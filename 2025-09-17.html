<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 17 September 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: 'rgba(107, 106, 101, 0.7)' // #6B6A65 with 70% opacity
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 17 September 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100 min-h-screen">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="mobile-quick-must-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="mobile-quick-should-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="mobile-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="mobile-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="mobile-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="mobile-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="mobile-quick-datasets" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="mobile-quick-reset" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-highly" checked>
                                            <label for="mobile-relevance-highly"></label>
                                        </div>
                                        <label for="mobile-relevance-highly" class="text-neutral-10 text-xl font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-moderately" checked>
                                            <label for="mobile-relevance-moderately"></label>
                                        </div>
                                        <label for="mobile-relevance-moderately" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-tangentially" checked>
                                            <label for="mobile-relevance-tangentially"></label>
                                        </div>
                                        <label for="mobile-relevance-tangentially" class="text-neutral-10 text-xl font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-not" checked>
                                            <label for="mobile-relevance-not"></label>
                                        </div>
                                        <label for="mobile-relevance-not" class="text-neutral-10 text-xl font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="desktop-quick-must-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="desktop-quick-should-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="desktop-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="desktop-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="desktop-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="desktop-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="desktop-quick-datasets" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="desktop-quick-reset" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-highly" checked>
                                            <label for="desktop-relevance-highly"></label>
                                        </div>
                                        <label for="desktop-relevance-highly" class="text-neutral-10 text-lg font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-moderately" checked>
                                            <label for="desktop-relevance-moderately"></label>
                                        </div>
                                        <label for="desktop-relevance-moderately" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-tangentially" checked>
                                            <label for="desktop-relevance-tangentially"></label>
                                        </div>
                                        <label for="desktop-relevance-tangentially" class="text-neutral-10 text-lg font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-not" checked>
                                            <label for="desktop-relevance-not"></label>
                                        </div>
                                        <label for="desktop-relevance-not" class="text-neutral-10 text-lg font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 17 September 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl min-h-screen">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2509.13603",
      "title": "Modernizing Facebook Scoped Search: Keyword and Embedding Hybrid\n  Retrieval with LLM Evaluation",
      "authors": [
        "Yongye Su",
        "Zeya Zhang",
        "Jane Kou",
        "Cheng Ju",
        "Shubhojeet Sarkar",
        "Yamin Wang",
        "Ji Liu",
        "Shengbo Guo"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Beyond general web-scale search, social network search uniquely enables users\nto retrieve information and discover potential connections within their social\ncontext. We introduce a framework of modernized Facebook Group Scoped Search by\nblending traditional keyword-based retrieval with embedding-based retrieval\n(EBR) to improve the search relevance and diversity of search results. Our\nsystem integrates semantic retrieval into the existing keyword search pipeline,\nenabling users to discover more contextually relevant group posts. To\nrigorously assess the impact of this blended approach, we introduce a novel\nevaluation framework that leverages large language models (LLMs) to perform\noffline relevance assessments, providing scalable and consistent quality\nbenchmarks. Our results demonstrate that the blended retrieval system\nsignificantly enhances user engagement and search quality, as validated by both\nonline metrics and LLM-based evaluation. This work offers practical insights\nfor deploying and evaluating advanced retrieval systems in large-scale,\nreal-world social platforms.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13603v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13603v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.366,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes a blended retrieval system for search enhancement and uses LLMs for evaluation, but it does not involve training a reward model on human-ranked data or applying reinforcement learning to fine-tune models. There is no mention of human feedback or RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on integrating keyword and embedding-based retrieval for social search, with LLM-based evaluation, but it does not adapt diffusion models for multi-step logical reasoning or iterative refinement of a Chain-of-Thought. No diffusion-related components are present.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13605",
      "title": "A Generalization of CLAP from 3D Localization to Image Processing, A\n  Connection With RANSAC & Hough Transforms",
      "authors": [
        "Ruochen Hou",
        "Gabriel I. Fernandez",
        "Alex Xu",
        "Dennis W. Hong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "In previous work, we introduced a 2D localization algorithm called CLAP,\nClustering to Localize Across $n$ Possibilities, which was used during our\nchampionship win in RoboCup 2024, an international autonomous humanoid soccer\ncompetition. CLAP is particularly recognized for its robustness against\noutliers, where clustering is employed to suppress noise and mitigate against\nerroneous feature matches. This clustering-based strategy provides an\nalternative to traditional outlier rejection schemes such as RANSAC, in which\ncandidates are validated by reprojection error across all data points. In this\npaper, CLAP is extended to a more general framework beyond 2D localization,\nspecifically to 3D localization and image stitching. We also show how CLAP,\nRANSAC, and Hough transforms are related. The generalization of CLAP is widely\napplicable to many different fields and can be a useful tool to deal with noise\nand uncertainty.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13605v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13605v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.315,
      "distributed_training_score": 0.279,
      "datasets_score": 0.24,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13612",
      "title": "Rest2Visual: Predicting Visually Evoked fMRI from Resting-State Scans",
      "authors": [
        "Chuyang Zhou",
        "Ziao Ji",
        "Daochang Liu",
        "Dongang Wang",
        "Chenyu Wang",
        "Chang Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Understanding how spontaneous brain activity relates to stimulus-driven\nneural responses is a fundamental challenge in cognitive neuroscience. While\ntask-based functional magnetic resonance imaging (fMRI) captures localized\nstimulus-evoked brain activation, its acquisition is costly, time-consuming,\nand difficult to scale across populations. In contrast, resting-state fMRI\n(rs-fMRI) is task-free and abundant, but lacks direct interpretability. We\nintroduce Rest2Visual, a conditional generative model that predicts visually\nevoked fMRI (ve-fMRI) from resting-state input and 2D visual stimuli. It\nfollows a volumetric encoder--decoder design, where multiscale 3D features from\nrs-fMRI are modulated by image embeddings via adaptive normalization, enabling\nspatially accurate, stimulus-specific activation synthesis. To enable model\ntraining, we construct a large-scale triplet dataset from the Natural Scenes\nDataset (NSD), aligning each rs-fMRI volume with stimulus images and their\ncorresponding ve-fMRI activation maps. Quantitative evaluation shows that the\npredicted activations closely match ground truth across standard similarity and\nrepresentational metrics, and support successful image reconstruction in\ndownstream decoding. Notably, the predicted maps preserve subject-specific\nstructure, demonstrating the model's capacity to generate individualized\nfunctional surrogates. Our results provide compelling evidence that\nindividualized spontaneous neural activity can be transformed into\nstimulus-aligned representations, opening new avenues for scalable, task-free\nfunctional brain modeling.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13612v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13612v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.294,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.312,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces Rest2Visual, a conditional generative model for predicting fMRI activations from resting-state scans and visual stimuli, using an encoder-decoder architecture with adaptive normalization. It does not involve diffusion models, iterative refinement processes, or any mechanisms for multi-step logical reasoning or Chain-of-Thought tasks. Therefore, it has no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13615",
      "title": "See, Think, Act: Teaching Multimodal Agents to Effectively Interact with\n  GUI by Identifying Toggles",
      "authors": [
        "Zongru Wu",
        "Rui Mao",
        "Zhiyuan Tian",
        "Pengzhou Cheng",
        "Tianjie Ju",
        "Zheng Wu",
        "Lingzhong Dong",
        "Haiyue Sheng",
        "Zhuosheng Zhang",
        "Gongshen Liu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "The advent of multimodal agents facilitates effective interaction within\ngraphical user interface (GUI), especially in ubiquitous GUI control. However,\ntheir inability to reliably execute toggle control instructions remains a key\nbottleneck. To investigate this, we construct a state control benchmark with\nbinary toggle instructions from public datasets. Evaluations of existing agents\ndemonstrate their unreliability, particularly when the current toggle state\nalready matches the desired state. To address the challenge, we propose\nState-aware Reasoning (StaR), a training method that teaches agents to perceive\nthe current toggle state, analyze the desired state from the instruction, and\nact accordingly. Experiments on three multimodal agents demonstrate that StaR\ncan improve toggle instruction execution accuracy by over 30\\%. Further\nevaluations on three public benchmarks show that StaR also enhances general\ntask performance. Finally, evaluations on a dynamic environment highlight the\npotential of StaR for real-world applications. Code, benchmark, and\nStaR-enhanced agents are available at https://github.com/ZrW00/StaR.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13615v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13615v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.326,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the proposal of State-aware Reasoning (StaR), a training method for multimodal agents to handle toggle controls in GUIs by perceiving states, analyzing instructions, and deciding actions. This involves step-by-step reasoning but does not mention or adapt the iterative refinement process of diffusion models for logical tasks. There is no evidence of treating Chain-of-Thought as a single entity for holistic correction over multiple steps using diffusion techniques, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13620",
      "title": "A reduced-order derivative-informed neural operator for subsurface\n  fluid-flow",
      "authors": [
        "Jeongjin",
        "Park",
        "Grant Bruer",
        "Huseyin Tuna Erdinc",
        "Abhinav Prakash Gahlot",
        "Felix J. Herrmann"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Neural operators have emerged as cost-effective surrogates for expensive\nfluid-flow simulators, particularly in computationally intensive tasks such as\npermeability inversion from time-lapse seismic data, and uncertainty\nquantification. In these applications, the fidelity of the surrogate's\ngradients with respect to system parameters is crucial, as the accuracy of\ndownstream tasks, such as optimization and Bayesian inference, relies directly\non the quality of the derivative information. Recent advances in\nphysics-informed methods have leveraged derivative information to improve\nsurrogate accuracy. However, incorporating explicit Jacobians can become\ncomputationally prohibitive, as the complexity typically scales quadratically\nwith the number of input parameters. To address this limitation, we propose\nDeFINO (Derivative-based Fisher-score Informed Neural Operator), a\nreduced-order, derivative-informed training framework. DeFINO integrates\nFourier neural operators (FNOs) with a novel derivative-based training strategy\nguided by the Fisher Information Matrix (FIM). By projecting Jacobians onto\ndominant eigen-directions identified by the FIM, DeFINO captures critical\nsensitivity information directly informed by observational data, significantly\nreducing computational expense. We validate DeFINO through synthetic\nexperiments in the context of subsurface multi-phase fluid-flow, demonstrating\nimprovements in gradient accuracy while maintaining robust forward predictions\nof underlying fluid dynamics. These results highlight DeFINO's potential to\noffer practical, scalable solutions for inversion problems in complex\nreal-world scenarios, all at substantially reduced computational cost.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13620v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13620v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.375,
      "datasets_score": 0.235,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13626",
      "title": "Mind the Gap: Aligning Knowledge Bases with User Needs to Enhance Mental\n  Health Retrieval",
      "authors": [
        "Amanda Chan",
        "James Jiayu Liu",
        "He Kai",
        "Onno P. Kampman"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Access to reliable mental health information is vital for early help-seeking,\nyet expanding knowledge bases is resource-intensive and often misaligned with\nuser needs. This results in poor performance of retrieval systems when\npresented concerns are not covered or expressed in informal or contextualized\nlanguage. We present an AI-based gap-informed framework for corpus augmentation\nthat authentically identifies underrepresented topics (gaps) by overlaying\nnaturalistic user data such as forum posts in order to prioritize expansions\nbased on coverage and usefulness. In a case study, we compare Directed\n(gap-informed augmentations) with Non-Directed augmentation (random additions),\nevaluating the relevance and usefulness of retrieved information across four\nretrieval-augmented generation (RAG) pipelines. Directed augmentation achieved\nnear-optimal performance with modest expansions--requiring only a 42% increase\nfor Query Transformation, 74% for Reranking and Hierarchical, and 318% for\nBaseline--to reach ~95% of the performance of an exhaustive reference corpus.\nIn contrast, Non-Directed augmentation required substantially larger and thus\npractically infeasible expansions to achieve comparable performance (232%,\n318%, 403%, and 763%, respectively). These results show that strategically\ntargeted corpus growth can reduce content creation demands while sustaining\nhigh retrieval and provision quality, offering a scalable approach for building\ntrusted health information repositories and supporting generative AI\napplications in high-stakes domains.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13626v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13626v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.314,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on an AI-based framework for augmenting knowledge bases in mental health retrieval using user data to identify gaps, but it does not involve training AI models with reinforcement learning, a reward model, or human-ranked data for fine-tuning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses corpus augmentation and RAG pipelines for mental health information retrieval, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for Chain-of-Thought.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13627",
      "title": "Secure, Scalable and Privacy Aware Data Strategy in Cloud",
      "authors": [
        "Vijay Kumar Butte",
        "Sujata Butte"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "The enterprises today are faced with the tough challenge of processing,\nstoring large amounts of data in a secure, scalable manner and enabling\ndecision makers to make quick, informed data driven decisions. This paper\naddresses this challenge and develops an effective enterprise data strategy in\nthe cloud. Various components of an effective data strategy are discussed and\narchitectures addressing security, scalability and privacy aspects are\nprovided.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13627v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13627v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.25,
      "weak_supervision_score": 0.289,
      "diffusion_reasoning_score": 0.276,
      "distributed_training_score": 0.365,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13629",
      "title": "SAMIR, an efficient registration framework via robust feature learning\n  from SAM",
      "authors": [
        "Yue He",
        "Min Liu",
        "Qinghao Liu",
        "Jiazheng Wang",
        "Yaonan Wang",
        "Hang Zhang",
        "Xiang Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Image registration is a fundamental task in medical image analysis.\nDeformations are often closely related to the morphological characteristics of\ntissues, making accurate feature extraction crucial. Recent weakly supervised\nmethods improve registration by incorporating anatomical priors such as\nsegmentation masks or landmarks, either as inputs or in the loss function.\nHowever, such weak labels are often not readily available, limiting their\npractical use. Motivated by the strong representation learning ability of\nvisual foundation models, this paper introduces SAMIR, an efficient medical\nimage registration framework that utilizes the Segment Anything Model (SAM) to\nenhance feature extraction. SAM is pretrained on large-scale natural image\ndatasets and can learn robust, general-purpose visual representations. Rather\nthan using raw input images, we design a task-specific adaptation pipeline\nusing SAM's image encoder to extract structure-aware feature embeddings,\nenabling more accurate modeling of anatomical consistency and deformation\npatterns. We further design a lightweight 3D head to refine features within the\nembedding space, adapting to local deformations in medical images.\nAdditionally, we introduce a Hierarchical Feature Consistency Loss to guide\ncoarse-to-fine feature matching and improve anatomical alignment. Extensive\nexperiments demonstrate that SAMIR significantly outperforms state-of-the-art\nmethods on benchmark datasets for both intra-subject cardiac image registration\nand inter-subject abdomen CT image registration, achieving performance\nimprovements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code\nwill be publicly available on GitHub following the acceptance of this paper.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13629v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13629v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.333,
      "distributed_training_score": 0.313,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13631",
      "title": "Federated Learning for Deforestation Detection: A Distributed Approach\n  with Satellite Imagery",
      "authors": [
        "Yuvraj Dutta",
        "Aaditya Sikder",
        "Basabdatta Palit"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "Accurate identification of deforestation from satellite images is essential\nin order to understand the geographical situation of an area. This paper\nintroduces a new distributed approach to identify as well as locate\ndeforestation across different clients using Federated Learning (FL). Federated\nLearning enables distributed network clients to collaboratively train a model\nwhile maintaining data privacy and security of the active users. In our\nframework, a client corresponds to an edge satellite center responsible for\nlocal data processing. Moreover, FL provides an advantage over centralized\ntraining method which requires combining data, thereby compromising with data\nsecurity of the clients. Our framework leverages the FLOWER framework with RAY\nframework to execute the distributed learning workload. Furthermore, efficient\nclient spawning is ensured by RAY as it can select definite amount of users to\ncreate an emulation environment. Our FL framework uses YOLOS-small (a Vision\nTransformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN\nwith a MobileNetV3 backbone models trained and tested on publicly available\ndatasets. Our approach provides us a different view for image\nsegmentation-based tasks on satellite imagery.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13631v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13631v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.476,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a Federated Learning (FL) framework for deforestation detection, which is a direct application of distributed training. It involves multiple clients (edge satellite centers) collaboratively training models without sharing data, using frameworks like FLOWER and RAY to handle distributed workloads, client spawning, and aggregation of model updates. This aligns closely with distributed training concepts, such as partitioning computation across nodes, reducing communication overhead, and enabling parallel processing, as evidenced by the use of FL algorithms like FedAvg and the evaluation of models on decentralized data.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces a federated learning framework for detecting deforestation from satellite imagery, emphasizing data privacy by enabling distributed training across edge satellite centers without sharing raw data. It utilizes the FLOWER and RAY frameworks to evaluate models such as YOLOS-small and Faster R-CNN with ResNet50 or MobileNetV3 backbones, employing aggregation strategies like FedAvg and FedAvgM, and demonstrates competitive performance compared to centralized methods while analyzing aspects like stability, scalability, and training efficiency.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing federated learning techniques with specific models for deforestation detection, offering a notable improvement in privacy-preserving applications for remote sensing, though it does not introduce entirely new problems or architectures.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like distributed computer vision and environmental monitoring, as it addresses practical privacy challenges in satellite imagery analysis, potentially influencing future research in these areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, practical contribution to applying federated learning in real-world environmental tasks, making it valuable for researchers in AI and remote sensing to understand privacy-preserving techniques in this domain.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/afc2fa22abc8c25313debcc390ff094656006b1b",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 5,
      "average_h_index": 1.6666666666666667,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yuvraj Dutta",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375735162"
        },
        {
          "name": "Aaditya Sikder",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685989"
        },
        {
          "name": "Basabdatta Palit",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2286013"
        }
      ]
    },
    {
      "id": "2509.13633",
      "title": "DeepLogit: A sequentially constrained explainable deep learning modeling\n  approach for transport policy analysis",
      "authors": [
        "Jeremy Oon",
        "Rakhi Manohar Mepparambath",
        "Ling Feng"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite the significant progress of deep learning models in multitude of\napplications, their adaption in planning and policy related areas remains\nchallenging due to the black-box nature of these models. In this work, we\ndevelop a set of DeepLogit models that follow a novel sequentially constrained\napproach in estimating deep learning models for transport policy analysis. In\nthe first step of the proposed approach, we estimate a convolutional neural\nnetwork (CNN) model with only linear terms, which is equivalent of a\nlinear-in-parameter multinomial logit model. We then estimate other deep\nlearning models by constraining the parameters that need interpretability at\nthe values obtained in the linear-in-parameter CNN model and including higher\norder terms or by introducing advanced deep learning architectures like\nTransformers. Our approach can retain the interpretability of the selected\nparameters, yet provides significantly improved model accuracy than the\ndiscrete choice model. We demonstrate our approach on a transit route choice\nexample using real-world transit smart card data from Singapore. This study\nshows the potential for a unifying approach, where theory-based discrete choice\nmodel (DCM) and data-driven AI models can leverage each other's strengths in\ninterpretability and predictive power. With the availability of larger datasets\nand more complex constructions, such approach can lead to more accurate models\nusing discrete choice models while maintaining its applicability in planning\nand policy-related areas. Our code is available on\nhttps://github.com/jeremyoon/route-choice/ .",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13633v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13633v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.427,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.49,
      "distributed_training_score": 0.365,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper develops a hybrid deep learning approach for transport policy analysis, focusing on interpretable models like DeepLogit, but it does not involve reinforcement learning, human feedback, or training models with human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses architectures like CNN and Transformers for sequential modeling in transport choices, but it lacks any components related to diffusion processes, iterative refinement, or multi-step logical reasoning as a holistic entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13642",
      "title": "LLM-I: LLMs are Naturally Interleaved Multimodal Creators",
      "authors": [
        "Zirun Guo",
        "Feng Zhang",
        "Kai Jia",
        "Tao Jin"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that\nreframes interleaved image-text generation as a tool-use problem. LLM-I is\ndesigned to overcome the \"one-tool\" bottleneck of current unified models, which\nare limited to synthetic imagery and struggle with tasks requiring factual\ngrounding or programmatic precision. Our framework empowers a central LLM or\nMLLM agent to intelligently orchestrate a diverse toolkit of specialized visual\ntools, including online image search, diffusion-based generation, code\nexecution, and image editing. The agent is trained to select and apply these\ntools proficiently via a Reinforcement Learning (RL) framework that features a\nhybrid reward system combining rule-based logic with judgments from LLM and\nMLLM evaluators. Trained on a diverse new dataset using four different model\nbackbones, LLM-I demonstrates state-of-the-art performance, outperforming\nexisting methods by a large margin across four benchmarks. We also introduce a\nnovel test-time scaling strategy that provides further performance gains.\nProject Page: https://github.com/ByteDance-BandAI/LLM-I.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13642v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13642v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.418,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.517,
      "distributed_training_score": 0.389,
      "datasets_score": 0.4,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper uses a Reinforcement Learning framework with a hybrid reward system involving rule-based logic and judgments from LLM/MLLM evaluators, but it does not involve human feedback or a reward model trained on human-ranked data. Therefore, it does not align with RLHF.",
      "weak_supervision_justification": "The paper mentions building a diverse new dataset for training, but it does not describe using weak supervision techniques, such as programmatically generating labels from noisy sources. There is no indication of relying on high-level or imprecise labeling methods instead of hand-labeled data.",
      "diffusion_reasoning_justification": "The paper includes diffusion-based generation as one of the tools in the toolkit, but it does not adapt diffusion for multi-step logical reasoning or treat a chain-of-thought as an entity for iterative refinement. The focus is on tool orchestration via RL, not diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contributions include introducing a diverse new dataset and benchmark for interleaved image-text generation, which directly involves creating and evaluating datasets for AI applications. This aligns closely with research on dataset creation, curation, and benchmarking.",
      "llm_score_status": "completed",
      "summary": "The paper introduces LLM-I, a flexible framework that reframes interleaved image-text generation as a tool-use problem, utilizing a central LLM or MLLM as an agent to orchestrate specialized tools such as online image search, diffusion-based generation, code execution, and image editing. Trained via a Reinforcement Learning approach with a hybrid reward system on a new diverse dataset, LLM-I demonstrates superior performance over existing methods across multiple benchmarks and includes a novel test-time scaling strategy for further improvements.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new paradigm by positioning LLMs as intelligent agents that orchestrate external tools for multimodal generation, significantly advancing beyond the limitations of unified end-to-end models.",
      "impact_score": "High",
      "impact_justification": "The work's flexible tool-augmented framework could broadly influence future research in multimodal AI and practical applications by enabling more scalable and context-aware content generation systems.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper presents a high-quality, innovative contribution with strong empirical results that advances multimodal generation, making it essential for researchers in AI to be aware of for its potential implications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/91bb20d24cfd9676f4f3743c7cdfeee00e1a49f2",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 4,
      "average_h_index": 2.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zirun Guo",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2310395988"
        },
        {
          "name": "Feng Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381987284"
        },
        {
          "name": "Kai Jia",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380688333"
        },
        {
          "name": "Tao Jin",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2334574553"
        }
      ]
    },
    {
      "id": "2509.13650",
      "title": "GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You\n  Commit?",
      "authors": [
        "Amena Amro",
        "Manar H. Alalfi"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As software development practices increasingly adopt AI-powered tools,\nensuring that such tools can support secure coding has become critical. This\nstudy evaluates the effectiveness of GitHub Copilot's recently introduced code\nreview feature in detecting security vulnerabilities. Using a curated set of\nlabeled vulnerable code samples drawn from diverse open-source projects\nspanning multiple programming languages and application domains, we\nsystematically assessed Copilot's ability to identify and provide feedback on\ncommon security flaws. Contrary to expectations, our results reveal that\nCopilot's code review frequently fails to detect critical vulnerabilities such\nas SQL injection, cross-site scripting (XSS), and insecure deserialization.\nInstead, its feedback primarily addresses low-severity issues, such as coding\nstyle and typographical errors. These findings expose a significant gap between\nthe perceived capabilities of AI-assisted code review and its actual\neffectiveness in supporting secure development practices. Our results highlight\nthe continued necessity of dedicated security tools and manual code audits to\nensure robust software security.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13650v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13650v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.308,
      "distributed_training_score": 0.273,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13652",
      "title": "Gaussian Alignment for Relative Camera Pose Estimation via Single-View\n  Reconstruction",
      "authors": [
        "Yumin Li",
        "Dylan Campbell"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Estimating metric relative camera pose from a pair of images is of great\nimportance for 3D reconstruction and localisation. However, conventional\ntwo-view pose estimation methods are not metric, with camera translation known\nonly up to a scale, and struggle with wide baselines and textureless or\nreflective surfaces. This paper introduces GARPS, a training-free framework\nthat casts this problem as the direct alignment of two independently\nreconstructed 3D scenes. GARPS leverages a metric monocular depth estimator and\na Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model\n(GMM) for each image. It then refines an initial pose from a feed-forward\ntwo-view pose estimator by optimising a differentiable GMM alignment objective.\nThis objective jointly considers geometric structure, view-independent colour,\nanisotropic covariance, and semantic feature consistency, and is robust to\nocclusions and texture-poor regions without requiring explicit 2D\ncorrespondences. Extensive experiments on the Real\\-Estate10K dataset\ndemonstrate that GARPS outperforms both classical and state-of-the-art\nlearning-based methods, including MASt3R. These results highlight the potential\nof bridging single-view perception with multi-view geometry to achieve robust\nand metric relative pose estimation.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13652v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13652v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.297,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.305,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13662",
      "title": "Deep Lookup Network",
      "authors": [
        "Yulan Guo",
        "Longguang Wang",
        "Wendong Mao",
        "Xiaoyu Dong",
        "Yingqian Wang",
        "Li Liu",
        "Wei An"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Convolutional neural networks are constructed with massive operations with\ndifferent types and are highly computationally intensive. Among these\noperations, multiplication operation is higher in computational complexity and\nusually requires {more} energy consumption with longer inference time than\nother operations, which hinders the deployment of convolutional neural networks\non mobile devices. In many resource-limited edge devices, complicated\noperations can be calculated via lookup tables to reduce computational cost.\nMotivated by this, in this paper, we introduce a generic and efficient lookup\noperation which can be used as a basic operation for the construction of neural\nnetworks. Instead of calculating the multiplication of weights and activation\nvalues, simple yet efficient lookup operations are adopted to compute their\nresponses. To enable end-to-end optimization of the lookup operation, we\nconstruct the lookup tables in a differentiable manner and propose several\ntraining strategies to promote their convergence. By replacing computationally\nexpensive multiplication operations with our lookup operations, we develop\nlookup networks for the image classification, image super-resolution, and point\ncloud classification tasks. It is demonstrated that our lookup networks can\nbenefit from the lookup operations to achieve higher efficiency in terms of\nenergy consumption and inference speed while maintaining competitive\nperformance to vanilla convolutional networks. Extensive experiments show that\nour lookup networks produce state-of-the-art performance on different tasks\n(both classification and regression tasks) and different data types (both\nimages and point clouds).",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13662v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13662v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.418,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is the development of a lookup operation to replace multiplication in neural networks, aiming to reduce computational complexity for efficient inference on resource-limited devices. It discusses training strategies for lookup tables but focuses on single-device optimization, not distributed training, parallel computing, or multi-node machine learning techniques such as data partitioning across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13664",
      "title": "Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs",
      "authors": [
        "Zhuoxuan Zhang",
        "Jinhao Duan",
        "Edward Kim",
        "Kaidi Xu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Ambiguity is pervasive in real-world questions, yet large language models\n(LLMs) often respond with confident answers rather than seeking clarification.\nIn this work, we show that question ambiguity is linearly encoded in the\ninternal representations of LLMs and can be both detected and controlled at the\nneuron level. During the model's pre-filling stage, we identify that a small\nnumber of neurons, as few as one, encode question ambiguity information. Probes\ntrained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance\non ambiguity detection and generalize across datasets, outperforming\nprompting-based and representation-based baselines. Layerwise analysis reveals\nthat AENs emerge from shallow layers, suggesting early encoding of ambiguity\nsignals in the model's processing pipeline. Finally, we show that through\nmanipulating AENs, we can control LLM's behavior from direct answering to\nabstention. Our findings reveal that LLMs form compact internal representations\nof question ambiguity, enabling interpretable and controllable behavior.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13664v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13664v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.428,
      "diffusion_reasoning_score": 0.473,
      "distributed_training_score": 0.347,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on analyzing and manipulating internal neuron representations in LLMs for question ambiguity, using probes trained on specific datasets. It does not involve training models with programmatically generated, noisy, or imprecise labels, which is the core of weak supervision. Instead, it relies on standard labeled datasets for experiments, without any mention of weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper examines how ambiguity is encoded in LLMs at the neuron level and how to control it, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. There is no component related to adapting diffusion for reasoning, making this topic unrelated.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13666",
      "title": "DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater\n  Monitoring",
      "authors": [
        "Zhenqi Wu",
        "Abhinav Modi",
        "Angelos Mavrogiannis",
        "Kaustubh Joshi",
        "Nikhil Chopra",
        "Yiannis Aloimonos",
        "Nare Karapetyan",
        "Ioannis Rekleitis",
        "Xiaomin Lin"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The ocean is warming and acidifying, increasing the risk of mass mortality\nevents for temperature-sensitive shellfish such as oysters. This motivates the\ndevelopment of long-term monitoring systems. However, human labor is costly and\nlong-duration underwater work is highly hazardous, thus favoring robotic\nsolutions as a safer and more efficient option. To enable underwater robots to\nmake real-time, environment-aware decisions without human intervention, we must\nequip them with an intelligent \"brain.\" This highlights the need for\npersistent,wide-area, and low-cost benthic monitoring. To this end, we present\nDREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term\nunderwater exploration and habitat monitoring. The results show that our\nframework is highly efficient in finding and exploring target objects (e.g.,\noysters, shipwrecks) without prior location information. In the\noyster-monitoring task, our framework takes 31.5% less time than the previous\nbaseline with the same amount of oysters. Compared to the vanilla VLM, it uses\n23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our\nframework successfully explores and maps the wreck without collisions,\nrequiring 27.5% fewer steps than the vanilla model and achieving 100% coverage,\nwhile the vanilla model achieves 60.23% average coverage in our shipwreck\nenvironments.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13666v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13666v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.418,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.355,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces a VLM-guided framework for underwater monitoring, focusing on real-time decision-making and chain-of-thought reasoning, but it does not discuss training models with programmatically generated labels from noisy sources. There is no mention of weak supervision techniques, such as using imprecise labels for model training.",
      "diffusion_reasoning_justification": "The paper uses chain-of-thought reasoning with VLMs for planning and decision-making, which involves step-by-step logical processes, but it does not incorporate diffusion models or their iterative refinement mechanisms for reasoning tasks. There is no evidence of adapting diffusion processes for holistic correction of reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13672",
      "title": "CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in\n  Chinese Literature Grammatical Error Correction",
      "authors": [
        "Shang Qin",
        "Jingheng Ye",
        "Yinghui Li",
        "Hai-Tao Zheng",
        "Qi Li",
        "Jinxiao Shan",
        "Zhixing Li",
        "Hong-Gee Kim"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The growing demand for automated writing assistance in diverse academic\ndomains highlights the need for robust Chinese Grammatical Error Correction\n(CGEC) systems that can adapt across disciplines. However, existing CGEC\nresearch largely lacks dedicated benchmarks for multi-disciplinary academic\nwriting, overlooking continual learning (CL) as a promising solution to handle\ndomain-specific linguistic variation and prevent catastrophic forgetting. To\nfill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning\nbenchmark for Chinese Literature Grammatical Error Correction, designed to\nevaluate adaptive CGEC across multiple academic fields. Our benchmark includes\n10,000 human-annotated sentences spanning 10 disciplines, each exhibiting\ndistinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating\ngrammatical error correction in a continual learning setting, simulating\nsequential exposure to diverse academic disciplines to reflect real-world\neditorial dynamics. We evaluate large language models under sequential tuning,\nparameter-efficient adaptation, and four representative CL algorithms, using\nboth standard GEC metrics and continual learning metrics adapted to task-level\nvariation. Experimental results reveal that regularization-based methods\nmitigate forgetting more effectively than replay-based or naive sequential\napproaches. Our benchmark provides a rigorous foundation for future research in\nadaptive grammatical error correction across diverse academic domains.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13672v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13672v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.393,
      "datasets_score": 0.396,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13676",
      "title": "Re-purposing SAM into Efficient Visual Projectors for MLLM-Based\n  Referring Image Segmentation",
      "authors": [
        "Xiaobo Yang",
        "Xiaojin Gong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recently, Referring Image Segmentation (RIS) frameworks that pair the\nMultimodal Large Language Model (MLLM) with the Segment Anything Model (SAM)\nhave achieved impressive results. However, adapting MLLM to segmentation is\ncomputationally intensive, primarily due to visual token redundancy. We observe\nthat traditional patch-wise visual projectors struggle to strike a balance\nbetween reducing the number of visual tokens and preserving semantic clarity,\noften retaining overly long token sequences to avoid performance drops.\nInspired by text tokenizers, we propose a novel semantic visual projector that\nleverages semantic superpixels generated by SAM to identify \"visual words\" in\nan image. By compressing and projecting semantic superpixels as visual tokens,\nour approach adaptively shortens the token sequence according to scene\ncomplexity while minimizing semantic loss in compression. To mitigate loss of\ninformation, we propose a semantic superpixel positional embedding to\nstrengthen MLLM's awareness of superpixel geometry and position, alongside a\nsemantic superpixel aggregator to preserve both fine-grained details inside\nsuperpixels and global context outside. Experiments show that our method cuts\nvisual tokens by 93% without compromising performance, notably speeding up MLLM\ntraining and inference, and outperforming existing compressive visual\nprojectors on RIS.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13676v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13676v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.389,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on improving efficiency in Multimodal Large Language Models (MLLMs) for Referring Image Segmentation (RIS) by using semantic superpixels and visual projectors, without any mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It deals with vision-language tasks like segmentation, which are unrelated to adapting diffusion for reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13677",
      "title": "AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise\n  Control in Text Generation",
      "authors": [
        "Xinxu Zhou",
        "Jiaqi Bai",
        "Zhenqi Sun",
        "Fanxiang Zeng",
        "Yue Liu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Although significant progress has been made in many tasks within the field of\nNatural Language Processing (NLP), Controlled Text Generation (CTG) continues\nto face numerous challenges, particularly in achieving fine-grained conditional\ncontrol over generation. Additionally, in real scenario and online\napplications, cost considerations, scalability, domain knowledge learning and\nmore precise control are required, presenting more challenge for CTG. This\npaper introduces a novel and scalable framework, AgentCTG, which aims to\nenhance precise and complex control over the text generation by simulating the\ncontrol and regulation mechanisms in multi-agent workflows. We explore various\ncollaboration methods among different agents and introduce an auto-prompt\nmodule to further enhance the generation effectiveness. AgentCTG achieves\nstate-of-the-art results on multiple public datasets. To validate its\neffectiveness in practical applications, we propose a new challenging\nCharacter-Driven Rewriting task, which aims to convert the original text into\nnew text that conform to specific character profiles and simultaneously\npreserve the domain knowledge. When applied to online navigation with\nrole-playing, our approach significantly enhances the driving experience\nthrough improved content delivery. By optimizing the generation of contextually\nrelevant text, we enable a more immersive interaction within online\ncommunities, fostering greater personalization and user engagement.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13677v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13677v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.395,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.343,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the introduction of AgentCTG, a multi-agent collaboration framework for controlled text generation, focusing on fine-grained control, auto-prompt modules, and tasks like Character-Driven Rewriting. While the introduction briefly mentions \"diffusion learning\" as part of traditional CTG methods, it does not involve adapting the iterative refinement process of diffusion models for multi-step logical reasoning or treating a Chain-of-Thought as a single entity for holistic correction. The paper emphasizes agent-based workflows and LLMs, with no clear component dedicated to diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13680",
      "title": "Prompt Stability in Code LLMs: Measuring Sensitivity across Emotion- and\n  Personality-Driven Variations",
      "authors": [
        "Wei Ma",
        "Yixiao Yang",
        "Jingquan Ge",
        "Xiaofei Xie",
        "Lingxiao Jiang"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Code generation models are widely used in software development, yet their\nsensitivity to prompt phrasing remains under-examined. Identical requirements\nexpressed with different emotions or communication styles can yield divergent\noutputs, while most benchmarks emphasize only peak performance. We present\nPromptSE (Prompt Sensitivity Evaluation), a framework that creates semantically\nequivalent prompt variants with emotion and personality templates, and that\nevaluates stability using probability aware continuous scoring or using binary\npass rates when logits are unavailable. The results are aggregated into a\nproposed area under curve metric (AUC-E) for cross model comparison. Across 14\nmodels from three families (Llama, Qwen, and DeepSeek), our study shows that\nperformance and stability behave as largely decoupled optimization objectives,\nand it reveals architectural and scale related patterns that challenge common\nassumptions about model robustness. The framework supports rapid screening for\nclosed-source models as well as detailed stability analysis in research\nsettings. PromptSE enables practitioners to quantify performance stability\ntrade offs for deployment and model selection, positioning prompt stability as\na complementary evaluation dimension alongside performance and fairness, and\ncontributing to more trustworthy AI-assisted software development tools.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13680v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13680v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.461,
      "weak_supervision_score": 0.427,
      "diffusion_reasoning_score": 0.45,
      "distributed_training_score": 0.355,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating prompt sensitivity in code generation models, introducing a framework for stability assessment, but does not involve training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper's main contribution is a framework for evaluating model stability using prompt variants, not a method for training models with programmatically generated labels or noisy supervision.",
      "diffusion_reasoning_justification": "The paper evaluates prompt stability in code LLMs without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning adaptations.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13681",
      "title": "FishBEV: Distortion-Resilient Bird's Eye View Segmentation with\n  Surround-View Fisheye Cameras",
      "authors": [
        "Hang Li",
        "Dianmo Sheng",
        "Qiankun Dong",
        "Zichun Wang",
        "Zhiwei Xu",
        "Tao Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "As a cornerstone technique for autonomous driving, Bird's Eye View (BEV)\nsegmentation has recently achieved remarkable progress with pinhole cameras.\nHowever, it is non-trivial to extend the existing methods to fisheye cameras\nwith severe geometric distortion, ambiguous multi-view correspondences and\nunstable temporal dynamics, all of which significantly degrade BEV performance.\nTo address these challenges, we propose FishBEV, a novel BEV segmentation\nframework specifically tailored for fisheye cameras. This framework introduces\nthree complementary innovations, including a Distortion-Resilient Multi-scale\nExtraction (DRME) backbone that learns robust features under distortion while\npreserving scale consistency, an Uncertainty-aware Spatial Cross-Attention\n(U-SCA) mechanism that leverages uncertainty estimation for reliable cross-view\nalignment, a Distance-aware Temporal Self-Attention (D-TSA) module that\nadaptively balances near field details and far field context to ensure temporal\ncoherence. Extensive experiments on the Synwoodscapes dataset demonstrate that\nFishBEV consistently outperforms SOTA baselines, regarding the performance\nevaluation of FishBEV on the surround-view fisheye BEV segmentation tasks.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13681v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13681v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.327,
      "distributed_training_score": 0.339,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13683",
      "title": "Improving Context Fidelity via Native Retrieval-Augmented Reasoning",
      "authors": [
        "Suyuchen Wang",
        "Jinlin Wang",
        "Xinyu Wang",
        "Shiqi Li",
        "Xiangru Tang",
        "Sirui Hong",
        "Xiao-Wen Chang",
        "Chenglin Wu",
        "Bang Liu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) often struggle with context fidelity, producing\ninconsistent answers when responding to questions based on provided\ninformation. Existing approaches either rely on expensive supervised\nfine-tuning to generate evidence post-answer or train models to perform web\nsearches without necessarily improving utilization of the given context. We\npropose CARE, a novel native retrieval-augmented reasoning framework that\nteaches LLMs to explicitly integrate in-context evidence within their reasoning\nprocess with the model's own retrieval capabilities. Our method requires\nlimited labeled evidence data while significantly enhancing both retrieval\naccuracy and answer generation performance through strategically retrieved\nin-context tokens in the reasoning chain. Extensive experiments on multiple\nreal-world and counterfactual QA benchmarks demonstrate that our approach\nsubstantially outperforms supervised fine-tuning, traditional\nretrieval-augmented generation methods, and external retrieval solutions. This\nwork represents a fundamental advancement in making LLMs more accurate,\nreliable, and efficient for knowledge-intensive tasks.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13683v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13683v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.509,
      "distributed_training_score": 0.318,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a native retrieval-augmented reasoning framework for LLMs, focusing on integrating in-context evidence through supervised fine-tuning and reinforcement learning. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for holistic correction. Thus, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13687",
      "title": "Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging\n  Classification",
      "authors": [
        "Kaniz Fatema",
        "Emad A. Mohammed",
        "Sukhjit Singh Sehra"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Effective and interpretable classification of medical images is a challenge\nin computer-aided diagnosis, especially in resource-limited clinical settings.\nThis study introduces spline-based Kolmogorov-Arnold Networks (KANs) for\naccurate medical image classification with limited, diverse datasets. The\nmodels include SBTAYLOR-KAN, integrating B-splines with Taylor series;\nSBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN,\nembedding B-splines in Morlet wavelet transforms. These approaches leverage\nspline-based function approximation to capture both local and global\nnonlinearities. The models were evaluated on brain MRI, chest X-rays,\ntuberculosis X-rays, and skin lesion images without preprocessing,\ndemonstrating the ability to learn directly from raw data. Extensive\nexperiments, including cross-dataset validation and data reduction analysis,\nshowed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93%\naccuracy, with a balanced F1-score, maintaining over 86% accuracy using only\n30% of the training data across three datasets. Despite class imbalance in the\nskin cancer dataset, experiments on both imbalanced and balanced versions\nshowed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy.\nUnlike traditional CNNs, which require millions of parameters (e.g., ResNet50\nwith 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872\ntrainable parameters, making it more suitable for constrained medical\nenvironments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used\nfor interpretability, highlighting relevant regions in medical images. This\nframework provides a lightweight, interpretable, and generalizable solution for\nmedical image classification, addressing the challenges of limited datasets and\ndata-scarce scenarios in clinical AI applications.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13687v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13687v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.288,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.344,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13688",
      "title": "CraftMesh: High-Fidelity Generative Mesh Manipulation via Poisson\n  Seamless Fusion",
      "authors": [
        "James Jincheng",
        "Youcheng Cai",
        "Ligang Liu"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Controllable, high-fidelity mesh editing remains a significant challenge in\n3D content creation. Existing generative methods often struggle with complex\ngeometries and fail to produce detailed results. We propose CraftMesh, a novel\nframework for high-fidelity generative mesh manipulation via Poisson Seamless\nFusion. Our key insight is to decompose mesh editing into a pipeline that\nleverages the strengths of 2D and 3D generative models: we edit a 2D reference\nimage, then generate a region-specific 3D mesh, and seamlessly fuse it into the\noriginal model. We introduce two core techniques: Poisson Geometric Fusion,\nwhich utilizes a hybrid SDF/Mesh representation with normal blending to achieve\nharmonious geometric integration, and Poisson Texture Harmonization for\nvisually consistent texture blending. Experimental results demonstrate that\nCraftMesh outperforms state-of-the-art methods, delivering superior global\nconsistency and local detail in complex editing tasks.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13688v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13688v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.327,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on generative mesh manipulation using diffusion models for 3D content creation and editing, such as in SDS-based and MVD-based approaches. However, it does not involve adapting diffusion for multi-step logical reasoning or treating a Chain-of-Thought as an entity for solving complex logical tasks. Instead, diffusion is used for visual generation and editing, which does not align with the topic's definition.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13702",
      "title": "DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination\n  Suppression in Large Language Models",
      "authors": [
        "Xiao Zheng"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Model (LLM) hallucination is a significant barrier to their\nreliable deployment. Current methods like Retrieval-Augmented Generation (RAG)\nare often reactive. We introduce **Dynamic Self-reinforcing Calibration for\nHallucination Suppression (DSCC-HS)**, a novel, proactive framework that\nintervenes during autoregressive decoding. Inspired by dual-process cognitive\ntheory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a\nFactual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During\ninference, these proxies dynamically steer a large target model by injecting a\nreal-time steering vector, which is the difference between FAP and HDP logits,\nat each decoding step. This plug-and-play approach requires no modification to\nthe target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS\nachieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%\nFactual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained\nthe highest FActScore of 46.50. These results validate DSCC-HS as a principled\nand efficient solution for enhancing LLM factuality.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13702v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13702v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.484,
      "weak_supervision_score": 0.446,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.402,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on training proxy models using contrastive optimization and data augmentation, without any mention of human feedback, reward models, or reinforcement learning techniques. It does not involve aligning models with human preferences through RL.",
      "weak_supervision_justification": "The paper employs data-augmented proxy training with strategies like question paraphrasing, answer perturbation, and external data supplementation to generate labels programmatically, relying on noisy or imprecise sources rather than extensive hand-labeled data, which aligns directly with weak supervision principles.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for multi-step logical reasoning; instead, it focuses on dynamic steering during autoregressive decoding using proxy models, with no components for holistically correcting reasoning paths.",
      "distributed_training_justification": "The paper discusses parameter-efficient training of a compact proxy model using LoRA, but it does not address distributed systems, parallel computing, or partitioning data/computation across multiple nodes for acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces DSCC-HS, a proactive framework designed to suppress hallucinations in large language models by leveraging a compact proxy model trained in two adversarial roles: a Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP), inspired by dual-process cognitive theory. The methodology involves iteratively training these proxies via contrastive logit-space optimization and using them during inference to dynamically steer the target model's output through a real-time steering vector derived from the difference in their logits, without requiring any modifications to the target model. Experimental results on benchmarks like TruthfulQA and BioGEN demonstrate state-of-the-art performance, with DSCC-HS achieving a 99.2% Factual Consistency Rate on TruthfulQA and the highest FActScore of 46.50 on BioGEN, highlighting its effectiveness in enhancing LLM factuality.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly innovative proactive framework with adversarial proxy models and dynamic steering, significantly advancing the state-of-the-art in hallucination suppression by addressing limitations of existing reactive methods.",
      "impact_score": "High",
      "impact_justification": "The work's plug-and-play design and demonstrated superior results could broadly influence future research in LLM reliability and applications in high-stakes domains like medicine and finance.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality, innovative solution to a critical issue in LLMs, making it essential for researchers focused on AI safety and natural language processing to be aware of its contributions.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0a72f9904be205f9c969ac43b4836dfce73e6ffe",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Xiao Zheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381290445"
        }
      ]
    },
    {
      "id": "2509.13704",
      "title": "InfraMind: A Novel Exploration-based GUI Agentic Framework for\n  Mission-critical Industrial Management",
      "authors": [
        "Liangtao Lin",
        "Zhaomeng Zhu",
        "Tianwei Zhang",
        "Yonggang Wen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Mission-critical industrial infrastructure, such as data centers,\nincreasingly depends on complex management software. Its operations, however,\npose significant challenges due to the escalating system complexity,\nmulti-vendor integration, and a shortage of expert operators. While Robotic\nProcess Automation (RPA) offers partial automation through handcrafted scripts,\nit suffers from limited flexibility and high maintenance costs. Recent advances\nin Large Language Model (LLM)-based graphical user interface (GUI) agents have\nenabled more flexible automation, yet these general-purpose agents face five\ncritical challenges when applied to industrial management, including unfamiliar\nelement understanding, precision and efficiency, state localization, deployment\nconstraints, and safety requirements. To address these issues, we propose\nInfraMind, a novel exploration-based GUI agentic framework specifically\ntailored for industrial management systems. InfraMind integrates five\ninnovative modules to systematically resolve different challenges in industrial\nmanagement: (1) systematic search-based exploration with virtual machine\nsnapshots for autonomous understanding of complex GUIs; (2) memory-driven\nplanning to ensure high-precision and efficient task execution; (3) advanced\nstate identification for robust localization in hierarchical interfaces; (4)\nstructured knowledge distillation for efficient deployment with lightweight\nmodels; and (5) comprehensive, multi-layered safety mechanisms to safeguard\nsensitive operations. Extensive experiments on both open-source and commercial\nDCIM platforms demonstrate that our approach consistently outperforms existing\nframeworks in terms of task success rate and operational efficiency, providing\na rigorous and scalable solution for industrial management automation.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13704v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13704v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.422,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.398,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on an exploration-based GUI agent framework using LLMs and systematic methods for industrial automation, with no mention of human feedback, reward models, or reinforcement learning techniques. It relies on autonomous exploration and knowledge distillation rather than aligning models with human preferences.",
      "weak_supervision_justification": "The paper involves programmatically generating knowledge, such as through systematic exploration to understand GUI elements and knowledge distillation from large to small models, which aligns with weak supervision by using noisy or imprecise sources for learning. However, it does not explicitly rely on weak supervision for training, as the core focus is on agentic frameworks rather than label generation for machine learning models.",
      "diffusion_reasoning_justification": "The paper describes reasoning through modules like memory-driven planning and a Perception-Reasoning-Action loop, but it does not involve diffusion models, iterative refinement of a chain-of-thought, or any multi-step logical reasoning based on diffusion processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "InfraMind is a novel framework designed to automate graphical user interfaces in mission-critical industrial management systems, such as data centers, by addressing key challenges like unfamiliar elements, precision, state localization, deployment constraints, and safety through five innovative modules: systematic search-based exploration, memory-driven planning, advanced state identification, structured knowledge distillation, and multi-layered safety mechanisms. The framework leverages large language models and virtual machine snapshots for efficient learning and execution, demonstrating superior task success rates and operational efficiency compared to existing methods in experiments on both open-source and commercial Data Center Infrastructure Management platforms.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new exploration-based framework with specialized modules tailored for industrial GUI automation, significantly advancing the state-of-the-art by addressing unique challenges in mission-critical systems that existing agents fail to handle effectively.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of AI for industrial automation, as it provides practical solutions for complex GUI management in critical infrastructures, though its specialized focus may limit wider influence.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with innovative approaches to GUI automation in industrial settings, making it valuable for researchers and practitioners in AI and software engineering, though it may not be essential for those outside these areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/68ebb4c3a20b549009042d8480c56b6e21ada552",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 11,
      "average_h_index": 4.75,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Liangtao Lin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380850473"
        },
        {
          "name": "Zhaomeng Zhu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2265294493"
        },
        {
          "name": "Tianwei Zhang",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2146333441"
        },
        {
          "name": "Yonggang Wen",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2114783855"
        }
      ]
    },
    {
      "id": "2509.13706",
      "title": "Automated Triaging and Transfer Learning of Incident Learning Safety\n  Reports Using Large Language Representational Models",
      "authors": [
        "Peter Beidler",
        "Mark Nguyen",
        "Kevin Lybarger",
        "Ola Holmberg",
        "Eric Ford",
        "John Kang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "PURPOSE: Incident reports are an important tool for safety and quality\nimprovement in healthcare, but manual review is time-consuming and requires\nsubject matter expertise. Here we present a natural language processing (NLP)\nscreening tool to detect high-severity incident reports in radiation oncology\nacross two institutions.\n  METHODS AND MATERIALS: We used two text datasets to train and evaluate our\nNLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA\nSAFRON (SF), all of which had severity scores labeled by clinical content\nexperts. We trained and evaluated two types of models: baseline support vector\nmachines (SVM) and BlueBERT which is a large language model pretrained on\nPubMed abstracts and hospitalized patient data. We assessed for\ngeneralizability of our model in two ways. First, we evaluated models trained\nusing Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that\nwas first fine-tuned on Inst.-train then on SF-train before testing on SF-test\nset. To further analyze model performance, we also examined a subset of 59\nreports from our Inst. dataset, which were manually edited for clarity.\n  RESULTS Classification performance on the Inst. test achieved AUROC 0.82\nusing SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,\nperformance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56\nusing BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,\nimproved the performance on SF test to AUROC 0.78. Performance of SVM, and\nBlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and\n0.74) was similar to human performance (AUROC 0.81).\n  CONCLUSION: In summary, we successfully developed cross-institution NLP\nmodels on incident report text from radiation oncology centers. These models\nwere able to detect high-severity reports similarly to humans on a curated\ndataset.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13706v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13706v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.354,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes using supervised learning techniques (SVM and fine-tuned BlueBERT) on human-labeled incident reports for classification and transfer learning, but it does not involve reinforcement learning, a reward model trained on human-ranked data, or any fine-tuning via reinforcement learning. RLHF specifically requires these elements, which are absent here.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13711",
      "title": "StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion\n  Models",
      "authors": [
        "Qiuyu Tang",
        "Joshua Krinsky",
        "Aparna Bharati"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The rapid advancement of generative models, particularly diffusion-based\napproaches, has inadvertently facilitated their potential for misuse. Such\nmodels enable malicious exploiters to replicate artistic styles that capture an\nartist's creative labor, personal vision, and years of dedication in an\ninexpensive manner. This has led to a rise in the need and exploration of\nmethods for protecting artworks against style mimicry. Although generic\ndiffusion models can easily mimic an artistic style, finetuning amplifies this\ncapability, enabling the model to internalize and reproduce the style with\nhigher fidelity and control. We hypothesize that certain cross-attention layers\nexhibit heightened sensitivity to artistic styles. Sensitivity is measured\nthrough activation strengths of attention layers in response to style and\ncontent representations, and assessing their correlations with features\nextracted from external models. Based on our findings, we introduce an\nefficient and lightweight protection strategy, StyleProtect, that achieves\neffective style defense against fine-tuned diffusion models by updating only\nselected cross-attention layers. Our experiments utilize a carefully curated\nartwork dataset based on WikiArt, comprising representative works from 30\nartists known for their distinctive and influential styles and cartoon\nanimations from the Anita dataset. The proposed method demonstrates promising\nperformance in safeguarding unique styles of artworks and anime from malicious\ndiffusion customization, while maintaining competitive imperceptibility.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13711v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13711v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.48,
      "distributed_training_score": 0.319,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of StyleProtect, a method to safeguard artistic styles in fine-tuned diffusion models by targeting specific layers to prevent style mimicry. It focuses on generative image synthesis and protection against misuse, with no discussion of adapting diffusion models for complex logical tasks, multi-step reasoning, or treating a 'Chain-of-Thought' as an entity. Thus, it does not relate to Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13713",
      "title": "UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation\n  with Visual Odometry",
      "authors": [
        "Tae-Wook Um",
        "Ki-Hyeon Kim",
        "Hyun-Duck Choi",
        "Hyo-Sung Ahn"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Monocular depth estimation has been increasingly adopted in robotics and\nautonomous driving for its ability to infer scene geometry from a single\ncamera. In self-supervised monocular depth estimation frameworks, the network\njointly generates and exploits depth and pose estimates during training,\nthereby eliminating the need for depth labels. However, these methods remain\nchallenged by uncertainty in the input data, such as low-texture or dynamic\nregions, which can cause reduced depth accuracy. To address this, we introduce\nUM-Depth, a framework that combines motion- and uncertainty-aware refinement to\nenhance depth accuracy at dynamic object boundaries and in textureless regions.\nSpecifically, we develop a teacherstudent training strategy that embeds\nuncertainty estimation into both the training pipeline and network\narchitecture, thereby strengthening supervision where photometric signals are\nweak. Unlike prior motion-aware approaches that incur inference-time overhead\nand rely on additional labels or auxiliary networks for real-time generation,\nour method uses optical flow exclusively within the teacher network during\ntraining, which eliminating extra labeling demands and any runtime cost.\nExtensive experiments on the KITTI and Cityscapes datasets demonstrate the\neffectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves\nstate-of-the-art results in both self-supervised depth and pose estimation on\nthe KITTI datasets.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13713v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13713v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.338,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves self-supervised monocular depth estimation, where the model is trained using photometric reconstruction errors from adjacent frames, programmatically generating supervisory signals without relying on hand-labeled depth data. This directly aligns with weak supervision, as it utilizes noisy or imprecise sources (e.g., photometric consistency) to create training labels, eliminating the need for precise ground truth.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper, titled UM-Depth, presents a self-supervised framework for monocular depth estimation that tackles uncertainties in dynamic and textureless regions by integrating a teacher-student architecture. The teacher network estimates depth, pose, and optical flow to identify problematic areas, guiding the student network to refine depth predictions using uncertainty masks and a triplet loss, resulting in state-of-the-art performance on the KITTI dataset and competitive results on Cityscapes.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper offers a notable improvement by combining existing ideas like teacher-student frameworks and uncertainty estimation in a new way to handle dynamic regions without additional runtime costs, advancing self-supervised monocular depth estimation effectively. However, it builds on established concepts rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of computer vision, particularly for applications in autonomous driving and robotics, due to its state-of-the-art results on key benchmarks. Nonetheless, its influence may be limited to specific areas of monocular depth estimation rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to self-supervised depth estimation with innovative refinements that achieve state-of-the-art results, making it important for researchers in computer vision to be aware of. While not groundbreaking enough to be essential for all, it offers practical insights and advancements in handling real-world challenges.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/07d9f535cbadc0bdfe614c803b56b1b79d966b57",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 2,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Tae-Wook Um",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2321963946"
        },
        {
          "name": "Ki-Hyeon Kim",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2151286902"
        },
        {
          "name": "Hyun-Duck Choi",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2151705124"
        },
        {
          "name": "Hyo-Sung Ahn",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380688080"
        }
      ]
    },
    {
      "id": "2509.13722",
      "title": "Mitigating Query Selection Bias in Referring Video Object Segmentation",
      "authors": [
        "Dingwei Zhang",
        "Dong Zhang",
        "Jinhui Tang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recently, query-based methods have achieved remarkable performance in\nReferring Video Object Segmentation (RVOS) by using textual static object\nqueries to drive cross-modal alignment. However, these static queries are\neasily misled by distractors with similar appearance or motion, resulting in\n\\emph{query selection bias}. To address this issue, we propose Triple Query\nFormer (TQF), which factorizes the referring query into three specialized\ncomponents: an appearance query for static attributes, an intra-frame\ninteraction query for spatial relations, and an inter-frame motion query for\ntemporal association. Instead of relying solely on textual embeddings, our\nqueries are dynamically constructed by integrating both linguistic cues and\nvisual guidance. Furthermore, we introduce two motion-aware aggregation modules\nthat enhance object token representations: Intra-frame Interaction Aggregation\nincorporates position-aware interactions among objects within a single frame,\nwhile Inter-frame Motion Aggregation leverages trajectory-guided alignment\nacross frames to ensure temporal coherence. Extensive experiments on multiple\nRVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our\nstructured query design and motion-aware aggregation modules.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13722v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13722v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.331,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of Triple Query Former (TQF) for improving Referring Video Object Segmentation by addressing query selection bias through specialized queries and aggregation modules. It focuses on computer vision techniques like cross-modal alignment and temporal coherence, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13735",
      "title": "State Space Models over Directed Graphs",
      "authors": [
        "Junzhi She",
        "Xunkai Li",
        "Rong-Hua Li",
        "Guoren Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Directed graphs are ubiquitous across numerous domains, where the\ndirectionality of edges encodes critical causal dependencies. However, existing\nGNNs and graph Transformers tailored for directed graphs face two major\nchallenges: (1) effectively capturing long-range causal dependencies derived\nfrom directed edges; (2) balancing accuracy and training efficiency when\nprocessing large-scale graph datasets. In recent years, state space models\n(SSMs) have achieved substantial progress in causal sequence tasks, and their\nvariants designed for graphs have demonstrated state-of-the-art accuracy while\nmaintaining high efficiency across various graph learning benchmarks. However,\nexisting graph state space models are exclusively designed for undirected\ngraphs, which limits their performance in directed graph learning. To this end,\nwe propose an innovative approach DirEgo2Token which sequentializes directed\ngraphs via k-hop ego graphs. This marks the first systematic extension of state\nspace models to the field of directed graph learning. Building upon this, we\ndevelop DirGraphSSM, a novel directed graph neural network architecture that\nimplements state space models on directed graphs via the message-passing\nmechanism. Experimental results demonstrate that DirGraphSSM achieves\nstate-of-the-art performance on three representative directed graph learning\ntasks while attaining competitive performance on two additional tasks with\n1.5$\\times $ to 2$\\times $ training speed improvements compared to existing\nstate-of-the-art models.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13735v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13735v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.384,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on extending state space models (SSMs) to directed graphs, emphasizing the capture of long-range causal dependencies through methods like DirEgo2Token and DirGraphSSM. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning on a 'Chain-of-Thought.' Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13747",
      "title": "Improving Generalized Visual Grounding with Instance-aware Joint\n  Learning",
      "authors": [
        "Ming Dai",
        "Wenxuan Cheng",
        "Jiang-Jiang Liu",
        "Lingfeng Yang",
        "Zhenhua Feng",
        "Wankou Yang",
        "Jingdong Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Generalized visual grounding tasks, including Generalized Referring\nExpression Comprehension (GREC) and Segmentation (GRES), extend the classical\nvisual grounding paradigm by accommodating multi-target and non-target\nscenarios. Specifically, GREC focuses on accurately identifying all referential\nobjects at the coarse bounding box level, while GRES aims for achieve\nfine-grained pixel-level perception. However, existing approaches typically\ntreat these tasks independently, overlooking the benefits of jointly training\nGREC and GRES to ensure consistent multi-granularity predictions and streamline\nthe overall process. Moreover, current methods often treat GRES as a semantic\nsegmentation task, neglecting the crucial role of instance-aware capabilities\nand the necessity of ensuring consistent predictions between instance-level\nboxes and masks. To address these limitations, we propose InstanceVG, a\nmulti-task generalized visual grounding framework equipped with instance-aware\ncapabilities, which leverages instance queries to unify the joint and\nconsistency predictions of instance-level boxes and masks. To the best of our\nknowledge, InstanceVG is the first framework to simultaneously tackle both GREC\nand GRES while incorporating instance-aware capabilities into generalized\nvisual grounding. To instantiate the framework, we assign each instance query a\nprior reference point, which also serves as an additional basis for target\nmatching. This design facilitates consistent predictions of points, boxes, and\nmasks for the same instance. Extensive experiments obtained on ten datasets\nacross four tasks demonstrate that InstanceVG achieves state-of-the-art\nperformance, significantly surpassing the existing methods in various\nevaluation metrics. The code and model will be publicly available at\nhttps://github.com/Dmmm1997/InstanceVG.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13747v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13747v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.336,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13754",
      "title": "Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person\n  Retrieval",
      "authors": [
        "Hao Yin",
        "Xin Man",
        "Feiyu Chen",
        "Jie Shao",
        "Heng Tao Shen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that\naims to retrieve the most relevant person images based on a given text query.\nThe key challenge in TIPR lies in achieving effective alignment between textual\nand visual modalities within a common latent space. To address this challenge,\nprior approaches incorporate attention mechanisms for implicit cross-modal\nlocal alignment. However, they lack the ability to verify whether all local\nfeatures are correctly aligned. Moreover, existing methods primarily focus on\nhard negative samples during model updates, with the goal of refining\ndistinctions between positive and negative pairs, often neglecting incorrectly\nmatched positive pairs. To alleviate these issues, we propose FMFA, a\ncross-modal Full-Mode Fine-grained Alignment framework, which enhances global\nmatching through explicit fine-grained alignment and existing implicit\nrelational reasoning -- hence the term ``full-mode\" -- without requiring\nadditional supervision. Specifically, we design an Adaptive Similarity\nDistribution Matching (A-SDM) module to rectify unmatched positive sample\npairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint\nembedding space, thereby achieving more precise global alignment. Additionally,\nwe introduce an Explicit Fine-grained Alignment (EFA) module, which makes up\nfor the lack of verification capability of implicit relational reasoning. EFA\nstrengthens explicit cross-modal fine-grained interactions by sparsifying the\nsimilarity matrix and employs a hard coding method for local alignment. Our\nproposed method is evaluated on three public datasets, achieving\nstate-of-the-art performance among all global matching methods. Our code is\navailable at https://github.com/yinhao1102/FMFA.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13754v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13754v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.34,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13755",
      "title": "Scrub It Out! Erasing Sensitive Memorization in Code Language Models via\n  Machine Unlearning",
      "authors": [
        "Zhaoyang Chu",
        "Yao Wan",
        "Zhikun Zhang",
        "Di Wang",
        "Zhou Yang",
        "Hongyu Zhang",
        "Pan Zhou",
        "Xuanhua Shi",
        "Hai Jin",
        "David Lo"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "While Code Language Models (CLMs) have demonstrated superior performance in\nsoftware engineering tasks such as code generation and summarization, recent\nempirical studies reveal a critical privacy vulnerability: these models exhibit\nunintended memorization of sensitive training data, enabling verbatim\nreproduction of confidential information when specifically prompted. To address\nthis issue, several approaches, including training data de-duplication and\ndifferential privacy augmentation, have been proposed. However, these methods\nrequire full-model retraining for deployed CLMs, which incurs substantial\ncomputational costs. In this paper, we aim to answer the following research\nquestion: Can sensitive information memorized by CLMs be erased effectively and\nefficiently?\n  We conduct a pioneering investigation into erasing sensitive memorization in\nCLMs through machine unlearning - a post-hoc modification method that removes\nspecific information from trained models without requiring full retraining.\nSpecifically, we first quantify the memorization risks of sensitive data within\nCLM training datasets and curate a high-risk dataset of 50,000 sensitive\nmemorized samples as unlearning targets. We study two widely used gradient\nascent-based unlearning approaches: the vanilla and constraint-based methods,\nand introduce CodeEraser, an advanced variant that selectively unlearns\nsensitive memorized segments in code while preserving the structural integrity\nand functional correctness of the surrounding code. Extensive experiments on\nthree families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder,\nvalidate the effectiveness and efficiency of CodeEraser in erasing targeted\nsensitive memorization while maintaining model utility.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13755v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13755v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.421,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.372,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper primarily focuses on machine unlearning techniques to erase sensitive memorization in Code Language Models, rather than on training models with programmatically generated labels. While the paper mentions curating a dataset of sensitive memorized samples, which might involve automated identification processes similar to weak supervision, this is not the main contribution and does not center on training with noisy or imprecise labels.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13756",
      "title": "Controllable-Continuous Color Editing in Diffusion Model via Color\n  Mapping",
      "authors": [
        "Yuqi Yang",
        "Dongliang Chang",
        "Yuanchen Fang",
        "Yi-Zhe SonG",
        "Zhanyu Ma",
        "Jun Guo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In recent years, text-driven image editing has made significant progress.\nHowever, due to the inherent ambiguity and discreteness of natural language,\ncolor editing still faces challenges such as insufficient precision and\ndifficulty in achieving continuous control. Although linearly interpolating the\nembedding vectors of different textual descriptions can guide the model to\ngenerate a sequence of images with varying colors, this approach lacks precise\ncontrol over the range of color changes in the output images. Moreover, the\nrelationship between the interpolation coefficient and the resulting image\ncolor is unknown and uncontrollable. To address these issues, we introduce a\ncolor mapping module that explicitly models the correspondence between the text\nembedding space and image RGB values. This module predicts the corresponding\nembedding vector based on a given RGB value, enabling precise color control of\nthe generated images while maintaining semantic consistency. Users can specify\na target RGB range to generate images with continuous color variations within\nthe desired range, thereby achieving finer-grained, continuous, and\ncontrollable color editing. Experimental results demonstrate that our method\nperforms well in terms of color continuity and controllability.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13756v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13756v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.491,
      "distributed_training_score": 0.287,
      "datasets_score": 0.257,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on controllable color editing in diffusion models for image generation, specifically introducing a color mapping module to handle RGB-based color changes. It does not involve adapting the diffusion process for complex logical tasks, such as multi-step reasoning or treating a 'Chain-of-Thought' as an entity for holistic correction. While diffusion models are used, the application is limited to visual editing, lacking any components for logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13760",
      "title": "Iterative Prompt Refinement for Safer Text-to-Image Generation",
      "authors": [
        "Jinwoo Jeon",
        "JunHyeok Oh",
        "Hayeong Lee",
        "Byung-Jun Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-Image (T2I) models have made remarkable progress in generating images\nfrom text prompts, but their output quality and safety still depend heavily on\nhow prompts are phrased. Existing safety methods typically refine prompts using\nlarge language models (LLMs), but they overlook the images produced, which can\nresult in unsafe outputs or unnecessary changes to already safe prompts. To\naddress this, we propose an iterative prompt refinement algorithm that uses\nVision Language Models (VLMs) to analyze both the input prompts and the\ngenerated images. By leveraging visual feedback, our method refines prompts\nmore effectively, improving safety while maintaining user intent and\nreliability comparable to existing LLM-based approaches. Additionally, we\nintroduce a new dataset labeled with both textual and visual safety signals\nusing off-the-shelf multi-modal LLM, enabling supervised fine-tuning.\nExperimental results demonstrate that our approach produces safer outputs\nwithout compromising alignment with user intent, offering a practical solution\nfor generating safer T2I content. Our code is available at\nhttps://github.com/ku-dmlab/IPR. \\textbf{\\textcolor{red}WARNING: This paper\ncontains examples of harmful or inappropriate images generated by models.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13760v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13760v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.475,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.475,
      "distributed_training_score": 0.328,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions a simplified RL formulation for training the prompt refiner, but it relies on automated evaluations from a Vision Language Model (VLM) rather than human feedback. There is no evidence of using human-ranked data to train a reward model or fine-tune the main model with human preferences, which are core to RLHF. Thus, it does not align with RLHF principles.",
      "weak_supervision_justification": "The paper creates a new dataset (ToxiClean-IT) by using an off-the-shelf multi-modal LLM to programmatically generate labels for textual and visual safety signals, which involves noisy or imprecise sources rather than hand-labeled data. This directly matches weak supervision, as it trains models with programmatically derived labels on a large scale.",
      "diffusion_reasoning_justification": "The paper involves iterative prompt refinement, which is a multi-step process, but it is applied to text-to-image generation for safety, not to complex logical tasks or reasoning paths using diffusion models. There is no clear adaptation of diffusion processes for holistic chain-of-thought correction, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper, titled \"Iterative Prompt Refinement for Safer Text-to-Image Generation,\" addresses the limitations of existing prompt-based safety methods for Text-to-Image (T2I) models by proposing an Iterative Prompt Refinement (IPR) framework that leverages Vision Language Models (VLMs) to analyze both input prompts and generated images iteratively. This approach aims to enhance output safety while preserving user intent, involving the creation of a new dataset for supervised fine-tuning and a simplified reinforcement learning formulation for training; experimental results show that IPR produces safer images with comparable intent alignment to prior methods.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by integrating VLMs for visual feedback in prompt refinement, which is a clever extension of existing LLM-based techniques rather than a completely new problem or architecture. This combination addresses overlooked aspects like image analysis, advancing safety in T2I generation without introducing a groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in AI safety for T2I models and multimodal systems, as it provides a practical method for safer generation that could be adopted or built upon in specific subfields like computer vision. However, its applicability is somewhat limited to generative AI ethics, reducing its broader commercial or widespread impact.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a strong, practical contribution to addressing safety in T2I models, making it essential for researchers focused on AI ethics and generative systems to be aware of this advancement. While not revolutionary, its methodological insights and empirical results provide valuable knowledge for the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1db09274ca0d79d5659336893b616c3da58e5ddd",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 1,
      "average_h_index": 0.25,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jinwoo Jeon",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381066943"
        },
        {
          "name": "JunHyeok Oh",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2364882091"
        },
        {
          "name": "Hayeong Lee",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2341650784"
        },
        {
          "name": "Byung-Jun Lee",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373466222"
        }
      ]
    },
    {
      "id": "2509.13761",
      "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical\n  Reasoning",
      "authors": [
        "Qikai Chang",
        "Zhenrong Zhang",
        "Pengfei Hu",
        "Jun Du",
        "Jiefeng Ma",
        "Yicheng Pan",
        "Jianshu Zhang",
        "Quan Liu",
        "Jianqing Gao"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Large Language Models (LLMs) have made remarkable progress in mathematical\nreasoning, but still continue to struggle with high-precision tasks like\nnumerical computation and formal symbolic manipulation. Integrating external\ntools has emerged as a promising approach to bridge this gap. Despite recent\nadvances, existing methods struggle with three key challenges: constructing\ntool-integrated reasoning data, performing fine-grained optimization, and\nenhancing inference. To overcome these limitations, we propose THOR\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\na multi-agent actor-critic-based pipeline for constructing high-quality\ndatasets of tool-integrated reasoning paths, aligning with the policy and\ngeneralizing well across diverse models. Second, to perform fine-grained\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\nfor both episode-level problem solving and step-level code generation. This is\nmotivated by our key insight that the success of an intermediate tool call is a\nstrong predictor of the final answer's correctness. Finally, THOR incorporates\na self-correction mechanism that leverages immediate tool feedback to\ndynamically revise erroneous reasoning paths during inference. Our approach\ndemonstrates strong generalization across diverse models, performing\neffectively in both reasoning and non-reasoning models. It further achieves\nstate-of-the-art performance for models of a similar scale on multiple\nmathematical benchmarks, while also delivering consistent improvements on code\nbenchmarks. Our code will be publicly available at\nhttps://github.com/JingMog/THOR.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13761v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13761v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.471,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.489,
      "distributed_training_score": 0.406,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning (RL) in its THOR framework, specifically an actor-critic-based pipeline for data construction and hierarchical optimization, but it does not involve human feedback, a reward model trained on human-ranked data, or alignment with human preferences. Instead, rewards are based on task success like answer correctness, making it standard RL without the human element required for RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on RL-based methods for tool-integrated reasoning, including hierarchical optimization and self-correction, but does not incorporate diffusion models, iterative refinement processes for Chain-of-Thought, or any multi-step logical reasoning adapted from diffusion techniques.",
      "distributed_training_justification": "The paper does not address distributed training, parallel computing, or multi-node machine learning strategies. Its contributions center on RL for mathematical reasoning and data construction, with no mention of partitioning data, architecture, or computation across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13762",
      "title": "Task-Aware Image Signal Processor for Advanced Visual Perception",
      "authors": [
        "Kai Chen",
        "Jin Xiao",
        "Leheng Zhang",
        "Kexuan Shi",
        "Shuhang Gu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In recent years, there has been a growing trend in computer vision towards\nexploiting RAW sensor data, which preserves richer information compared to\nconventional low-bit RGB images. Early studies mainly focused on enhancing\nvisual quality, while more recent efforts aim to leverage the abundant\ninformation in RAW data to improve the performance of visual perception tasks\nsuch as object detection and segmentation. However, existing approaches still\nface two key limitations: large-scale ISP networks impose heavy computational\noverhead, while methods based on tuning traditional ISP pipelines are\nrestricted by limited representational capacity.To address these issues, we\npropose Task-Aware Image Signal Processing (TA-ISP), a compact RAW-to-RGB\nframework that produces task-oriented representations for pretrained vision\nmodels. Instead of heavy dense convolutional pipelines, TA-ISP predicts a small\nset of lightweight, multi-scale modulation operators that act at global,\nregional, and pixel scales to reshape image statistics across different spatial\nextents. This factorized control significantly expands the range of spatially\nvarying transforms that can be represented while keeping memory usage,\ncomputation, and latency tightly constrained. Evaluated on several RAW-domain\ndetection and segmentation benchmarks under both daytime and nighttime\nconditions, TA-ISP consistently improves downstream accuracy while markedly\nreducing parameter count and inference time, making it well suited for\ndeployment on resource-constrained devices.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13762v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13762v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.39,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13766",
      "title": "NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World\n  Benchmark Dataset",
      "authors": [
        "Huichun Liu",
        "Xiaosong Li",
        "Yang Liu",
        "Xiaoqi Cheng",
        "Haishu Tan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Visual degradation caused by rain streak artifacts in low-light conditions\nsignificantly hampers the performance of nighttime surveillance and autonomous\nnavigation. Existing image deraining techniques are primarily designed for\ndaytime conditions and perform poorly under nighttime illumination due to the\nspatial heterogeneity of rain distribution and the impact of light-dependent\nstripe visibility. In this paper, we propose a novel Nighttime Deraining\nLocation-enhanced Perceptual Network(NDLPNet) that effectively captures the\nspatial positional information and density distribution of rain streaks in\nlow-light environments. Specifically, we introduce a Position Perception Module\n(PPM) to capture and leverage spatial contextual information from input data,\nenhancing the model's capability to identify and recalibrate the importance of\ndifferent feature channels. The proposed nighttime deraining network can\neffectively remove the rain streaks as well as preserve the crucial background\ninformation. Furthermore, We construct a night scene rainy (NSR) dataset\ncomprising 900 image pairs, all based on real-world nighttime scenes, providing\na new benchmark for nighttime deraining task research. Extensive qualitative\nand quantitative experimental evaluations on both existing datasets and the NSR\ndataset consistently demonstrate our method outperform the state-of-the-art\n(SOTA) methods in nighttime deraining tasks. The source code and dataset is\navailable at https://github.com/Feecuin/NDLPNet.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13766v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13766v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.344,
      "datasets_score": 0.388,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13767",
      "title": "VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in\n  Real-time MRI",
      "authors": [
        "Daiqi Liu",
        "Tomás Arias-Vergara",
        "Johannes Enk",
        "Fangxu Xing",
        "Maureen Stone",
        "Jerry L. Prince",
        "Jana Hutter",
        "Andreas Maier",
        "Jonghye Woo",
        "Paula Andrea Pérez-Toro"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurately segmenting articulatory structures in real-time magnetic resonance\nimaging (rtMRI) remains challenging, as most existing methods rely almost\nentirely on visual cues. Yet synchronized acoustic and phonological signals\nprovide complementary context that can enrich visual information and improve\nprecision. In this paper, we introduce VocSegMRI, a multimodal framework that\nintegrates video, audio, and phonological inputs through cross-attention fusion\nfor dynamic feature alignment. To further enhance cross-modal representation,\nwe incorporate a contrastive learning objective that improves segmentation\nperformance even when the audio modality is unavailable at inference. Evaluated\non a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-art\nperformance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance\n(HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines.\nAblation studies confirm the contributions of cross-attention and contrastive\nlearning to segmentation precision and robustness. These results highlight the\nvalue of integrative multimodal modeling for accurate vocal tract analysis.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13767v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13767v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.332,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13768",
      "title": "Generative Image Coding with Diffusion Prior",
      "authors": [
        "Jianhui Chang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "As generative technologies advance, visual content has evolved into a complex\nmix of natural and AI-generated images, driving the need for more efficient\ncoding techniques that prioritize perceptual quality. Traditional codecs and\nlearned methods struggle to maintain subjective quality at high compression\nratios, while existing generative approaches face challenges in visual fidelity\nand generalization. To this end, we propose a novel generative coding framework\nleveraging diffusion priors to enhance compression performance at low bitrates.\nOur approach employs a pre-optimized encoder to generate generalized\ncompressed-domain representations, integrated with the pretrained model's\ninternal features via a lightweight adapter and an attentive fusion module.\nThis framework effectively leverages existing pretrained diffusion models and\nenables efficient adaptation to different pretrained models for new\nrequirements with minimal retraining costs. We also introduce a distribution\nrenormalization method to further enhance reconstruction fidelity. Extensive\nexperiments show that our method (1) outperforms existing methods in visual\nfidelity across low bitrates, (2) improves compression performance by up to 79%\nover H.266/VVC, and (3) offers an efficient solution for AI-generated content\nwhile being adaptable to broader content types.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13768v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13768v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.582,
      "distributed_training_score": 0.348,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for generative image coding and compression, specifically leveraging diffusion priors to enhance visual fidelity and reconstruction quality at low bitrates. While diffusion models' iterative refinement process is employed for image generation, there is no component involving multi-step logical reasoning, chain-of-thought processing, or adaptation of diffusion for solving complex logical tasks. The work is centered on visual content and perceptual quality, not reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13769",
      "title": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for\n  Autonomous Driving",
      "authors": [
        "Yuechen Luo",
        "Fang Li",
        "Shaoqing Xu",
        "Zhiyi Lai",
        "Lei Yang",
        "Qimao Chen",
        "Ziang Luo",
        "Zixun Xie",
        "Shengyin Jiang",
        "Jiaxin Liu",
        "Long Chen",
        "Bing Wang",
        "Zhi-xin Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "While reasoning technology like Chain of Thought (CoT) has been widely\nadopted in Vision Language Action (VLA) models, it demonstrates promising\ncapabilities in end to end autonomous driving. However, recent efforts to\nintegrate CoT reasoning often fall short in simple scenarios, introducing\nunnecessary computational overhead without improving decision quality. To\naddress this, we propose AdaThinkDrive, a novel VLA framework with a dual mode\nreasoning mechanism inspired by fast and slow thinking. First, our framework is\npretrained on large scale autonomous driving (AD) scenarios using both question\nanswering (QA) and trajectory datasets to acquire world knowledge and driving\ncommonsense. During supervised fine tuning (SFT), we introduce a two mode\ndataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the\nmodel to distinguish between scenarios that require reasoning. Furthermore, an\nAdaptive Think Reward strategy is proposed in conjunction with the Group\nRelative Policy Optimization (GRPO), which rewards the model for selectively\napplying CoT by comparing trajectory quality across different reasoning modes.\nExtensive experiments on the Navsim benchmark show that AdaThinkDrive achieves\na PDMS of 90.3, surpassing the best vision only baseline by 1.7 points.\nMoreover, ablations show that AdaThinkDrive surpasses both the never Think and\nalways Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also\nreduces inference time by 14% compared to the always Think baseline,\ndemonstrating its ability to balance accuracy and efficiency through adaptive\nreasoning.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13769v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13769v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.541,
      "distributed_training_score": 0.375,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses Group Relative Policy Optimization (GRPO) as a reinforcement learning method with rewards based on trajectory quality and benchmarks, but it does not involve human feedback, such as training a reward model on human-ranked data. Therefore, it does not align with the definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper incorporates Chain of Thought (CoT) for reasoning in autonomous driving but does not adapt diffusion models or their iterative refinement processes for logical tasks. There is no mention of treating reasoning paths as entities for multi-step correction via diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13773",
      "title": "MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based\n  Instruction Recommendation",
      "authors": [
        "Zhipeng Bian",
        "Jieming Zhu",
        "Xuyang Xie",
        "Quanyu Dai",
        "Zhou Zhao",
        "Zhenhua Dong"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "The rapid advancement of generative AI technologies is driving the\nintegration of diverse AI-powered services into smartphones, transforming how\nusers interact with their devices. To simplify access to predefined AI\nservices, this paper introduces MIRA, a pioneering framework for task\ninstruction recommendation that enables intuitive one-touch AI tasking on\nsmartphones. With MIRA, users can long-press on images or text objects to\nreceive contextually relevant instruction recommendations for executing AI\ntasks. Our work introduces three key innovations: 1) A multimodal large\nlanguage model (MLLM)-based recommendation pipeline with structured reasoning\nto extract key entities, infer user intent, and generate precise instructions;\n2) A template-augmented reasoning mechanism that integrates high-level\nreasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based\nconstrained decoding strategy that restricts outputs to predefined instruction\ncandidates, ensuring coherent and intent-aligned suggestions. Through\nevaluation using a real-world annotated datasets and a user study, MIRA has\ndemonstrated substantial improvements in the accuracy of instruction\nrecommendation. The encouraging results highlight MIRA's potential to\nrevolutionize the way users engage with AI services on their smartphones,\noffering a more seamless and efficient experience.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13773v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13773v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.426,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.366,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a framework for AI instruction recommendation using MLLMs, with innovations in reasoning and decoding strategies. It mentions evaluation via user studies and datasets, but there is no indication of training a reward model or using reinforcement learning to fine-tune models based on human-ranked feedback. Thus, it does not involve RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes structured reasoning, template-augmented mechanisms, and constrained decoding for task inference, but it does not adapt diffusion models for iterative refinement of reasoning paths or treat chain-of-thought as a holistically corrected entity. There is no mention of diffusion-based processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13775",
      "title": "Exploring Data and Parameter Efficient Strategies for Arabic Dialect\n  Identifications",
      "authors": [
        "Vani Kanjirangat",
        "Ljiljana Dolamic",
        "Fabio Rinaldi"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper discusses our exploration of different data-efficient and\nparameter-efficient approaches to Arabic Dialect Identification (ADI). In\nparticular, we investigate various soft-prompting strategies, including\nprefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA\nreparameterizations. For the data-efficient strategy, we analyze hard prompting\nwith zero-shot and few-shot inferences to analyze the dialect identification\ncapabilities of Large Language Models (LLMs). For the parameter-efficient PEFT\napproaches, we conducted our experiments using Arabic-specific encoder models\non several major datasets. We also analyzed the n-shot inferences on\nopen-source decoder-only models, a general multilingual model (Phi-3.5), and an\nArabic-specific one(SILMA). We observed that the LLMs generally struggle to\ndifferentiate the dialectal nuances in the few-shot or zero-shot setups. The\nsoft-prompted encoder variants perform better, while the LoRA-based fine-tuned\nmodels perform best, even surpassing full fine-tuning.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13775v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13775v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.394,
      "datasets_score": 0.402,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on data-efficient and parameter-efficient strategies for Arabic Dialect Identification, such as prompting and fine-tuning, with no mention of reinforcement learning, human feedback, or training a reward model.",
      "weak_supervision_justification": "The paper examines zero-shot and few-shot learning for dialect identification but does not involve programmatically generating labels from noisy or imprecise sources, which is central to weak supervision.",
      "diffusion_reasoning_justification": "The paper does not discuss diffusion models, iterative refinement for reasoning, or multi-step logical processes; it centers on prompting and fine-tuning strategies for dialect identification.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper evaluates and benchmarks multiple Arabic Dialect Identification datasets using various models and strategies, involving analysis and comparative performance, though the primary focus is on fine-tuning methods rather than dataset creation or curation.",
      "llm_score_status": "completed",
      "summary": "This paper explores data-efficient and parameter-efficient strategies for Arabic Dialect Identification (ADI), focusing on evaluating hard prompting in zero-shot and few-shot settings with large language models (LLMs), as well as soft-prompting techniques and LoRA fine-tuning on Arabic-specific encoder models. The methodology involves experiments across multiple ADI datasets using models like Phi-3.5 and SILMA, revealing that LLMs perform poorly in low-data scenarios, soft-prompted encoders show improved results, and LoRA-based fine-tuning achieves the best performance, even surpassing full fine-tuning.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper applies existing techniques like prompting and LoRA to the specific context of Arabic Dialect Identification, offering a clever combination for a known problem in a underrepresented language, thus providing a notable improvement rather than a groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in low-resource language processing and dialect identification by demonstrating effective strategies for Arabic, potentially leading to citations and applications within NLP subfields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable insights into efficient fine-tuning for Arabic NLP tasks, making it a significant contribution for researchers in computational linguistics or AI focused on low-resource languages, though it may not be essential for broader audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/fe841e5decc46d38498c04f241d16b35be08bc08",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 5,
      "average_h_index": 3.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Vani Kanjirangat",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2125681709"
        },
        {
          "name": "Ljiljana Dolamic",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/1680665"
        },
        {
          "name": "Fabio Rinaldi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2315306404"
        }
      ]
    },
    {
      "id": "2509.13776",
      "title": "Morphology-optimized Multi-Scale Fusion: Combining Local Artifacts and\n  Mesoscopic Semantics for Deepfake Detection and Localization",
      "authors": [
        "Chao Shuai",
        "Gaojian Wang",
        "Kun Pan",
        "Tong Wu",
        "Fanli Jin",
        "Haohan Tan",
        "Mengxiang Li",
        "Zhenguang Liu",
        "Feng Lin",
        "Kui Ren"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "While the pursuit of higher accuracy in deepfake detection remains a central\ngoal, there is an increasing demand for precise localization of manipulated\nregions. Despite the remarkable progress made in classification-based\ndetection, accurately localizing forged areas remains a significant challenge.\nA common strategy is to incorporate forged region annotations during model\ntraining alongside manipulated images. However, such approaches often neglect\nthe complementary nature of local detail and global semantic context, resulting\nin suboptimal localization performance. Moreover, an often-overlooked aspect is\nthe fusion strategy between local and global predictions. Naively combining the\noutputs from both branches can amplify noise and errors, thereby undermining\nthe effectiveness of the localization.\n  To address these issues, we propose a novel approach that independently\npredicts manipulated regions using both local and global perspectives. We\nemploy morphological operations to fuse the outputs, effectively suppressing\nnoise while enhancing spatial coherence. Extensive experiments reveal the\neffectiveness of each module in improving the accuracy and robustness of\nforgery localization.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13776v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13776v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.374,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a morphology-optimized multi-scale fusion framework for deepfake detection and localization, focusing on combining local and mesoscopic features using morphological operations. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13782",
      "title": "Who is Introducing the Failure? Automatically Attributing Failures of\n  Multi-Agent Systems via Spectrum Analysis",
      "authors": [
        "Yu Ge",
        "Linna Xie",
        "Zhong Li",
        "Yu Pei",
        "Tian Zhang"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Large Language Model Powered Multi-Agent Systems (MASs) are increasingly\nemployed to automate complex real-world problems, such as programming and\nscientific discovery. Despite their promising, MASs are not without their\nflaws. However, failure attribution in MASs - pinpointing the specific agent\nactions responsible for failures - remains underexplored and labor-intensive,\nposing significant challenges for debugging and system improvement. To bridge\nthis gap, we propose FAMAS, the first spectrum-based failure attribution\napproach for MASs, which operates through systematic trajectory replay and\nabstraction, followed by spectrum analysis.The core idea of FAMAS is to\nestimate, from variations across repeated MAS executions, the likelihood that\neach agent action is responsible for the failure. In particular, we propose a\nnovel suspiciousness formula tailored to MASs, which integrates two key factor\ngroups, namely the agent behavior group and the action behavior group, to\naccount for the agent activation patterns and the action activation patterns\nwithin the execution trajectories of MASs. Through expensive evaluations\nagainst 12 baselines on the Who and When benchmark, FAMAS demonstrates superior\nperformance by outperforming all the methods in comparison.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13782v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13782v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.327,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13784",
      "title": "CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate\n  Scheduling",
      "authors": [
        "Hanfang Liang",
        "Bing Wang",
        "Shizhen Zhang",
        "Wen Jiang",
        "Yizhuo Yang",
        "Weixiang Guo",
        "Shenghai Yuan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Event cameras capture asynchronous pixel-level brightness changes with\nmicrosecond temporal resolution, offering unique advantages for high-speed\nvision tasks. Existing methods often convert event streams into intermediate\nrepresentations such as frames, voxel grids, or point clouds, which inevitably\nrequire predefined time windows and thus introduce window latency. Meanwhile,\npointwise detection methods face computational challenges that prevent\nreal-time efficiency due to their high computational cost. To overcome these\nlimitations, we propose the Variable-Rate Spatial Event Mamba, a novel\narchitecture that directly processes raw event streams without intermediate\nrepresentations. Our method introduces a lightweight causal spatial\nneighborhood encoder to efficiently capture local geometric relations, followed\nby Mamba-based state space models for scalable temporal modeling with linear\ncomplexity. During inference, a controller adaptively adjusts the processing\nspeed according to the event rate, achieving an optimal balance between window\nlatency and inference latency.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13784v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13784v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.276,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.346,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13789",
      "title": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise\n  Caching",
      "authors": [
        "Hanshuai Cui",
        "Zhiqing Tang",
        "Zhifei Xu",
        "Zhi Yao",
        "Wenyi Zeng",
        "Weijia Jia"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13789v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13789v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.49,
      "distributed_training_score": 0.407,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on accelerating inference in Diffusion Transformers for video generation by caching features, which is a generative task. It does not involve adapting diffusion models for multi-step logical reasoning, chain-of-thought processes, or iterative refinement of complex logical tasks.",
      "distributed_training_justification": "The paper proposes a training-free method for accelerating inference in diffusion models through feature caching, without any discussion of distributed training, parallel computing, multi-node setups, or partitioning data/computation across processors for model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13790",
      "title": "Teaching According to Talents! Instruction Tuning LLMs with\n  Competence-Aware Curriculum Learning",
      "authors": [
        "Yangning Li",
        "Tingwei Lu",
        "Yinghui Li",
        "Yankai Chen",
        "Wei-Chieh Huang",
        "Wenhao Jiang",
        "Hui Wang",
        "Hai-Tao Zheng",
        "Philip S. Yu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Efficient instruction tuning aims to enhance the ultimate performance of\nlarge language models (LLMs) trained on a given instruction dataset. Curriculum\nlearning as a typical data organization strategy has shown preliminary\neffectiveness in instruction tuning. However, current curriculum tuning methods\nsuffer from the curriculum rigidity, since they rely solely on static heuristic\ndifficulty metrics. These methods fail to adapt to the evolving capabilities of\nmodels during training, resulting in a fixed and potentially sub-optimal\nlearning trajectory. To address the issue, Competence-Aware Multi-Perspective\ncUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS\noffers several advantages: (1) Dynamic selection for sub-curriculum. (2)\nCompetency-aware adjustment to the curriculum schedule. (3) Multiple\ndifficulty-based scheduling. Extensive experiments prove the superior\nperformance of CAMPUS, compared to other state-of-the-art baselines for\nefficient instruction tuning.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13790v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13790v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.384,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a competence-aware curriculum learning framework for instruction tuning of LLMs, using adversarial learning to assess data difficulty. It does not involve training a reward model on human-ranked data or using reinforcement learning to fine-tune the model based on human feedback, which are core elements of RLHF.",
      "weak_supervision_justification": "The paper employs programmatically derived difficulty metrics and a lightweight scoring model trained with adversarial learning to organize instruction data, which somewhat aligns with weak supervision by using noisy or heuristic sources for assessments rather than hand-labeled data. However, the primary focus is on curriculum learning for instruction tuning, not on training models with weakly supervised labels.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13792",
      "title": "Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust\n  Spacecraft 6-DoF Pose Estimation",
      "authors": [
        "Inder Pal Singh",
        "Nidhal Eddine Chenni",
        "Abd El Rahman Shabayek",
        "Arunkumar Rathinam",
        "Djamila Aouada"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous\nspace operations such as rendezvous, docking, and in-orbit servicing. Hybrid\npipelines that combine object detection, keypoint regression, and\nPerspective-n-Point (PnP) solvers have recently achieved strong results on\nsynthetic datasets, yet their performance deteriorates sharply on real or\nlab-generated imagery due to the persistent synthetic-to-real domain gap.\nExisting unsupervised domain adaptation approaches aim to mitigate this issue\nbut often underperform when a modest number of labeled target samples are\navailable. In this work, we propose the first Supervised Domain Adaptation\n(SDA) framework tailored for SPE keypoint regression. Building on the Learning\nInvariant Representation and Risk (LIRR) paradigm, our method jointly optimizes\ndomain-invariant representations and task-specific risk using both labeled\nsynthetic and limited labeled real data, thereby reducing generalization error\nunder domain shift. Extensive experiments on the SPEED+ benchmark demonstrate\nthat our approach consistently outperforms source-only, fine-tuning, and oracle\nbaselines. Notably, with only 5% labeled target data, our method matches or\nsurpasses oracle performance trained on larger fractions of labeled data. The\nframework is lightweight, backbone-agnostic, and computationally efficient,\noffering a practical pathway toward robust and deployable spacecraft pose\nestimation in real-world space environments.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13792v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13792v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.432,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.402,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on supervised domain adaptation for spacecraft pose estimation using labeled synthetic and real data, without any involvement of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper relies on directly labeled synthetic and limited real data for training, rather than programmatically generating labels from noisy or imprecise sources, which is the core of weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper does not discuss distributed training, parallel computing, or multi-node strategies; it instead emphasizes a lightweight framework for domain adaptation without reference to accelerating training across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13795",
      "title": "SWA-PF: Semantic-Weighted Adaptive Particle Filter for Memory-Efficient\n  4-DoF UAV Localization in GNSS-Denied Environments",
      "authors": [
        "Jiayu Yuan",
        "Ming Dai",
        "Enhui Zheng",
        "Chao Su",
        "Nanxing Chen",
        "Qiming Hu",
        "Shibo Zhu",
        "Yibin Cao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision-based Unmanned Aerial Vehicle (UAV) localization systems have been\nextensively investigated for Global Navigation Satellite System (GNSS)-denied\nenvironments. However, existing retrieval-based approaches face limitations in\ndataset availability and persistent challenges including suboptimal real-time\nperformance, environmental sensitivity, and limited generalization capability,\nparticularly in dynamic or temporally varying environments. To overcome these\nlimitations, we present a large-scale Multi-Altitude Flight Segments dataset\n(MAFS) for variable altitude scenarios and propose a novel Semantic-Weighted\nAdaptive Particle Filter (SWA-PF) method. This approach integrates robust\nsemantic features from both UAV-captured images and satellite imagery through\ntwo key innovations: a semantic weighting mechanism and an optimized particle\nfiltering architecture. Evaluated using our dataset, the proposed method\nachieves 10x computational efficiency gain over feature extraction methods,\nmaintains global positioning errors below 10 meters, and enables rapid 4 degree\nof freedom (4-DoF) pose estimation within seconds using accessible\nlow-resolution satellite maps. Code and dataset will be available at\nhttps://github.com/YuanJiayuuu/SWA-PF.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13795v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13795v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.314,
      "distributed_training_score": 0.335,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13801",
      "title": "Masked Feature Modeling Enhances Adaptive Segmentation",
      "authors": [
        "Wenlve Zhou",
        "Zhiheng Zhou",
        "Tiantao Xian",
        "Yikui Zhai",
        "Weibin Wu",
        "Biyun Ma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Unsupervised domain adaptation (UDA) for semantic segmentation aims to\ntransfer models from a labeled source domain to an unlabeled target domain.\nWhile auxiliary self-supervised tasks-particularly contrastive learning-have\nimproved feature discriminability, masked modeling approaches remain\nunderexplored in this setting, largely due to architectural incompatibility and\nmisaligned optimization objectives. We propose Masked Feature Modeling (MFM), a\nnovel auxiliary task that performs feature masking and reconstruction directly\nin the feature space. Unlike existing masked modeling methods that reconstruct\nlow-level inputs or perceptual features (e.g., HOG or visual tokens), MFM\naligns its learning target with the main segmentation task, ensuring\ncompatibility with standard architectures like DeepLab and DAFormer without\nmodifying the inference pipeline. To facilitate effective reconstruction, we\nintroduce a lightweight auxiliary module, Rebuilder, which is trained jointly\nbut discarded during inference, adding zero computational overhead at test\ntime. Crucially, MFM leverages the segmentation decoder to classify the\nreconstructed features, tightly coupling the auxiliary objective with the\npixel-wise prediction task to avoid interference with the primary task.\nExtensive experiments across various architectures and UDA benchmarks\ndemonstrate that MFM consistently enhances segmentation performance, offering a\nsimple, efficient, and generalizable strategy for unsupervised domain-adaptive\nsemantic segmentation.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13801v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13801v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.422,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.364,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on Unsupervised Domain Adaptation (UDA) for semantic segmentation, which uses labeled source data and unlabeled target data to adapt models, aligning with weak supervision by relying on noisy or unlabeled data rather than fully hand-labeled datasets. However, the main contribution—MFM as an auxiliary task—emphasizes feature masking and reconstruction for better adaptation, rather than directly addressing the generation or use of programmatically created labels. Thus, while UDA incorporates elements of weak supervision, the paper's core innovation is more specific to domain adaptation techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Masked Feature Modeling (MFM), a novel auxiliary task for unsupervised domain adaptation (UDA) in semantic segmentation, which addresses limitations of existing methods by performing masking and reconstruction directly in the feature space to improve feature discriminability and align with the primary segmentation objective. It proposes a lightweight Rebuilder module for feature reconstruction during training, which is discarded at inference, and demonstrates through experiments that MFM enhances performance across various architectures and UDA benchmarks without adding computational overhead.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining masked modeling with feature space operations for UDA segmentation, offering a new way to address architectural incompatibilities and optimization conflicts in existing methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of UDA for semantic segmentation due to its efficient and generalizable approach, though its influence may remain confined to specific computer vision applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to domain adaptation techniques in segmentation, making it important for researchers in computer vision to be aware of its methods and results.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7b4f303356a8ba0e63e80b2647185c7e7b7942ee",
      "total_authors": 6,
      "authors_found": 5,
      "highest_h_index": 5,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Wenlve Zhou",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/152326064"
        },
        {
          "name": "Zhiheng Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378982394"
        },
        {
          "name": "Tiantao Xian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2368905313"
        },
        {
          "name": "Yikui Zhai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381244224"
        },
        {
          "name": "Weibin Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380714105"
        },
        {
          "name": "Biyun Ma",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2509.13805",
      "title": "Towards a Physics Foundation Model",
      "authors": [
        "Florian Wiesner",
        "Matthias Wessling",
        "Stephen Baek"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Foundation models have revolutionized natural language processing through a\n``train once, deploy anywhere'' paradigm, where a single pre-trained model\nadapts to countless downstream tasks without retraining. Access to a Physics\nFoundation Model (PFM) would be transformative -- democratizing access to\nhigh-fidelity simulations, accelerating scientific discovery, and eliminating\nthe need for specialized solver development. Yet current physics-aware machine\nlearning approaches remain fundamentally limited to single, narrow domains and\nrequire retraining for each new system. We present the General Physics\nTransformer (GPhyT), trained on 1.8 TB of diverse simulation data, that\ndemonstrates foundation model capabilities are achievable for physics. Our key\ninsight is that transformers can learn to infer governing dynamics from\ncontext, enabling a single model to simulate fluid-solid interactions, shock\nwaves, thermal convection, and multi-phase dynamics without being told the\nunderlying equations. GPhyT achieves three critical breakthroughs: (1) superior\nperformance across multiple physics domains, outperforming specialized\narchitectures by up to 29x, (2) zero-shot generalization to entirely unseen\nphysical systems through in-context learning, and (3) stable long-term\npredictions through 50-timestep rollouts. By establishing that a single model\ncan learn generalizable physical principles from data alone, this work opens\nthe path toward a universal PFM that could transform computational science and\nengineering.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13805v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13805v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.417,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on developing a transformer-based model for physics simulations, emphasizing in-context learning for physical dynamics, but it does not involve diffusion models, iterative refinement for logical tasks, or any multi-step reasoning processes related to diffusion. There is no component for treating a chain-of-thought as an entity for correction, making it unrelated to this topic.",
      "distributed_training_justification": "The paper mentions large-scale GPU computing in the context of training foundation models, which could imply distributed training techniques, but it does not discuss specific algorithms, systems, or strategies for partitioning data, architecture, or computation across multiple nodes. The main contribution is the model's architecture and capabilities for physics, not distributed training methods.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13809",
      "title": "Data-Efficient Spectral Classification of Hyperspectral Data Using\n  MiniROCKET and HDC-MiniROCKET",
      "authors": [
        "Nick Theisen",
        "Kenny Schlegel",
        "Dietrich Paulus",
        "Peer Neubert"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The classification of pixel spectra of hyperspectral images, i.e. spectral\nclassification, is used in many fields ranging from agricultural, over medical\nto remote sensing applications and is currently also expanding to areas such as\nautonomous driving. Even though for full hyperspectral images the\nbest-performing methods exploit spatial-spectral information, performing\nclassification solely on spectral information has its own advantages, e.g.\nsmaller model size and thus less data required for training. Moreover, spectral\ninformation is complementary to spatial information and improvements on either\npart can be used to improve spatial-spectral approaches in the future.\nRecently, 1D-Justo-LiuNet was proposed as a particularly efficient model with\nvery few parameters, which currently defines the state of the art in spectral\nclassification. However, we show that with limited training data the model\nperformance deteriorates. Therefore, we investigate MiniROCKET and\nHDC-MiniROCKET for spectral classification to mitigate that problem. The model\nextracts well-engineered features without trainable parameters in the feature\nextraction part and is therefore less vulnerable to limited training data. We\nshow that even though MiniROCKET has more parameters it outperforms\n1D-Justo-LiuNet in limited data scenarios and is mostly on par with it in the\ngeneral case",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13809v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13809v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.308,
      "distributed_training_score": 0.388,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13834",
      "title": "Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology\n  Segmentation",
      "authors": [
        "Nguyen Lan Vi Vu",
        "Thanh-Huy Nguyen",
        "Thien Nguyen",
        "Daisuke Kihara",
        "Tianyang Wang",
        "Xingjian Li",
        "Min Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Semi-supervised learning has been employed to alleviate the need for\nextensive labeled data for histopathology image segmentation, but existing\nmethods struggle with noisy pseudo-labels due to ambiguous gland boundaries and\nmorphological misclassification. This paper introduces Semi-MOE, to the best of\nour knowledge, the first multi-task Mixture-of-Experts framework for\nsemi-supervised histopathology image segmentation. Our approach leverages three\nspecialized expert networks: A main segmentation expert, a signed distance\nfield regression expert, and a boundary prediction expert, each dedicated to\ncapturing distinct morphological features. Subsequently, the Multi-Gating\nPseudo-labeling module dynamically aggregates expert features, enabling a\nrobust fuse-and-refine pseudo-labeling mechanism. Furthermore, to eliminate\nmanual tuning while dynamically balancing multiple learning objectives, we\npropose an Adaptive Multi-Objective Loss. Extensive experiments on GlaS and\nCRAG benchmarks show that our method outperforms state-of-the-art approaches in\nlow-label settings, highlighting the potential of MoE-based architectures in\nadvancing semi-supervised segmentation. Our code is available at\nhttps://github.com/vnlvi2k3/Semi-MoE.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13834v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13834v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.27,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.315,
      "datasets_score": 0.287,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, Semi-MoE, employs semi-supervised learning to generate and refine pseudo-labels for histopathology segmentation, which aligns closely with weak supervision. It programmatically creates labels from noisy sources (e.g., via the Multi-Gating Pseudo-labeling module and expert networks) to train on unlabeled data, reducing reliance on hand-labeled data, as defined in the topic. This direct application of pseudo-labeling techniques makes the paper highly relevant.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces Semi-MoE, a pioneering multi-task Mixture-of-Experts framework for semi-supervised histopathology image segmentation, aiming to overcome challenges like noisy pseudo-labels and ambiguous boundaries by employing three specialized experts for main segmentation, signed distance field regression, and boundary prediction. The methodology includes a Multi-Gating Pseudo-labeling module for dynamic feature aggregation and an Adaptive Multi-Objective Loss for balancing tasks, with experiments on GlaS and CRAG benchmarks showing state-of-the-art performance in low-label settings, outperforming existing approaches.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new multi-task Mixture-of-Experts framework specifically for semi-supervised histopathology segmentation, which is the first of its kind and significantly advances the state-of-the-art by addressing limitations in existing methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in semi-supervised medical image segmentation by providing a robust framework that could be adapted to other domains, though its impact is primarily confined to the histopathology subfield.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution with innovative techniques and superior results, making it essential for researchers in computer vision and medical imaging to be aware of for advancing semi-supervised methods.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f4a5f20a689341498b3db859682855b07b1c3ea9",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 2,
      "average_h_index": 0.7142857142857143,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Nguyen Lan Vi Vu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2355319636"
        },
        {
          "name": "Thanh-Huy Nguyen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2369628042"
        },
        {
          "name": "Thien Nguyen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381135695"
        },
        {
          "name": "Daisuke Kihara",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2308470897"
        },
        {
          "name": "Tianyang Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2290112112"
        },
        {
          "name": "Xingjian Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2307292971"
        },
        {
          "name": "Min Xu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2361865038"
        }
      ]
    },
    {
      "id": "2509.13836",
      "title": "Diving into Mitigating Hallucinations from a Vision Perspective for\n  Large Vision-Language Models",
      "authors": [
        "Weihang Wang",
        "Xinhao Li",
        "Ziyue Wang",
        "Yan Pang",
        "Jielei Zhang",
        "Peiyi Li",
        "Qiang Zhang",
        "Longwen Gao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Object hallucination in Large Vision-Language Models (LVLMs) significantly\nimpedes their real-world applicability. As the primary component for accurately\ninterpreting visual information, the choice of visual encoder is pivotal. We\nhypothesize that the diverse training paradigms employed by different visual\nencoders instill them with distinct inductive biases, which leads to their\ndiverse hallucination performances. Existing benchmarks typically focus on\ncoarse-grained hallucination detection and fail to capture the diverse\nhallucinations elaborated in our hypothesis. To systematically analyze these\neffects, we introduce VHBench-10, a comprehensive benchmark with approximately\n10,000 samples for evaluating LVLMs across ten fine-grained hallucination\ncategories. Our evaluations confirm encoders exhibit unique hallucination\ncharacteristics. Building on these insights and the suboptimality of simple\nfeature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.\nIt employs global visual features to generate routing signals, dynamically\naggregating visual features from multiple specialized experts. Comprehensive\nexperiments confirm the effectiveness of VisionWeaver in significantly reducing\nhallucinations and improving overall model performance.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13836v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13836v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.466,
      "distributed_training_score": 0.342,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on mitigating hallucinations in Large Vision-Language Models by analyzing visual encoders, introducing a benchmark, and proposing a Context-Aware Routing Network. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13846",
      "title": "Consistent View Alignment Improves Foundation Models for 3D Medical\n  Image Segmentation",
      "authors": [
        "Puru Vaish",
        "Felix Meister",
        "Tobias Heimann",
        "Christoph Brune",
        "Jelmer M. Wolterink"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Many recent approaches in representation learning implicitly assume that\nuncorrelated views of a data point are sufficient to learn meaningful\nrepresentations for various downstream tasks. In this work, we challenge this\nassumption and demonstrate that meaningful structure in the latent space does\nnot emerge naturally. Instead, it must be explicitly induced. We propose a\nmethod that aligns representations from different views of the data to align\ncomplementary information without inducing false positives. Our experiments\nshow that our proposed self-supervised learning method, Consistent View\nAlignment, improves performance for downstream tasks, highlighting the critical\nrole of structured view alignment in learning effective representations. Our\nmethod achieved first and second place in the MICCAI 2025 SSL3D challenge when\nusing a Primus vision transformer and ResEnc convolutional neural network,\nrespectively. The code and pretrained model weights are released at\nhttps://github.com/Tenbatsu24/LatentCampus.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13846v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13846v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.333,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13848",
      "title": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation",
      "authors": [
        "Jiayi Pan",
        "Jiaming Xu",
        "Yongkang Zhou",
        "Guohao Dai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13848v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13848v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.579,
      "distributed_training_score": 0.446,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on accelerating inference in diffusion models for image generation tasks, such as feature caching and self-speculation to reduce computation. It does not involve adapting diffusion models for multi-step logical reasoning, chain-of-thought processes, or holistic correction of reasoning paths.",
      "distributed_training_justification": "The paper discusses inference optimization techniques for diffusion models on a single GPU, without addressing distributed training, parallel computing across multiple nodes, or strategies for partitioning data/models to accelerate training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13854",
      "title": "Understanding the Process of Human-AI Value Alignment",
      "authors": [
        "Jack McKinlay",
        "Marina De Vos",
        "Janina A. Hoffmann",
        "Andreas Theodorou"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Background: Value alignment in computer science research is often used to\nrefer to the process of aligning artificial intelligence with humans, but the\nway the phrase is used often lacks precision. Objectives: In this paper, we\nconduct a systematic literature review to advance the understanding of value\nalignment in artificial intelligence by characterising the topic in the context\nof its research literature. We use this to suggest a more precise definition of\nthe term. Methods: We analyse 172 value alignment research articles that have\nbeen published in recent years and synthesise their content using thematic\nanalyses. Results: Our analysis leads to six themes: value alignment drivers &\napproaches; challenges in value alignment; values in value alignment; cognitive\nprocesses in humans and AI; human-agent teaming; and designing and developing\nvalue-aligned systems. Conclusions: By analysing these themes in the context of\nthe literature we define value alignment as an ongoing process between humans\nand autonomous agents that aims to express and implement abstract values in\ndiverse contexts, while managing the cognitive limits of both humans and AI\nagents and also balancing the conflicting ethical and political demands\ngenerated by the values in different groups. Our analysis gives rise to a set\nof research challenges and opportunities in the field of value alignment for\nfuture work.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13854v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13854v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.513,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.302,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a systematic literature review and thematic analysis of value alignment in AI, focusing on defining the concept and identifying themes across 172 papers. It briefly references related ideas, such as cooperative inverse reinforcement learning from Hadfield-Menell et al., which is a precursor to RLHF, but does not directly discuss or implement systems involving human feedback for training reward models and fine-tuning via reinforcement learning. Thus, while it touches on broader alignment concepts that could indirectly relate to RLHF, it is not a primary focus.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13857",
      "title": "InterKey: Cross-modal Intersection Keypoints for Global Localization on\n  OpenStreetMap",
      "authors": [
        "Nguyen Hoang Khoi Tran",
        "Julie Stephany Berrio",
        "Mao Shan",
        "Stewart Worrall"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Reliable global localization is critical for autonomous vehicles, especially\nin environments where GNSS is degraded or unavailable, such as urban canyons\nand tunnels. Although high-definition (HD) maps provide accurate priors, the\ncost of data collection, map construction, and maintenance limits scalability.\nOpenStreetMap (OSM) offers a free and globally available alternative, but its\ncoarse abstraction poses challenges for matching with sensor data. We propose\nInterKey, a cross-modal framework that leverages road intersections as\ndistinctive landmarks for global localization. Our method constructs compact\nbinary descriptors by jointly encoding road and building imprints from point\nclouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation,\norientation determination, and area-equalized sampling strategies, enabling\nrobust cross-modal matching. Experiments on the KITTI dataset demonstrate that\nInterKey achieves state-of-the-art accuracy, outperforming recent baselines by\na large margin. The framework generalizes to sensors that can produce dense\nstructural point clouds, offering a scalable and cost-effective solution for\nrobust vehicle localization.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13857v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13857v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.343,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13858",
      "title": "EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics",
      "authors": [
        "Qianxin Xia",
        "Jiawei Du",
        "Guoming Lu",
        "Zhiyong Shu",
        "Jielei Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Dataset distillation aims to synthesize a compact dataset from the original\nlarge-scale one, enabling highly efficient learning while preserving\ncompetitive model performance. However, traditional techniques primarily\ncapture low-level visual features, neglecting the high-level semantic and\nstructural information inherent in images. In this paper, we propose EDITS, a\nnovel framework that exploits the implicit textual semantics within the image\ndata to achieve enhanced distillation. First, external texts generated by a\nVision Language Model (VLM) are fused with image features through a Global\nSemantic Query module, forming the prior clustered buffer. Local Semantic\nAwareness then selects representative samples from the buffer to construct\nimage and text prototypes, with the latter produced by guiding a Large Language\nModel (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype\nGuidance strategy generates the final synthetic dataset through a diffusion\nmodel. Extensive experiments confirm the effectiveness of our method.Source\ncode is available in: https://github.com/einsteinxia/EDITS.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13858v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13858v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.421,
      "diffusion_reasoning_score": 0.467,
      "distributed_training_score": 0.397,
      "datasets_score": 0.462,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper uses a Vision Language Model (VLM) to generate textual descriptions, which could be viewed as programmatically derived labels, potentially introducing noise similar to weak supervision. However, the primary focus is on dataset distillation for efficient learning, not on training models with weak labels as a core methodology. Thus, while there is a minor connection, it is not central to the paper's contributions.",
      "diffusion_reasoning_justification": "The paper employs a diffusion model solely for synthesizing images in the dataset distillation process, focusing on generative tasks rather than multi-step logical reasoning or iterative refinement of a 'Chain-of-Thought'. There is no evidence of adapting diffusion for complex reasoning tasks, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is a framework for enhancing dataset distillation, which involves creating synthetic datasets that preserve semantic information, directly aligning with research on creating and curating datasets for machine learning. It includes methodologies for dataset synthesis, evaluation through experiments, and addresses efficiency in dataset usage, fitting core aspects of dataset-focused research.",
      "llm_score_status": "completed",
      "summary": "EDITS is a novel framework designed to improve dataset distillation by incorporating implicit textual semantics from images, addressing the limitations of traditional methods that focus only on low-level visual features. The methodology involves using a Vision Language Model to generate external texts, fusing them with image features via a Global Semantic Query module to create a clustered buffer, then employing Local Semantic Awareness to select representative samples for image and text prototypes, and finally synthesizing the distilled dataset using Dual Prototype Guidance with a diffusion model. Extensive experiments demonstrate that this approach enhances the representational capacity and performance of distilled datasets, making it more effective for efficient learning in computer vision tasks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing technologies like Vision Language Models and diffusion models to integrate textual semantics into dataset distillation, offering a notable improvement over methods that overlook high-level semantics. While it advances the field by addressing a specific gap, it does not introduce a entirely new problem or technique but rather refines current approaches.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in dataset distillation and computer vision by demonstrating the benefits of incorporating textual semantics, potentially leading to more efficient and semantically rich synthetic datasets. However, its applicability may be limited to specific subfields like image synthesis, making widespread impact uncertain.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong and valuable contribution to dataset distillation techniques, particularly for researchers in computer vision interested in semantic enhancements, making it worth reading for its practical insights and experimental validation. While not essential for all, it provides important advancements that could inform future work in the area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/eddfa2ba59deebc0d1236d31d504992e68172022",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 2,
      "average_h_index": 1.2,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Qianxin Xia",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2279915540"
        },
        {
          "name": "Jiawei Du",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381233888"
        },
        {
          "name": "Guoming Lu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2294509739"
        },
        {
          "name": "Zhiyong Shu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2349848098"
        },
        {
          "name": "Jielei Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2294337065"
        }
      ]
    },
    {
      "id": "2509.13863",
      "title": "LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray\n  Laminography Reconstruction",
      "authors": [
        "Chu Chen",
        "Ander Biguri",
        "Jean-Michel Morel",
        "Raymond H. Chan",
        "Carola-Bibiane Schönlieb",
        "Jizhou Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "X-ray Computed Laminography (CL) is essential for non-destructive inspection\nof plate-like structures in applications such as microchips and composite\nbattery materials, where traditional computed tomography (CT) struggles due to\ngeometric constraints. However, reconstructing high-quality volumes from\nlaminographic projections remains challenging, particularly under highly\nsparse-view acquisition conditions. In this paper, we propose a reconstruction\nalgorithm, namely LamiGauss, that combines Gaussian Splatting radiative\nrasterization with a dedicated detector-to-world transformation model\nincorporating the laminographic tilt angle. LamiGauss leverages an\ninitialization strategy that explicitly filters out common laminographic\nartifacts from the preliminary reconstruction, preventing redundant Gaussians\nfrom being allocated to false structures and thereby concentrating model\ncapacity on representing the genuine object. Our approach effectively optimizes\ndirectly from sparse projections, enabling accurate and efficient\nreconstruction with limited data. Extensive experiments on both synthetic and\nreal datasets demonstrate the effectiveness and superiority of the proposed\nmethod over existing techniques. LamiGauss uses only 3$\\%$ of full views to\nachieve superior performance over the iterative method optimized on a full\ndataset.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13863v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13863v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.255,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.336,
      "distributed_training_score": 0.297,
      "datasets_score": 0.233,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13864",
      "title": "Distractor-Aware Memory-Based Visual Object Tracking",
      "authors": [
        "Jovana Videnovic",
        "Matej Kristan",
        "Alan Lukezic"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent emergence of memory-based video segmentation methods such as SAM2 has\nled to models with excellent performance in segmentation tasks, achieving\nleading results on numerous benchmarks. However, these modes are not fully\nadjusted for visual object tracking, where distractors (i.e., objects visually\nsimilar to the target) pose a key challenge. In this paper we propose a\ndistractor-aware drop-in memory module and introspection-based management\nmethod for SAM2, leading to DAM4SAM. Our design effectively reduces the\ntracking drift toward distractors and improves redetection capability after\nobject occlusion. To facilitate the analysis of tracking in the presence of\ndistractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM\noutperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results\non ten. Furthermore, integrating the proposed distractor-aware memory into a\nreal-time tracker EfficientTAM leads to 11% improvement and matches tracking\nquality of the non-real-time SAM2.1-L on multiple tracking and segmentation\nbenchmarks, while integration with edge-based tracker EdgeTAM delivers 4%\nperformance boost, demonstrating a very good generalization across\narchitectures.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13864v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13864v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.35,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13866",
      "title": "Masked Diffusion Models as Energy Minimization",
      "authors": [
        "Sitong Chen",
        "Shen Nie",
        "Jiacheng Sun",
        "Zijin Feng",
        "Zhenguo Li",
        "Ji-Rong Wen",
        "Chongxuan Li"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "We present a systematic theoretical framework that interprets masked\ndiffusion models (MDMs) as solutions to energy minimization problems in\ndiscrete optimal transport. Specifically, we prove that three distinct energy\nformulations--kinetic, conditional kinetic, and geodesic energy--are\nmathematically equivalent under the structure of MDMs, and that MDMs minimize\nall three when the mask schedule satisfies a closed-form optimality condition.\nThis unification not only clarifies the theoretical foundations of MDMs, but\nalso motivates practical improvements in sampling. By parameterizing\ninterpolation schedules via Beta distributions, we reduce the schedule design\nspace to a tractable 2D search, enabling efficient post-training tuning without\nmodel modification. Experiments on synthetic and real-world benchmarks\ndemonstrate that our energy-inspired schedules outperform hand-crafted\nbaselines, particularly in low-step sampling settings.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13866v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13866v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.317,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.571,
      "distributed_training_score": 0.371,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper primarily contributes to the theoretical understanding of Masked Diffusion Models (MDMs) as energy minimization in optimal transport, with applications in generative tasks like text and image generation. While it mentions experiments on benchmarks including mathematical reasoning tasks, it does not specifically adapt the iterative refinement process of diffusion for solving complex logical tasks or treat a Chain-of-Thought as a single entity for holistic correction. Thus, the connection is indirect, as the core focus is on general sampling improvements rather than multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13873",
      "title": "Invisible Yet Detected: PelFANet with Attention-Guided Anatomical Fusion\n  for Pelvic Fracture Diagnosis",
      "authors": [
        "Siam Tahsin Bhuiyan",
        "Rashedur Rahman",
        "Sefatul Wasi",
        "Naomi Yagi",
        "Syoji Kobashi",
        "Ashraful Islam",
        "Saadia Binte Alam"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Pelvic fractures pose significant diagnostic challenges, particularly in\ncases where fracture signs are subtle or invisible on standard radiographs. To\naddress this, we introduce PelFANet, a dual-stream attention network that fuses\nraw pelvic X-rays with segmented bone images to improve fracture\nclassification. The network employs Fused Attention Blocks (FABlocks) to\niteratively exchange and refine features from both inputs, capturing global\ncontext and localized anatomical detail. Trained in a two-stage pipeline with a\nsegmentation-guided approach, PelFANet demonstrates superior performance over\nconventional methods. On the AMERI dataset, it achieves 88.68% accuracy and\n0.9334 AUC on visible fractures, while generalizing effectively to invisible\nfracture cases with 82.29% accuracy and 0.8688 AUC, despite not being trained\non them. These results highlight the clinical potential of anatomy-aware\ndual-input architectures for robust fracture detection, especially in scenarios\nwith subtle radiographic presentations.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13873v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13873v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.261,
      "weak_supervision_score": 0.279,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.276,
      "datasets_score": 0.272,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13879",
      "title": "Combining Evidence and Reasoning for Biomedical Fact-Checking",
      "authors": [
        "Mariano Barone",
        "Antonio Romano",
        "Giuseppe Riccio",
        "Marco Postiglione",
        "Vincenzo Moscato"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https: //github.com/PRAISELab-PicusLab/CER.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13879v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13879v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.415,
      "distributed_training_score": 0.299,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for biomedical fact-checking that combines evidence retrieval, large language model reasoning, and supervised veracity prediction. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. The reasoning is based on LLMs and retrieval-augmented generation, which differs fundamentally from the topic's focus.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13880",
      "title": "An Exhaustive DPLL Approach to Model Counting over Integer Linear\n  Constraints with Simplification Techniques",
      "authors": [
        "Mingwei Zhang",
        "Zhenhao Gu",
        "Liangda Fang",
        "Cunjing Ge",
        "Ziliang Chen",
        "Zhao-Rong Lai",
        "Quanlong Guan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Linear constraints are one of the most fundamental constraints in fields such\nas computer science, operations research and optimization. Many applications\nreduce to the task of model counting over integer linear constraints (MCILC).\nIn this paper, we design an exact approach to MCILC based on an exhaustive DPLL\narchitecture. To improve the efficiency, we integrate several effective\nsimplification techniques from mixed integer programming into the architecture.\nWe compare our approach to state-of-the-art MCILC counters and propositional\nmodel counters on 2840 random and 4131 application benchmarks. Experimental\nresults show that our approach significantly outperforms all exact methods in\nrandom benchmarks solving 1718 instances while the state-of-the-art approach\nonly computes 1470 instances. In addition, our approach is the only approach to\nsolve all 4131 application instances.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13880v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13880v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.268,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.309,
      "datasets_score": 0.254,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13883",
      "title": "EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person\n  View",
      "authors": [
        "Zhen Xu",
        "Guorui Lu",
        "Chang Gao",
        "Qinyu Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Hand tracking holds great promise for intuitive interaction paradigms, but\nframe-based methods often struggle to meet the requirements of accuracy, low\nlatency, and energy efficiency, especially in resource-constrained settings\nsuch as Extended Reality (XR) devices. Event cameras provide $\\mu$s-level\ntemporal resolution at mW-level power by asynchronously sensing brightness\nchanges. In this work, we present EvHand-FPV, a lightweight framework for\negocentric First-Person-View 3D hand tracking from a single event camera. We\nconstruct an event-based FPV dataset that couples synthetic training data with\n3D labels and real event data with 2D labels for evaluation to address the\nscarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based\nregion of interest (ROI) that localizes the hand region via geometric cues,\ncombined with an end-to-end mapping strategy that embeds ROI offsets into the\nnetwork to reduce computation without explicit reconstruction, and a multi-task\nlearning strategy with an auxiliary geometric feature head that improves\nrepresentations without test-time overhead. On our real FPV test set,\nEvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from\n11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It\nalso maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results\ndemonstrate accurate and efficient egocentric event-based hand tracking\nsuitable for on-device XR applications. The dataset and code are available at\nhttps://github.com/zen5x5/EvHand-FPV.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13883v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13883v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.298,
      "diffusion_reasoning_score": 0.298,
      "distributed_training_score": 0.337,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13888",
      "title": "Combating Biomedical Misinformation through Multi-modal Claim Detection\n  and Evidence-based Verification",
      "authors": [
        "Mariano Barone",
        "Antonio Romano",
        "Giuseppe Riccio",
        "Marco Postiglione",
        "Vincenzo Moscato"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https://github.com/PRAISELab-PicusLab/CER",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13888v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13888v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.296,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13892",
      "title": "Synthetic Data Generation for Screen Time and App Usage",
      "authors": [
        "Gustavo Kruger",
        "Nikhil Sachdeva",
        "Michael Sobolev"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Smartphone usage data can provide valuable insights for understanding\ninteraction with technology and human behavior. However, collecting\nlarge-scale, in-the-wild smartphone usage logs is challenging due to high\ncosts, privacy concerns, under representative user samples and biases like\nnon-response that can skew results. These challenges call for exploring\nalternative approaches to obtain smartphone usage datasets. In this context,\nlarge language models (LLMs) such as Open AI's ChatGPT present a novel approach\nfor synthetic smartphone usage data generation, addressing limitations of\nreal-world data collection. We describe a case study on how four prompt\nstrategies influenced the quality of generated smartphone usage data. We\ncontribute with insights on prompt design and measures of data quality,\nreporting a prompting strategy comparison combining two factors, prompt level\nof detail (describing a user persona, describing the expected results\ncharacteristics) and seed data inclusion (with versus without an initial real\nusage example). Our findings suggest that using LLMs to generate structured and\nbehaviorally plausible smartphone use datasets is feasible for some use cases,\nespecially when using detailed prompts. Challenges remain in capturing diverse\nnuances of human behavioral patterns in a single synthetic dataset, and\nevaluating tradeoffs between data fidelity and diversity, suggesting the need\nfor use-case-specific evaluation metrics and future research with more diverse\nseed data and different LLM models.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13892v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13892v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.418,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.315,
      "datasets_score": 0.414,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on using pre-existing LLMs for synthetic data generation via prompt strategies, with no mention of training models, human feedback loops, reward models, or reinforcement learning techniques. It does not involve aligning AI with human preferences through RLHF.",
      "weak_supervision_justification": "The paper involves generating synthetic data programmatically using LLMs and prompts, which could loosely relate to weak supervision by creating noisy or imprecise data sources. However, it does not focus on training machine learning models with programmatically generated labels, making the connection indirect and not central to the paper's contributions.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is on creating and evaluating synthetic datasets for smartphone usage, including methodologies for generation, analysis of data quality, and assessment of structural and behavioral realism, which directly aligns with research on dataset creation, curation, and evaluation for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper explores the use of large language models (LLMs) like ChatGPT to generate synthetic smartphone usage data as an alternative to collecting real-world data, which is hindered by costs, privacy issues, and biases. Through a case study, the authors investigate four prompt strategies varying in detail and seed data inclusion, evaluating the generated data on structural compliance and behavioral realism, and find that detailed prompts enhance data quality but challenges persist in capturing nuanced human behaviors, highlighting the need for tailored evaluation metrics.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying LLMs to generate synthetic smartphone usage data, combining existing AI techniques in a new context to address data collection challenges, though it does not introduce a entirely new problem or architecture. This approach builds on prior work in synthetic data and prompt engineering but offers a clever adaptation for human-computer interaction studies.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in HCI and AI subfields by providing practical insights into synthetic data generation for behavioral studies, potentially reducing reliance on real data collection. However, its applicability is somewhat limited to specific use cases like hypothesis generation, making widespread commercial impact uncertain.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable insights for researchers in AI and HCI interested in synthetic data methods, as it provides a solid case study with practical prompt strategies and evaluations. While not essential for all audiences, it represents a strong contribution that could inform ongoing work in behavioral data generation.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3904362eb6820eb6f7aaeae41b615084344f34fa",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Gustavo Kruger",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685249"
        },
        {
          "name": "Nikhil Sachdeva",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685457"
        },
        {
          "name": "Michael Sobolev",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685899"
        }
      ]
    },
    {
      "id": "2509.13895",
      "title": "FedSSG: Expectation-Gated and History-Aware Drift Alignment for\n  Federated Learning",
      "authors": [
        "Zhanting Zhou",
        "Jinshan Lai",
        "Fengchun Zhang",
        "Zeqin Wu",
        "Fengli Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Non-IID data and partial participation induce client drift and inconsistent\nlocal optima in federated learning, causing unstable convergence and accuracy\nloss. We present FedSSG, a stochastic sampling-guided, history-aware drift\nalignment method. FedSSG maintains a per-client drift memory that accumulates\nlocal model differences as a lightweight sketch of historical gradients;\ncrucially, it gates both the memory update and the local alignment term by a\nsmooth function of the observed/expected participation ratio (a\nphase-by-expectation signal derived from the server sampler). This\nstatistically grounded gate stays weak and smooth when sampling noise dominates\nearly, then strengthens once participation statistics stabilize, contracting\nthe local-global gap without extra communication. Across CIFAR-10/100 with\n100/500 clients and 2-15 percent participation, FedSSG consistently outperforms\nstrong drift-aware baselines and accelerates convergence; on our benchmarks it\nimproves test accuracy by up to a few points (e.g., about +0.9 on CIFAR-10 and\nabout +2.7 on CIFAR-100 on average over the top-2 baseline) and yields about\n4.5x faster target-accuracy convergence on average. The method adds only O(d)\nclient memory and a constant-time gate, and degrades gracefully to a mild\nregularizer under near-IID or uniform sampling. FedSSG shows that sampling\nstatistics can be turned into a principled, history-aware phase control to\nstabilize and speed up federated training.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13895v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13895v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.319,
      "distributed_training_score": 0.435,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on federated learning techniques to address client drift and improve model convergence in distributed settings, with no mention of human feedback, reward models, or reinforcement learning for aligning AI with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution, FedSSG, is a method for federated learning that enhances distributed training by managing client drift and partial participation across multiple nodes, directly relating to distributed training, parallel computing, and multi-node machine learning algorithms.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces FedSSG, a novel method for federated learning that addresses client drift caused by non-IID data and partial participation by maintaining a per-client drift memory of historical gradients and gating updates based on observed versus expected participation ratios. This expectation-gated, history-aware approach dynamically adjusts alignment strength to reduce the local-global gap without additional communication, resulting in faster convergence and improved test accuracy by up to 2.7 points on CIFAR-10/100 benchmarks compared to baselines, while adding only minimal memory and computational overhead.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining history-aware drift memory with a statistically grounded gating mechanism based on participation ratios, extending existing drift-aware methods like FedDyn and FedDC in a clever way without introducing an entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in federated learning research due to its practical enhancements in handling client drift and partial participation, though its influence may remain confined to this specific subfield rather than broader applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a strong, valuable contribution to federated learning by improving convergence and accuracy with an efficient method, making it essential for researchers focused on FL algorithms and challenges like client drift.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/205689be6343768911bbd53bbc85d7424e0957a5",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 4,
      "average_h_index": 0.8,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zhanting Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380939020"
        },
        {
          "name": "Jinshan Lai",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2158143408"
        },
        {
          "name": "Fengchun Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2370066226"
        },
        {
          "name": "Zeqin Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380794076"
        },
        {
          "name": "Fengli Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381987253"
        }
      ]
    },
    {
      "id": "2509.13905",
      "title": "Do Large Language Models Understand Word Senses?",
      "authors": [
        "Domenico Meconi",
        "Simone Stirpe",
        "Federico Martelli",
        "Leonardo Lavalle",
        "Roberto Navigli"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Understanding the meaning of words in context is a fundamental capability for\nLarge Language Models (LLMs). Despite extensive evaluation efforts, the extent\nto which LLMs show evidence that they truly grasp word senses remains\nunderexplored. In this paper, we address this gap by evaluating both i) the\nWord Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,\ncomparing their performance to state-of-the-art systems specifically designed\nfor the task, and ii) the ability of two top-performing open- and closed-source\nLLMs to understand word senses in three generative settings: definition\ngeneration, free-form explanation, and example generation. Notably, we find\nthat, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve\nperformance on par with specialized WSD systems, while also demonstrating\ngreater robustness across domains and levels of difficulty. In the generation\ntasks, results reveal that LLMs can explain the meaning of words in context up\nto 98\\% accuracy, with the highest performance observed in the free-form\nexplanation task, which best aligns with their generative capabilities.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13905v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13905v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.461,
      "distributed_training_score": 0.343,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates Large Language Models (LLMs) on Word Sense Disambiguation (WSD) and generative tasks related to word meanings, focusing on models like GPT-4o and DeepSeek-V3. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. There is no mention of adapting diffusion for reasoning tasks, making the paper entirely unrelated.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13907",
      "title": "White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic\n  Segmentation",
      "authors": [
        "Jiyun Im",
        "SuBeen Lee",
        "Miso Lee",
        "Jae-Pil Heo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-point\nlabels for an unlabeled point cloud, given only a few labeled examples. To\nextract discriminative representations from the limited support set, existing\nmethods have constructed prototypes using conventional algorithms such as\nfarthest point sampling. However, we point out that its initial randomness\nsignificantly affects FS-PCS performance and that the prototype generation\nprocess remains underexplored despite its prevalence. This motivates us to\ninvestigate an advanced prototype generation method based on attention\nmechanism. Despite its potential, we found that vanilla module suffers from the\ndistributional gap between learnable prototypical tokens and support features.\nTo overcome this, we propose White Aggregation and Restoration Module (WARM),\nwhich resolves the misalignment by sandwiching cross-attention between\nwhitening and coloring transformations. Specifically, whitening aligns the\nsupport features to prototypical tokens before attention process, and\nsubsequently coloring restores the original distribution to the attended\ntokens. This simple yet effective design enables robust attention, thereby\ngenerating representative prototypes by capturing the semantic relationships\namong support features. Our method achieves state-of-the-art performance with a\nsignificant margin on multiple FS-PCS benchmarks, demonstrating its\neffectiveness through extensive experiments.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13907v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13907v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.426,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.359,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on Few-Shot 3D Point Cloud Segmentation (FS-PCS), which uses a small number of labeled examples to perform segmentation, aligning with weak supervision by reducing reliance on large, perfectly labeled datasets. However, the main contribution is a specific module (WARM) for prototype generation rather than a broad technique for generating or handling noisy/imprecise labels, making it moderately relevant rather than highly so.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the limitations in Few-Shot 3D Point Cloud Segmentation (FS-PCS) by introducing the White Aggregation and Restoration Module (WARM), which enhances prototype generation through an attention mechanism combined with whitening and coloring transformations to align feature distributions and improve semantic representation from limited support sets. The methodology involves applying whitening to standardize support features before cross-attention and then coloring to restore them, resulting in more robust prototypes and achieving state-of-the-art performance on multiple FS-PCS benchmarks as validated through extensive experiments.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by extending the attention mechanism with whitening and coloring transformations to address distributional gaps in prototype generation for FS-PCS, effectively combining existing ideas in a new way for this specific task. While it builds on established concepts like attention, it introduces a clever adaptation that advances the field without creating an entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of 3D point cloud segmentation and few-shot learning, as it achieves state-of-the-art results and addresses a key challenge in handling limited data. However, its influence may be confined to specialized applications in computer vision rather than broadly across disciplines.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to FS-PCS with innovative methodology and demonstrated performance gains, making it essential for researchers in 3D vision to be aware of for advancing few-shot techniques. While not groundbreaking for all audiences, it offers practical insights that could inspire further developments.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/919db1934f9fb93700e5c0c8d47a048a2c3c466e",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 6,
      "average_h_index": 2.75,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Jiyun Im",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380689132"
        },
        {
          "name": "Subeen Lee",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/83320444"
        },
        {
          "name": "Miso Lee",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2109462203"
        },
        {
          "name": "Jae-pil Heo",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2238133392"
        }
      ]
    },
    {
      "id": "2509.13914",
      "title": "Ensemble of Pre-Trained Models for Long-Tailed Trajectory Prediction",
      "authors": [
        "Divya Thuremella",
        "Yi Yang",
        "Simon Wanna",
        "Lars Kunze",
        "Daniele De Martini"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This work explores the application of ensemble modeling to the\nmultidimensional regression problem of trajectory prediction for vehicles in\nurban environments. As newer and bigger state-of-the-art prediction models for\nautonomous driving continue to emerge, an important open challenge is the\nproblem of how to combine the strengths of these big models without the need\nfor costly re-training. We show how, perhaps surprisingly, combining\nstate-of-the-art deep learning models out-of-the-box (without retraining or\nfine-tuning) with a simple confidence-weighted average method can enhance the\noverall prediction. Indeed, while combining trajectory prediction models is not\nstraightforward, this simple approach enhances performance by 10% over the best\nprediction model, especially in the long-tailed metrics. We show that this\nperformance improvement holds on both the NuScenes and Argoverse datasets, and\nthat these improvements are made across the dataset distribution. The code for\nour work is open source.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13914v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13914v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.373,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on ensembling pre-trained models for trajectory prediction in autonomous driving, using a confidence-weighted average to combine outputs. It does not involve diffusion models, iterative refinement for logical tasks, or any multi-step reasoning processes as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13919",
      "title": "Towards Rationale-Answer Alignment of LVLMs via Self-Rationale\n  Calibration",
      "authors": [
        "Yuanchen Wu",
        "Ke Yan",
        "Shouhong Ding",
        "Ziyin Zhou",
        "Xiaoqiang Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have manifested strong visual question\nanswering capability. However, they still struggle with aligning the rationale\nand the generated answer, leading to inconsistent reasoning and incorrect\nresponses. To this end, this paper introduces the Self-Rationale Calibration\n(SRC) framework to iteratively calibrate the alignment between rationales and\nanswers. SRC begins by employing a lightweight \"rationale fine-tuning\"\napproach, which modifies the model's response format to require a rationale\nbefore deriving an answer without explicit prompts. Next, SRC searches for a\ndiverse set of candidate responses from the fine-tuned LVLMs for each sample,\nfollowed by a proposed pairwise scoring strategy using a tailored scoring\nmodel, R-Scorer, to evaluate both rationale quality and factual consistency of\ncandidates. Based on a confidence-weighted preference curation process, SRC\ndecouples the alignment calibration into a preference fine-tuning manner,\nleading to significant improvements of LVLMs in perception, reasoning, and\ngeneralization across multiple benchmarks. Our results emphasize the\nrationale-oriented alignment in exploring the potential of LVLMs.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13919v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13919v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.437,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.474,
      "distributed_training_score": 0.305,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper employs preference alignment techniques similar to Direct Preference Optimization (DPO), which is inspired by RLHF methods, to fine-tune LVLMs based on scored candidate responses. However, it uses an automated LLM-based scorer (R-Scorer) for evaluation rather than human-ranked data or reinforcement learning from human feedback, making it only indirectly related to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on iterative calibration of rationales and answers in LVLMs using fine-tuning and scoring strategies, but it does not involve diffusion models, iterative refinement processes for logical tasks, or treating Chain-of-Thought as a holistic entity for multi-step correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13922",
      "title": "Towards Robust Defense against Customization via Protective Perturbation\n  Resistant to Diffusion-based Purification",
      "authors": [
        "Wenkui Yang",
        "Jie Cao",
        "Junxian Duan",
        "Ran He"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Diffusion models like Stable Diffusion have become prominent in visual\nsynthesis tasks due to their powerful customization capabilities, which also\nintroduce significant security risks, including deepfakes and copyright\ninfringement. In response, a class of methods known as protective perturbation\nemerged, which mitigates image misuse by injecting imperceptible adversarial\nnoise. However, purification can remove protective perturbations, thereby\nexposing images again to the risk of malicious forgery. In this work, we\nformalize the anti-purification task, highlighting challenges that hinder\nexisting approaches, and propose a simple diagnostic protective perturbation\nnamed AntiPure. AntiPure exposes vulnerabilities of purification within the\n\"purification-customization\" workflow, owing to two guidance mechanisms: 1)\nPatch-wise Frequency Guidance, which reduces the model's influence over\nhigh-frequency components in the purified image, and 2) Erroneous Timestep\nGuidance, which disrupts the model's denoising strategy across different\ntimesteps. With additional guidance, AntiPure embeds imperceptible\nperturbations that persist under representative purification settings,\nachieving effective post-customization distortion. Experiments show that, as a\nstress test for purification, AntiPure achieves minimal perceptual discrepancy\nand maximal distortion, outperforming other protective perturbation methods\nwithin the purification-customization workflow.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13922v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13922v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.456,
      "distributed_training_score": 0.323,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on protective perturbations in diffusion models for image synthesis and security, specifically addressing adversarial attacks and purification resistance in visual tasks. It does not involve adapting the iterative refinement process of diffusion models for multi-step logical reasoning or treating a 'Chain-of-Thought' as an entity for complex logical tasks. While diffusion models' iterative processes are mentioned, they are applied to image generation and denoising, not reasoning, so there is no relevant component.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13926",
      "title": "MAP: End-to-End Autonomous Driving with Map-Assisted Planning",
      "authors": [
        "Huilin Yin",
        "Yiming Kan",
        "Daniel Watzenig"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In recent years, end-to-end autonomous driving has attracted increasing\nattention for its ability to jointly model perception, prediction, and planning\nwithin a unified framework. However, most existing approaches underutilize the\nonline mapping module, leaving its potential to enhance trajectory planning\nlargely untapped. This paper proposes MAP (Map-Assisted Planning), a novel\nmap-assisted end-to-end trajectory planning framework. MAP explicitly\nintegrates segmentation-based map features and the current ego status through a\nPlan-enhancing Online Mapping module, an Ego-status-guided Planning module, and\na Weight Adapter based on current ego status. Experiments conducted on the\nDAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6%\nreduction in L2 displacement error, a 56.2% reduction in off-road rate, and a\n44.5% improvement in overall score compared to the UniV2X baseline, even\nwithout post-processing. Furthermore, it achieves top ranking in Track 2 of the\nEnd-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS\nWorkshop @CVPR2025, outperforming the second-best model by 39.5% in terms of\noverall score. These results highlight the effectiveness of explicitly\nleveraging semantic map features in planning and suggest new directions for\nimproving structure design in end-to-end autonomous driving systems. Our code\nis available at https://gitee.com/kymkym/map.git",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13926v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13926v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.393,
      "distributed_training_score": 0.341,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13927",
      "title": "DSpAST: Disentangled Representations for Spatial Audio Reasoning with\n  Large Language Models",
      "authors": [
        "Kevin Wilkinghoff",
        "Zheng-Hua Tan"
      ],
      "categories": [
        "eess.AS (Audio and Speech Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.SD (Sound)"
      ],
      "abstract": "Reasoning about spatial audio with large language models requires a spatial\naudio encoder as an acoustic front-end to obtain audio embeddings for further\nprocessing. Such an encoder needs to capture all information required to detect\nthe type of sound events, as well as the direction and distance of their\ncorresponding sources. Accomplishing this with a single audio encoder is\ndemanding as the information required for each of these tasks is mostly\nindependent of each other. As a result, the performance obtained with a single\nencoder is often worse than when using task-specific audio encoders. In this\nwork, we present DSpAST, a novel audio encoder based on SpatialAST that learns\ndisentangled representations of spatial audio while having only 0.2% additional\nparameters. Experiments on SpatialSoundQA with the spatial audio reasoning\nsystem BAT demonstrate that DSpAST significantly outperforms SpatialAST.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13927v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13927v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.463,
      "distributed_training_score": 0.373,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of DSpAST, a spatial audio encoder that learns disentangled representations for tasks like sound event detection, direction, and distance estimation using large language models. It does not involve diffusion models, iterative refinement processes, or any multi-step logical reasoning mechanisms as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13936",
      "title": "Noise-Level Diffusion Guidance: Well Begun is Half Done",
      "authors": [
        "Harvey Mannering",
        "Zhiwu Huang",
        "Adam Prugel-Bennett"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Diffusion models have achieved state-of-the-art image generation. However,\nthe random Gaussian noise used to start the diffusion process influences the\nfinal output, causing variations in image quality and prompt adherence.\nExisting noise-level optimization approaches generally rely on extra dataset\nconstruction, additional networks, or backpropagation-based optimization,\nlimiting their practicality. In this paper, we propose Noise Level Guidance\n(NLG), a simple, efficient, and general noise-level optimization approach that\nrefines initial noise by increasing the likelihood of its alignment with\ngeneral guidance - requiring no additional training data, auxiliary networks,\nor backpropagation. The proposed NLG approach provides a unified framework\ngeneralizable to both conditional and unconditional diffusion models,\naccommodating various forms of diffusion-level guidance. Extensive experiments\non five standard benchmarks demonstrate that our approach enhances output\ngeneration quality and input condition adherence. By seamlessly integrating\nwith existing guidance methods while maintaining computational efficiency, our\nmethod establishes NLG as a practical and scalable enhancement to diffusion\nmodels. Code can be found at\nhttps://github.com/harveymannering/NoiseLevelGuidance.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13936v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13936v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.41,
      "diffusion_reasoning_score": 0.504,
      "distributed_training_score": 0.321,
      "datasets_score": 0.281,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on optimizing initial noise in diffusion models for image generation, without any involvement in training models using programmatically generated labels or noisy sources. It does not address weak supervision techniques, as the method NLG operates during the generation process and requires no additional training data or label generation.",
      "diffusion_reasoning_justification": "The paper applies diffusion models solely to image generation by refining initial noise, and does not adapt the diffusion process for multi-step logical reasoning, chain-of-thought refinement, or solving complex logical tasks. There is no component for holistic correction of reasoning paths, limiting it to visual synthesis rather than reasoning applications.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13939",
      "title": "Can Current AI Models Count What We Mean, Not What They See? A Benchmark\n  and Systematic Evaluation",
      "authors": [
        "Gia Khanh Nguyen",
        "Yifeng Huang",
        "Minh Hoai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Visual counting is a fundamental yet challenging task, especially when users\nneed to count objects of a specific type in complex scenes. While recent\nmodels, including class-agnostic counting models and large vision-language\nmodels (VLMs), show promise in counting tasks, their ability to perform\nfine-grained, intent-driven counting remains unclear. In this paper, we\nintroduce PairTally, a benchmark dataset specifically designed to evaluate\nfine-grained visual counting. Each of the 681 high-resolution images in\nPairTally contains two object categories, requiring models to distinguish and\ncount based on subtle differences in shape, size, color, or semantics. The\ndataset includes both inter-category (distinct categories) and intra-category\n(closely related subcategories) settings, making it suitable for rigorous\nevaluation of selective counting capabilities. We benchmark a variety of\nstate-of-the-art models, including exemplar-based methods, language-prompted\nmodels, and large VLMs. Our results show that despite recent advances, current\nmodels struggle to reliably count what users intend, especially in fine-grained\nand visually ambiguous cases. PairTally provides a new foundation for\ndiagnosing and improving fine-grained visual counting systems.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13939v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13939v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.359,
      "datasets_score": 0.404,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of the PairTally dataset, a new benchmark specifically designed for evaluating fine-grained visual counting in AI models. It details dataset creation, including curation methodologies (e.g., selecting high-resolution images with paired object categories), and uses it for benchmarking and systematic evaluation of various models. This directly aligns with research on creating, analyzing, and benchmarking datasets for machine learning and AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces PairTally, a new benchmark dataset comprising 681 high-resolution images with pairs of object categories to evaluate the fine-grained visual counting capabilities of AI models, particularly in distinguishing subtle differences in shape, size, color, or semantics. The authors systematically benchmark ten state-of-the-art models, including class-agnostic counters, object detectors, and large vision-language models, revealing that current models often fail to accurately count user-intended objects in ambiguous scenarios, thus highlighting the need for advancements in intent-driven counting systems.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark dataset and systematic evaluation framework for fine-grained visual counting, which addresses a significant gap in existing methods and advances the state-of-the-art by diagnosing model limitations in intent-driven tasks.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the computer vision subfield for improving counting models, as it provides a specialized benchmark that could enhance future research on fine-grained tasks, though its influence may be limited outside this area.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper represents a strong and valuable contribution with its innovative dataset and thorough evaluations, making it essential for researchers in visual counting and AI to understand current model shortcomings and guide future developments.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/dc24bda7d68ebd2385fab62648b21925488b99a0",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 2,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Gia Khanh Nguyen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685852"
        },
        {
          "name": "Yifeng Huang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2239057739"
        },
        {
          "name": "Minh Hoai",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2304195223"
        }
      ]
    },
    {
      "id": "2509.13941",
      "title": "An Empirical Study on Failures in Automated Issue Solving",
      "authors": [
        "Simiao Liu",
        "Fang Liu",
        "Liehao Li",
        "Xin Tan",
        "Yinghao Zhu",
        "Xiaoli Lian",
        "Li Zhang"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Automated issue solving seeks to autonomously identify and repair defective\ncode snippets across an entire codebase. SWE-Bench has emerged as the most\nwidely adopted benchmark for evaluating progress in this area. While LLM-based\nagentic tools show great promise, they still fail on a substantial portion of\ntasks. Moreover, current evaluations primarily report aggregate issue-solving\nrates, which obscure the underlying causes of success and failure, making it\nchallenging to diagnose model weaknesses or guide targeted improvements. To\nbridge this gap, we first analyze the performance and efficiency of three SOTA\ntools, spanning both pipeline-based and agentic architectures, in automated\nissue solving tasks of SWE-Bench-Verified under varying task characteristics.\nFurthermore, to move from high-level performance metrics to underlying cause\nanalysis, we conducted a systematic manual analysis of 150 failed instances.\nFrom this analysis, we developed a comprehensive taxonomy of failure modes\ncomprising 3 primary phases, 9 main categories, and 25 fine-grained\nsubcategories. Then we systematically analyze the distribution of the\nidentified failure modes, the results reveal distinct failure fingerprints\nbetween the two architectural paradigms, with the majority of agentic failures\nstemming from flawed reasoning and cognitive deadlocks. Motivated by these\ninsights, we propose a collaborative Expert-Executor framework. It introduces a\nsupervisory Expert agent tasked with providing strategic oversight and\ncourse-correction for a primary Executor agent. This architecture is designed\nto correct flawed reasoning and break the cognitive deadlocks that frequently\nlead to failure. Experiments show that our framework solves 22.2% of previously\nintractable issues for a leading single agent. These findings pave the way for\nbuilding more robust agents through diagnostic evaluation and collaborative\ndesign.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13941v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13941v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.417,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.352,
      "datasets_score": 0.401,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper does not involve weak supervision techniques, as it relies on manual analysis and annotation of failure instances to create a taxonomy and dataset, rather than programmatically generating labels from noisy or imprecise sources.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper directly contributes to dataset research by creating, analyzing, and releasing a new publicly available dataset of 342 annotated failure instances for AI applications in automated issue solving, including methodologies for curation and analysis.",
      "llm_score_status": "completed",
      "summary": "This paper presents an empirical study on failures in automated issue solving using the SWE-Bench-Verified benchmark, analyzing the performance and efficiency of three state-of-the-art tools to identify distinct failure patterns across pipeline-based and agentic architectures. Through manual analysis of 150 failed instances, the authors develop a comprehensive taxonomy of failure modes and propose a collaborative Expert-Executor framework, which mitigates key failures by providing oversight and course-correction, ultimately solving 22.2% of previously intractable issues and paving the way for more robust AI agents in software engineering.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel taxonomy of failure modes and a new collaborative framework for automated issue solving, significantly advancing the state-of-the-art by providing diagnostic tools and mitigation strategies that were previously underdeveloped. This represents a truly new approach to understanding and addressing failures in LLM-based agents.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfields of AI and software engineering, as it offers a publicly available dataset and framework for improving agent robustness. However, its influence may be limited to specific applications in automated code repair rather than broader domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a high-quality contribution with practical insights and a new framework that advances AI-driven software engineering, making it valuable for researchers in the field. While essential for those working on LLM agents, it may not be critical for a general audience.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7942253c4806b4f1200515a95a743313c1ee10bb",
      "total_authors": 7,
      "authors_found": 6,
      "highest_h_index": 9,
      "average_h_index": 3.6666666666666665,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Simiao Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380788663"
        },
        {
          "name": "Fang Liu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2298831829"
        },
        {
          "name": "Liehao Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380804665"
        },
        {
          "name": "Xin Tan",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yinghao Zhu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2297307867"
        },
        {
          "name": "Xiaoli Lian",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/1839458"
        },
        {
          "name": "Li Zhang",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2152828011"
        }
      ]
    },
    {
      "id": "2509.13965",
      "title": "MetricNet: Recovering Metric Scale in Generative Navigation Policies",
      "authors": [
        "Abhijeet Nayak",
        "Débora N. P. Oliveira",
        "Samiran Gode",
        "Cordelia Schmid",
        "Wolfram Burgard"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Generative navigation policies have made rapid progress in improving\nend-to-end learned navigation. Despite their promising results, this paradigm\nhas two structural problems. First, the sampled trajectories exist in an\nabstract, unscaled space without metric grounding. Second, the control strategy\ndiscards the full path, instead moving directly towards a single waypoint. This\nleads to short-sighted and unsafe actions, moving the robot towards obstacles\nthat a complete and correctly scaled path would circumvent. To address these\nissues, we propose MetricNet, an effective add-on for generative navigation\nthat predicts the metric distance between waypoints, grounding policy outputs\nin real-world coordinates. We evaluate our method in simulation with a new\nbenchmarking framework and show that executing MetricNet-scaled waypoints\nsignificantly improves both navigation and exploration performance. Beyond\nsimulation, we further validate our approach in real-world experiments.\nFinally, we propose MetricNav, which integrates MetricNet into a navigation\npolicy to guide the robot away from obstacles while still moving towards the\ngoal.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13965v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13965v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.375,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs diffusion-based policies to generate sequences of waypoints for navigation, which involves iterative refinement processes. However, it does not adapt diffusion for complex logical tasks or multi-step reasoning like a 'Chain-of-Thought' entity. Instead, the focus is on scaling trajectories for real-world robot navigation, making it only peripherally related to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13968",
      "title": "Exploring Major Transitions in the Evolution of Biological Cognition\n  With Artificial Neural Networks",
      "authors": [
        "Konstantinos Voudouris",
        "Andrew Barron",
        "Marta Halina",
        "Colin Klein",
        "Matishalin Patel"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.FL (Formal Languages and Automata Theory)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Transitional accounts of evolution emphasise a few changes that shape what is\nevolvable, with dramatic consequences for derived lineages. More recently it\nhas been proposed that cognition might also have evolved via a series of major\ntransitions that manipulate the structure of biological neural networks,\nfundamentally changing the flow of information. We used idealised models of\ninformation flow, artificial neural networks (ANNs), to evaluate whether\nchanges in information flow in a network can yield a transitional change in\ncognitive performance. We compared networks with feed-forward, recurrent and\nlaminated topologies, and tested their performance learning artificial grammars\nthat differed in complexity, controlling for network size and resources. We\ndocumented a qualitative expansion in the types of input that recurrent\nnetworks can process compared to feed-forward networks, and a related\nqualitative increase in performance for learning the most complex grammars. We\nalso noted how the difficulty in training recurrent networks poses a form of\ntransition barrier and contingent irreversibility -- other key features of\nevolutionary transitions. Not all changes in network topology confer a\nperformance advantage in this task set. Laminated networks did not outperform\nnon-laminated networks in grammar learning. Overall, our findings show how some\nchanges in information flow can yield transitions in cognitive performance.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13968v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13968v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.298,
      "diffusion_reasoning_score": 0.433,
      "distributed_training_score": 0.32,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines the effects of different neural network topologies (e.g., feed-forward, recurrent, laminated) on cognitive performance in artificial grammar learning, focusing on evolutionary transitions in cognition. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13978",
      "title": "LLM Agents for Interactive Workflow Provenance: Reference Architecture\n  and Evaluation Methodology",
      "authors": [
        "Renan Souza",
        "Timothy Poteet",
        "Brian Etz",
        "Daniel Rosendo",
        "Amal Gueroudji",
        "Woong Shin",
        "Prasanna Balaprakash",
        "Rafael Ferreira da Silva"
      ],
      "categories": [
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.AI (Artificial Intelligence)",
        "cs.DB (Databases)"
      ],
      "abstract": "Modern scientific discovery increasingly relies on workflows that process\ndata across the Edge, Cloud, and High Performance Computing (HPC) continuum.\nComprehensive and in-depth analyses of these data are critical for hypothesis\nvalidation, anomaly detection, reproducibility, and impactful findings.\nAlthough workflow provenance techniques support such analyses, at large scale,\nthe provenance data become complex and difficult to analyze. Existing systems\ndepend on custom scripts, structured queries, or static dashboards, limiting\ndata interaction. In this work, we introduce an evaluation methodology,\nreference architecture, and open-source implementation that leverages\ninteractive Large Language Model (LLM) agents for runtime data analysis. Our\napproach uses a lightweight, metadata-driven design that translates natural\nlanguage into structured provenance queries. Evaluations across LLaMA, GPT,\nGemini, and Claude, covering diverse query classes and a real-world chemistry\nworkflow, show that modular design, prompt tuning, and Retrieval-Augmented\nGeneration (RAG) enable accurate and insightful LLM agent responses beyond\nrecorded provenance.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13978v2",
      "pdf_url": "http://arxiv.org/pdf/2509.13978v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.363,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves using LLM agents for interactive workflow provenance through prompt tuning and RAG, without any mention of training a reward model on human-ranked data or fine-tuning via reinforcement learning. It does not incorporate human feedback mechanisms for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on LLM agents for natural language query translation and provenance analysis using techniques like RAG and prompt engineering, but it does not adapt diffusion processes for multi-step logical reasoning or treat reasoning paths as entities for iterative refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13987",
      "title": "Differential Privacy in Federated Learning: Mitigating Inference Attacks\n  with Randomized Response",
      "authors": [
        "Ozer Ozturk",
        "Busra Buyuktanir",
        "Gozde Karatas Baydogmus",
        "Kazim Yildiz"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Machine learning models used for distributed architectures consisting of\nservers and clients require large amounts of data to achieve high accuracy.\nData obtained from clients are collected on a central server for model\ntraining. However, storing data on a central server raises concerns about\nsecurity and privacy. To address this issue, a federated learning architecture\nhas been proposed. In federated learning, each client trains a local model\nusing its own data. The trained models are periodically transmitted to the\ncentral server. The server then combines the received models using federated\naggregation algorithms to obtain a global model. This global model is\ndistributed back to the clients, and the process continues in a cyclical\nmanner. Although preventing data from leaving the clients enhances security,\ncertain concerns still remain. Attackers can perform inference attacks on the\nobtained models to approximate the training dataset, potentially causing data\nleakage. In this study, differential privacy was applied to address the\naforementioned security vulnerability, and a performance analysis was\nconducted. The Data-Unaware Classification Based on Association (duCBA)\nalgorithm was used as the federated aggregation method. Differential privacy\nwas implemented on the data using the Randomized Response technique, and the\ntrade-off between security and performance was examined under different epsilon\nvalues. As the epsilon value decreased, the model accuracy declined, and class\nprediction imbalances were observed. This indicates that higher levels of\nprivacy do not always lead to practical outcomes and that the balance between\nsecurity and performance must be carefully considered.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13987v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13987v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.41,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on Federated Learning (FL), a form of distributed training where models are trained locally on client devices and aggregated centrally, which aligns with concepts of partitioning computation across nodes. However, the primary contribution is the integration of Differential Privacy via Randomized Response to mitigate inference attacks, rather than advancing distributed training techniques for acceleration or efficiency. Thus, while FL inherently involves distributed elements, the paper's emphasis on privacy makes it moderately relevant to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper explores the application of differential privacy in federated learning to mitigate inference attacks, using the Randomized Response technique to protect client data while employing the Data-Unaware Classification Based on Association (duCBA) algorithm for model aggregation. The core objectives are to enhance security in distributed machine learning architectures and analyze the trade-off between privacy and performance; the methodology involves implementing differential privacy on client-side data and evaluating model accuracy under varying epsilon values, revealing that higher privacy levels (lower epsilon) lead to reduced accuracy and class prediction imbalances, emphasizing the need for balancing security and efficacy.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining differential privacy with the duCBA algorithm in federated learning to address inference attacks, though it builds on existing techniques rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of privacy-preserving machine learning, as it provides practical insights into the privacy-performance trade-off in federated learning.",
      "recommendation_score": "Can Skip",
      "recommendation_justification": "While the paper offers valuable analysis on a relevant topic, its contribution is incremental and specific to federated learning security, making it non-essential for readers outside this niche.",
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13990",
      "title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency",
      "authors": [
        "Colin Hong",
        "Xu Guo",
        "Anand Chaanan Singh",
        "Esha Choukse",
        "Dmitrii Ustiugov"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.13990v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13990v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.474,
      "distributed_training_score": 0.423,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on optimizing Self-Consistency (SC) in LLMs by pruning redundant reasoning chains based on similarity, which is a test-time scaling technique. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for holistic correction. There is no mention of adapting diffusion-based methods for multi-step logical reasoning, making this paper unrelated to the topic.",
      "distributed_training_justification": "The paper addresses inference-time efficiency for LLMs, specifically through thought pruning in Self-Consistency to reduce latency and computational overhead. It does not discuss distributed training, parallel computing for model training, or strategies for partitioning data/computation across multiple nodes or processors, as it is focused solely on test-time optimizations rather than training processes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14001",
      "title": "MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment",
      "authors": [
        "Elena Camuffo",
        "Francesco Barbato",
        "Mete Ozay",
        "Simone Milani",
        "Umberto Michieli"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),\na knowledge distillation approach that transfers region-level multimodal\nsemantics from a large vision-language teacher (e.g., LLaVa) into a lightweight\nvision-only object detector student (e.g., YOLO). A translation module maps\nstudent features into a joint space, where the training of the student and\ntranslator is guided by a dual-objective loss that enforces both local\nalignment and global relational consistency. Unlike prior approaches focused on\ndense or global alignment, MOCHA operates at the object level, enabling\nefficient transfer of semantics without modifying the teacher or requiring\ntextual input at inference. We validate our method across four personalized\ndetection benchmarks under few-shot regimes. Results show consistent gains over\nbaselines, with a +10.1 average score improvement. Despite its compact\narchitecture, MOCHA reaches performance on par with larger multimodal models,\nproving its suitability for real-world deployment.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14001v2",
      "pdf_url": "http://arxiv.org/pdf/2509.14001v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.388,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on knowledge distillation for transferring multimodal semantics in object detection, using a vision-language teacher and a vision-only student model. It involves feature alignment and relational consistency but does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, it lacks any components related to diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14003",
      "title": "RFM-Editing: Rectified Flow Matching for Text-guided Audio Editing",
      "authors": [
        "Liting Gao",
        "Yi Yuan",
        "Yaru Chen",
        "Yuelan Cheng",
        "Zhenbo Li",
        "Juan Wen",
        "Shubin Zhang",
        "Wenwu Wang"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Diffusion models have shown remarkable progress in text-to-audio generation.\nHowever, text-guided audio editing remains in its early stages. This task\nfocuses on modifying the target content within an audio signal while preserving\nthe rest, thus demanding precise localization and faithful editing according to\nthe text prompt. Existing training-based and zero-shot methods that rely on\nfull-caption or costly optimization often struggle with complex editing or lack\npracticality. In this work, we propose a novel end-to-end efficient rectified\nflow matching-based diffusion framework for audio editing, and construct a\ndataset featuring overlapping multi-event audio to support training and\nbenchmarking in complex scenarios. Experiments show that our model achieves\nfaithful semantic alignment without requiring auxiliary captions or masks,\nwhile maintaining competitive editing quality across metrics.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14003v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14003v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.494,
      "distributed_training_score": 0.341,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on diffusion models for text-guided audio editing and does not involve any elements of reinforcement learning, human feedback, reward models, or alignment with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion models for audio generation and editing, which involves iterative refinement for generative tasks, but it does not adapt diffusion for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14008",
      "title": "Hala Technical Report: Building Arabic-Centric Instruction & Translation\n  Models at Scale",
      "authors": [
        "Hasan Abed Al Kader Hammoud",
        "Mohammad Zbeeb",
        "Bernard Ghanem"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We present Hala, a family of Arabic-centric instruction and translation\nmodels built with our translate-and-tune pipeline. We first compress a strong\nAR$\\leftrightarrow$EN teacher to FP8 (yielding $\\sim$2$\\times$ higher\nthroughput with no quality loss) and use it to create high-fidelity bilingual\nsupervision. A lightweight language model LFM2-1.2B is then fine-tuned on this\ndata and used to translate high-quality English instruction sets into Arabic,\nproducing a million-scale corpus tailored to instruction following. We train\nHala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to\nbalance Arabic specialization with base-model strengths. On Arabic-centric\nbenchmarks, Hala achieves state-of-the-art results within both the \"nano\"\n($\\leq$2B) and \"small\" (7-9B) categories, outperforming their bases. We release\nmodels, data, evaluation, and recipes to accelerate research in Arabic NLP.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14008v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14008v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.401,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on building Arabic-centric models through translation and fine-tuning pipelines, using datasets like Open-Orca and synthetic data, but it does not involve human feedback, reward models, or reinforcement learning techniques for alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses training models at various scales (e.g., 350M to 9B parameters) and model compression for efficiency, which may relate to computational scaling, but it does not specifically address distributed training algorithms, parallel computing strategies, or multi-node systems as a core contribution.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14012",
      "title": "Performance Optimization of YOLO-FEDER FusionNet for Robust Drone\n  Detection in Visually Complex Environments",
      "authors": [
        "Tamara R. Lenhard",
        "Andreas Weinmann",
        "Tobias Koch"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Drone detection in visually complex environments remains challenging due to\nbackground clutter, small object scale, and camouflage effects. While generic\nobject detectors like YOLO exhibit strong performance in low-texture scenes,\ntheir effectiveness degrades in cluttered environments with low\nobject-background separability. To address these limitations, this work\npresents an enhanced iteration of YOLO-FEDER FusionNet -- a detection framework\nthat integrates generic object detection with camouflage object detection\ntechniques. Building upon the original architecture, the proposed iteration\nintroduces systematic advancements in training data composition, feature fusion\nstrategies, and backbone design. Specifically, the training process leverages\nlarge-scale, photo-realistic synthetic data, complemented by a small set of\nreal-world samples, to enhance robustness under visually complex conditions.\nThe contribution of intermediate multi-scale FEDER features is systematically\nevaluated, and detection performance is comprehensively benchmarked across\nmultiple YOLO-based backbone configurations. Empirical results indicate that\nintegrating intermediate FEDER features, in combination with backbone upgrades,\ncontributes to notable performance improvements. In the most promising\nconfiguration -- YOLO-FEDER FusionNet with a YOLOv8l backbone and FEDER\nfeatures derived from the DWD module -- these enhancements lead to a FNR\nreduction of up to 39.1 percentage points and a mAP increase of up to 62.8\npercentage points at an IoU threshold of 0.5, compared to the initial baseline.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14012v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14012v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.331,
      "distributed_training_score": 0.385,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14030",
      "title": "CrowdAgent: Multi-Agent Managed Multi-Source Annotation System",
      "authors": [
        "Maosheng Qin",
        "Renyu Zhu",
        "Mingxuan Xia",
        "Chenkai Chen",
        "Zhen Zhu",
        "Minmin Lin",
        "Junbo Zhao",
        "Lu Xu",
        "Changjie Fan",
        "Runze Wu",
        "Haobo Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "High-quality annotated data is a cornerstone of modern Natural Language\nProcessing (NLP). While recent methods begin to leverage diverse annotation\nsources-including Large Language Models (LLMs), Small Language Models (SLMs),\nand human experts-they often focus narrowly on the labeling step itself. A\ncritical gap remains in the holistic process control required to manage these\nsources dynamically, addressing complex scheduling and quality-cost trade-offs\nin a unified manner. Inspired by real-world crowdsourcing companies, we\nintroduce CrowdAgent, a multi-agent system that provides end-to-end process\ncontrol by integrating task assignment, data annotation, and quality/cost\nmanagement. It implements a novel methodology that rationally assigns tasks,\nenabling LLMs, SLMs, and human experts to advance synergistically in a\ncollaborative annotation workflow. We demonstrate the effectiveness of\nCrowdAgent through extensive experiments on six diverse multimodal\nclassification tasks. The source code and video demo are available at\nhttps://github.com/QMMMS/CrowdAgent.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14030v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14030v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.462,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.391,
      "datasets_score": 0.415,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces a multi-agent system for managing annotations from diverse sources like LLMs, SLMs, and humans, which often produce noisy or imprecise labels. This aligns with weak supervision by programmatically aggregating and refining labels to generate training data, but the primary focus is on process control and quality management rather than directly training models with weak labels. Thus, it is relevant but not central to weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper presents CrowdAgent as a system for creating high-quality annotated datasets through multi-source collaboration, including experiments on six multimodal classification tasks. This directly contributes to dataset curation methodologies, quality assurance, and efficient dataset creation for AI applications, making it highly pertinent to research on datasets.",
      "llm_score_status": "completed",
      "summary": "CrowdAgent is a multi-agent system designed to manage and optimize the annotation process from diverse sources, including Large Language Models (LLMs), Small Language Models (SLMs), and human experts, by addressing gaps in dynamic task assignment, quality assurance, and cost management. The methodology involves four core components—Annotation Agents for labeling, Quality Assurance Agent for evaluation, Financing Agent for cost monitoring, and Scheduling Agent for task allocation—demonstrating effectiveness through experiments on six multimodal classification tasks, with the system providing a user-friendly platform for efficient, high-quality data annotation.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by integrating multi-agent systems to dynamically manage multi-source annotation, combining existing ideas in a new way to address quality-cost trade-offs that were previously underexplored. However, it builds on established concepts like crowdsourcing and hybrid annotation methods rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research and applications in AI-driven data annotation by providing a practical framework for optimizing quality and cost, potentially leading to citations and adaptations in subfields like NLP and multimodal tasks. Nonetheless, its impact may be confined to specific areas and not broadly transformative across all AI domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to annotation management systems, making it essential for researchers and practitioners in AI data processing to understand its innovative approach. While not groundbreaking enough for a \"Must Read,\" it provides practical insights that could enhance related workflows.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2d060a99823d824fa218bd1544e87813954b7530",
      "total_authors": 11,
      "authors_found": 10,
      "highest_h_index": 11,
      "average_h_index": 1.6,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Maosheng Qin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380689079"
        },
        {
          "name": "Renyu Zhu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2268398357"
        },
        {
          "name": "Mingxuan Xia",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2293428901"
        },
        {
          "name": "Chenkai Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380794312"
        },
        {
          "name": "Zhen Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2321925890"
        },
        {
          "name": "Minmin Lin",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2269045029"
        },
        {
          "name": "Junbo Zhao",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2261754498"
        },
        {
          "name": "Lu Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380797052"
        },
        {
          "name": "Changjie Fan",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Runze Wu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2268432921"
        },
        {
          "name": "Haobo Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2367087514"
        }
      ]
    },
    {
      "id": "2509.14031",
      "title": "You Are What You Train: Effects of Data Composition on Training\n  Context-aware Machine Translation Models",
      "authors": [
        "Paweł Mąka",
        "Yusuf Can Semerci",
        "Jan Scholtes",
        "Gerasimos Spanakis"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Achieving human-level translations requires leveraging context to ensure\ncoherence and handle complex phenomena like pronoun disambiguation. Sparsity of\ncontextually rich examples in the standard training data has been hypothesized\nas the reason for the difficulty of context utilization. In this work, we\nsystematically validate this claim in both single- and multilingual settings by\nconstructing training datasets with a controlled proportions of contextually\nrelevant examples. We demonstrate a strong association between training data\nsparsity and model performance confirming sparsity as a key bottleneck.\nImportantly, we reveal that improvements in one contextual phenomenon do no\ngeneralize to others. While we observe some cross-lingual transfer, it is not\nsignificantly higher between languages within the same sub-family. Finally, we\npropose and empirically evaluate two training strategies designed to leverage\nthe available data. These strategies improve context utilization, resulting in\naccuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in\nsingle- and multilingual settings respectively.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14031v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14031v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.424,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.406,
      "datasets_score": 0.416,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves programmatically annotating and selecting training examples using tools like ctxPro, which could be seen as a form of weak supervision through automated labeling. However, it does not primarily focus on generating noisy or imprecise labels for training; instead, it emphasizes data composition for context-aware MT, making this only a minor connection.",
      "diffusion_reasoning_justification": "The paper focuses on context-aware machine translation and data composition effects, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. There is no component involving diffusion-based techniques for reasoning tasks.",
      "distributed_training_justification": "The paper does not address distributed training, parallel computing, or partitioning data/computation across multiple nodes. It is centered on training data composition and strategies for machine translation models, without any discussion of acceleration techniques via distributed systems.",
      "datasets_justification": "The paper's main contribution involves creating, analyzing, and evaluating datasets by constructing training sets with controlled proportions of contextually rich examples, using tools like ctxPro for annotation and benchmarking performance on specific phenomena. This directly aligns with research on dataset curation, analysis, and evaluation for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper investigates the impact of data composition, specifically the sparsity of contextually rich examples, on the performance of context-aware machine translation (MT) models, aiming to validate the hypothesis that such sparsity hinders effective context utilization. The authors construct training datasets with controlled proportions of contextually relevant examples for various linguistic phenomena and evaluate model performance in both single- and multilingual settings using metrics like BLEU and COMET, revealing a strong correlation between data density and translation accuracy, limited generalization across phenomena, modest cross-lingual transfer, and proposing two training strategies that improve accuracy by up to 8 percentage points.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by systematically validating the sparsity hypothesis and proposing new training strategies, which cleverly combine existing ideas to address a known problem in context-aware MT. However, it does not introduce a entirely new problem or architecture, making it an incremental advancement rather than a groundbreaking one.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in machine translation by highlighting the importance of data composition and providing practical training strategies, potentially leading to better MT models in specific subfields. Nonetheless, its applicability may be limited to context-aware MT, reducing its broader commercial or widespread impact.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable insights and empirical evidence on a key challenge in MT, making it a strong contribution for researchers in AI and language processing. While not essential for all, it is highly relevant for those working on contextual models and data optimization strategies.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/731e9f27bc656b14f9f0f5b83832009fb80d96a7",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 18,
      "average_h_index": 5.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Pawel Mkaka",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2335563830"
        },
        {
          "name": "Yusuf Can Semerci",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2045069003"
        },
        {
          "name": "Jan Scholtes",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2282468902"
        },
        {
          "name": "Gerasimos Spanakis",
          "h_index": 18,
          "profile_url": "https://www.semanticscholar.org/author/3266578"
        }
      ]
    },
    {
      "id": "2509.14033",
      "title": "SAIL-VL2 Technical Report",
      "authors": [
        "Weijie Yin",
        "Yongjie Ye",
        "Fangxun Shu",
        "Yue Liao",
        "Zijian Kang",
        "Hongyuan Dong",
        "Haiyang Yu",
        "Dingkang Yang",
        "Jiacong Wang",
        "Han Wang",
        "Wenzhuo Liu",
        "Xiao Liang",
        "Shuicheng Yan",
        "Chao Feng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)\nfor comprehensive multimodal understanding and reasoning. As the successor to\nSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B\nparameter scales across diverse image and video benchmarks, demonstrating\nstrong capabilities from fine-grained perception to complex reasoning. Its\neffectiveness is driven by three core innovations. First, a large-scale data\ncuration pipeline with scoring and filtering strategies enhances both quality\nand distribution across captioning, OCR, QA, and video data, improving training\nefficiency. Second, a progressive training framework begins with a powerful\npre-trained vision encoder (SAIL-ViT), advances through multimodal\npre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that\nsystematically strengthens model capabilities. Third, architectural advances\nextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.\nWith these contributions, SAIL-VL2 demonstrates competitive performance across\n106 datasets and achieves state-of-the-art results on challenging reasoning\nbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass\nleaderboard, SAIL-VL2-2B ranks first among officially released open-source\nmodels under the 4B parameter scale, while serving as an efficient and\nextensible foundation for the open-source multimodal community.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14033v2",
      "pdf_url": "http://arxiv.org/pdf/2509.14033v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.456,
      "distributed_training_score": 0.402,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces SAIL-VL2, focusing on vision-language models with innovations in data curation, progressive training (including SFT-RL hybrid and Chain-of-Thought strategies), and efficient architectures like Mixture-of-Experts. It does not mention or utilize diffusion models or their iterative refinement processes for reasoning tasks. There is no component for treating a Chain-of-Thought as a holistic entity for multi-step correction via diffusion.",
      "distributed_training_justification": "The paper discusses challenges in scaling model parameters and training data, emphasizing efficiency through architectural designs (e.g., Mixture-of-Experts) and training strategies, but it does not cover distributed training techniques, parallel computing algorithms, or systems for partitioning data/computation across multiple nodes or processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14036",
      "title": "SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting\n  for Question-Based Sign Language Translation",
      "authors": [
        "Zekang Liu",
        "Wei Feng",
        "Fanhua Shang",
        "Lianyu Hu",
        "Jichao Feng",
        "Liqing Gao"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Sign Language Translation (SLT) bridges the communication gap between deaf\npeople and hearing people, where dialogue provides crucial contextual cues to\naid in translation. Building on this foundational concept, this paper proposes\nQuestion-based Sign Language Translation (QB-SLT), a novel task that explores\nthe efficient integration of dialogue. Unlike gloss (sign language\ntranscription) annotations, dialogue naturally occurs in communication and is\neasier to annotate. The key challenge lies in aligning multimodality features\nwhile leveraging the context of the question to improve translation. To address\nthis issue, we propose a cross-modality Self-supervised Learning with Sigmoid\nSelf-attention Weighting (SSL-SSAW) fusion method for sign language\ntranslation. Specifically, we employ contrastive learning to align\nmultimodality features in QB-SLT, then introduce a Sigmoid Self-attention\nWeighting (SSAW) module for adaptive feature extraction from question and sign\nlanguage sequences. Additionally, we leverage available question text through\nself-supervised learning to enhance representation and translation\ncapabilities. We evaluated our approach on newly constructed CSL-Daily-QA and\nPHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,\neasily accessible question assistance can achieve or even surpass the\nperformance of gloss assistance. Furthermore, visualization results demonstrate\nthe effectiveness of incorporating dialogue in improving translation quality.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14036v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14036v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.322,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper proposes using naturally occurring questions as auxiliary information for sign language translation, replacing expensive gloss annotations. This aligns with weak supervision, as questions are easier to obtain and may introduce noise or imprecision compared to precise labels, allowing the model to be trained without relying on perfectly hand-labeled data. The use of self-supervised learning to leverage these questions further supports this, though the paper does not explicitly focus on programmatically generating labels.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces Question-based Sign Language Translation (QB-SLT), a novel task that utilizes naturally occurring questions as auxiliary information to translate sign language videos into text, aiming to reduce the high costs associated with traditional gloss annotations. The proposed SSL-SSAW method employs contrastive learning to align multimodal features from video and text, incorporates a Sigmoid Self-attention Weighting module for adaptive feature extraction and fusion, and uses self-supervised learning on question text to enhance representation and translation accuracy, achieving state-of-the-art performance on new datasets like CSL-Daily-QA and PHOENIX-2014T-QA, where it matches or exceeds gloss-based approaches.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new task (QB-SLT) and a clever combination of contrastive learning and self-attention weighting to integrate questions with sign language features, advancing existing methods without creating an entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in sign language translation and multimodal AI by demonstrating the effectiveness of low-cost annotations like questions, potentially leading to broader adoption in accessibility-focused applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to AI and computational linguistics through its innovative approach to sign language translation, making it essential for researchers in multimodal learning and accessibility.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3a6b8e2ed3061fe756a043f7f0d442090d84f5c7",
      "total_authors": 6,
      "authors_found": 5,
      "highest_h_index": 10,
      "average_h_index": 6.2,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Zekang Liu",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2109371407"
        },
        {
          "name": "Wei Feng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Fanhua Shang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2263864781"
        },
        {
          "name": "Lianyu Hu",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/1994488088"
        },
        {
          "name": "Jichao Feng",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2314293529"
        },
        {
          "name": "Liqing Gao",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2148992126"
        }
      ]
    },
    {
      "id": "2509.14037",
      "title": "PhenoGnet: A Graph-Based Contrastive Learning Framework for Disease\n  Similarity Prediction",
      "authors": [
        "Ranga Baminiwatte",
        "Kazi Jewel Rana",
        "Aaron J. Masino"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Understanding disease similarity is critical for advancing diagnostics, drug\ndiscovery, and personalized treatment strategies. We present PhenoGnet, a novel\ngraph-based contrastive learning framework designed to predict disease\nsimilarity by integrating gene functional interaction networks with the Human\nPhenotype Ontology (HPO). PhenoGnet comprises two key components: an intra-view\nmodel that separately encodes gene and phenotype graphs using Graph\nConvolutional Networks (GCNs) and Graph Attention Networks (GATs), and a cross\nview model implemented as a shared weight multilayer perceptron (MLP) that\naligns gene and phenotype embeddings through contrastive learning. The model is\ntrained using known gene phenotype associations as positive pairs and randomly\nsampled unrelated pairs as negatives. Diseases are represented by the mean\nembeddings of their associated genes and/or phenotypes, and pairwise similarity\nis computed via cosine similarity. Evaluation on a curated benchmark of 1,100\nsimilar and 866 dissimilar disease pairs demonstrates strong performance, with\ngene based embeddings achieving an AUCPR of 0.9012 and AUROC of 0.8764,\noutperforming existing state of the art methods. Notably, PhenoGnet captures\nlatent biological relationships beyond direct overlap, offering a scalable and\ninterpretable solution for disease similarity prediction. These results\nunderscore its potential for enabling downstream applications in rare disease\nresearch and precision medicine.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14037v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14037v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.315,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14040",
      "title": "Prompt2Auto: From Motion Prompt to Automated Control via\n  Geometry-Invariant One-Shot Gaussian Process Learning",
      "authors": [
        "Zewen Yang",
        "Xiaobing Dai",
        "Dongfa Zhang",
        "Yu Li",
        "Ziyang Meng",
        "Bingkun Huang",
        "Hamid Sadeghian",
        "Sami Haddadin"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "Learning from demonstration allows robots to acquire complex skills from\nhuman demonstrations, but conventional approaches often require large datasets\nand fail to generalize across coordinate transformations. In this paper, we\npropose Prompt2Auto, a geometry-invariant one-shot Gaussian process (GeoGP)\nlearning framework that enables robots to perform human-guided automated\ncontrol from a single motion prompt. A dataset-construction strategy based on\ncoordinate transformations is introduced that enforces invariance to\ntranslation, rotation, and scaling, while supporting multi-step predictions.\nMoreover, GeoGP is robust to variations in the user's motion prompt and\nsupports multi-skill autonomy. We validate the proposed approach through\nnumerical simulations with the designed user graphical interface and two\nreal-world robotic experiments, which demonstrate that the proposed method is\neffective, generalizes across tasks, and significantly reduces the\ndemonstration burden. Project page is available at:\nhttps://prompt2auto.github.io",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14040v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14040v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.32,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Learning from Demonstration (LfD) using a geometry-invariant one-shot Gaussian Process (GeoGP) framework for robot control from a single motion prompt. It involves direct learning from human demonstrations via supervised regression, without any elements of reinforcement learning, reward modeling, or fine-tuning based on human-ranked data. RLHF specifically requires training a reward model from ranked human feedback and using it in a reinforcement learning loop, which is absent here. Thus, there is no direct or indirect connection to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14049",
      "title": "Comprehensive Evaluation of CNN-Based Audio Tagging Models on\n  Resource-Constrained Devices",
      "authors": [
        "Jordi Grau-Haro",
        "Ruben Ribes-Serrano",
        "Javier Naranjo-Alcazar",
        "Marta Garcia-Ballesteros",
        "Pedro Zuccarello"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Convolutional Neural Networks (CNNs) have demonstrated exceptional\nperformance in audio tagging tasks. However, deploying these models on\nresource-constrained devices like the Raspberry Pi poses challenges related to\ncomputational efficiency and thermal management. In this paper, a comprehensive\nevaluation of multiple convolutional neural network (CNN) architectures for\naudio tagging on the Raspberry Pi is conducted, encompassing all 1D and 2D\nmodels from the Pretrained Audio Neural Networks (PANNs) framework, a\nConvNeXt-based model adapted for audio classification, as well as MobileNetV3\narchitectures. In addition, two PANNs-derived networks, CNN9 and CNN13,\nrecently proposed, are also evaluated. To enhance deployment efficiency and\nportability across diverse hardware platforms, all models are converted to the\nOpen Neural Network Exchange (ONNX) format. Unlike previous works that focus on\na single model, our analysis encompasses a broader range of architectures and\ninvolves continuous 24-hour inference sessions to assess performance stability.\nOur experiments reveal that, with appropriate model selection and optimization,\nit is possible to maintain consistent inference latency and manage thermal\nbehavior effectively over extended periods. These findings provide valuable\ninsights for deploying audio tagging models in real-world edge computing\nscenarios.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14049v2",
      "pdf_url": "http://arxiv.org/pdf/2509.14049v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.387,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14051",
      "title": "PROFUSEme: PROstate Cancer Biochemical Recurrence Prediction via FUSEd\n  Multi-modal Embeddings",
      "authors": [
        "Suhang You",
        "Carla Pitarch-Abaigar",
        "Sanket Kachole",
        "Sumedh Sonawane",
        "Juhyung Ha",
        "Anish Sudarshan Gada",
        "David Crandall",
        "Rakesh Shiradkar",
        "Spyridon Bakas"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Almost 30% of prostate cancer (PCa) patients undergoing radical prostatectomy\n(RP) experience biochemical recurrence (BCR), characterized by increased\nprostate specific antigen (PSA) and associated with increased mortality.\nAccurate early prediction of BCR, at the time of RP, would contribute to prompt\nadaptive clinical decision-making and improved patient outcomes. In this work,\nwe propose prostate cancer BCR prediction via fused multi-modal embeddings\n(PROFUSEme), which learns cross-modal interactions of clinical, radiology, and\npathology data, following an intermediate fusion configuration in combination\nwith Cox Proportional Hazard regressors. Quantitative evaluation of our\nproposed approach reveals superior performance, when compared with late fusion\nconfigurations, yielding a mean C-index of 0.861 ($\\sigma=0.112$) on the\ninternal 5-fold nested cross-validation framework, and a C-index of 0.7107 on\nthe hold out data of CHIMERA 2025 challenge validation leaderboard.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14051v2",
      "pdf_url": "http://arxiv.org/pdf/2509.14051v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.297,
      "datasets_score": 0.281,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14055",
      "title": "Wan-Animate: Unified Character Animation and Replacement with Holistic\n  Replication",
      "authors": [
        "Gang Cheng",
        "Xin Gao",
        "Li Hu",
        "Siqi Hu",
        "Mingyang Huang",
        "Chaonan Ji",
        "Ju Li",
        "Dechao Meng",
        "Jinwei Qi",
        "Penchong Qiao",
        "Zhen Shen",
        "Yafei Song",
        "Ke Sun",
        "Linrui Tian",
        "Feng Wang",
        "Guangyuan Wang",
        "Qi Wang",
        "Zhongjian Wang",
        "Jiayu Xiao",
        "Sheng Xu",
        "Bang Zhang",
        "Peng Zhang",
        "Xindi Zhang",
        "Zhe Zhang",
        "Jingren Zhou",
        "Lian Zhuo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce Wan-Animate, a unified framework for character animation and\nreplacement. Given a character image and a reference video, Wan-Animate can\nanimate the character by precisely replicating the expressions and movements of\nthe character in the video to generate high-fidelity character videos.\nAlternatively, it can integrate the animated character into the reference video\nto replace the original character, replicating the scene's lighting and color\ntone to achieve seamless environmental integration. Wan-Animate is built upon\nthe Wan model. To adapt it for character animation tasks, we employ a modified\ninput paradigm to differentiate between reference conditions and regions for\ngeneration. This design unifies multiple tasks into a common symbolic\nrepresentation. We use spatially-aligned skeleton signals to replicate body\nmotion and implicit facial features extracted from source images to reenact\nexpressions, enabling the generation of character videos with high\ncontrollability and expressiveness. Furthermore, to enhance environmental\nintegration during character replacement, we develop an auxiliary Relighting\nLoRA. This module preserves the character's appearance consistency while\napplying the appropriate environmental lighting and color tone. Experimental\nresults demonstrate that Wan-Animate achieves state-of-the-art performance. We\nare committed to open-sourcing the model weights and its source code.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14055v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14055v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.313,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14057",
      "title": "Machines are more productive than humans until they aren't, and vice\n  versa",
      "authors": [
        "Riccardo Zanardelli"
      ],
      "categories": [
        "econ.GN (General Economics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "With the growth of artificial skills, organizations are increasingly\nconfronting the problem of optimizing skill policy decisions guided by economic\nprinciples. This paper addresses the underlying complexity of this challenge by\ndeveloping an in-silico framework based on Monte Carlo simulations grounded in\nempirical realism to analyze the economic impact of human and machine skills,\nindividually or jointly deployed, in the execution of tasks presenting varying\nlevels of complexity. Our results provide quantitative support for the\nestablished notions that automation tends to be the most economically-effective\nstrategy for tasks characterized by low-to-medium generalization difficulty,\nwhile automation may struggle to match the economic utility of human skills in\nmore complex scenarios. Critically, our simulations highlight that, when a high\nlevel of generalization is required and the cost of errors is high, combining\nhuman and machine skills can be the most effective strategy, but only if\ngenuine augmentation is achieved. In contrast, when failing to realize this\nsynergy, the human-machine policy is severely penalized by the inherent costs\nof its dual skill structure, causing it to destroy value and become the worst\nchoice from an economic perspective. The takeaway for decision-makers is\nunambiguous: in complex and critical contexts, simply allocating human and\nmachine skills to a task may be insufficient, and a human-machine skill policy\nis neither a silver-bullet solution nor a low-risk compromise. Rather, it is a\ncritical opportunity to boost competitiveness that demands a strong\norganizational commitment to enabling augmentation. Also, our findings show\nthat improving the cost-effectiveness of machine skills over time, while\nuseful, does not replace the fundamental need to focus on achieving\naugmentation.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14057v4",
      "pdf_url": "http://arxiv.org/pdf/2509.14057v4",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.412,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.383,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses human-machine interaction and feedback in the context of augmentation and economic decision-making, such as in feedback loops for automation, but it does not involve training AI models using reinforcement learning from human feedback. The focus is on simulations for skill allocation, not RLHF techniques.",
      "weak_supervision_justification": "The paper uses Monte Carlo simulations to analyze economic impacts of human and machine skills but does not address training machine learning models with programmatically generated or noisy labels, which is the core of weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14060",
      "title": "VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by\n  Visual Semantic Enhancement",
      "authors": [
        "Jun Du",
        "Weiwei Xing",
        "Ming Li",
        "Fei Richard Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Current multi-object tracking (MOT) algorithms typically overlook issues\ninherent in low-quality videos, leading to significant degradation in tracking\nperformance when confronted with real-world image deterioration. Therefore,\nadvancing the application of MOT algorithms in real-world low-quality video\nscenarios represents a critical and meaningful endeavor. To address the\nchallenges posed by low-quality scenarios, inspired by vision-language models,\nthis paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking\nframework (VSE-MOT). Specifically, we first design a tri-branch architecture\nthat leverages a vision-language model to extract global visual semantic\ninformation from images and fuse it with query vectors. Subsequently, to\nfurther enhance the utilization of visual semantic information, we introduce\nthe Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion\nModule (VSFM). The MOT-Adapter adapts the extracted global visual semantic\ninformation to suit multi-object tracking tasks, while the VSFM improves the\nefficacy of feature fusion. Through extensive experiments, we validate the\neffectiveness and superiority of the proposed method in real-world low-quality\nvideo scenarios. Its tracking performance metrics outperform those of existing\nmethods by approximately 8% to 20%, while maintaining robust performance in\nconventional scenarios.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14060v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14060v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.314,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14061",
      "title": "Queen Detection in Beehives via Environmental Sensor Fusion for\n  Low-Power Edge Computing",
      "authors": [
        "Chiara De Luca",
        "Elisa Donati"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Queen bee presence is essential for the health and stability of honeybee\ncolonies, yet current monitoring methods rely on manual inspections that are\nlabor-intensive, disruptive, and impractical for large-scale beekeeping. While\nrecent audio-based approaches have shown promise, they often require high power\nconsumption, complex preprocessing, and are susceptible to ambient noise. To\novercome these limitations, we propose a lightweight, multimodal system for\nqueen detection based on environmental sensor fusion-specifically, temperature,\nhumidity, and pressure differentials between the inside and outside of the\nhive. Our approach employs quantized decision tree inference on a commercial\nSTM32 microcontroller, enabling real-time, low-power edge computing without\ncompromising accuracy. We show that our system achieves over 99% queen\ndetection accuracy using only environmental inputs, with audio features\noffering no significant performance gain. This work presents a scalable and\nsustainable solution for non-invasive hive monitoring, paving the way for\nautonomous, precision beekeeping using off-the-shelf, energy-efficient\nhardware.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14061v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14061v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.344,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14084",
      "title": "AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with\n  Anomaly-Aware Calibration",
      "authors": [
        "Jingyi Yuan",
        "Jianxiong Ye",
        "Wenkang Chen",
        "Chenqiang Gao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary\nnovel categories, offering a scalable and annotation-efficient solution.\nTraditionally, most ZSAD works have been based on the CLIP model, which\nperforms anomaly detection by calculating the similarity between visual and\ntext embeddings. Recently, vision foundation models such as DINOv3 have\ndemonstrated strong transferable representation capabilities. In this work, we\nare the first to adapt DINOv3 for ZSAD. However, this adaptation presents two\nkey challenges: (i) the domain bias between large-scale pretraining data and\nanomaly detection tasks leads to feature misalignment; and (ii) the inherent\nbias toward global semantics in pretrained representations often leads to\nsubtle anomalies being misinterpreted as part of the normal foreground objects,\nrather than being distinguished as abnormal regions. To overcome these\nchallenges, we introduce AD-DINOv3, a novel vision-language multimodal\nframework designed for ZSAD. Specifically, we formulate anomaly detection as a\nmultimodal contrastive learning problem, where DINOv3 is employed as the visual\nbackbone to extract patch tokens and a CLS token, and the CLIP text encoder\nprovides embeddings for both normal and abnormal prompts. To bridge the domain\ngap, lightweight adapters are introduced in both modalities, enabling their\nrepresentations to be recalibrated for the anomaly detection task. Beyond this\nbaseline alignment, we further design an Anomaly-Aware Calibration Module\n(AACM), which explicitly guides the CLS token to attend to anomalous regions\nrather than generic foreground semantics, thereby enhancing discriminability.\nExtensive experiments on eight industrial and medical benchmarks demonstrate\nthat AD-DINOv3 consistently matches or surpasses state-of-the-art methods.The\ncode will be available at https://github.com/Kaisor-Yuan/AD-DINOv3.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14084v2",
      "pdf_url": "http://arxiv.org/pdf/2509.14084v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.365,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14093",
      "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework",
      "authors": [
        "Kerui Huang",
        "Shuhan Liu",
        "Xing Hu",
        "Tongtong Xu",
        "Lingfeng Bao",
        "Xin Xia"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14093v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14093v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.54,
      "distributed_training_score": 0.378,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an adaptive framework called SEER for compressing Chain-of-Thought (CoT) reasoning in LLMs to improve efficiency in tasks like code generation and math reasoning. It uses techniques such as Best-of-N sampling and adaptive filtering, with no mention of diffusion models, iterative refinement processes, or adapting diffusion for logical tasks. The paper does not treat CoT as a single entity for holistic correction over multiple steps as described in the topic, making it entirely unrelated.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14097",
      "title": "Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for\n  Audio-Visual Video Parsing",
      "authors": [
        "Yaru Chen",
        "Ruohao Guo",
        "Liting Gao",
        "Yang Xiang",
        "Qingyu Luo",
        "Zhenbo Li",
        "Wenwu Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible,\nvisible, and audio-visual events without temporal annotations. Previous work\nhas emphasized refining global predictions through contrastive or collaborative\nlearning, but neglected stable segment-level supervision and class-aware\ncross-modal alignment. To address this, we propose two strategies: (1) an\nexponential moving average (EMA)-guided pseudo supervision framework that\ngenerates reliable segment-level masks via adaptive thresholds or top-k\nselection, offering stable temporal guidance beyond video-level labels; and (2)\na class-aware cross-modal agreement (CMA) loss that aligns audio and visual\nembeddings at reliable segment-class pairs, ensuring consistency across\nmodalities while preserving temporal structure. Evaluations on LLP and UnAV-100\ndatasets shows that our method achieves state-of-the-art (SOTA) performance\nacross multiple metrics.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14097v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14097v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.445,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.315,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution focuses on weakly-supervised audio-visual video parsing, where it uses video-level annotations to generate reliable segment-level pseudo labels via an EMA-guided framework. This directly aligns with weak supervision, as it programmatically creates training labels from high-level, noisy sources (video-level labels) to enable precise event localization, without relying on hand-labeled data. The approach addresses key challenges in weak supervision, such as noise reduction and dynamic label refinement, making it a core example of the topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper tackles weakly-supervised audio-visual video parsing (AVVP) by introducing an exponential moving average (EMA)-guided pseudo supervision framework to generate stable segment-level masks from video-level labels, and a class-aware cross-modal agreement (CMA) loss to align audio and visual embeddings at reliable segments. Their method addresses key challenges in stable supervision and precise cross-modal alignment, achieving state-of-the-art performance on the LLP and UnAV-100 datasets, demonstrating significant improvements in event detection accuracy.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining EMA for dynamic pseudo supervision and class-aware alignment, which cleverly adapts existing techniques to enhance weakly-supervised AVVP beyond previous static methods. However, it builds on established concepts like teacher-student frameworks rather than introducing an entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in audio-visual parsing and weakly-supervised learning within computer vision and multimedia subfields, as evidenced by its state-of-the-art results on benchmark datasets. Nonetheless, its impact may be limited to specific applications in multimodal event detection rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, innovative contribution to AVVP with practical advancements and SOTA results, making it essential for researchers in computer vision and multimedia to stay informed. While not groundbreaking across all AI, it provides valuable insights for those working on weakly-supervised multimodal tasks.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5b07ab0e67a4597bfcf9153f14b7d82f941004fb",
      "total_authors": 7,
      "authors_found": 6,
      "highest_h_index": 7,
      "average_h_index": 2.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Yaru Chen",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2257091353"
        },
        {
          "name": "Ruohao Guo",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2152318597"
        },
        {
          "name": "Liting Gao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380787736"
        },
        {
          "name": "Yang Xiang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381271609"
        },
        {
          "name": "Qingyu Luo",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhenbo Li",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2257090959"
        },
        {
          "name": "Wenwu Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379039596"
        }
      ]
    },
    {
      "id": "2509.14104",
      "title": "CSMoE: An Efficient Remote Sensing Foundation Model with Soft\n  Mixture-of-Experts",
      "authors": [
        "Leonard Hackel",
        "Tom Burgert",
        "Begüm Demir"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Self-supervised learning through masked autoencoders has attracted great\nattention for remote sensing (RS) foundation model (FM) development, enabling\nimproved representation learning across diverse sensors and downstream tasks.\nHowever, existing RS FMs often either suffer from substantial computational\ncomplexity during both training and inference or exhibit limited\nrepresentational capacity. These issues restrict their practical applicability\nin RS. To address this limitation, we propose an adaptation for enhancing the\nefficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism\ninto the FM. The integration of Soft MoEs into the FM allows modality-specific\nexpert specialization alongside shared cross-sensor representation learning. To\ndemonstrate the effectiveness of our adaptation, we apply it on the\nCross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor\nMixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic\ndescriptor-driven sampling strategy for the construction of a representative\nand diverse training set to train our CSMoE model. Extensive experiments on\nscene classification, semantic segmentation, and content-based image retrieval\ndemonstrate that our adaptation yields a reduction in computational\nrequirements while maintaining or improving representational performance.\nCompared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off\nbetween representational capacity, accuracy, and computational efficiency. On\naverage, CSMoE achieves more than twice the computational efficiency of\nexisting RS FMs, while maintaining competitive performance across all\nexperiments. These results show the effectiveness of the proposed adaptation\nfor creating computationally efficient RS FMs. The code for the model, the\ntraining set creation, and the model weights will be available at\nhttps://git.tu-berlin.de/rsim/csmoe.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14104v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14104v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.387,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14119",
      "title": "Generative AI for Misalignment-Resistant Virtual Staining to Accelerate\n  Histopathology Workflows",
      "authors": [
        "Jiabo MA",
        "Wenqiang Li",
        "Jinbang Li",
        "Ziyi Liu",
        "Linshan Wu",
        "Fengtao Zhou",
        "Li Liang",
        "Ronald Cheong Kin Chan",
        "Terence T. W. Wong",
        "Hao Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate histopathological diagnosis often requires multiple differently\nstained tissue sections, a process that is time-consuming, labor-intensive, and\nenvironmentally taxing due to the use of multiple chemical stains. Recently,\nvirtual staining has emerged as a promising alternative that is faster,\ntissue-conserving, and environmentally friendly. However, existing virtual\nstaining methods face significant challenges in clinical applications,\nprimarily due to their reliance on well-aligned paired data. Obtaining such\ndata is inherently difficult because chemical staining processes can distort\ntissue structures, and a single tissue section cannot undergo multiple staining\nprocedures without damage or loss of information. As a result, most available\nvirtual staining datasets are either unpaired or roughly paired, making it\ndifficult for existing methods to achieve accurate pixel-level supervision. To\naddress this challenge, we propose a robust virtual staining framework\nfeaturing cascaded registration mechanisms to resolve spatial mismatches\nbetween generated outputs and their corresponding ground truth. Experimental\nresults demonstrate that our method significantly outperforms state-of-the-art\nmodels across five datasets, achieving an average improvement of 3.2% on\ninternal datasets and 10.1% on external datasets. Moreover, in datasets with\nsubstantial misalignment, our approach achieves a remarkable 23.8% improvement\nin peak signal-to-noise ratio compared to baseline models. The exceptional\nrobustness of the proposed method across diverse datasets simplifies the data\nacquisition process for virtual staining and offers new insights for advancing\nits development.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14119v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14119v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.367,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14120",
      "title": "Deceptive Beauty: Evaluating the Impact of Beauty Filters on Deepfake\n  and Morphing Attack Detection",
      "authors": [
        "Sara Concas",
        "Simone Maurizio La Cava",
        "Andrea Panzino",
        "Ester Masala",
        "Giulia Orrù",
        "Gian Luca Marcialis"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Digital beautification through social media filters has become increasingly\npopular, raising concerns about the reliability of facial images and videos and\nthe effectiveness of automated face analysis. This issue is particularly\ncritical for digital manipulation detectors, systems aiming at distinguishing\nbetween genuine and manipulated data, especially in cases involving deepfakes\nand morphing attacks designed to deceive humans and automated facial\nrecognition. This study examines whether beauty filters impact the performance\nof deepfake and morphing attack detectors. We perform a comprehensive analysis,\nevaluating multiple state-of-the-art detectors on benchmark datasets before and\nafter applying various smoothing filters. Our findings reveal performance\ndegradation, highlighting vulnerabilities introduced by facial enhancements and\nunderscoring the need for robust detection models resilient to such\nalterations.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14120v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14120v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.29,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14142",
      "title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,\n  Results, Discussion, and Outlook",
      "authors": [
        "Peng Xu",
        "Shengwu Xiong",
        "Jiajun Zhang",
        "Yaxiong Chen",
        "Bowen Zhou",
        "Chen Change Loy",
        "David A. Clifton",
        "Kyoung Mu Lee",
        "Luc Van Gool",
        "Ruiming He",
        "Ruilin Yao",
        "Xinwei Long",
        "Jirui Huang",
        "Kai Tian",
        "Sa Yang",
        "Yihua Shao",
        "Jin Feng",
        "Yue Zhong",
        "Jiakai Zhou",
        "Cheng Tang",
        "Tianyu Zou",
        "Yifang Zhang",
        "Junming Liang",
        "Guoyou Li",
        "Zhaoxiang Wang",
        "Qiang Zhou",
        "Yichen Zhao",
        "Shili Xiong",
        "Hyeongjin Nam",
        "Jaerin Lee",
        "Jaeyoung Chung",
        "JoonKyu Park",
        "Junghun Oh",
        "Kanggeon Lee",
        "Wooseok Lee",
        "Juneyoung Ro",
        "Turghun Osman",
        "Can Hu",
        "Chaoyang Liao",
        "Cheng Chen",
        "Chengcheng Han",
        "Chenhao Qiu",
        "Chong Peng",
        "Cong Xu",
        "Dailin Li",
        "Feiyu Wang",
        "Feng Gao",
        "Guibo Zhu",
        "Guopeng Tang",
        "Haibo Lu",
        "Han Fang",
        "Han Qi",
        "Hanxiao Wu",
        "Haobo Cheng",
        "Hongbo Sun",
        "Hongyao Chen",
        "Huayong Hu",
        "Hui Li",
        "Jiaheng Ma",
        "Jiang Yu",
        "Jianing Wang",
        "Jie Yang",
        "Jing He",
        "Jinglin Zhou",
        "Jingxuan Li",
        "Josef Kittler",
        "Lihao Zheng",
        "Linnan Zhao",
        "Mengxi Jia",
        "Muyang Yan",
        "Nguyen Thanh Thien",
        "Pu Luo",
        "Qi Li",
        "Shien Song",
        "Shijie Dong",
        "Shuai Shao",
        "Shutao Li",
        "Taofeng Xue",
        "Tianyang Xu",
        "Tianyi Gao",
        "Tingting Li",
        "Wei Zhang",
        "Weiyang Su",
        "Xiaodong Dong",
        "Xiao-Jun Wu",
        "Xiaopeng Zhou",
        "Xin Chen",
        "Xin Wei",
        "Xinyi You",
        "Xudong Kang",
        "Xujie Zhou",
        "Xusheng Liu",
        "Yanan Wang",
        "Yanbin Huang",
        "Yang Liu",
        "Yang Yang",
        "Yanglin Deng",
        "Yashu Kang",
        "Ye Yuan",
        "Yi Wen",
        "Yicen Tian",
        "Yilin Tao",
        "Yin Tang",
        "Yipeng Lin",
        "Yiqing Wang",
        "Yiting Xi",
        "Yongkang Yu",
        "Yumei Li",
        "Yuxin Qin",
        "Yuying Chen",
        "Yuzhe Cen",
        "Zhaofan Zou",
        "Zhaohong Liu",
        "Zhehao Shen",
        "Zhenglin Du",
        "Zhengyang Li",
        "Zhenni Huang",
        "Zhenwei Shao",
        "Zhilong Song",
        "Zhiyong Feng",
        "Zhiyu Wang",
        "Zhou Yu",
        "Ziang Li",
        "Zihan Zhai",
        "Zijian Zhang",
        "Ziyang Peng",
        "Ziyun Xiao",
        "Zongshu Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim\nto bring together different approaches in multimodal machine learning and LLMs\nvia a large benchmark. We hope it better allows researchers to follow the\nstate-of-the-art in this very dynamic area. Meanwhile, a growing number of\ntestbeds have boosted the evolution of general-purpose large language models.\nThus, this year's MARS2 focuses on real-world and specialized scenarios to\nbroaden the multimodal reasoning applications of MLLMs. Our organizing team\nreleased two tailored datasets Lens and AdsQA as test sets, which support\ngeneral reasoning in 12 daily scenarios and domain-specific reasoning in\nadvertisement videos, respectively. We evaluated 40+ baselines that include\nboth generalist MLLMs and task-specific models, and opened up three competition\ntracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question\nAnswering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative\nAdvertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and\nindustrial institutions have registered and 40+ valid submissions (out of\n1200+) have been included in our ranking lists. Our datasets, code sets (40+\nbaselines and 15+ participants' methods), and rankings are publicly available\non the MARS2 workshop website and our GitHub organization page\nhttps://github.com/mars2workshop/, where our updates and announcements of\nupcoming events will be continuously provided.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14142v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14142v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.391,
      "datasets_score": 0.469,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on multimodal reasoning challenges, datasets, and evaluations using LLMs and MLLMs, but it does not mention diffusion models, iterative refinement processes, or any adaptation of diffusion for logical reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves creating, releasing, and evaluating two new datasets (Lens and AdsQA) for multimodal reasoning tasks, including their curation from real-world sources, benchmarking with baselines, and analysis in competition tracks, which aligns directly with research on dataset creation, analysis, and evaluation.",
      "llm_score_status": "completed",
      "summary": "The MARS2 2025 Challenge on Multimodal Reasoning aims to advance multimodal machine learning and large language models by introducing two new datasets, Lens and AdsQA, designed for general reasoning in 12 daily scenarios and domain-specific reasoning in advertisement videos, respectively. The challenge evaluated over 40 baselines across three tracks—Visual Grounding in Real-world Scenarios, Visual Question Answering with Spatial Awareness, and Visual Reasoning in Creative Advertisement Videos—resulting in participation from 76 teams with more than 1200 submissions, and it provides publicly available datasets, codes, and benchmarks to foster further research on complex, non-stepwise reasoning tasks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing tailored datasets and competition tracks for real-world multimodal reasoning scenarios, combining existing ideas in a new way to address gaps in current benchmarks. However, it does not introduce a truly novel problem or technique, as it builds on established concepts in multimodal machine learning.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research within the subfield of multimodal reasoning by providing publicly available datasets and benchmarks that can be built upon. While it may not have broad commercial applications immediately, it offers valuable resources for academic and industrial AI development.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution by organizing a comprehensive challenge with new datasets and benchmarks, making it essential for researchers in multimodal AI to stay informed. However, it is not groundbreaking enough to be considered must-read for those outside the specific subfield.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8e84909fcadd50be875ce2522d9413d9a9cc6c53",
      "total_authors": 128,
      "authors_found": 118,
      "highest_h_index": 24,
      "average_h_index": 1.0084745762711864,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Peng Xu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Shengwu Xiong",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2325146287"
        },
        {
          "name": "Jiajun Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362645993"
        },
        {
          "name": "Yaxiong Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2360838839"
        },
        {
          "name": "Bowen Zhou",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2310664569"
        },
        {
          "name": "Chen Change Loy",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2376314804"
        },
        {
          "name": "David Clifton",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2355419127"
        },
        {
          "name": "Kyoung Mu Lee",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2267862169"
        },
        {
          "name": "L. V. Gool",
          "h_index": 24,
          "profile_url": "https://www.semanticscholar.org/author/2246990749"
        },
        {
          "name": "Ruiming He",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2383961870"
        },
        {
          "name": "Rui Yao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2352107831"
        },
        {
          "name": "Xinwei Long",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2261501697"
        },
        {
          "name": "Jirui Huang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2287516494"
        },
        {
          "name": "Kai Tian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2322672437"
        },
        {
          "name": "Sa Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375953793"
        },
        {
          "name": "Yihua Shao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jin Feng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2159709554"
        },
        {
          "name": "Yue Zhong",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jiakai Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380691188"
        },
        {
          "name": "Cheng Tang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380802801"
        },
        {
          "name": "Tianyu Zou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362627227"
        },
        {
          "name": "Yifang Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2293711008"
        },
        {
          "name": "Junming Liang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380848951"
        },
        {
          "name": "Guoyou Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362638366"
        },
        {
          "name": "Zhaoxiang Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381060607"
        },
        {
          "name": "Qiang Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2364116896"
        },
        {
          "name": "Yichen Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2282665416"
        },
        {
          "name": "Shili Xiong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2371071654"
        },
        {
          "name": "Hyeongjin Nam",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2177210577"
        },
        {
          "name": "Jaerin Lee",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/71710083"
        },
        {
          "name": "Jaeyoung Chung",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "JoonKyu Park",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380698139"
        },
        {
          "name": "Junghun Oh",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2110913667"
        },
        {
          "name": "K. Lee",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2211986119"
        },
        {
          "name": "Wooseok Lee",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380952057"
        },
        {
          "name": "Juneyoung Ro",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685810"
        },
        {
          "name": "Turghun Osman",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380686031"
        },
        {
          "name": "Can Hu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380839015"
        },
        {
          "name": "Chao Liao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2255944710"
        },
        {
          "name": "Cheng Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381426429"
        },
        {
          "name": "Chengcheng Han",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Chenhao Qiu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372602182"
        },
        {
          "name": "Chong Peng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2233563996"
        },
        {
          "name": "Cong Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376270647"
        },
        {
          "name": "Daili Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2350430550"
        },
        {
          "name": "Feiyu Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354943646"
        },
        {
          "name": "Feng Gao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2370173487"
        },
        {
          "name": "Guibo Zhu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2364916885"
        },
        {
          "name": "Gu-xiu Tang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354017945"
        },
        {
          "name": "Haibo Lu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380694296"
        },
        {
          "name": "Han Fang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Han Qi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2171737175"
        },
        {
          "name": "Han Wu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2269776516"
        },
        {
          "name": "Hao Cheng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2269858499"
        },
        {
          "name": "Hongbo Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376011144"
        },
        {
          "name": "Hongyao Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363825795"
        },
        {
          "name": "Huayong Hu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380752759"
        },
        {
          "name": "Hui Li",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2155494585"
        },
        {
          "name": "Jiaheng Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376441926"
        },
        {
          "name": "Jiang Yu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380865861"
        },
        {
          "name": "Jianing Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2348878507"
        },
        {
          "name": "Jie Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2366785213"
        },
        {
          "name": "Jing He",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2373701867"
        },
        {
          "name": "Jinglin Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377759501"
        },
        {
          "name": "Jingxuan Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375947577"
        },
        {
          "name": "J. Kittler",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2193203460"
        },
        {
          "name": "Lihao Zheng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2302448325"
        },
        {
          "name": "Linnan Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381069347"
        },
        {
          "name": "Meng-Xiao Jia",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2314797548"
        },
        {
          "name": "Muyang Yan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381295314"
        },
        {
          "name": "Nguyen Thanh Thien",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/1388178524"
        },
        {
          "name": "Pu Luo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376267424"
        },
        {
          "name": "Qi Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2227868023"
        },
        {
          "name": "S. Song",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2252241397"
        },
        {
          "name": "Shijie Dong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381268299"
        },
        {
          "name": "Shuai Shao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379398883"
        },
        {
          "name": "Shutao Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380797199"
        },
        {
          "name": "Taofeng Xue",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2325587937"
        },
        {
          "name": "Tianyang Xu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Tianyi Gao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Tingting Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2358599912"
        },
        {
          "name": "Wei Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372612434"
        },
        {
          "name": "Wei Su",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2327960751"
        },
        {
          "name": "Xiaodong Dong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2260853089"
        },
        {
          "name": "Xiao-Jun Wu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2285402611"
        },
        {
          "name": "Xiaopeng Zhou",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2155799655"
        },
        {
          "name": "Xin Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2260023714"
        },
        {
          "name": "Xin Wei",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2310320886"
        },
        {
          "name": "Xin You",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2310458773"
        },
        {
          "name": "Xu Kang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377954250"
        },
        {
          "name": "Xujie Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381130949"
        },
        {
          "name": "Xusheng Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380691729"
        },
        {
          "name": "Yanan Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2307742017"
        },
        {
          "name": "Yanbin Huang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2318985828"
        },
        {
          "name": "Yang Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379985045"
        },
        {
          "name": "Yang Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2267864112"
        },
        {
          "name": "Yanglin Deng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2317133102"
        },
        {
          "name": "Yashu Kang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380800070"
        },
        {
          "name": "Ye Yuan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375089452"
        },
        {
          "name": "Yi Wen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2215813580"
        },
        {
          "name": "Yi-He Tian",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2282354045"
        },
        {
          "name": "Yilin Tao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382460120"
        },
        {
          "name": "Yin Tang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379939119"
        },
        {
          "name": "Yi-Miao Lin",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2107915473"
        },
        {
          "name": "Yiqing Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2330150289"
        },
        {
          "name": "Yiting Xi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380688377"
        },
        {
          "name": "Yongkang Yu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380755261"
        },
        {
          "name": "Yumei Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376276023"
        },
        {
          "name": "Yuxin Qin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2351717260"
        },
        {
          "name": "Yuying Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2135803321"
        },
        {
          "name": "Yuzhe Cen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685578"
        },
        {
          "name": "Zhao-ju Zou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2252187796"
        },
        {
          "name": "Zhao-Ji Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2371162686"
        },
        {
          "name": "Zhehao Shen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2383880572"
        },
        {
          "name": "Zhenglin Du",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhengyang Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354707639"
        },
        {
          "name": "Zhenni Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380838920"
        },
        {
          "name": "Zhenwei Shao",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2210731803"
        },
        {
          "name": "Zhi-Gang Song",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2256341898"
        },
        {
          "name": "Zhi-Tao Feng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2178833785"
        },
        {
          "name": "Zhiyu Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2368563604"
        },
        {
          "name": "Zhou Yu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2281792709"
        },
        {
          "name": "Ziang Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2349380923"
        },
        {
          "name": "Zihan Zhai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685856"
        },
        {
          "name": "Zijian Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2327347389"
        },
        {
          "name": "Zi-Qi Peng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2313267646"
        },
        {
          "name": "Ziyun Xiao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zongshu Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2339240186"
        }
      ]
    },
    {
      "id": "2509.14149",
      "title": "An Exploratory Study on Abstract Images and Visual Representations\n  Learned from Them",
      "authors": [
        "Haotian Li",
        "Jianbo Jiao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Imagine living in a world composed solely of primitive shapes, could you\nstill recognise familiar objects? Recent studies have shown that abstract\nimages-constructed by primitive shapes-can indeed convey visual semantic\ninformation to deep learning models. However, representations obtained from\nsuch images often fall short compared to those derived from traditional raster\nimages. In this paper, we study the reasons behind this performance gap and\ninvestigate how much high-level semantic content can be captured at different\nabstraction levels. To this end, we introduce the Hierarchical Abstraction\nImage Dataset (HAID), a novel data collection that comprises abstract images\ngenerated from normal raster images at multiple levels of abstraction. We then\ntrain and evaluate conventional vision systems on HAID across various tasks\nincluding classification, segmentation, and object detection, providing a\ncomprehensive study between rasterised and abstract image representations. We\nalso discuss if the abstract image can be considered as a potentially effective\nformat for conveying visual semantic information and contributing to vision\ntasks.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14149v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14149v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.317,
      "datasets_score": 0.388,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves studying abstract images, datasets like HAID, and their impact on vision tasks such as classification and segmentation. It does not discuss diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14151",
      "title": "BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View\n  3D Object Detection",
      "authors": [
        "Rongyu Zhang",
        "Jiaming Liu",
        "Xiaoqi Li",
        "Xiaowei Chi",
        "Dan Wang",
        "Li Du",
        "Yuan Du",
        "Shanghang Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision-centric Bird's Eye View (BEV) perception holds considerable promise\nfor autonomous driving. Recent studies have prioritized efficiency or accuracy\nenhancements, yet the issue of domain shift has been overlooked, leading to\nsubstantial performance degradation upon transfer. We identify major domain\ngaps in real-world cross-domain scenarios and initiate the first effort to\naddress the Domain Adaptation (DA) challenge in multi-view 3D object detection\nfor BEV perception. Given the complexity of BEV perception approaches with\ntheir multiple components, domain shift accumulation across multi-geometric\nspaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain\nadaptation. In this paper, we introduce an innovative geometric-aware\nteacher-student framework, BEVUDA++, to diminish this issue, comprising a\nReliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model.\nSpecifically, RDT effectively blends target LiDAR with dependable depth\npredictions to generate depth-aware information based on uncertainty\nestimation, enhancing the extraction of Voxel and BEV features that are\nessential for understanding the target domain. To collaboratively reduce the\ndomain shift, GCS maps features from multiple spaces into a unified geometric\nembedding space, thereby narrowing the gap in data distribution between the two\ndomains. Additionally, we introduce a novel Uncertainty-guided Exponential\nMoving Average (UEMA) to further reduce error accumulation due to domain shifts\ninformed by previously obtained uncertainty guidance. To demonstrate the\nsuperiority of our proposed method, we execute comprehensive experiments in\nfour cross-domain scenarios, securing state-of-the-art performance in BEV 3D\nobject detection tasks, e.g., 12.9\\% NDS and 9.5\\% mAP enhancement on Day-Night\nadaptation.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14151v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14151v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.371,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14165",
      "title": "Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High\n  Resolutions",
      "authors": [
        "Michal Szczepanski",
        "Martyna Poreba",
        "Karim Haroun"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Vision Transformers (ViTs) achieve state-of-the-art performance in semantic\nsegmentation but are hindered by high computational and memory costs. To\naddress this, we propose STEP (SuperToken and Early-Pruning), a hybrid\ntoken-reduction framework that combines dynamic patch merging and token pruning\nto enhance efficiency without significantly compromising accuracy. At the core\nof STEP is dCTS, a lightweight CNN-based policy network that enables flexible\nmerging into superpatches. Encoder blocks integrate also early-exits to remove\nhigh-confident supertokens, lowering computational load. We evaluate our method\non high-resolution semantic segmentation benchmarks, including images up to\n1024 x 1024, and show that when dCTS is applied alone, the token count can be\nreduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching\nscheme. This yields a 2.6x reduction in computational cost and a 3.4x increase\nin throughput when using ViT-Large as the backbone. Applying the full STEP\nframework further improves efficiency, reaching up to a 4x reduction in\ncomputational complexity and a 1.7x gain in inference speed, with a maximum\naccuracy drop of no more than 2.0%. With the proposed STEP configurations, up\nto 40% of tokens can be confidently predicted and halted before reaching the\nfinal encoder layer.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14165v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14165v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.281,
      "weak_supervision_score": 0.307,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.413,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is the development of the STEP framework for efficient Vision Transformers in semantic segmentation, focusing on token pruning and dynamic patch merging to reduce computational costs on a single GPU. It does not address distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation, making it unrelated to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14172",
      "title": "TGPO: Tree-Guided Preference Optimization for Robust Web Agent\n  Reinforcement Learning",
      "authors": [
        "Ziyuan Chen",
        "Zhenghui Zhao",
        "Zhangye Han",
        "Miancan Liu",
        "Xianhang Ye",
        "Yiqing Li",
        "Hongbo Min",
        "Jinkui Ren",
        "Xiantao Zhang",
        "Guitao Cao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "With the rapid advancement of large language models and vision-language\nmodels, employing large models as Web Agents has become essential for automated\nweb interaction. However, training Web Agents with reinforcement learning faces\ncritical challenges including credit assignment misallocation, prohibitively\nhigh annotation costs, and reward sparsity. To address these issues, we propose\nTree-Guided Preference Optimization (TGPO), an offline reinforcement learning\nframework that proposes a tree-structured trajectory representation merging\nsemantically identical states across trajectories to eliminate label conflicts.\nOur framework incorporates a Process Reward Model that automatically generates\nfine-grained rewards through subgoal progress, redundancy detection, and action\nverification. Additionally, a dynamic weighting mechanism prioritizes\nhigh-impact decision points during training. Experiments on Online-Mind2Web and\nour self-constructed C-WebShop datasets demonstrate that TGPO significantly\noutperforms existing methods, achieving higher success rates with fewer\nredundant steps.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14172v2",
      "pdf_url": "http://arxiv.org/pdf/2509.14172v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.507,
      "weak_supervision_score": 0.415,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.393,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes TGPO, an offline RL framework that automates reward generation using a Process Reward Model, without relying on human-ranked data or a separate reward model trained on human feedback. It focuses on algorithmic improvements for web agents, not aligning models with human preferences, so it does not qualify as RLHF.",
      "weak_supervision_justification": "The paper's TGPO framework uses programmatic methods to generate fine-grained rewards automatically from trajectories, such as through subgoal progress and redundancy detection, reducing the need for manual annotations. This aligns directly with weak supervision by leveraging noisy or imprecise sources to create training signals, addressing the challenge of high annotation costs.",
      "diffusion_reasoning_justification": "The paper focuses on tree-structured trajectory representation and RL for web agents, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. It deals with preference optimization in RL, not adapting diffusion for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning framework aimed at addressing key challenges in training web agents, including credit assignment misallocation, high annotation costs, and reward sparsity. TGPO employs a tree-structured trajectory representation to merge semantically identical states, integrates a Process Reward Model for automatic fine-grained reward generation, and uses dynamic weighting to focus on high-impact decisions, resulting in superior performance with higher success rates and fewer redundant actions on datasets like Online-Mind2Web and C-WebShop compared to existing methods.",
      "novelty_score": "High",
      "novelty_justification": "TGPO introduces a truly new technique by combining tree-structured trajectory representation with automated reward modeling, significantly advancing the state-of-the-art in web agent reinforcement learning by effectively resolving longstanding challenges.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of web agents and offline reinforcement learning due to its improved efficiency and success rates, though its influence may be confined to specific AI applications rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, innovative contribution to web agent training that addresses critical issues, making it essential for researchers in machine learning and AI to understand its methods and potential applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0221b14ea30d3c0b4941c00e0065daa968bd9154",
      "total_authors": 10,
      "authors_found": 9,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ziyuan Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381349880"
        },
        {
          "name": "Zhenghui Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381130286"
        },
        {
          "name": "Zhangye Han",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381145932"
        },
        {
          "name": "Miancan Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380770722"
        },
        {
          "name": "Xianhang Ye",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yiqing Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380779592"
        },
        {
          "name": "Hongbo Min",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380690049"
        },
        {
          "name": "Jinkui Ren",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381379017"
        },
        {
          "name": "Xiantao Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380690535"
        },
        {
          "name": "Guitao Cao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380726670"
        }
      ]
    },
    {
      "id": "2509.14180",
      "title": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation\n  Framework for Personal Finance LLMs",
      "authors": [
        "Akhil Theerthala"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Personalized financial advice requires consideration of user goals,\nconstraints, risk tolerance, and jurisdiction. Prior LLM work has focused on\nsupport systems for investors and financial planners. Simultaneously, numerous\nrecent studies examine broader personal finance tasks, including budgeting,\ndebt management, retirement, and estate planning, through agentic pipelines\nthat incur high maintenance costs, yielding less than 25% of their expected\nfinancial returns. In this study, we introduce a novel and reproducible\nframework that integrates relevant financial context with behavioral finance\nstudies to construct supervision data for end-to-end advisors. Using this\nframework, we create a 19k sample reasoning dataset and conduct a comprehensive\nfine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test\nsplit and a blind LLM-jury study, we demonstrate that through careful data\ncuration and behavioral integration, our 8B model achieves performance\ncomparable to significantly larger baselines (14-32B parameters) across factual\naccuracy, fluency, and personalization metrics while incurring 80% lower costs\nthan the larger counterparts.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14180v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14180v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.467,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.51,
      "distributed_training_score": 0.326,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on generating a dataset via a framework and fine-tuning a model (Qwen-3-8B) using supervision data verified by LLM juries, but it does not involve training a reward model or using reinforcement learning to align with human preferences. There is no mention of RLHF elements like policy optimization based on human feedback.",
      "weak_supervision_justification": "The paper describes programmatically generating a 19k sample dataset using a framework that integrates financial context and verifies generations with LLM juries, which aligns with weak supervision by relying on noisy, automated labeling sources rather than hand-labeled data. However, it is not the primary focus, as the emphasis is on data curation for fine-tuning rather than purely weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper introduces a framework for synthesizing reasoning chains but does not mention or adapt diffusion models for iterative refinement in logical tasks. It focuses on chain-of-thought generation and verification without any multi-step diffusion-based processes for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces a novel framework for synthesizing behaviorally-grounded reasoning chains to generate supervision data for training large language models (LLMs) in personal finance, addressing limitations in existing agentic systems by integrating financial context, behavioral finance, and psychological insights. The methodology involves creating a 19k sample dataset and fine-tuning the Qwen-3-8B model, which achieves performance comparable to larger models (14-32B parameters) in accuracy, fluency, and personalization while reducing costs by 80%, as demonstrated through held-out tests and LLM-jury evaluations.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing financial and behavioral concepts into a new data-generation framework for LLMs, offering a clever way to enhance personalization and reduce costs compared to agentic pipelines. However, it builds on prior work in LLMs for finance rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in AI for personal finance by providing a cost-effective framework for building trustworthy LLMs, potentially leading to more efficient applications in subfields like financial advising. Nonetheless, its applicability may be limited to specific domains and not broadly transformative across all AI areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, practical contribution with its innovative data-centric approach to improving LLMs for finance, making it valuable for researchers in AI and computational language to understand for advancing personalized systems. While insightful, it is not essential for those outside the immediate subfield.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/cad6d2a7dbb55275e8c235f9961772aecf61744d",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Akhil Theerthala",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380687911"
        }
      ]
    },
    {
      "id": "2509.14181",
      "title": "Bridging Past and Future: Distribution-Aware Alignment for Time Series\n  Forecasting",
      "authors": [
        "Yifan Hu",
        "Jie Yang",
        "Tian Zhou",
        "Peiyuan Liu",
        "Yujin Tang",
        "Rong Jin",
        "Liang Sun"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Although contrastive and other representation-learning methods have long been\nexplored in vision and NLP, their adoption in modern time series forecasters\nremains limited. We believe they hold strong promise for this domain. To unlock\nthis potential, we explicitly align past and future representations, thereby\nbridging the distributional gap between input histories and future targets. To\nthis end, we introduce TimeAlign, a lightweight, plug-and-play framework that\nestablishes a new representation paradigm, distinct from contrastive learning,\nby aligning auxiliary features via a simple reconstruction task and feeding\nthem back into any base forecaster. Extensive experiments across eight\nbenchmarks verify its superior performance. Further studies indicate that the\ngains arise primarily from correcting frequency mismatches between historical\ninputs and future outputs. Additionally, we provide two theoretical\njustifications for how reconstruction improves forecasting generalization and\nhow alignment increases the mutual information between learned representations\nand predicted targets. The code is available at\nhttps://github.com/TROUBADOUR000/TimeAlign.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14181v2",
      "pdf_url": "http://arxiv.org/pdf/2509.14181v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.383,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14191",
      "title": "MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for\n  High-Fidelity Mapping",
      "authors": [
        "Zhihao Cao",
        "Hanyu Wu",
        "Li Wa Tang",
        "Zizhou Luo",
        "Zihan Zhu",
        "Wei Zhang",
        "Marc Pollefeys",
        "Martin R. Oswald"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent progress in dense SLAM has primarily targeted monocular setups, often\nat the expense of robustness and geometric coverage. We present MCGS-SLAM, the\nfirst purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting\n(3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM\nfuses dense RGB inputs from multiple viewpoints into a unified, continuously\noptimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines\nposes and depths via dense photometric and geometric residuals, while a scale\nconsistency module enforces metric alignment across views using low-rank\npriors. The system supports RGB input and maintains real-time performance at\nlarge scale. Experiments on synthetic and real-world datasets show that\nMCGS-SLAM consistently yields accurate trajectories and photorealistic\nreconstructions, usually outperforming monocular baselines. Notably, the wide\nfield of view from multi-camera input enables reconstruction of side-view\nregions that monocular setups miss, critical for safe autonomous operation.\nThese results highlight the promise of multi-camera Gaussian Splatting SLAM for\nhigh-fidelity mapping in robotics and autonomous driving.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14191v2",
      "pdf_url": "http://arxiv.org/pdf/2509.14191v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.266,
      "weak_supervision_score": 0.307,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.337,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14195",
      "title": "Hierarchical Learning for Maze Navigation: Emergence of Mental\n  Representations via Second-Order Learning",
      "authors": [
        "Shalima Binta Manir",
        "Tim Oates"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Mental representation, characterized by structured internal models mirroring\nexternal environments, is fundamental to advanced cognition but remains\nchallenging to investigate empirically. Existing theory hypothesizes that\nsecond-order learning -- learning mechanisms that adapt first-order learning\n(i.e., learning about the task/domain) -- promotes the emergence of such\nenvironment-cognition isomorphism. In this paper, we empirically validate this\nhypothesis by proposing a hierarchical architecture comprising a Graph\nConvolutional Network (GCN) as a first-order learner and an MLP controller as a\nsecond-order learner. The GCN directly maps node-level features to predictions\nof optimal navigation paths, while the MLP dynamically adapts the GCN's\nparameters when confronting structurally novel maze environments. We\ndemonstrate that second-order learning is particularly effective when the\ncognitive system develops an internal mental map structurally isomorphic to the\nenvironment. Quantitative and qualitative results highlight significant\nperformance improvements and robust generalization on unseen maze tasks,\nproviding empirical support for the pivotal role of structured mental\nrepresentations in maximizing the effectiveness of second-order learning.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14195v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14195v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.466,
      "distributed_training_score": 0.32,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on hierarchical learning using Graph Convolutional Networks (GCNs) and Multi-Layer Perceptrons (MLPs) for maze navigation and the emergence of mental representations through second-order learning. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic. There is no component related to treating a 'Chain-of-Thought' as a single entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14199",
      "title": "Dense Video Understanding with Gated Residual Tokenization",
      "authors": [
        "Haichao Zhang",
        "Wenhao Chai",
        "Shwai He",
        "Ang Li",
        "Yun Fu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "High temporal resolution is essential for capturing fine-grained details in\nvideo understanding. However, current video large language models (VLLMs) and\nbenchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or\nkeyframe selection, discarding dense temporal information. This compromise\navoids the high cost of tokenizing every frame, which otherwise leads to\nredundant computation and linear token growth as video length increases. While\nthis trade-off works for slowly changing content, it fails for tasks like\nlecture comprehension, where information appears in nearly every frame and\nrequires precise temporal alignment. To address this gap, we introduce Dense\nVideo Understanding (DVU), which enables high-FPS video comprehension by\nreducing both tokenization time and token overhead. Existing benchmarks are\nalso limited, as their QA pairs focus on coarse content changes. We therefore\npropose DIVE (Dense Information Video Evaluation), the first benchmark designed\nfor dense temporal reasoning. To make DVU practical, we present Gated Residual\nTokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated\nTokenization uses pixel-level motion estimation to skip static regions during\ntokenization, achieving sub-linear growth in token count and compute. (2)\nSemantic-Scene Intra-Tokenization Merging fuses tokens across static regions\nwithin a scene, further reducing redundancy while preserving dynamic semantics.\nExperiments on DIVE show that GRT outperforms larger VLLM baselines and scales\npositively with FPS. These results highlight the importance of dense temporal\ninformation and demonstrate that GRT enables efficient, scalable high-FPS video\nunderstanding.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14199v2",
      "pdf_url": "http://arxiv.org/pdf/2509.14199v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.466,
      "distributed_training_score": 0.426,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on efficient tokenization for high-FPS video understanding using techniques like motion estimation and token merging, with no mention of diffusion models, iterative refinement processes, or adapting diffusion for multi-step logical reasoning.",
      "distributed_training_justification": "The paper discusses patch-level parallelization in tokenization to improve efficiency, but it does not address distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation in multi-node machine learning setups.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14216",
      "title": "A Universal Banach--Bregman Framework for Stochastic Iterations:\n  Unifying Stochastic Mirror Descent, Learning and LLM Training",
      "authors": [
        "Johnny R. Zhang",
        "Xiaomei Mi",
        "Gaoyuan Du",
        "Qianyi Sun",
        "Shiqi Wang",
        "Jiaxuan Li",
        "Wenhua Zhou"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Stochastic optimization powers the scalability of modern artificial\nintelligence, spanning machine learning, deep learning, reinforcement learning,\nand large language model training. Yet, existing theory remains largely\nconfined to Hilbert spaces, relying on inner-product frameworks and\northogonality. This paradigm fails to capture non-Euclidean settings, such as\nmirror descent on simplices, Bregman proximal methods for sparse learning,\nnatural gradient descent in information geometry, or\nKullback--Leibler-regularized language model training. Unlike Euclidean-based\nHilbert-space methods, this approach embraces general Banach spaces. This work\nintroduces a pioneering Banach--Bregman framework for stochastic iterations,\nestablishing Bregman geometry as a foundation for next-generation optimization.\nIt (i) provides a unified template via Bregman projections and Bregman--Fejer\nmonotonicity, encompassing stochastic approximation, mirror descent, natural\ngradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations\n($\\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and\nelucidating their acceleration effect; and (iii) delivers convergence theorems\nspanning almost-sure boundedness to geometric rates, validated on synthetic and\nreal-world tasks. Empirical studies across machine learning (UCI benchmarks),\ndeep learning (e.g., Transformer training), reinforcement learning\n(actor--critic), and large language models (WikiText-2 with distilGPT-2) show\nup to 20% faster convergence, reduced variance, and enhanced accuracy over\nclassical baselines. These results position Banach--Bregman geometry as a\ncornerstone unifying optimization theory and practice across core AI paradigms.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14216v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14216v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.394,
      "distributed_training_score": 0.429,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on a unified Banach-Bregman framework for stochastic optimization, which enhances methods like mirror descent and natural gradient descent, and includes empirical studies on tasks such as Transformer training and LLM training. While these tasks often involve distributed training in practice, the paper does not directly address distributed computing, parallel processing, or strategies for partitioning data/computation across multiple nodes. Thus, it has a loose connection through improved optimization techniques that could potentially benefit distributed systems, but it is not a core focus.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14223",
      "title": "Fresh in memory: Training-order recency is linearly encoded in language\n  model activations",
      "authors": [
        "Dmitrii Krasheninnikov",
        "Richard E. Turner",
        "David Krueger"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "We show that language models' activations linearly encode when information\nwas learned during training. Our setup involves creating a model with a known\ntraining order by sequentially fine-tuning Llama-3.2-1B on six disjoint but\notherwise similar datasets about named entities. We find that the average\nactivations of test samples corresponding to the six training datasets encode\nthe training order: when projected into a 2D subspace, these centroids are\narranged exactly in the order of training and lie on a straight line. Further,\nwe show that linear probes can accurately (~90%) distinguish \"early\" vs. \"late\"\nentities, generalizing to entities unseen during the probes' own training. The\nmodel can also be fine-tuned to explicitly report an unseen entity's training\nstage (~80% accuracy). Interestingly, the training-order encoding does not seem\nattributable to simple differences in activation magnitudes, losses, or model\nconfidence. Our paper demonstrates that models are capable of differentiating\ninformation by its acquisition time, and carries significant implications for\nhow they might manage conflicting data and respond to knowledge modifications.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14223v2",
      "pdf_url": "http://arxiv.org/pdf/2509.14223v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.387,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on how language model activations encode training order through sequential fine-tuning, without any mention of human feedback, reward models, or reinforcement learning techniques. It does not involve aligning models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines activation patterns in language models related to training sequences, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. There is no component adapting diffusion for reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14227",
      "title": "Cinéaste: A Fine-grained Contextual Movie Question Answering\n  Benchmark",
      "authors": [
        "Nisarg A. Shah",
        "Amir Ziai",
        "Chaitanya Ekanadham",
        "Vishal M. Patel"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "While recent advancements in vision-language models have improved video\nunderstanding, diagnosing their capacity for deep, narrative comprehension\nremains a challenge. Existing benchmarks often test short-clip recognition or\nuse template-based questions, leaving a critical gap in evaluating fine-grained\nreasoning over long-form narrative content. To address these gaps, we introduce\n$\\mathsf{Cin\\acute{e}aste}$, a comprehensive benchmark for long-form movie\nunderstanding. Our dataset comprises 3,119 multiple-choice question-answer\npairs derived from 1,805 scenes across 200 diverse movies, spanning five novel\nfine-grained contextual reasoning categories. We use GPT-4o to generate\ndiverse, context-rich questions by integrating visual descriptions, captions,\nscene titles, and summaries, which require deep narrative understanding. To\nensure high-quality evaluation, our pipeline incorporates a two-stage filtering\nprocess: Context-Independence filtering ensures questions require video\ncontext, while Contextual Veracity filtering validates factual consistency\nagainst the movie content, mitigating hallucinations. Experiments show that\nexisting MLLMs struggle on $\\mathsf{Cin\\acute{e}aste}$; our analysis reveals\nthat long-range temporal reasoning is a primary bottleneck, with the top\nopen-source model achieving only 63.15\\% accuracy. This underscores significant\nchallenges in fine-grained contextual understanding and the need for\nadvancements in long-form movie comprehension.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14227v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14227v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.321,
      "datasets_score": 0.432,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a new benchmark for movie question answering using GPT-4o for QA generation and evaluating MLLMs, but it does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical reasoning tasks. There is no mention of treating a chain-of-thought as a single entity for correction, making this topic unrelated.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new benchmark dataset, Cinéaste, including its creation, curation methodologies (e.g., using GPT-4o with a two-stage filtering process), and evaluation of models on it. This directly aligns with research on dataset creation, benchmarking, and analysis for AI applications, such as video understanding.",
      "llm_score_status": "completed",
      "summary": "The paper introduces Cinéaste, a new benchmark designed to evaluate fine-grained contextual understanding in long-form movies, addressing limitations in existing datasets by focusing on deep narrative comprehension through 3,119 multiple-choice QA pairs derived from 1,805 scenes across 200 movies and categorized into five reasoning types: Visual Reasoning, State Changes, Temporal Ordering, Cause and Effect, and Message Understanding. The methodology involves using GPT-4o to generate context-rich questions from multimodal inputs, followed by a two-stage filtering process to ensure contextual dependency and factual accuracy, with experiments demonstrating that state-of-the-art MLLMs achieve only around 63% accuracy, primarily struggling with long-range temporal reasoning and underscoring the need for improved video understanding capabilities.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark with novel reasoning categories and a robust QA generation pipeline incorporating advanced filtering, significantly advancing the state-of-the-art in evaluating fine-grained narrative comprehension in long-form videos.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research in multi-modal models by providing a comprehensive benchmark that highlights key limitations in long-form video understanding, potentially leading to broader advancements in AI for narrative comprehension and commercial applications like video analysis tools.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to computer vision and AI research by offering a new diagnostic tool for MLLMs, making it essential for researchers focused on video understanding to be aware of its insights and implications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e0394f5cd8f42d59185970c07f26098caea2515b",
      "total_authors": 4,
      "authors_found": 3,
      "highest_h_index": 12,
      "average_h_index": 4.333333333333333,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Nisarg A. Shah",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2256478022"
        },
        {
          "name": "Amir Ziai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380686322"
        },
        {
          "name": "Chaitanya Ekanadham",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/1779306"
        },
        {
          "name": "Vishal M. Patel",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2509.14232",
      "title": "GenExam: A Multidisciplinary Text-to-Image Exam",
      "authors": [
        "Zhaokai Wang",
        "Penghao Yin",
        "Xiangyu Zhao",
        "Changyao Tian",
        "Yu Qiao",
        "Wenhai Wang",
        "Jifeng Dai",
        "Gen Luo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Exams are a fundamental test of expert-level intelligence and require\nintegrated understanding, reasoning, and generation. Existing exam-style\nbenchmarks mainly focus on understanding and reasoning tasks, and current\ngeneration benchmarks emphasize the illustration of world knowledge and visual\nconcepts, neglecting the evaluation of rigorous drawing exams. We introduce\nGenExam, the first benchmark for multidisciplinary text-to-image exams,\nfeaturing 1,000 samples across 10 subjects with exam-style prompts organized\nunder a four-level taxonomy. Each problem is equipped with ground-truth images\nand fine-grained scoring points to enable a precise evaluation of semantic\ncorrectness and visual plausibility. Experiments show that even\nstate-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve\nless than 15% strict scores, and most models yield almost 0%, suggesting the\ngreat challenge of our benchmark. By framing image generation as an exam,\nGenExam offers a rigorous assessment of models' ability to integrate\nunderstanding, reasoning, and generation, providing insights on the path to\ngeneral AGI. Our benchmark and evaluation code are released at\nhttps://github.com/OpenGVLab/GenExam.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14232v2",
      "pdf_url": "http://arxiv.org/pdf/2509.14232v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.291,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.323,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces GenExam, a benchmark for evaluating text-to-image models on multidisciplinary exam-style tasks, focusing on understanding, reasoning, and generation capabilities. However, it does not discuss or adapt the iterative refinement process of diffusion models for multi-step logical reasoning tasks. The paper evaluates existing models like GPT-Image-1 and Gemini-2.5-Flash-Image but does not feature any components related to treating a 'Chain-of-Thought' as an entity for holistic correction in a diffusion-based framework. Thus, there is no clear connection to diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14233",
      "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language\n  Environments",
      "authors": [
        "Alejandro Hernández-Cano",
        "Alexander Hägele",
        "Allen Hao Huang",
        "Angelika Romanou",
        "Antoni-Joan Solergibert",
        "Barna Pasztor",
        "Bettina Messmer",
        "Dhia Garbaya",
        "Eduard Frank Ďurech",
        "Ido Hakimi",
        "Juan García Giraldo",
        "Mete Ismayilzada",
        "Negar Foroutan",
        "Skander Moalla",
        "Tiancheng Chen",
        "Vinko Sabolčec",
        "Yixuan Xu",
        "Michael Aerni",
        "Badr AlKhamissi",
        "Ines Altemir Marinas",
        "Mohammad Hossein Amani",
        "Matin Ansaripour",
        "Ilia Badanin",
        "Harold Benoit",
        "Emanuela Boros",
        "Nicholas Browning",
        "Fabian Bösch",
        "Maximilian Böther",
        "Niklas Canova",
        "Camille Challier",
        "Clement Charmillot",
        "Jonathan Coles",
        "Jan Deriu",
        "Arnout Devos",
        "Lukas Drescher",
        "Daniil Dzenhaliou",
        "Maud Ehrmann",
        "Dongyang Fan",
        "Simin Fan",
        "Silin Gao",
        "Miguel Gila",
        "María Grandury",
        "Diba Hashemi",
        "Alexander Hoyle",
        "Jiaming Jiang",
        "Mark Klein",
        "Andrei Kucharavy",
        "Anastasiia Kucherenko",
        "Frederike Lübeck",
        "Roman Machacek",
        "Theofilos Manitaras",
        "Andreas Marfurt",
        "Kyle Matoba",
        "Simon Matrenok",
        "Henrique Mendoncça",
        "Fawzi Roberto Mohamed",
        "Syrielle Montariol",
        "Luca Mouchel",
        "Sven Najem-Meyer",
        "Jingwei Ni",
        "Gennaro Oliva",
        "Matteo Pagliardini",
        "Elia Palme",
        "Andrei Panferov",
        "Léo Paoletti",
        "Marco Passerini",
        "Ivan Pavlov",
        "Auguste Poiroux",
        "Kaustubh Ponkshe",
        "Nathan Ranchin",
        "Javi Rando",
        "Mathieu Sauser",
        "Jakhongir Saydaliev",
        "Muhammad Ali Sayfiddinov",
        "Marian Schneider",
        "Stefano Schuppli",
        "Marco Scialanga",
        "Andrei Semenov",
        "Kumar Shridhar",
        "Raghav Singhal",
        "Anna Sotnikova",
        "Alexander Sternfeld",
        "Ayush Kumar Tarun",
        "Paul Teiletche",
        "Jannis Vamvas",
        "Xiaozhe Yao",
        "Hao Zhao Alexander Ilic",
        "Ana Klimovic",
        "Andreas Krause",
        "Caglar Gulcehre",
        "David Rosenthal",
        "Elliott Ash",
        "Florian Tramèr",
        "Joost VandeVondele",
        "Livio Veraldi",
        "Martin Rajman",
        "Thomas Schulthess",
        "Torsten Hoefler",
        "Antoine Bosselut",
        "Martin Jaggi",
        "Imanol Schlag"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14233v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14233v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.452,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.436,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions QRPO (Quantile Reward Policy Optimization) as a related publication and innovation for post-training, which could involve reward-based optimization similar to RLHF elements. However, the main contributions focus on data compliance, multilingual training, and general post-training, without explicitly describing human feedback, reward models, or RL for fine-tuning the main models.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper details training the Apertus models on up to 4096 GPUs, implementing innovations like xIELU, AdEMAMix, and QRPO to stabilize large-scale training. This directly addresses distributed training techniques, parallel computing, and multi-node strategies for accelerating LLM training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The Apertus project introduces a suite of fully open large language models (LLMs) trained on 15T tokens from over 1800 languages, focusing on data compliance by using only openly available data, respecting opt-outs via robots.txt, and filtering out toxic and personally identifiable content, while employing the Goldfish objective to prevent memorization. The methodology includes pretraining at 8B and 70B scales, extensive multilingual coverage with 40% non-English data, and full transparency through released artifacts like code and datasets; key findings show that Apertus models achieve state-of-the-art performance on multilingual benchmarks without sacrificing task effectiveness, positioning them as compliant and democratized tools for global AI development.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing techniques like data compliance filtering and the Goldfish objective with large-scale multilingual training, offering a clever adaptation for known issues in open LLMs rather than introducing a entirely new problem or architecture.",
      "impact_score": "High",
      "impact_justification": "This work could significantly influence future research and applications in ethical AI, multilingual models, and open-source practices by providing fully reproducible, compliant models that set new standards for global accessibility and transparency.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to AI ethics and multilingualism, offering essential resources and insights that researchers in computation and language, artificial intelligence, and machine learning should be aware of for advancing compliant LLM development.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/58980617aec8a54e395db736538904922e0ff79f",
      "total_authors": 101,
      "authors_found": 101,
      "highest_h_index": 13,
      "average_h_index": 2.3168316831683167,
      "notable_authors_count": 14,
      "author_h_indexes": [
        {
          "name": "Alejandro Hern'andez-Cano",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363579690"
        },
        {
          "name": "Alexander Hagele",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2168096895"
        },
        {
          "name": "Allen Hao Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381064273"
        },
        {
          "name": "Angelika Romanou",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/1910588458"
        },
        {
          "name": "Antoni-Joan Solergibert",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356857672"
        },
        {
          "name": "Barna Pásztor",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/1902022482"
        },
        {
          "name": "Bettina Messmer",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2219037377"
        },
        {
          "name": "Dhia Garbaya",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363574237"
        },
        {
          "name": "Eduard Frank vDurech",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685776"
        },
        {
          "name": "Ido Hakimi",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/66116534"
        },
        {
          "name": "Juan Garc'ia Giraldo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685587"
        },
        {
          "name": "Mete Ismayilzada",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2190955045"
        },
        {
          "name": "Negar Foroutan",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/9737058"
        },
        {
          "name": "Skander Moalla",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2277120112"
        },
        {
          "name": "Tiancheng Chen",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2262515307"
        },
        {
          "name": "Vinko Sabolvcec",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685902"
        },
        {
          "name": "Yixuan Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362521599"
        },
        {
          "name": "Michael Aerni",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2122909997"
        },
        {
          "name": "Badr AlKhamissi",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2006905770"
        },
        {
          "name": "In'es Altemir Marinas",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378130527"
        },
        {
          "name": "Mohammad Hossein Amani",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2165468984"
        },
        {
          "name": "Matin Ansaripour",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2187427125"
        },
        {
          "name": "Ilia Badanin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685699"
        },
        {
          "name": "Harold Benoit",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685470"
        },
        {
          "name": "Emanuela Boros",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2291371995"
        },
        {
          "name": "Nicholas Browning",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685949"
        },
        {
          "name": "Fabian Bosch",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685791"
        },
        {
          "name": "Maximilian Bother",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2180394609"
        },
        {
          "name": "Niklas Canova",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685752"
        },
        {
          "name": "Camille Challier",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2371991566"
        },
        {
          "name": "Clement Charmillot",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380686024"
        },
        {
          "name": "Jonathan Coles",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685703"
        },
        {
          "name": "Jan Deriu",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/145116511"
        },
        {
          "name": "Arnout Devos",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354927517"
        },
        {
          "name": "Lukas Drescher",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2200298923"
        },
        {
          "name": "Daniil Dzenhaliou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2333242340"
        },
        {
          "name": "Maud Ehrmann",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2260862717"
        },
        {
          "name": "Dongyang Fan",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2284762232"
        },
        {
          "name": "Simin Fan",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2261456109"
        },
        {
          "name": "Silin Gao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2282084596"
        },
        {
          "name": "M. Gila",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/144594270"
        },
        {
          "name": "María Grandury",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2176184513"
        },
        {
          "name": "Diba Hashemi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2320311352"
        },
        {
          "name": "A. Hoyle",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372801340"
        },
        {
          "name": "J. Jiang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2353622744"
        },
        {
          "name": "Mark Klein",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380690598"
        },
        {
          "name": "Andrei Kucharavy",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2319814659"
        },
        {
          "name": "Anastasiia Kucherenko",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372540541"
        },
        {
          "name": "Frederike Lubeck",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2186741163"
        },
        {
          "name": "Roman Machácek",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376405130"
        },
        {
          "name": "Theofilos-Ioannis Manitaras",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/102919099"
        },
        {
          "name": "Andreas Marfurt",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/1896289"
        },
        {
          "name": "Kyle Matoba",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2268398512"
        },
        {
          "name": "Simon Matrenok",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373388950"
        },
        {
          "name": "Henrique Mendonccca",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685930"
        },
        {
          "name": "Fawzi Mohamed",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2372414408"
        },
        {
          "name": "Syrielle Montariol",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2261392096"
        },
        {
          "name": "Luca Mouchel",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685760"
        },
        {
          "name": "Sven Najem-Meyer",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2132447856"
        },
        {
          "name": "Jingwei Ni",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381223532"
        },
        {
          "name": "Gennaro Oliva",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685756"
        },
        {
          "name": "Matteo Pagliardini",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2435537"
        },
        {
          "name": "Elia Palme",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2372970184"
        },
        {
          "name": "Andrei Panferov",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2279022089"
        },
        {
          "name": "L'eo Paoletti",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685500"
        },
        {
          "name": "Marco Passerini",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2217270215"
        },
        {
          "name": "Ivan Pavlov",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685680"
        },
        {
          "name": "Auguste Poiroux",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2305679963"
        },
        {
          "name": "Kaustubh Ponkshe",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2154100332"
        },
        {
          "name": "Nathan Ranchin",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2341330236"
        },
        {
          "name": "Javier Rando",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2330425551"
        },
        {
          "name": "Mathieu Sauser",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685529"
        },
        {
          "name": "Jakhongir Saydaliev",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363346090"
        },
        {
          "name": "Muhammad Ali Sayfiddinov",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685058"
        },
        {
          "name": "Marian Schneider",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381238386"
        },
        {
          "name": "Stefano Schuppli",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2372965867"
        },
        {
          "name": "Marco Scialanga",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2203146831"
        },
        {
          "name": "Andrei Semenov",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2266390331"
        },
        {
          "name": "Kumar Shridhar",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2266467511"
        },
        {
          "name": "Raghav Singhal",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2325907550"
        },
        {
          "name": "Anna Sotnikova",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2289611422"
        },
        {
          "name": "Alexander Sternfeld",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2328403828"
        },
        {
          "name": "A. Tarun",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2141034263"
        },
        {
          "name": "Paul Teiletche",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2322796738"
        },
        {
          "name": "Jannis Vamvas",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2269142027"
        },
        {
          "name": "Xiaozhe Yao",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2267428071"
        },
        {
          "name": "Hao Zhao Alexander Ilic",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685684"
        },
        {
          "name": "Ana Klimovic",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2285439911"
        },
        {
          "name": "Andreas Krause",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2326294629"
        },
        {
          "name": "Caglar Gulcehre",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2288101023"
        },
        {
          "name": "David Rosenthal",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353304168"
        },
        {
          "name": "Elliott Ash",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2365837281"
        },
        {
          "name": "Florian Tramèr",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2267733649"
        },
        {
          "name": "Joost VandeVondele",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2205688883"
        },
        {
          "name": "Livio Veraldi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2353303938"
        },
        {
          "name": "Martin Rajman",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2265861432"
        },
        {
          "name": "T. Schulthess",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2258514724"
        },
        {
          "name": "Torsten Hoefler",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2372233409"
        },
        {
          "name": "Antoine Bosselut",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2284866282"
        },
        {
          "name": "Martin Jaggi",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2328413457"
        },
        {
          "name": "Imanol Schlag",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/35328044"
        }
      ]
    },
    {
      "id": "2509.14303",
      "title": "FlowDrive: Energy Flow Field for End-to-End Autonomous Driving",
      "authors": [
        "Hao Jiang",
        "Zhipeng Zhang",
        "Yu Gao",
        "Zhigang Sun",
        "Yiru Wang",
        "Yuwen Heng",
        "Shuo Wang",
        "Jinhao Chai",
        "Zhuo Chen",
        "Hao Zhao",
        "Hao Sun",
        "Xi Zhang",
        "Anqing Jiang",
        "Chuan Hu"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in end-to-end autonomous driving leverage multi-view images\nto construct BEV representations for motion planning. In motion planning,\nautonomous vehicles need considering both hard constraints imposed by\ngeometrically occupied obstacles (e.g., vehicles, pedestrians) and soft,\nrule-based semantics with no explicit geometry (e.g., lane boundaries, traffic\npriors). However, existing end-to-end frameworks typically rely on BEV features\nlearned in an implicit manner, lacking explicit modeling of risk and guidance\npriors for safe and interpretable planning. To address this, we propose\nFlowDrive, a novel framework that introduces physically interpretable\nenergy-based flow fields-including risk potential and lane attraction fields-to\nencode semantic priors and safety cues into the BEV space. These flow-aware\nfeatures enable adaptive refinement of anchor trajectories and serve as\ninterpretable guidance for trajectory generation. Moreover, FlowDrive decouples\nmotion intent prediction from trajectory denoising via a conditional diffusion\nplanner with feature-level gating, alleviating task interference and enhancing\nmultimodal diversity. Experiments on the NAVSIM v2 benchmark demonstrate that\nFlowDrive achieves state-of-the-art performance with an EPDMS of 86.3,\nsurpassing prior baselines in both safety and planning quality. The project is\navailable at https://astrixdrive.github.io/FlowDrive.github.io/.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14303v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14303v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.368,
      "datasets_score": 0.266,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a conditional diffusion planner for trajectory denoising in autonomous driving, which involves iterative refinement similar to diffusion models. However, it applies this to spatial and generative tasks like trajectory generation rather than multi-step logical reasoning or treating a 'Chain-of-Thought' as a holistic entity for complex logical tasks. Thus, while diffusion is a core component, it does not align with the topic's focus on logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14304",
      "title": "Deploying UDM Series in Real-Life Stuttered Speech Applications: A\n  Clinical Evaluation Framework",
      "authors": [
        "Eric Zhang",
        "Li Wei",
        "Sarah Chen",
        "Michael Wang"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Stuttered and dysfluent speech detection systems have traditionally suffered\nfrom the trade-off between accuracy and clinical interpretability. While\nend-to-end deep learning models achieve high performance, their black-box\nnature limits clinical adoption. This paper looks at the Unconstrained\nDysfluency Modeling (UDM) series-the current state-of-the-art framework\ndeveloped by Berkeley that combines modular architecture, explicit phoneme\nalignment, and interpretable outputs for real-world clinical deployment.\nThrough extensive experiments involving patients and certified speech-language\npathologists (SLPs), we demonstrate that UDM achieves state-of-the-art\nperformance (F1: 0.89+-0.04) while providing clinically meaningful\ninterpretability scores (4.2/5.0). Our deployment study shows 87% clinician\nacceptance rate and 34% reduction in diagnostic time. The results provide\nstrong evidence that UDM represents a practical pathway toward AI-assisted\nspeech therapy in clinical environments.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14304v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14304v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.383,
      "datasets_score": 0.376,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating and deploying the UDM framework for stuttered speech detection, emphasizing clinical experiments and interpretability, but does not involve training AI models using human feedback to fine-tune via reinforcement learning. There is no mention of a reward model or RL-based optimization.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses deep learning models for speech detection with modular architecture and phoneme alignment, but it does not incorporate diffusion-based processes for multi-step logical reasoning or iterative refinement of a Chain-of-Thought. No elements of diffusion models are present.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14335",
      "title": "Beyond Classification: Evaluating LLMs for Fine-Grained Automatic\n  Malware Behavior Auditing",
      "authors": [
        "Xinran Zheng",
        "Xingzhi Qian",
        "Yiling He",
        "Shuo Yang",
        "Lorenzo Cavallaro"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Automated malware classification has achieved strong detection performance.\nYet, malware behavior auditing seeks causal and verifiable explanations of\nmalicious activities -- essential not only to reveal what malware does but also\nto substantiate such claims with evidence. This task is challenging, as\nadversarial intent is often hidden within complex, framework-heavy\napplications, making manual auditing slow and costly. Large Language Models\n(LLMs) could help address this gap, but their auditing potential remains\nlargely unexplored due to three limitations: (1) scarce fine-grained\nannotations for fair assessment; (2) abundant benign code obscuring malicious\nsignals; and (3) unverifiable, hallucination-prone outputs undermining\nattribution credibility. To close this gap, we introduce MalEval, a\ncomprehensive framework for fine-grained Android malware auditing, designed to\nevaluate how effectively LLMs support auditing under real-world constraints.\nMalEval provides expert-verified reports and an updated sensitive API list to\nmitigate ground truth scarcity and reduce noise via static reachability\nanalysis. Function-level structural representations serve as intermediate\nattribution units for verifiable evaluation. Building on this, we define four\nanalyst-aligned tasks -- function prioritization, evidence attribution,\nbehavior synthesis, and sample discrimination -- together with domain-specific\nmetrics and a unified workload-oriented score. We evaluate seven widely used\nLLMs on a curated dataset of recent malware and misclassified benign apps,\noffering the first systematic assessment of their auditing capabilities.\nMalEval reveals both promising potential and critical limitations across audit\nstages, providing a reproducible benchmark and foundation for future research\non LLM-enhanced malware behavior auditing. MalEval is publicly available at\nhttps://github.com/ZhengXR930/MalEval.git",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14335v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14335v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.413,
      "weak_supervision_score": 0.412,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.323,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating existing LLMs for malware auditing tasks, such as function prioritization and evidence attribution, using a curated dataset with expert-verified reports. It does not involve training or fine-tuning models with human feedback via reinforcement learning, as required for RLHF.",
      "weak_supervision_justification": "The paper deals with the scarcity of fine-grained annotations and uses expert-verified data and indirect indicators for evaluation, which touches on challenges in supervision. However, it primarily relies on manually verified ground truth rather than programmatically generating noisy labels, making it only loosely connected to weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14343",
      "title": "Near-Real-Time Resource Slicing for QoS Optimization in 5G O-RAN using\n  Deep Reinforcement Learning",
      "authors": [
        "Peihao Yan",
        "Jie Lu",
        "Huacheng Zeng",
        "Y. Thomas Hou"
      ],
      "categories": [
        "eess.SY (Systems and Control)",
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)"
      ],
      "abstract": "Open-Radio Access Network (O-RAN) has become an important paradigm for 5G and\nbeyond radio access networks. This paper presents an xApp called xSlice for the\nNear-Real-Time (Near-RT) RAN Intelligent Controller (RIC) of 5G O-RANs. xSlice\nis an online learning algorithm that adaptively adjusts MAC-layer resource\nallocation in response to dynamic network states, including time-varying\nwireless channel conditions, user mobility, traffic fluctuations, and changes\nin user demand. To address these network dynamics, we first formulate the\nQuality-of-Service (QoS) optimization problem as a regret minimization problem\nby quantifying the QoS demands of all traffic sessions through weighting their\nthroughput, latency, and reliability. We then develop a deep reinforcement\nlearning (DRL) framework that utilizes an actor-critic model to combine the\nadvantages of both value-based and policy-based updating methods. A graph\nconvolutional network (GCN) is incorporated as a component of the DRL framework\nfor graph embedding of RAN data, enabling xSlice to handle a dynamic number of\ntraffic sessions. We have implemented xSlice on an O-RAN testbed with 10\nsmartphones and conducted extensive experiments to evaluate its performance in\nrealistic scenarios. Experimental results show that xSlice can reduce\nperformance regret by 67% compared to the state-of-the-art solutions. Source\ncode is available on GitHub [1].",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14343v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14343v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.314,
      "distributed_training_score": 0.391,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14353",
      "title": "DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene\n  Interaction via Guided Diffusion",
      "authors": [
        "Dvij Kalaria",
        "Sudarshan S Harithas",
        "Pushkal Katara",
        "Sangkyung Kwak",
        "Sarthak Bhagat",
        "Shankar Sastry",
        "Srinath Sridhar",
        "Sai Vemprala",
        "Ashish Kapoor",
        "Jonathan Chung-Kuan Huang"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We introduce DreamControl, a novel methodology for learning autonomous\nwhole-body humanoid skills. DreamControl leverages the strengths of diffusion\nmodels and Reinforcement Learning (RL): our core innovation is the use of a\ndiffusion prior trained on human motion data, which subsequently guides an RL\npolicy in simulation to complete specific tasks of interest (e.g., opening a\ndrawer or picking up an object). We demonstrate that this human motion-informed\nprior allows RL to discover solutions unattainable by direct RL, and that\ndiffusion models inherently promote natural looking motions, aiding in\nsim-to-real transfer. We validate DreamControl's effectiveness on a Unitree G1\nrobot across a diverse set of challenging tasks involving simultaneous lower\nand upper body control and object interaction. Project website at\nhttps://genrobo.github.io/DreamControl/",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14353v3",
      "pdf_url": "http://arxiv.org/pdf/2509.14353v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.436,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.505,
      "distributed_training_score": 0.348,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses a diffusion prior trained on human motion data to guide RL for humanoid control, but it does not involve training a reward model on human-ranked data or using human feedback to align the AI with preferences. Instead, the human data serves as a pre-trained prior, making this unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs diffusion models for generating human-like motion sequences to guide RL, focusing on physical tasks like robot control. It does not adapt diffusion for multi-step logical reasoning or treat a 'Chain-of-Thought' as an entity for holistic correction, as required for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14360",
      "title": "Embodied sensorimotor control: computational modeling of the neural\n  control of movement",
      "authors": [
        "Muhammad Noman Almani",
        "John Lazzari",
        "Jeff Walker",
        "Shreya Saxena"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We review how sensorimotor control is dictated by interacting neural\npopulations, optimal feedback mechanisms, and the biomechanics of bodies.\nFirst, we outline the distributed anatomical loops that shuttle sensorimotor\nsignals between cortex, subcortical regions, and spinal cord. We then summarize\nevidence that neural population activity occupies low-dimensional, dynamically\nevolving manifolds during planning and execution of movements. Next, we\nsummarize literature explaining motor behavior through the lens of optimal\ncontrol theory, which clarifies the role of internal models and feedback during\nmotor control. Finally, recent studies on embodied sensorimotor control address\ngaps within each framework by aiming to elucidate neural population activity\nthrough the explicit control of musculoskeletal dynamics. We close by\ndiscussing open problems and opportunities: multi-tasking and cognitively rich\nbehavior, multi-regional circuit models, and the level of anatomical detail\nneeded in body and network models. Together, this review and recent advances\npoint towards reaching an integrative account of the neural control of\nmovement.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14360v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14360v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.29,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.283,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14382",
      "title": "Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents",
      "authors": [
        "Daniel Röder",
        "Akhil Juneja",
        "Roland Roller",
        "Sven Schmeier"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Web agents powered by large language models (LLMs) can autonomously perform\ncomplex, multistep tasks in dynamic web environments. However, current\nevaluations mostly focus on the overall success while overlooking intermediate\nerrors. This limits insight into failure modes and hinders systematic\nimprovement. This work analyzes existing benchmarks and highlights the lack of\nfine-grained diagnostic tools. To address this gap, we propose a modular\nevaluation framework that decomposes agent pipelines into interpretable stages\nfor detailed error analysis. Using the SeeAct framework and the Mind2Web\ndataset as a case study, we show how this approach reveals actionable\nweaknesses missed by standard metrics - paving the way for more robust and\ngeneralizable web agents.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14382v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14382v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.447,
      "distributed_training_score": 0.37,
      "datasets_score": 0.384,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on developing a modular evaluation framework for analyzing errors in web agent pipelines, using existing benchmarks like Mind2Web and enhancing agent architectures for better diagnostics. It does not involve training machine learning models with programmatically generated labels, noisy sources, or weak supervision techniques, as its primary contribution is in evaluation and error analysis rather than model training.",
      "diffusion_reasoning_justification": "The paper proposes a framework for fine-grained error analysis in LLM-based web agents, including stages like subgoal planning and action selection, but it does not incorporate diffusion models, iterative refinement processes, or treat reasoning paths as entities for holistic correction. There is no mention of multi-step logical reasoning using diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14383",
      "title": "RLBind: Adversarial-Invariant Cross-Modal Alignment for Unified Robust\n  Embeddings",
      "authors": [
        "Yuhong Lu"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Unified multi-modal encoders that bind vision, audio, and other sensors into\na shared embedding space are attractive building blocks for robot perception\nand decision-making. However, on-robot deployment exposes the vision branch to\nadversarial and natural corruptions, making robustness a prerequisite for\nsafety. Prior defenses typically align clean and adversarial features within\nCLIP-style encoders and overlook broader cross-modal correspondence, yielding\nmodest gains and often degrading zero-shot transfer. We introduce RLBind, a\ntwo-stage adversarial-invariant cross-modal alignment framework for robust\nunified embeddings. Stage 1 performs unsupervised fine-tuning on\nclean-adversarial pairs to harden the visual encoder. Stage 2 leverages\ncross-modal correspondence by minimizing the discrepancy between\nclean/adversarial features and a text anchor, while enforcing class-wise\ndistributional alignment across modalities. Extensive experiments on Image,\nAudio, Thermal, and Video data show that RLBind consistently outperforms the\nLanguageBind backbone and standard fine-tuning baselines in both clean accuracy\nand norm-bounded adversarial robustness. By improving resilience without\nsacrificing generalization, RLBind provides a practical path toward safer\nmulti-sensor perception stacks for embodied robots in navigation, manipulation,\nand other autonomy settings.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14383v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14383v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.458,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.348,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on adversarial robustness in multi-modal encoders for robotics, introducing a two-stage alignment framework (RLBind). It does not involve human feedback, reward models, or reinforcement learning for model alignment. The \"RL\" in RLBind appears to be an acronym for the method, not related to reinforcement learning from human feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses cross-modal alignment and robustness against adversarial attacks in multi-modal encoders, with no mention of diffusion models, iterative refinement for logical reasoning, or multi-step chain-of-thought processes. It is centered on perception and embeddings, not reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14388",
      "title": "eIQ Neutron: Redefining Edge-AI Inference with Integrated NPU and\n  Compiler Innovations",
      "authors": [
        "Lennart Bamberg",
        "Filippo Minnella",
        "Roberto Bosio",
        "Fabrizio Ottati",
        "Yuebin Wang",
        "Jongmin Lee",
        "Luciano Lavagno",
        "Adam Fuks"
      ],
      "categories": [
        "cs.AR (Hardware Architecture)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Neural Processing Units (NPUs) are key to enabling efficient AI inference in\nresource-constrained edge environments. While peak tera operations per second\n(TOPS) is often used to gauge performance, it poorly reflects real-world\nperformance and typically rather correlates with higher silicon cost. To\naddress this, architects must focus on maximizing compute utilization, without\nsacrificing flexibility. This paper presents the eIQ Neutron efficient-NPU,\nintegrated into a commercial flagship MPU, alongside co-designed compiler\nalgorithms. The architecture employs a flexible, data-driven design, while the\ncompiler uses a constrained programming approach to optimize compute and data\nmovement based on workload characteristics. Compared to the leading embedded\nNPU and compiler stack, our solution achieves an average speedup of 1.8x (4x\npeak) at equal TOPS and memory resources across standard AI-benchmarks. Even\nagainst NPUs with double the compute and memory resources, Neutron delivers up\nto 3.3x higher performance.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14388v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14388v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.472,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on optimizing AI inference for edge devices using NPUs and compiler innovations, emphasizing hardware-software co-design for efficient computation and data movement. It does not address distributed training, parallel computing across multiple nodes, or strategies for accelerating model training through data/model partitioning. The core contributions are centered on inference performance, not training processes, making it unrelated to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14391",
      "title": "Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in\n  Quantized Long-Context LLMs",
      "authors": [
        "Ye Qiao",
        "Sitao Huang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Extending LLM context windows is crucial for long range tasks. RoPE-based\nposition interpolation (PI) methods like linear and frequency-aware scaling\nextend input lengths without retraining, while post-training quantization (PTQ)\nenables practical deployment. We show that combining PI with PTQ degrades\naccuracy due to coupled effects long context aliasing, dynamic range dilation,\naxis grid anisotropy, and outlier shifting that induce position-dependent logit\nnoise. We provide the first systematic analysis of PI plus PTQ and introduce\ntwo diagnostics: Interpolation Pressure (per-band phase scaling sensitivity)\nand Tail Inflation Ratios (outlier shift from short to long contexts). To\naddress this, we propose Q-ROAR, a RoPE-aware, weight-only stabilization that\ngroups RoPE dimensions into a few frequency bands and performs a small search\nover per-band scales for W_Q,W_K, with an optional symmetric variant to\npreserve logit scale. The diagnostics guided search uses a tiny long-context\ndev set and requires no fine-tuning, kernel, or architecture changes.\nEmpirically, Q-ROAR recovers up to 0.7% accuracy on standard tasks and reduces\nGovReport perplexity by more than 10%, while preserving short-context\nperformance and compatibility with existing inference stacks.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14391v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14391v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.42,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.389,
      "datasets_score": 0.265,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on techniques for extending context windows in LLMs through RoPE-based position interpolation and quantization, specifically introducing Q-ROAR to address accuracy issues in long-context scenarios. It does not involve human feedback, reward models, reinforcement learning for alignment, or any elements of RLHF. Therefore, the paper's contributions are entirely unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14404",
      "title": "A Taxonomy of Prompt Defects in LLM Systems",
      "authors": [
        "Haoye Tian",
        "Chong Wang",
        "BoYang Yang",
        "Lyuye Zhang",
        "Yang Liu"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.PL (Programming Languages)"
      ],
      "abstract": "Large Language Models (LLMs) have become key components of modern software,\nwith prompts acting as their de-facto programming interface. However, prompt\ndesign remains largely empirical and small mistakes can cascade into\nunreliable, insecure, or inefficient behavior. This paper presents the first\nsystematic survey and taxonomy of prompt defects, recurring ways that prompts\nfail to elicit their intended behavior from LLMs. We organize defects along six\ndimensions: (1) Specification and Intent, (2) Input and Content, (3) Structure\nand Formatting, (4) Context and Memory, (5) Performance and Efficiency, and (6)\nMaintainability and Engineering. Each dimension is refined into fine-grained\nsubtypes, illustrated with concrete examples and root cause analysis. Grounded\nin software engineering principles, we show how these defects surface in real\ndevelopment workflows and examine their downstream effects. For every subtype,\nwe distill mitigation strategies that span emerging prompt engineering\npatterns, automated guardrails, testing harnesses, and evaluation frameworks.\nWe then summarize these strategies in a master taxonomy that links defect,\nimpact, and remedy. We conclude with open research challenges and a call for\nrigorous engineering-oriented methodologies to ensure that LLM-driven systems\nare dependable by design.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14404v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14404v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.316,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a taxonomy of prompt defects in LLM systems and strategies for prompt engineering, without any mention of training AI models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper addresses defects in prompt design for LLMs and does not discuss machine learning approaches involving programmatically generated labels, noisy data sources, or training models with weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14420",
      "title": "Class-invariant Test-Time Augmentation for Domain Generalization",
      "authors": [
        "Zhicheng Lin",
        "Xiaolin Wu",
        "Xi Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Deep models often suffer significant performance degradation under\ndistribution shifts. Domain generalization (DG) seeks to mitigate this\nchallenge by enabling models to generalize to unseen domains. Most prior\napproaches rely on multi-domain training or computationally intensive test-time\nadaptation. In contrast, we propose a complementary strategy: lightweight\ntest-time augmentation. Specifically, we develop a novel Class-Invariant\nTest-Time Augmentation (CI-TTA) technique. The idea is to generate multiple\nvariants of each input image through elastic and grid deformations that\nnevertheless belong to the same class as the original input. Their predictions\nare aggregated through a confidence-guided filtering scheme that remove\nunreliable outputs, ensuring the final decision relies on consistent and\ntrustworthy cues. Extensive Experiments on PACS and Office-Home datasets\ndemonstrate consistent gains across different DG algorithms and backbones,\nhighlighting the effectiveness and generality of our approach.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14420v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14420v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.303,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.384,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14436",
      "title": "When Content is Goliath and Algorithm is David: The Style and Semantic\n  Effects of Generative Search Engine",
      "authors": [
        "Lijia Ma",
        "Juan Qin",
        "Xingchen Xu",
        "Yong Tan"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generative search engines (GEs) leverage large language models (LLMs) to\ndeliver AI-generated summaries with website citations, establishing novel\ntraffic acquisition channels while fundamentally altering the search engine\noptimization landscape. To investigate the distinctive characteristics of GEs,\nwe collect data through interactions with Google's generative and conventional\nsearch platforms, compiling a dataset of approximately ten thousand websites\nacross both channels. Our empirical analysis reveals that GEs exhibit\npreferences for citing content characterized by significantly higher\npredictability for underlying LLMs and greater semantic similarity among\nselected sources. Through controlled experiments utilizing retrieval augmented\ngeneration (RAG) APIs, we demonstrate that these citation preferences emerge\nfrom intrinsic LLM tendencies to favor content aligned with their generative\nexpression patterns. Motivated by applications of LLMs to optimize website\ncontent, we conduct additional experimentation to explore how LLM-based content\npolishing by website proprietors alters AI summaries, finding that such\npolishing paradoxically enhances information diversity within AI summaries.\nFinally, to assess the user-end impact of LLM-induced information increases, we\ndesign a generative search engine and recruit Prolific participants to conduct\na randomized controlled experiment involving an information-seeking and writing\ntask. We find that higher-educated users exhibit minimal changes in their final\noutputs' information diversity but demonstrate significantly reduced task\ncompletion time when original sites undergo polishing. Conversely,\nlower-educated users primarily benefit through enhanced information density in\ntheir task outputs while maintaining similar completion times across\nexperimental groups.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14436v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14436v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.329,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves analyzing generative search engines using LLMs and RAG for content selection, citation preferences, and user impacts, with no reference to diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14438",
      "title": "Simulating a Bias Mitigation Scenario in Large Language Models",
      "authors": [
        "Kiana Kiashemshaki",
        "Mohammad Jalili Torkamani",
        "Negin Mahmoudi",
        "Meysam Shirdel Bilehsavar"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have fundamentally transformed the field of\nnatural language processing; however, their vulnerability to biases presents a\nnotable obstacle that threatens both fairness and trust. This review offers an\nextensive analysis of the bias landscape in LLMs, tracing its roots and\nexpressions across various NLP tasks. Biases are classified into implicit and\nexplicit types, with particular attention given to their emergence from data\nsources, architectural designs, and contextual deployments. This study advances\nbeyond theoretical analysis by implementing a simulation framework designed to\nevaluate bias mitigation strategies in practice. The framework integrates\nmultiple approaches including data curation, debiasing during model training,\nand post-hoc output calibration and assesses their impact in controlled\nexperimental settings. In summary, this work not only synthesizes existing\nknowledge on bias in LLMs but also contributes original empirical validation\nthrough simulation of mitigation strategies.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14438v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14438v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.435,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.398,
      "datasets_score": 0.428,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on bias analysis and mitigation in LLMs through simulation, data curation, and training adjustments, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper discusses data curation and augmentation as part of bias mitigation strategies, which could indirectly relate to weak supervision by handling noisy data sources, but it does not primarily focus on programmatically generating labels or weak supervision methods.",
      "diffusion_reasoning_justification": "The paper addresses bias in LLMs and simulates mitigation strategies, with no reference to diffusion models, iterative refinement for reasoning, or multi-step logical processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper involves data curation and analysis as part of its simulation framework for bias mitigation, which relates to dataset handling and evaluation, though it is not primarily focused on creating, benchmarking, or deeply analyzing new datasets.",
      "llm_score_status": "completed",
      "summary": "This paper reviews the landscape of biases in Large Language Models (LLMs), classifying them into implicit and explicit types stemming from data sources, model architecture, and deployment contexts, while highlighting their implications across NLP tasks. It introduces a simulation framework to empirically evaluate bias mitigation strategies, including data curation, fairness-aware training, and post-hoc output calibration, through controlled experiments, and synthesizes existing knowledge with original findings on their effectiveness.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a simulation framework that combines existing bias mitigation techniques in a practical way to evaluate their impact, representing a notable improvement on known problems rather than a completely new concept. While it advances empirical validation, it builds on prior research without introducing a groundbreaking problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI fairness and NLP, as it provides a practical framework for testing bias mitigation strategies in LLMs. However, its influence may be limited to specific applications rather than broadly transforming the field or commercial practices.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a strong, valuable contribution through its review and empirical simulation of bias mitigation, making it essential for researchers focused on AI ethics and fairness to be aware of. While not exceptional enough to be a must-read, it offers practical insights that advance the discussion on LLM biases.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1975403be0f6e6da458d85c38f2bae8f3d195b39",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 2,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Kiana Kiashemshaki",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2374048412"
        },
        {
          "name": "Mohammad Jalili Torkamani",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2332355474"
        },
        {
          "name": "Negin Mahmoudi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2374049678"
        },
        {
          "name": "Meysam Shirdel Bilehsavar",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376531918"
        }
      ]
    },
    {
      "id": "2509.14448",
      "title": "VCBench: Benchmarking LLMs in Venture Capital",
      "authors": [
        "Rick Chen",
        "Joseph Ternasky",
        "Afriyie Samuel Kwesi",
        "Ben Griffin",
        "Aaron Ontoyin Yin",
        "Zakari Salifu",
        "Kelvin Amoaba",
        "Xianling Mu",
        "Fuat Alican",
        "Yigit Ihlamur"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets\naccelerate progress toward artificial general intelligence (AGI). We introduce\nVCBench, the first benchmark for predicting founder success in venture capital\n(VC), a domain where signals are sparse, outcomes are uncertain, and even top\ninvestors perform modestly. At inception, the market index achieves a precision\nof 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1\nfirms are 2.9x better. VCBench provides 9,000 anonymized founder profiles,\nstandardized to preserve predictive features while resisting identity leakage,\nwith adversarial tests showing more than 90% reduction in re-identification\nrisk. We evaluate nine state-of-the-art large language models (LLMs).\nDeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the\nhighest F0.5, and most models surpass human benchmarks. Designed as a public\nand evolving resource available at vcbench.com, VCBench establishes a\ncommunity-driven standard for reproducible and privacy-preserving evaluation of\nAGI in early-stage venture forecasting.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14448v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14448v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.441,
      "weak_supervision_score": 0.424,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.392,
      "datasets_score": 0.436,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on introducing a benchmark dataset for evaluating LLMs in venture capital, including data standardization, anonymization, and performance comparisons. It does not involve training models using human feedback, reward models, or reinforcement learning techniques, which are core to RLHF. Thus, there is no connection to this topic.",
      "weak_supervision_justification": "The paper addresses noisy and inconsistent data sources (e.g., LinkedIn and Crunchbase) with issues like format irregularity and entry irregularity, and develops a pipeline for data cleaning and anonymization. This indirectly relates to weak supervision, as it involves handling imprecise labels and sources to generate a dataset. However, the paper does not explicitly focus on training models with weakly supervised techniques, making it only moderately relevant.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and introduction of VCBench, a new benchmark dataset for founder success prediction in venture capital. It covers dataset curation methodologies (e.g., standardization, filtering, enrichment, and anonymization), evaluation of LLMs on the dataset, and establishment of a public leaderboard, directly aligning with research on creating, analyzing, and benchmarking datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "VCBench introduces the first benchmark for evaluating large language models (LLMs) in predicting founder success within venture capital, addressing challenges like data sparsity and uncertainty by creating a dataset of 9,000 anonymized founder profiles through a pipeline of standardization, filtering, enrichment, and anonymization. The methodology evaluates nine state-of-the-art LLMs, revealing that models like DeepSeek-V3 achieve over six times the baseline precision and surpass human performance benchmarks, while establishing a public, evolving resource with a leaderboard to foster reproducible and privacy-preserving advancements in AGI for early-stage venture forecasting.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark for founder success prediction in venture capital, which is a novel application area for LLMs and significantly advances benchmarking in AI for financial decision-making.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI applications in finance and venture capital, as it provides a standardized tool for evaluation, though its influence may remain niche rather than widespread.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, innovative benchmark that advances AI in venture capital, making it valuable for researchers in AI and finance to understand and potentially utilize.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/4718e3c4b30bd60a77a4e76c26acb7d3a9822ebb",
      "total_authors": 10,
      "authors_found": 10,
      "highest_h_index": 3,
      "average_h_index": 0.8,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Rick Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381143754"
        },
        {
          "name": "Joseph Ternasky",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2355646176"
        },
        {
          "name": "Afriyie Samuel Kwesi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381055451"
        },
        {
          "name": "Ben Griffin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2364672102"
        },
        {
          "name": "Aaron Ontoyin Yin",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2330397046"
        },
        {
          "name": "Zakari Salifu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379935214"
        },
        {
          "name": "Kelvin Amoaba",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379935510"
        },
        {
          "name": "Xianling Mu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363571770"
        },
        {
          "name": "Fuat Alican",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/46942208"
        },
        {
          "name": "Yigit Ihlamur",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2275202217"
        }
      ]
    },
    {
      "id": "2509.14456",
      "title": "Correct-Detect: Balancing Performance and Ambiguity Through the Lens of\n  Coreference Resolution in LLMs",
      "authors": [
        "Amber Shore",
        "Russell Scheinberg",
        "Ameeta Agrawal",
        "So Young Lee"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) are intended to reflect human linguistic\ncompetencies. But humans have access to a broad and embodied context, which is\nkey in detecting and resolving linguistic ambiguities, even in isolated text\nspans. A foundational case of semantic ambiguity is found in the task of\ncoreference resolution: how is a pronoun related to an earlier person mention?\nThis capability is implicit in nearly every downstream task, and the presence\nof ambiguity at this level can alter performance significantly. We show that\nLLMs can achieve good performance with minimal prompting in both coreference\ndisambiguation and the detection of ambiguity in coreference, however, they\ncannot do both at the same time. We present the CORRECT-DETECT trade-off:\nthough models have both capabilities and deploy them implicitly, successful\nperformance balancing these two abilities remains elusive.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14456v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14456v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.32,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating LLMs' performance in coreference resolution and ambiguity detection using human judgments from the AmbiCoref dataset, but it does not involve training or fine-tuning models with a reward model based on human-ranked data, as required for RLHF. There is no mention of reinforcement learning or model alignment through human feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines coreference resolution in LLMs and identifies a trade-off in handling ambiguity, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described. There is no component related to treating Chain-of-Thought as a single entity for reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14474",
      "title": "From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial\n  General Intelligence",
      "authors": [
        "Meltem Subasioglu",
        "Nevzat Subasioglu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "The debate around Artificial General Intelligence (AGI) remains open due to\ntwo fundamentally different goals: replicating human-like performance versus\nreplicating human-like cognitive processes. We argue that current\nperformance-based definitions are inadequate because they provide no clear,\nmechanism-focused roadmap for research, and they fail to properly define the\nqualitative nature of genuine intelligence. Drawing inspiration from the human\nbrain, we propose a new paradigm that shifts the focus from external mimicry to\nthe development of foundational cognitive architectures. We define True\nIntelligence (TI) as a system characterized by six core components: embodied\nsensory fusion, core directives, dynamic schemata creation, a\nhighly-interconnected multi-expert architecture, an orchestration layer, and\nlastly, the unmeasurable quality of Interconnectedness, which we hypothesize\nresults in consciousness and a subjective experience. We propose a practical,\nfive-level taxonomy of AGI based on the number of the first five measurable\ncomponents a system exhibits. This framework provides a clear path forward with\ndevelopmental milestones that directly address the challenge of building\ngenuinely intelligent systems. We contend that once a system achieves Level-5\nAGI by implementing all five measurable components, the difference between it\nand TI remains as a purely philosophical debate. For practical purposes - and\ngiven theories indicate consciousness is an emergent byproduct of integrated,\nhigher-order cognition - we conclude that a fifth-level AGI is functionally and\npractically equivalent to TI. This work synthesizes diverse insights from\nanalytical psychology, schema theory, metacognition, modern brain architectures\nand latest works in AI to provide the first holistic, mechanism-based\ndefinition of AGI that offers a clear and actionable path for the research\ncommunity.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14474v2",
      "pdf_url": "http://arxiv.org/pdf/2509.14474v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.325,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a conceptual framework for Artificial General Intelligence (AGI) emphasizing cognitive architectures inspired by the human brain, including components like embodied sensory fusion and dynamic schemata creation. It does not discuss diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14476",
      "title": "AToken: A Unified Tokenizer for Vision",
      "authors": [
        "Jiasen Lu",
        "Liangchen Song",
        "Mingze Xu",
        "Byeongjoo Ahn",
        "Yanjun Wang",
        "Chen Chen",
        "Afshin Dehghan",
        "Yinfei Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "We present AToken, the first unified visual tokenizer that achieves both\nhigh-fidelity reconstruction and semantic understanding across images, videos,\nand 3D assets. Unlike existing tokenizers that specialize in either\nreconstruction or understanding for single modalities, AToken encodes these\ndiverse visual inputs into a shared 4D latent space, unifying both tasks and\nmodalities in a single framework. Specifically, we introduce a pure transformer\narchitecture with 4D rotary position embeddings to process visual inputs of\narbitrary resolutions and temporal durations. To ensure stable training, we\nintroduce an adversarial-free training objective that combines perceptual and\nGram matrix losses, achieving state-of-the-art reconstruction quality. By\nemploying a progressive training curriculum, AToken gradually expands from\nsingle images, videos, and 3D, and supports both continuous and discrete latent\ntokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01\nrFVD with 40.2% MSRVTT retrieval for videos, and 28.28 PSNR with 90.9%\nclassification accuracy for 3D.. In downstream applications, AToken enables\nboth visual generation tasks (e.g., image generation with continuous and\ndiscrete tokens, text-to-video generation, image-to-3D synthesis) and\nunderstanding tasks (e.g., multimodal LLMs), achieving competitive performance\nacross all benchmarks. These results shed light on the next-generation\nmultimodal AI systems built upon unified visual tokenization.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14476v2",
      "pdf_url": "http://arxiv.org/pdf/2509.14476v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.38,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces AToken, a unified visual tokenizer for handling images, videos, and 3D assets using a transformer architecture, focusing on reconstruction and semantic understanding. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning processes, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14480",
      "title": "Process-Supervised Reinforcement Learning for Interactive Multimodal\n  Tool-Use Agents",
      "authors": [
        "Weiting Tan",
        "Xinghua Qu",
        "Ming Tu",
        "Meng Ge",
        "Andy T. Liu",
        "Philipp Koehn",
        "Lu Lu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Effective interactive tool use requires agents to master Tool Integrated\nReasoning (TIR): a complex process involving multi-turn planning and\nlong-context dialogue management. To train agents for this dynamic process,\nparticularly in multi-modal contexts, we introduce a sandbox environment for\nreinforcement learning (RL) that supports interleaved speech-text rollouts. Our\ncore strategy, Turn-level Adjudicated Reinforcement Learning (TARL), addresses\nthe challenge of credit assignment in long-horizon tasks by employing a Large\nLanguage Model (LLM) as a judge to provide turn-level evaluation. To enhance\nexploration, we integrate a mixed-task training curriculum with mathematical\nreasoning problems. This unified approach boosts the task pass rate on the\ntext-based $\\tau$-bench by over 6% compared to strong RL baselines. Crucially,\nwe demonstrate our framework's suitability for fine-tuning a multi-modal\nfoundation model for agentic tasks. By training a base multi-modal LLM on\ninterleaved speech-text rollouts, we equip it with tool-use abilities, paving\nthe way for more natural, voice-driven interactive agents.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14480v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14480v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.487,
      "weak_supervision_score": 0.432,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.366,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses an LLM as a judge for turn-level rewards in RL, which is automated and not based on human-ranked data or a separate reward model trained on human preferences. Thus, it does not align with RLHF, which requires human feedback.",
      "weak_supervision_justification": "The paper employs an LLM as a judge to programmatically generate turn-level evaluations and rewards, which can be seen as a form of weak supervision due to its reliance on noisy, high-level sources rather than precise hand-labeled data. However, the primary focus is on RL, not weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper focuses on RL methods like TARL for tool-use agents and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. There is no mention of adapting diffusion for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces a framework for training interactive multimodal agents using reinforcement learning, specifically proposing Turn-level Adjudicated Reinforcement Learning (TARL), which utilizes a Large Language Model as a judge to provide turn-level rewards in a sandbox environment supporting speech and text interactions. By incorporating mixed-task training with mathematical problems to enhance exploration, the approach achieves up to 6% higher pass rates on text-based benchmarks and over 20% on multimodal tasks compared to baselines, demonstrating its effectiveness for fine-tuning multimodal LLMs for tool-use abilities.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces TARL, a novel technique for turn-level credit assignment in RL using an LLM judge, which significantly advances training for long-horizon, multimodal interactive agents. This represents a meaningful step forward in addressing challenges like exploration and credit assignment in complex tool-use scenarios.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future research in AI agents, particularly in multimodal interactions and voice-driven applications, by providing a scalable framework for training more natural and effective interactive systems. Its demonstration of improving agent performance could lead to broader adoption in commercial tools like voice assistants.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality contribution with innovative methods for RL in multimodal agents, making it valuable for researchers in AI and language processing to understand advancements in interactive tool use. While not groundbreaking across all fields, its specific innovations warrant attention for those working on agentic systems.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0c8bd4625dbf1aabfb3f1c8501ebf36b8f747bfd",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 6,
      "average_h_index": 0.8571428571428571,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Weiting Tan",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/28000727"
        },
        {
          "name": "Xinghua Qu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381015104"
        },
        {
          "name": "Ming Tu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381037652"
        },
        {
          "name": "Meng Ge",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381043606"
        },
        {
          "name": "Andy T. Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381252608"
        },
        {
          "name": "Philipp Koehn",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381057440"
        },
        {
          "name": "Lu Lu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381314499"
        }
      ]
    },
    {
      "id": "2509.14485",
      "title": "Beyond the high score: Prosocial ability profiles of multi-agent\n  populations",
      "authors": [
        "Marko Tesic",
        "Yue Zhao",
        "Joel Z. Leibo",
        "Rakshit S. Trivedi",
        "Jose Hernandez-Orallo"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The development and evaluation of social capabilities in AI agents require\ncomplex environments where competitive and cooperative behaviours naturally\nemerge. While game-theoretic properties can explain why certain teams or agent\npopulations outperform others, more abstract behaviours, such as convention\nfollowing, are harder to control in training and evaluation settings. The\nMelting Pot contest is a social AI evaluation suite designed to assess the\ncooperation capabilities of AI systems. In this paper, we apply a Bayesian\napproach known as Measurement Layouts to infer the capability profiles of\nmulti-agent systems in the Melting Pot contest. We show that these capability\nprofiles not only predict future performance within the Melting Pot suite but\nalso reveal the underlying prosocial abilities of agents. Our analysis\nindicates that while higher prosocial capabilities sometimes correlate with\nbetter performance, this is not a universal trend-some lower-scoring agents\nexhibit stronger cooperation abilities. Furthermore, we find that\ntop-performing contest submissions are more likely to achieve high scores in\nscenarios where prosocial capabilities are not required. These findings,\ntogether with reports that the contest winner used a hard-coded solution\ntailored to specific environments, suggest that at least one top-performing\nteam may have optimised for conditions where cooperation was not necessary,\npotentially exploiting limitations in the evaluation framework. We provide\nrecommendations for improving the annotation of cooperation demands and propose\nfuture research directions to account for biases introduced by different\ntesting environments. Our results demonstrate that Measurement Layouts offer\nboth strong predictive accuracy and actionable insights, contributing to a more\ntransparent and generalisable approach to evaluating AI systems in complex\nsocial settings.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.14485v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14485v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.438,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.308,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves applying a Bayesian approach (Measurement Layouts) to evaluate prosocial abilities in multi-agent systems within the Melting Pot contest, focusing on cooperative behaviors in reinforcement learning agents. However, it does not mention or involve human feedback, such as training a reward model on human-ranked data or fine-tuning models to align with human preferences. Since RLHF specifically requires human feedback elements, which are absent here, the paper is not relevant to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15233",
      "title": "Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided\n  Role-playing Agents",
      "authors": [
        "Xueqiao Zhang",
        "Chao Zhang",
        "Jingtao Xu",
        "Yifan Zhu",
        "Xin Shi",
        "Yi Yang",
        "Yawei Luo"
      ],
      "categories": [
        "cs.MM (Multimedia)",
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Role-playing agents (RPAs) have attracted growing interest for their ability\nto simulate immersive and interactive characters. However, existing approaches\nprimarily focus on static role profiles, overlooking the dynamic perceptual\nabilities inherent to humans. To bridge this gap, we introduce the concept of\ndynamic role profiles by incorporating video modality into RPAs. To support\nthis, we construct Role-playing-Video60k, a large-scale, high-quality dataset\ncomprising 60k videos and 700k corresponding dialogues. Based on this dataset,\nwe develop a comprehensive RPA framework that combines adaptive temporal\nsampling with both dynamic and static role profile representations.\nSpecifically, the dynamic profile is created by adaptively sampling video\nframes and feeding them to the LLM in temporal order, while the static profile\nconsists of (1) character dialogues from training videos during fine-tuning,\nand (2) a summary context from the input video during inference. This joint\nintegration enables RPAs to generate greater responses. Furthermore, we propose\na robust evaluation method covering eight metrics. Experimental results\ndemonstrate the effectiveness of our framework, highlighting the importance of\ndynamic role profiles in developing RPAs.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.15233v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15233v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.323,
      "datasets_score": 0.422,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes the creation and introduction of a new dataset, Role-playing-Video60k, comprising 60k videos and 700k dialogues, specifically designed for machine learning and AI applications in role-playing agents. It details dataset curation methodologies, such as sourcing from social media platforms, and provides a comparison with existing datasets, which aligns directly with research on creating, analyzing, and benchmarking datasets.",
      "llm_score_status": "completed",
      "summary": "This paper introduces a novel approach to role-playing agents by integrating video modality to create dynamic role profiles, addressing the limitations of static profiles in existing systems. The authors construct a large-scale dataset, Role-playing-Video60k, comprising 60k videos and 700k dialogues, and propose a framework that combines adaptive temporal sampling of video frames with both dynamic and static profile representations to enhance response generation in RPAs, demonstrating superior performance and state-of-the-art results in human-likeness through extensive experiments.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by being the first to integrate video modality into role-playing agents, significantly advancing the field through the creation of dynamic profiles and a new dataset. This represents a substantial departure from existing static-focused methods, marking a clear innovation in multimodal AI for interactive characters.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of multimodal role-playing agents, as it provides a new dataset and framework that could enhance immersive AI applications. However, its influence may be limited to specific areas like computer vision and language processing rather than broadly across all AI domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with innovative integration of video into RPAs, making it valuable for researchers in multimodal AI and character simulation. While not essential for all audiences, it provides significant insights that could inform future work in interactive agents.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5eec6853074e24180b4f284bfe4acb251dc5b392",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 6,
      "average_h_index": 2.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Xueqiao Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354284344"
        },
        {
          "name": "Chao Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2355114858"
        },
        {
          "name": "Jingtao Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381348057"
        },
        {
          "name": "Yifan Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363573637"
        },
        {
          "name": "Xin Shi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362757249"
        },
        {
          "name": "Yi Yang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2258673289"
        },
        {
          "name": "Yawei Luo",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2261893045"
        }
      ]
    },
    {
      "id": "2509.15234",
      "title": "Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in\n  Chest X-rays",
      "authors": [
        "Hanbin Ko",
        "Gihun Cho",
        "Inhyeok Baek",
        "Donguk Kim",
        "Joonbeom Koo",
        "Changi Kim",
        "Dongheon Lee",
        "Chang Min Park"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision-language pretraining has advanced image-text alignment, yet progress\nin radiology remains constrained by the heterogeneity of clinical reports,\nincluding abbreviations, impression-only notes, and stylistic variability.\nUnlike general-domain settings where more data often leads to better\nperformance, naively scaling to large collections of noisy reports can plateau\nor even degrade model learning. We ask whether large language model (LLM)\nencoders can provide robust clinical representations that transfer across\ndiverse styles and better guide image-text alignment. We introduce LLM2VEC4CXR,\na domain-adapted LLM encoder for chest X-ray reports, and LLM2CLIP4CXR, a\ndual-tower framework that couples this encoder with a vision backbone.\nLLM2VEC4CXR improves clinical text understanding over BERT-based baselines,\nhandles abbreviations and style variation, and achieves strong clinical\nalignment on report-level metrics. LLM2CLIP4CXR leverages these embeddings to\nboost retrieval accuracy and clinically oriented scores, with stronger\ncross-dataset generalization than prior medical CLIP variants. Trained on 1.6M\nCXR studies from public and private sources with heterogeneous and noisy\nreports, our models demonstrate that robustness -- not scale alone -- is the\nkey to effective multimodal learning. We release models to support further\nresearch in medical image-text representation learning.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.15234v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15234v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.431,
      "distributed_training_score": 0.336,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using LLM encoders for image-text retrieval in chest X-rays, emphasizing domain adaptation for clinical reports and multimodal alignment. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical reasoning tasks. Therefore, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15235",
      "title": "ViSpec: Accelerating Vision-Language Models with Vision-Aware\n  Speculative Decoding",
      "authors": [
        "Jialiang Kang",
        "Han Shu",
        "Wenshuo Li",
        "Yingjie Zhai",
        "Xinghao Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), yet its application to vision-language models\n(VLMs) remains underexplored, with existing methods achieving only modest\nspeedups (<1.5x). This gap is increasingly significant as multimodal\ncapabilities become central to large-scale models. We hypothesize that large\nVLMs can effectively filter redundant image information layer by layer without\ncompromising textual comprehension, whereas smaller draft models struggle to do\nso. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a\nnovel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor\nmodule to compress image tokens into a compact representation, which is\nseamlessly integrated into the draft model's attention mechanism while\npreserving original image positional information. Additionally, we extract a\nglobal feature vector for each input image and augment all subsequent text\ntokens with this feature to enhance multimodal coherence. To overcome the\nscarcity of multimodal datasets with long assistant responses, we curate a\nspecialized training dataset by repurposing existing datasets and generating\nextended outputs using the target VLM with modified prompts. Our training\nstrategy mitigates the risk of the draft model exploiting direct access to the\ntarget model's hidden states, which could otherwise lead to shortcut learning\nwhen training solely on target model outputs. Extensive experiments validate\nViSpec, achieving, to our knowledge, the first substantial speedup in VLM\nspeculative decoding. Code is available at\nhttps://github.com/KangJialiang/ViSpec.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.15235v4",
      "pdf_url": "http://arxiv.org/pdf/2509.15235v4",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.455,
      "distributed_training_score": 0.387,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of Vision-Aware Speculative Decoding (ViSpec) to accelerate inference in vision-language models by compressing image tokens and integrating them into a draft model. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15236",
      "title": "ChannelFlow-Tools: A Standardized Dataset Creation Pipeline for 3D\n  Obstructed Channel Flows",
      "authors": [
        "Shubham Kavane",
        "Kajol Kulkarni",
        "Harald Koestler"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present ChannelFlow-Tools, a configuration-driven framework that\nstandardizes the end-to-end path from programmatic CAD solid generation to\nML-ready inputs and targets for 3D obstructed channel flows. The toolchain\nintegrates geometry synthesis with feasibility checks, signed distance field\n(SDF) voxelization, automated solver orchestration on HPC (waLBerla LBM), and\nCartesian resampling to co-registered multi-resolution tensors. A single\nHydra/OmegaConf configuration governs all stages, enabling deterministic\nreproduction and controlled ablations. As a case study, we generate 10k+ scenes\nspanning Re=100-15000 with diverse shapes and poses. An end-to-end evaluation\nof storage trade-offs directly from the emitted artifacts, a minimal 3D U-Net\nat 128x32x32, and example surrogate models with dataset size illustrate that\nthe standardized representations support reproducible ML training.\nChannelFlow-Tools turns one-off dataset creation into a reproducible,\nconfigurable pipeline for CFD surrogate modeling.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.15236v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15236v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.366,
      "datasets_score": 0.417,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of ChannelFlow-Tools, a framework for creating standardized, ML-ready datasets for 3D obstructed channel flows in CFD. It covers dataset creation through programmatic geometry synthesis, simulation, and resampling, resulting in a new dataset of over 10,000 scenes. The paper also includes benchmarking with a 3D U-Net to demonstrate dataset utility, directly aligning with research on creating, benchmarking, and evaluating datasets for ML and AI applications. This focus on reproducible dataset pipelines and empirical evaluations makes it a strong match for the topic.",
      "llm_score_status": "completed",
      "summary": "ChannelFlow-Tools is a configuration-driven framework designed to standardize the creation of datasets for 3D obstructed channel flows, encompassing geometry synthesis using CadQuery, signed distance field (SDF) voxelization with OpenVDB, high-performance computing (HPC) simulations via waLBerla lattice-Boltzmann method (LBM), and resampling into multi-resolution tensors for machine learning (ML) readiness. The paper demonstrates its methodology by generating over 10,000 scenes across various Reynolds numbers, ensuring reproducibility through a single Hydra/OmegaConf configuration, and evaluates its effectiveness with a minimal 3D U-Net model, showing predictable error reductions as dataset size increases, thereby facilitating scalable and comparable ML applications in computational fluid dynamics (CFD).",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by integrating and standardizing existing tools into a configurable pipeline for CFD dataset creation, addressing a gap in reproducible workflows, though it does not introduce entirely new problems or techniques.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in AI applications for CFD, as it provides a shareable framework that enhances reproducibility and comparability in subfields like surrogate modeling, but its influence may remain limited to specialized engineering domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a significant, practical contribution by standardizing dataset pipelines for CFD, making it valuable for researchers in AI and engineering who focus on fluid dynamics, though it is not essential for those outside this niche.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/9fe092f25593d678b1e4c96c287a258ab70e03b8",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 1,
      "average_h_index": 0.3333333333333333,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Shubham Kavane",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2101382774"
        },
        {
          "name": "Kajol Kulkarni",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2352641075"
        },
        {
          "name": "Harald Koestler",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381257516"
        }
      ]
    },
    {
      "id": "2509.15237",
      "title": "MICA: Multi-Agent Industrial Coordination Assistant",
      "authors": [
        "Di Wen",
        "Kunyu Peng",
        "Junwei Zheng",
        "Yufan Chen",
        "Yitain Shi",
        "Jiale Wei",
        "Ruiping Liu",
        "Kailun Yang",
        "Rainer Stiefelhagen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Industrial workflows demand adaptive and trustworthy assistance that can\noperate under limited computing, connectivity, and strict privacy constraints.\nIn this work, we present MICA (Multi-Agent Industrial Coordination Assistant),\na perception-grounded and speech-interactive system that delivers real-time\nguidance for assembly, troubleshooting, part queries, and maintenance. MICA\ncoordinates five role-specialized language agents, audited by a safety checker,\nto ensure accurate and compliant support. To achieve robust step understanding,\nwe introduce Adaptive Step Fusion (ASF), which dynamically blends expert\nreasoning with online adaptation from natural speech feedback. Furthermore, we\nestablish a new multi-agent coordination benchmark across representative task\ncategories and propose evaluation metrics tailored to industrial assistance,\nenabling systematic comparison of different coordination topologies. Our\nexperiments demonstrate that MICA consistently improves task success,\nreliability, and responsiveness over baseline structures, while remaining\ndeployable on practical offline hardware. Together, these contributions\nhighlight MICA as a step toward deployable, privacy-preserving multi-agent\nassistants for dynamic factory environments. The source code will be made\npublicly available at https://github.com/Kratos-Wen/MICA.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.15237v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15237v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.381,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15238",
      "title": "Generating Plans for Belief-Desire-Intention (BDI) Agents Using\n  Alternating-Time Temporal Logic (ATL)",
      "authors": [
        "Dylan Léveillé"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "Belief-Desire-Intention (BDI) is a framework for modelling agents based on\ntheir beliefs, desires, and intentions. Plans are a central component of BDI\nagents, and define sequences of actions that an agent must undertake to achieve\na certain goal. Existing approaches to plan generation often require\nsignificant manual effort, and are mainly focused on single-agent systems. As a\nresult, in this work, we have developed a tool that automatically generates BDI\nplans using Alternating-Time Temporal Logic (ATL). By using ATL, the plans\ngenerated accommodate for possible competition or cooperation between the\nagents in the system. We demonstrate the effectiveness of the tool by\ngenerating plans for an illustrative game that requires agent collaboration to\nachieve a shared goal. We show that the generated plans allow the agents to\nsuccessfully attain this goal.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.15238v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15238v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.226,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.225,
      "datasets_score": 0.238,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15239",
      "title": "KNARsack: Teaching Neural Algorithmic Reasoners to Solve\n  Pseudo-Polynomial Problems",
      "authors": [
        "Stjepan Požgaj",
        "Dobrik Georgiev",
        "Marin Šilić",
        "Petar Veličković"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Neural algorithmic reasoning (NAR) is a growing field that aims to embed\nalgorithmic logic into neural networks by imitating classical algorithms. In\nthis extended abstract, we detail our attempt to build a neural algorithmic\nreasoner that can solve Knapsack, a pseudo-polynomial problem bridging\nclassical algorithms and combinatorial optimisation, but omitted in standard\nNAR benchmarks. Our neural algorithmic reasoner is designed to closely follow\nthe two-phase pipeline for the Knapsack problem, which involves first\nconstructing the dynamic programming table and then reconstructing the solution\nfrom it. The approach, which models intermediate states through dynamic\nprogramming supervision, achieves better generalization to larger problem\ninstances than a direct-prediction baseline that attempts to select the optimal\nsubset only from the problem inputs.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.15239v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15239v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.346,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is developing a neural algorithmic reasoner for the Knapsack problem using a two-phase dynamic programming pipeline, focusing on supervising intermediate computation steps. It does not involve diffusion models, iterative refinement processes typical of diffusion (e.g., noise addition and removal for holistic reasoning paths), or any adaptation of diffusion for multi-step logical tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15241",
      "title": "M-PACE: Mother Child Framework for Multimodal Compliance",
      "authors": [
        "Shreyash Verma",
        "Amit Kesari",
        "Vinayak Trivedi",
        "Anupam Purwar",
        "Ratnesh Jamidar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Ensuring that multi-modal content adheres to brand, legal, or\nplatform-specific compliance standards is an increasingly complex challenge\nacross domains. Traditional compliance frameworks typically rely on disjointed,\nmulti-stage pipelines that integrate separate modules for image classification,\ntext extraction, audio transcription, hand-crafted checks, and rule-based\nmerges. This architectural fragmentation increases operational overhead,\nhampers scalability, and hinders the ability to adapt to dynamic guidelines\nefficiently. With the emergence of Multimodal Large Language Models (MLLMs),\nthere is growing potential to unify these workflows under a single,\ngeneral-purpose framework capable of jointly processing visual and textual\ncontent. In light of this, we propose Multimodal Parameter Agnostic Compliance\nEngine (M-PACE), a framework designed for assessing attributes across\nvision-language inputs in a single pass. As a representative use case, we apply\nM-PACE to advertisement compliance, demonstrating its ability to evaluate over\n15 compliance-related attributes. To support structured evaluation, we\nintroduce a human-annotated benchmark enriched with augmented samples that\nsimulate challenging real-world conditions, including visual obstructions and\nprofanity injection. M-PACE employs a mother-child MLLM setup, demonstrating\nthat a stronger parent MLLM evaluating the outputs of smaller child models can\nsignificantly reduce dependence on human reviewers, thereby automating quality\ncontrol. Our analysis reveals that inference costs reduce by over 31 times,\nwith the most efficient models (Gemini 2.0 Flash as child MLLM selected by\nmother MLLM) operating at 0.0005 per image, compared to 0.0159 for Gemini 2.5\nPro with comparable accuracy, highlighting the trade-off between cost and\noutput quality achieved in real time by M-PACE in real life deployment over\nadvertising data.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.15241v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15241v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.426,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.373,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses human-annotated benchmarks and meta-evaluation (e.g., Cohen’s kappa) to assess MLLM outputs, which involves human feedback for evaluation and alignment. However, it does not describe training a reward model or using reinforcement learning to fine-tune models, focusing instead on model selection and compliance checking. Thus, it is only tangentially related to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a framework using MLLMs for multimodal compliance with a mother-child setup, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. There is no mention of adapting diffusion for tasks like Chain-of-Thought processing.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15242",
      "title": "ProFusion: 3D Reconstruction of Protein Complex Structures from\n  Multi-view AFM Images",
      "authors": [
        "Jaydeep Rade",
        "Md Hasibul Hasan Hasib",
        "Meric Ozturk",
        "Baboucarr Faal",
        "Sheng Yang",
        "Dipali G. Sashital",
        "Vincenzo Venditti",
        "Baoyu Chen",
        "Soumik Sarkar",
        "Adarsh Krishnamurthy",
        "Anwesha Sarkar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "AI-based in silico methods have improved protein structure prediction but\noften struggle with large protein complexes (PCs) involving multiple\ninteracting proteins due to missing 3D spatial cues. Experimental techniques\nlike Cryo-EM are accurate but costly and time-consuming. We present ProFusion,\na hybrid framework that integrates a deep learning model with Atomic Force\nMicroscopy (AFM), which provides high-resolution height maps from random\norientations, naturally yielding multi-view data for 3D reconstruction.\nHowever, generating a large-scale AFM imaging data set sufficient to train deep\nlearning models is impractical. Therefore, we developed a virtual AFM framework\nthat simulates the imaging process and generated a dataset of ~542,000 proteins\nwith multi-view synthetic AFM images. We train a conditional diffusion model to\nsynthesize novel views from unposed inputs and an instance-specific Neural\nRadiance Field (NeRF) model to reconstruct 3D structures. Our reconstructed 3D\nprotein structures achieve an average Chamfer Distance within the AFM imaging\nresolution, reflecting high structural fidelity. Our method is extensively\nvalidated on experimental AFM images of various PCs, demonstrating strong\npotential for accurate, cost-effective protein complex structure prediction and\nrapid iterative validation using AFM experiments.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.15242v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15242v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.371,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15243",
      "title": "Multi-Modal Interpretability for Enhanced Localization in\n  Vision-Language Models",
      "authors": [
        "Muhammad Imran",
        "Yugyung Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in vision-language models have significantly expanded the\nfrontiers of automated image analysis. However, applying these models in\nsafety-critical contexts remains challenging due to the complex relationships\nbetween objects, subtle visual cues, and the heightened demand for transparency\nand reliability. This paper presents the Multi-Modal Explainable Learning\n(MMEL) framework, designed to enhance the interpretability of vision-language\nmodels while maintaining high performance. Building upon prior work in\ngradient-based explanations for transformer architectures (Grad-eclip), MMEL\nintroduces a novel Hierarchical Semantic Relationship Module that enhances\nmodel interpretability through multi-scale feature processing, adaptive\nattention weighting, and cross-modal alignment. Our approach processes features\nat multiple semantic levels to capture relationships between image regions at\ndifferent granularities, applying learnable layer-specific weights to balance\ncontributions across the model's depth. This results in more comprehensive\nvisual explanations that highlight both primary objects and their contextual\nrelationships with improved precision. Through extensive experiments on\nstandard datasets, we demonstrate that by incorporating semantic relationship\ninformation into gradient-based attribution maps, MMEL produces more focused\nand contextually aware visualizations that better reflect how vision-language\nmodels process complex scenes. The MMEL framework generalizes across various\ndomains, offering valuable insights into model decisions for applications\nrequiring high interpretability and reliability.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.15243v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15243v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.329,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.324,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of the Multi-Modal Explainable Learning (MMEL) framework for improving interpretability in vision-language models, focusing on gradient-based explanations, hierarchical semantic relationships, and cross-modal alignment in models like CLIP. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, which are central to the topic of Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15246",
      "title": "GenCAD-3D: CAD Program Generation using Multimodal Latent Space\n  Alignment and Synthetic Dataset Balancing",
      "authors": [
        "Nomi Yu",
        "Md Ferdous Alam",
        "A. John Hart",
        "Faez Ahmed"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "CAD programs, structured as parametric sequences of commands that compile\ninto precise 3D geometries, are fundamental to accurate and efficient\nengineering design processes. Generating these programs from nonparametric data\nsuch as point clouds and meshes remains a crucial yet challenging task,\ntypically requiring extensive manual intervention. Current deep generative\nmodels aimed at automating CAD generation are significantly limited by\nimbalanced and insufficiently large datasets, particularly those lacking\nrepresentation for complex CAD programs. To address this, we introduce\nGenCAD-3D, a multimodal generative framework utilizing contrastive learning for\naligning latent embeddings between CAD and geometric encoders, combined with\nlatent diffusion models for CAD sequence generation and retrieval.\nAdditionally, we present SynthBal, a synthetic data augmentation strategy\nspecifically designed to balance and expand datasets, notably enhancing\nrepresentation of complex CAD geometries. Our experiments show that SynthBal\nsignificantly boosts reconstruction accuracy, reduces the generation of invalid\nCAD models, and markedly improves performance on high-complexity geometries,\nsurpassing existing benchmarks. These advancements hold substantial\nimplications for streamlining reverse engineering and enhancing automation in\nengineering design. We will publicly release our datasets and code, including a\nset of 51 3D-printed and laser-scanned parts on our project site.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.15246v1",
      "pdf_url": "http://arxiv.org/pdf/2509.15246v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.433,
      "distributed_training_score": 0.374,
      "datasets_score": 0.389,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses latent diffusion models for CAD sequence generation and retrieval, which involves iterative refinement processes typical of diffusion models. However, it applies this to generating parametric CAD programs from geometric data, not to solving complex logical tasks or treating a 'Chain-of-Thought' as a holistic entity for reasoning. Since there is no clear component for multi-step logical reasoning, the relevance is indirect and limited to the shared mechanism of iterative refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.15248",
      "title": "Synthetic bootstrapped pretraining",
      "authors": [
        "Zitong Yang",
        "Aonan Zhang",
        "Hong Liu",
        "Tatsunori Hashimoto",
        "Emmanuel Candès",
        "Chong Wang",
        "Ruoming Pang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM)\npretraining procedure that first learns a model of relations between documents\nfrom the pretraining dataset and then leverages it to synthesize a vast new\ncorpus for joint training. While the standard pretraining teaches LMs to learn\ncausal correlations among tokens within a single document, it is not designed\nto efficiently model the rich, learnable inter-document correlations that can\npotentially lead to better performance. We validate SBP by designing a\ncompute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T\ntokens from scratch. We find SBP consistently improves upon a strong repetition\nbaseline and delivers a significant fraction of performance improvement\nattainable by an oracle upper bound with access to 20x more unique data.\nQualitative analysis reveals that the synthesized documents go beyond mere\nparaphrases -- SBP first abstracts a core concept from the seed material and\nthen crafts a new narration on top of it. Besides strong empirical performance,\nSBP admits a natural Bayesian interpretation: the synthesizer implicitly learns\nto abstract the latent concepts shared between related documents.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.15248v2",
      "pdf_url": "http://arxiv.org/pdf/2509.15248v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.393,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's SBP method programmatically generates synthetic data from existing documents by identifying similarities and training a conditional model, which aligns with weak supervision's use of high-level, noisy sources to create training material without hand-labeling. However, SBP focuses on data synthesis for language model pretraining rather than directly generating labels for supervised tasks, making it moderately relevant rather than highly so.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for reasoning, or any multi-step logical processes for tasks like Chain-of-Thought. It centers on synthetic data generation for language model pretraining, with no components related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Synthetic Bootstrapped Pretraining (SBP), a novel method for language model pretraining that addresses inter-document correlations by first identifying semantically similar document pairs from the existing dataset, training a conditional synthesizer to generate new related documents, and then using this synthesized corpus for joint training alongside the original data. This approach improves model performance by capturing previously overlooked correlations, as demonstrated through experiments on a 3B-parameter model trained on up to 1T tokens, where SBP outperforms a repetition baseline and achieves significant gains comparable to having access to 20 times more unique data, while also providing a Bayesian interpretation of the synthesizer's role in abstracting latent concepts.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new pretraining technique that models inter-document correlations through synthetic data generation, significantly advancing the state-of-the-art in language model training by addressing limitations in standard methods. This innovation represents a fresh approach to data utilization in a data-constrained era, rather than mere incremental improvements.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and commercial applications in AI by enabling more efficient use of existing data amidst the depletion of high-quality web text. Its empirical validation and scalable framework suggest it could be built upon to enhance language model development in data-scarce environments.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a strong, valuable contribution to language model pretraining that addresses a timely challenge, making it essential for researchers in AI and computational linguistics to be aware of its insights and methodologies. While not revolutionary across all fields, its practical implications and rigorous evaluation warrant attention from relevant experts.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e5c3fdb980ba0f5b9a2f9b2487125dc5704654f2",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 8,
      "average_h_index": 3.5714285714285716,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Zitong Yang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2283514777"
        },
        {
          "name": "Aonan Zhang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2291744374"
        },
        {
          "name": "Hong Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382555011"
        },
        {
          "name": "Tatsunori Hashimoto",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2294362683"
        },
        {
          "name": "Emmanuel J. Candès",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2283307289"
        },
        {
          "name": "Chong Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381715411"
        },
        {
          "name": "Ruoming Pang",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2238621132"
        }
      ]
    },
    {
      "id": "2509.16250",
      "title": "A study on Deep Convolutional Neural Networks, transfer learning, and\n  Mnet model for Cervical Cancer Detection",
      "authors": [
        "Saifuddin Sagor",
        "Md Taimur Ahad",
        "Faruk Ahmed",
        "Rokonozzaman Ayon",
        "Sanzida Parvin"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Early and accurate detection through Pap smear analysis is critical to\nimproving patient outcomes and reducing mortality of Cervical cancer.\nState-of-the-art (SOTA) Convolutional Neural Networks (CNNs) require\nsubstantial computational resources, extended training time, and large\ndatasets. In this study, a lightweight CNN model, S-Net (Simple Net), is\ndeveloped specifically for cervical cancer detection and classification using\nPap smear images to address these limitations. Alongside S-Net, six SOTA CNNs\nwere evaluated using transfer learning, including multi-path (DenseNet201,\nResNet152), depth-based (Serasnet152), width-based multi-connection (Xception),\ndepth-wise separable convolutions (MobileNetV2), and spatial exploitation-based\n(VGG19). All models, including S-Net, achieved comparable accuracy, with S-Net\nreaching 99.99%. However, S-Net significantly outperforms the SOTA CNNs in\nterms of computational efficiency and inference time, making it a more\npractical choice for real-time and resource-constrained applications. A major\nlimitation in CNN-based medical diagnosis remains the lack of transparency in\nthe decision-making process. To address this, Explainable AI (XAI) techniques,\nsuch as SHAP, LIME, and Grad-CAM, were employed to visualize and interpret the\nkey image regions influencing model predictions. The novelty of this study lies\nin the development of a highly accurate yet computationally lightweight model\n(S-Net) caPable of rapid inference while maintaining interpretability through\nXAI integration. Furthermore, this work analyzes the behavior of SOTA CNNs,\ninvestigates the effects of negative transfer learning on Pap smear images, and\nexamines pixel intensity patterns in correctly and incorrectly classified\nsamples.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.16250v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16250v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.281,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.323,
      "distributed_training_score": 0.347,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16251",
      "title": "R-Net: A Reliable and Resource-Efficient CNN for Colorectal Cancer\n  Detection with XAI Integration",
      "authors": [
        "Rokonozzaman Ayon",
        "Md Taimur Ahad",
        "Bo Song",
        "Yan Li"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "State-of-the-art (SOTA) Convolutional Neural Networks (CNNs) are criticized\nfor their extensive computational power, long training times, and large\ndatasets. To overcome this limitation, we propose a reasonable network (R-Net),\na lightweight CNN only to detect and classify colorectal cancer (CRC) using the\nEnteroscope Biopsy Histopathological Hematoxylin and Eosin Image Dataset\n(EBHI). Furthermore, six SOTA CNNs, including Multipath-based CNNs\n(DenseNet121, ResNet50), Depth-based CNNs (InceptionV3), width-based\nmulti-connection CNNs (Xception), depth-wise separable convolutions\n(MobileNetV2), spatial exploitation-based CNNs (VGG16), Transfer learning, and\ntwo ensemble models are also tested on the same dataset. The ensemble models\nare a multipath-depth-width combination (DenseNet121-InceptionV3-Xception) and\na multipath-depth-spatial combination (ResNet18-InceptionV3-VGG16). However,\nthe proposed R-Net lightweight achieved 99.37% accuracy, outperforming\nMobileNet (95.83%) and ResNet50 (96.94%). Most importantly, to understand the\ndecision-making of R-Net, Explainable AI such as SHAP, LIME, and Grad-CAM are\nintegrated to visualize which parts of the EBHI image contribute to the\ndetection and classification process of R-Net. The main novelty of this\nresearch lies in building a reliable, lightweight CNN R-Net that requires fewer\ncomputing resources yet maintains strong prediction results. SOTA CNNs,\ntransfer learning, and ensemble models also extend our knowledge on CRC\nclassification and detection. XAI functionality and the impact of pixel\nintensity on correct and incorrect classification images are also some\nnovelties in CRC detection and classification.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.16251v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16251v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.322,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16254",
      "title": "Imaging Modalities-Based Classification for Lung Cancer Detection",
      "authors": [
        "Sajim Ahmed",
        "Muhammad Zain Chaudhary",
        "Muhammad Zohaib Chaudhary",
        "Mahmoud Abbass",
        "Ahmed Sherif",
        "Mohammad Mahbubur Rahman Khan Mamun"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Lung cancer continues to be the predominant cause of cancer-related mortality\nglobally. This review analyzes various approaches, including advanced image\nprocessing methods, focusing on their efficacy in interpreting CT scans, chest\nradiographs, and biological markers. Notably, we identify critical gaps in the\nprevious surveys, including the need for robust models that can generalize\nacross diverse populations and imaging modalities. This comprehensive synthesis\naims to serve as a foundational resource for researchers and clinicians,\nguiding future efforts toward more accurate and efficient lung cancer\ndetection. Key findings reveal that 3D CNN architectures integrated with CT\nscans achieve the most superior performances, yet challenges such as high false\npositives, dataset variability, and computational complexity persist across\nmodalities.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.16254v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16254v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.264,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.319,
      "distributed_training_score": 0.293,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16256",
      "title": "HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in\n  Low-Resource African Language",
      "authors": [
        "Asiya Ibrahim Zanga",
        "Salisu Mamman Abdulrahman",
        "Abubakar Ado",
        "Abdulkadir Abubakar Bichi",
        "Lukman Aliyu Jibril",
        "Abdulmajid Babangida Umar",
        "Alhassan Adamu",
        "Shamsuddeen Hassan Muhammad",
        "Bashir Salisu Abubakar"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The development of Natural Language Processing (NLP) tools for low-resource\nlanguages is critically hindered by the scarcity of annotated datasets. This\npaper addresses this fundamental challenge by introducing HausaMovieReview, a\nnovel benchmark dataset comprising 5,000 YouTube comments in Hausa and\ncode-switched English. The dataset was meticulously annotated by three\nindependent annotators, demonstrating a robust agreement with a Fleiss' Kappa\nscore of 0.85 between annotators. We used this dataset to conduct a comparative\nanalysis of classical models (Logistic Regression, Decision Tree, K-Nearest\nNeighbors) and fine-tuned transformer models (BERT and RoBERTa). Our results\nreveal a key finding: the Decision Tree classifier, with an accuracy and\nF1-score 89.72% and 89.60% respectively, significantly outperformed the deep\nlearning models. Our findings also provide a robust baseline, demonstrating\nthat effective feature engineering can enable classical models to achieve\nstate-of-the-art performance in low-resource contexts, thereby laying a solid\nfoundation for future research.\n  Keywords: Hausa, Kannywood, Low-Resource Languages, NLP, Sentiment Analysis",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.16256v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16256v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.288,
      "distributed_training_score": 0.285,
      "datasets_score": 0.42,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new benchmark dataset, HausaMovieReview, for sentiment analysis in a low-resource language. It details the dataset's creation, including annotation by three independent annotators with a high Fleiss' Kappa score, and uses it for benchmarking various models. This directly aligns with research on creating, analyzing, and benchmarking datasets for AI and machine learning applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces HausaMovieReview, a new benchmark dataset consisting of 5,000 annotated YouTube comments in Hausa and code-switched English, aimed at addressing the lack of resources for sentiment analysis in low-resource African languages. The authors meticulously annotated the dataset, achieving high inter-annotator agreement, and conducted a comparative analysis of classical machine learning models and fine-tuned transformers, revealing that the Decision Tree classifier outperformed others with 89.72% accuracy and 89.60% F1-score, highlighting the potential of feature engineering in low-resource settings.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper introduces a new dataset for a low-resource language, which is a clever combination of existing sentiment analysis techniques applied to an underrepresented context, but it does not introduce a entirely new problem or architecture. This represents a notable improvement by providing a benchmark that advances research in African languages without fundamentally altering the state-of-the-art methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of NLP for low-resource languages, as it provides a valuable dataset and baseline for future sentiment analysis research in African contexts. However, its influence may be limited to specific areas rather than broadly affecting general AI or commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by offering a new dataset and practical insights for researchers in low-resource language processing, making it essential for those working in related NLP subfields. While not groundbreaking for the entire field, it provides important resources that warrant attention from informed readers.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/54e738e03574aea62d07e1c63d675612a0eb30ef",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 16,
      "average_h_index": 4.111111111111111,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Asiya Ibrahim Zanga",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381362726"
        },
        {
          "name": "S. Abdulrahman",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2199216"
        },
        {
          "name": "Abubakar Ado",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/52327459"
        },
        {
          "name": "Abdulkadir Abubakar Bichi",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/100809019"
        },
        {
          "name": "Lukman Aliyu Jibril",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381363866"
        },
        {
          "name": "Abdulmajid Babangida Umar",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381362276"
        },
        {
          "name": "Alhassan Adamu",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/144383586"
        },
        {
          "name": "Shamsuddeen Hassan Muhammad",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/7744881"
        },
        {
          "name": "Bashir Salisu Abubakar",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/80072778"
        }
      ]
    },
    {
      "id": "2509.18159",
      "title": "PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal\n  Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization\n  on the Kvasir Dataset",
      "authors": [
        "Akwasi Asare",
        "Ulas Bagci"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Colorectal cancer (CRC) remains one of the leading causes of cancer-related\nmorbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as\ncritical precursors according to the World Health Organization (WHO). Early and\naccurate segmentation of polyps during colonoscopy is essential for reducing\nCRC progression, yet manual delineation is labor-intensive and prone to\nobserver variability. Deep learning methods have demonstrated strong potential\nfor automated polyp analysis, but their limited interpretability remains a\nbarrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an\nexplainable deep learning framework that integrates the U-Net architecture with\nGradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp\nsegmentation. The model was trained and evaluated on the Kvasir-SEG dataset of\n1000 annotated endoscopic images. Experimental results demonstrate robust\nsegmentation performance, achieving a mean Intersection over Union (IoU) of\n0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)\non training and validation sets. Grad-CAM visualizations further confirmed that\npredictions were guided by clinically relevant regions, enhancing transparency\nand trust in the model's decisions. By coupling high segmentation accuracy with\ninterpretability, PolypSeg-GradCAM represents a step toward reliable,\ntrustworthy AI-assisted colonoscopy and improved early colorectal cancer\nprevention.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.18159v2",
      "pdf_url": "http://arxiv.org/pdf/2509.18159v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.264,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.336,
      "distributed_training_score": 0.267,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18160",
      "title": "PerceptronCARE: A Deep Learning-Based Intelligent Teleophthalmology\n  Application for Diabetic Retinopathy Diagnosis",
      "authors": [
        "Akwasi Asare",
        "Isaac Baffour Senkyire",
        "Emmanuel Freeman",
        "Mary Sagoe",
        "Simon Hilary Ayinedenaba Aluze-Ele",
        "Kelvin Kwao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Diabetic retinopathy is a leading cause of vision loss among adults and a\nmajor global health challenge, particularly in underserved regions. This study\npresents PerceptronCARE, a deep learning-based teleophthalmology application\ndesigned for automated diabetic retinopathy detection using retinal images. The\nsystem was developed and evaluated using multiple convolutional neural\nnetworks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine\nthe optimal balance between accuracy and computational efficiency. The final\nmodel classifies disease severity with an accuracy of 85.4%, enabling real-time\nscreening in clinical and telemedicine settings. PerceptronCARE integrates\ncloud-based scalability, secure patient data management, and a multi-user\nframework, facilitating early diagnosis, improving doctor-patient interactions,\nand reducing healthcare costs. This study highlights the potential of AI-driven\ntelemedicine solutions in expanding access to diabetic retinopathy screening,\nparticularly in remote and resource-constrained environments.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.18160v2",
      "pdf_url": "http://arxiv.org/pdf/2509.18160v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.292,
      "distributed_training_score": 0.364,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18161",
      "title": "Developing Training Procedures for Piecewise-linear Spline Activation\n  Functions in Neural Networks",
      "authors": [
        "William H Patty"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Activation functions in neural networks are typically selected from a set of\nempirically validated, commonly used static functions such as ReLU, tanh, or\nsigmoid. However, by optimizing the shapes of a network's activation functions,\nwe can train models that are more parameter-efficient and accurate by assigning\nmore optimal activations to the neurons. In this paper, I present and compare 9\ntraining methodologies to explore dual-optimization dynamics in neural networks\nwith parameterized linear B-spline activation functions. The experiments\nrealize up to 94% lower end model error rates in FNNs and 51% lower rates in\nCNNs compared to traditional ReLU-based models. These gains come at the cost of\nadditional development and training complexity as well as end model latency.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.18161v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18161v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.38,
      "datasets_score": 0.238,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18165",
      "title": "Self Identity Mapping",
      "authors": [
        "Xiuding Cai",
        "Yaoyao Zhu",
        "Linjie Fu",
        "Dong Miao",
        "Yu Yao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Regularization is essential in deep learning to enhance generalization and\nmitigate overfitting. However, conventional techniques often rely on\nheuristics, making them less reliable or effective across diverse settings. We\npropose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic\nregularization framework that leverages an inverse mapping mechanism to enhance\nrepresentation learning. By reconstructing the input from its transformed\noutput, SIM reduces information loss during forward propagation and facilitates\nsmoother gradient flow. To address computational inefficiencies, We instantiate\nSIM as $ \\rho\\text{SIM} $ by incorporating patch-level feature sampling and\nprojection-based method to reconstruct latent features, effectively lowering\ncomplexity. As a model-agnostic, task-agnostic regularizer, SIM can be\nseamlessly integrated as a plug-and-play module, making it applicable to\ndifferent network architectures and tasks.\n  We extensively evaluate $\\rho\\text{SIM}$ across three tasks: image\nclassification, few-shot prompt learning, and domain generalization.\nExperimental results show consistent improvements over baseline methods,\nhighlighting $\\rho\\text{SIM}$'s ability to enhance representation learning\nacross various tasks. We also demonstrate that $\\rho\\text{SIM}$ is orthogonal\nto existing regularization methods, boosting their effectiveness. Moreover, our\nresults confirm that $\\rho\\text{SIM}$ effectively preserves semantic\ninformation and enhances performance in dense-to-dense tasks, such as semantic\nsegmentation and image translation, as well as in non-visual domains including\naudio classification and time series anomaly detection. The code is publicly\navailable at https://github.com/XiudingCai/SIM-pytorch.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.18165v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18165v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.363,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18168",
      "title": "HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics",
      "authors": [
        "Dong Liu",
        "Yanxuan Yu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Semantic parsing of long documents remains challenging due to quadratic\ngrowth in pairwise composition and memory requirements. We introduce\n\\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that\ndecomposes an input of length $N$ into $M$ meaningful segments, constructs\n\\emph{Local Semantic Graphs} on each segment, and extracts compact\n\\emph{summary nodes} to form a \\emph{Global Graph Memory}. HSGM supports\n\\emph{incremental updates} -- only newly arrived segments incur local graph\nconstruction and summary-node integration -- while \\emph{Hierarchical Query\nProcessing} locates relevant segments via top-$K$ retrieval over summary nodes\nand then performs fine-grained reasoning within their local graphs.\n  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to\n$O\\!\\left(N\\,k + (N/k)^2\\right)$, with segment size $k \\ll N$, and we derive\nFrobenius-norm bounds on the approximation error introduced by node\nsummarization and sparsification thresholds. Empirically, on three benchmarks\n-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),\nand legal event extraction -- HSGM achieves \\emph{2--4$\\times$ inference\nspeedup}, \\emph{$>60\\%$ reduction} in peak memory, and \\emph{$\\ge 95\\%$} of\nbaseline accuracy. Our approach unlocks scalable, accurate semantic modeling\nfor ultra-long texts, enabling real-time and resource-constrained NLP\napplications.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.18168v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18168v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.382,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces HSGM, a framework for hierarchical graph-based memory to handle scalable semantic parsing of long texts, focusing on graph construction, summarization, and efficient querying. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning on a 'Chain-of-Thought'. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18170",
      "title": "MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients\n  for Label-Inference-Free Gradient Inversion",
      "authors": [
        "Zhanting Zhou",
        "Jinbo Wang",
        "Zeqin Wu",
        "Fengli Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We study gradient inversion in the challenging single round averaged gradient\nSAG regime where per sample cues are entangled within a single batch mean\ngradient. We introduce MAGIA a momentum based adaptive correction on gradient\ninversion attack a novel label inference free framework that senses latent per\nimage signals by probing random data subsets. MAGIA objective integrates two\ncore innovations 1 a closed form combinatorial rescaling that creates a\nprovably tighter optimization bound and 2 a momentum based mixing of whole\nbatch and subset losses to ensure reconstruction robustness. Extensive\nexperiments demonstrate that MAGIA significantly outperforms advanced methods\nachieving high fidelity multi image reconstruction in large batch scenarios\nwhere prior works fail. This is all accomplished with a computational footprint\ncomparable to standard solvers and without requiring any auxiliary information.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.18170v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18170v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.335,
      "datasets_score": 0.26,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18174",
      "title": "Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR",
      "authors": [
        "Khalil Hennara",
        "Muhammad Hreden",
        "Mohamed Motasim Hamed",
        "Ahmad Bastati",
        "Zeina Aldallal",
        "Sara Chrouf",
        "Safwan AlModhayan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Arabic document OCR remains a challenging task due to the language's cursive\nscript, diverse fonts, diacritics, and right-to-left orientation. While modern\nMultimodal Large Language Models (MLLMs) have advanced document understanding\nfor high-resource languages, their performance on Arabic remains limited. In\nthis work, we introduce Baseer, a vision-language model fine-tuned specifically\nfor Arabic document OCR. Leveraging a large-scale dataset combining synthetic\nand real-world documents, Baseer is trained using a decoder-only fine-tuning\nstrategy to adapt a pre-trained MLLM while preserving general visual features.\nWe also present Misraj-DocOCR, a high-quality, expert-verified benchmark\ndesigned for rigorous evaluation of Arabic OCR systems. Our experiments show\nthat Baseer significantly outperforms existing open-source and commercial\nsolutions, achieving a WER of 0.25 and establishing a new state-of-the-art in\nthe domain of Arabic document OCR. Our results highlight the benefits of\ndomain-specific adaptation of general-purpose MLLMs and establish a strong\nbaseline for high-accuracy OCR on morphologically rich languages like Arabic.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.18174v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18174v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.332,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18176",
      "title": "A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground\n  Deformation in Eastern Ireland",
      "authors": [
        "Wendong Yao",
        "Saeed Azadnejad",
        "Binhua Huang",
        "Shane Donohue",
        "Soumyabrata Dev"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Monitoring ground displacement is crucial for urban infrastructure stability\nand mitigating geological hazards. However, forecasting future deformation from\nsparse Interferometric Synthetic Aperture Radar (InSAR) time-series data\nremains a significant challenge. This paper introduces a novel deep learning\nframework that transforms these sparse point measurements into a dense\nspatio-temporal tensor. This methodological shift allows, for the first time,\nthe direct application of advanced computer vision architectures to this\nforecasting problem. We design and implement a hybrid Convolutional Neural\nNetwork and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to\nsimultaneously learn spatial patterns and temporal dependencies from the\ngenerated data tensor. The model's performance is benchmarked against powerful\nmachine learning baselines, Light Gradient Boosting Machine and LASSO\nregression, using Sentinel-1 data from eastern Ireland. Results demonstrate\nthat the proposed architecture provides significantly more accurate and\nspatially coherent forecasts, establishing a new performance benchmark for this\ntask. Furthermore, an interpretability analysis reveals that baseline models\noften default to simplistic persistence patterns, highlighting the necessity of\nour integrated spatio-temporal approach to capture the complex dynamics of\nground deformation. Our findings confirm the efficacy and potential of\nspatio-temporal deep learning for high-resolution deformation forecasting.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.18176v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18176v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.354,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18177",
      "title": "A Framework for Generating Artificial Datasets to Validate Absolute and\n  Relative Position Concepts",
      "authors": [
        "George Corrêa de Araújo",
        "Helena de Almeida Maia",
        "Helio Pedrini"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In this paper, we present the Scrapbook framework, a novel methodology\ndesigned to generate extensive datasets for probing the learned concepts of\nartificial intelligence (AI) models. The framework focuses on fundamental\nconcepts such as object recognition, absolute and relative positions, and\nattribute identification. By generating datasets with a large number of\nquestions about individual concepts and a wide linguistic variation, the\nScrapbook framework aims to validate the model's understanding of these basic\nelements before tackling more complex tasks. Our experimental findings reveal\nthat, while contemporary models demonstrate proficiency in recognizing and\nenumerating objects, they encounter challenges in comprehending positional\ninformation and addressing inquiries with additional constraints. Specifically,\nthe MobileVLM-V2 model showed significant answer disagreements and plausible\nwrong answers, while other models exhibited a bias toward affirmative answers\nand struggled with questions involving geometric shapes and positional\ninformation, indicating areas for improvement in understanding and consistency.\nThe proposed framework offers a valuable instrument for generating diverse and\ncomprehensive datasets, which can be utilized to systematically assess and\nenhance the performance of AI models.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.18177v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18177v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.413,
      "weak_supervision_score": 0.438,
      "diffusion_reasoning_score": 0.45,
      "distributed_training_score": 0.379,
      "datasets_score": 0.52,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on generating datasets to validate AI models' understanding of concepts like positions and objects, with no mention of human feedback, reinforcement learning, or training models to align with human preferences.",
      "weak_supervision_justification": "The paper introduces a framework for generating datasets to probe and evaluate AI models, but it does not involve training models using noisy or programmatically generated labels, which is the core of weak supervision.",
      "diffusion_reasoning_justification": "The paper deals with dataset generation for testing basic concepts in AI models and does not incorporate diffusion models, iterative refinement, or multi-step logical reasoning processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the Scrapbook framework for creating and evaluating datasets to assess AI models' understanding of concepts, directly aligning with research on dataset generation, benchmarking, and analysis for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces the Scrapbook framework, a methodology for generating extensive artificial datasets to evaluate AI models' understanding of fundamental concepts such as object recognition, absolute and relative positions, and attribute identification. By creating datasets with a high volume of varied questions focused on individual concepts, the framework enables systematic probing; experimental results with models like MobileVLM-V2, TinyGPT-V, and MiniGPT-4 demonstrate strong performance in object detection and counting but reveal weaknesses in handling positional information and questions with additional constraints, underscoring the need for improvements in AI model consistency and comprehension.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing dataset generation techniques tailored to deeply probe specific AI concepts, offering a notable improvement over general visual question answering datasets by focusing on individual concepts extensively. While not entirely groundbreaking, it advances the state-of-the-art in AI model validation by providing a structured framework for targeted evaluation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research within subfields of AI and computer vision by offering a tool for assessing and improving model understanding of basic concepts, potentially leading to better bias mitigation and performance enhancements. However, its applicability may be limited to specific areas like visual question answering and model pre-training, rather than broader commercial or interdisciplinary impacts.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a valuable, practical framework for evaluating AI models' conceptual understanding, making it essential for researchers focused on AI reliability and bias. It represents a strong contribution that advances model assessment techniques, though it may not be critical for those outside specific AI subfields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c2d4066ecdaa44a7262f6b4c4f3761991a0cead5",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 7,
      "average_h_index": 2.6666666666666665,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "George Correa de Ara'ujo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2187299688"
        },
        {
          "name": "H. Maia",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/8125221"
        },
        {
          "name": "Hélio Pedrini",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2281342509"
        }
      ]
    },
    {
      "id": "2509.18178",
      "title": "Foam-Agent 2.0: An End-to-End Composable Multi-Agent Framework for\n  Automating CFD Simulation in OpenFOAM",
      "authors": [
        "Ling Yue",
        "Nithin Somasekharan",
        "Tingwen Zhang",
        "Yadi Cao",
        "Shaowu Pan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Computational Fluid Dynamics (CFD) is an essential simulation tool in\nengineering, yet its steep learning curve and complex manual setup create\nsignificant barriers. To address these challenges, we introduce Foam-Agent, a\nmulti-agent framework that automates the entire end-to-end OpenFOAM workflow\nfrom a single natural language prompt. Our key innovations address critical\ngaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:\nFoam-Agent is the first system to manage the full simulation pipeline,\nincluding advanced pre-processing with a versatile Meshing Agent capable of\nhandling external mesh files and generating new geometries via Gmsh, automatic\ngeneration of HPC submission scripts, and post-simulation visualization via\nParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,\nthe framework uses Model Context Protocol (MCP) to expose its core functions as\ndiscrete, callable tools. This allows for flexible integration and use by other\nagentic systems, such as Claude-code, for more exploratory workflows. 3.\nHigh-Fidelity Configuration Generation: We achieve superior accuracy through a\nHierarchical Multi-Index RAG for precise context retrieval and a\ndependency-aware generation process that ensures configuration consistency.\nEvaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%\nsuccess rate with Claude 3.5 Sonnet, significantly outperforming existing\nframeworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the\nexpertise barrier for CFD, demonstrating how specialized multi-agent systems\ncan democratize complex scientific computing. The code is public at\nhttps://github.com/csml-rpi/Foam-Agent.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.18178v2",
      "pdf_url": "http://arxiv.org/pdf/2509.18178v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.37,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a multi-agent framework for automating CFD simulations using LLMs, focusing on iterative debugging, planning, and tool integration, but it does not involve diffusion models or adapt the iterative refinement process of diffusion for logical tasks. There is no mention of treating Chain-of-Thought as a holistic entity via diffusion-based methods, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.20369",
      "title": "AI-driven formative assessment and adaptive learning in data-science\n  education: Evaluating an LLM-powered virtual teaching assistant",
      "authors": [
        "Fadjimata I Anaroua",
        "Qing Li",
        "Yan Tang",
        "Hong P. Liu"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "This paper presents VITA (Virtual Teaching Assistants), an adaptive\ndistributed learning (ADL) platform that embeds a large language model\n(LLM)-powered chatbot (BotCaptain) to provide dialogic support, interoperable\nanalytics, and integrity-aware assessment for workforce preparation in data\nscience. The platform couples context-aware conversational tutoring with\nformative-assessment patterns designed to promote reflective reasoning. The\npaper describes an end-to-end data pipeline that transforms chat logs into\nExperience API (xAPI) statements, instructor dashboards that surface outliers\nfor just-in-time intervention, and an adaptive pathway engine that routes\nlearners among progression, reinforcement, and remediation content. The paper\nalso benchmarks VITA conceptually against emerging tutoring architectures,\nincluding retrieval-augmented generation (RAG)--based assistants and Learning\nTools Interoperability (LTI)--integrated hubs, highlighting trade-offs among\ncontent grounding, interoperability, and deployment complexity. Contributions\ninclude a reusable architecture for interoperable conversational analytics, a\ncatalog of patterns for integrity-preserving formative assessment, and a\npractical blueprint for integrating adaptive pathways into data-science\ncourses. The paper concludes with implementation lessons and a roadmap (RAG\nintegration, hallucination mitigation, and LTI~1.3 / OpenID Connect) to guide\nmulti-course evaluations and broader adoption. In light of growing demand and\nscalability constraints in traditional instruction, the approach illustrates\nhow conversational AI can support engagement, timely feedback, and personalized\nlearning at scale. Future work will refine the platform's adaptive intelligence\nand examine applicability across varied educational settings.",
      "published_date": "2025-09-17",
      "arxiv_url": "http://arxiv.org/abs/2509.20369v1",
      "pdf_url": "http://arxiv.org/pdf/2509.20369v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.365,
      "datasets_score": 0.422,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on applying pre-trained LLMs (e.g., OpenAI's models) in an educational platform for adaptive learning and assessments, but it does not describe or involve the process of RLHF. There is no mention of training a reward model, using human-ranked data, or fine-tuning models with reinforcement learning based on human feedback. The work is about deployment and integration, not alignment techniques like RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses LLM-powered chatbots for conversational tutoring and adaptive learning, including reflective reasoning and formative assessments, but it does not incorporate diffusion-based models or processes for multi-step logical reasoning. There is no reference to iterative refinement of a 'Chain-of-Thought' as a holistic entity, making this topic unrelated to the paper's contributions.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper describes a data pipeline that transforms chat logs into xAPI statements for analytics and assessments, which involves data handling and visualization in an educational context. However, it does not primarily focus on creating, analyzing, benchmarking, or evaluating datasets for ML/AI applications; instead, it uses data for adaptive learning, making it only peripherally related to dataset research.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    }
  ],
  "total_papers": 170,
  "date": "2025-09-17"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Relevance Filter State Management
        let currentRelevanceFilters = {
            highlyRelevant: true,
            moderatelyRelevant: true,
            tangentiallyRelevant: true,
            notRelevant: true
        };
        
        let pendingRelevanceFilters = { ...currentRelevanceFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function calculateRelevanceScore(paper) {
            let score = 0;
            
            // Only consider topics that are currently selected/enabled in the topic filter
            const topicsToConsider = [];
            if (currentTopicFilters.rlhf) topicsToConsider.push('rlhf_relevance');
            if (currentTopicFilters.weakSupervision) topicsToConsider.push('weak_supervision_relevance');
            if (currentTopicFilters.diffusionReasoning) topicsToConsider.push('diffusion_reasoning_relevance');
            if (currentTopicFilters.distributedTraining) topicsToConsider.push('distributed_training_relevance');
            if (currentTopicFilters.datasets) topicsToConsider.push('datasets_relevance');
            
            // If no topics are selected, return 0
            if (topicsToConsider.length === 0) return 0;
            
            // Weighted scoring system
            const relevanceWeights = {
                'Highly Relevant': 4,
                'Moderately Relevant': 3,
                'Tangentially Relevant': 2,
                'Not Relevant': 1
            };
            
            // Sum up scores for selected topics only
            for (let topicField of topicsToConsider) {
                const relevance = paper[topicField];
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = relevance === "not_validated" ? "Not Relevant" : relevance;
                score += relevanceWeights[normalizedRelevance] || 1; // Default to 1 if unknown
            }
            
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'relevance_high':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(b) - calculateRelevanceScore(a));
                    break;
                case 'relevance_low':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(a) - calculateRelevanceScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'relevance_high': 'Relevance (Highest to Lowest)',
                'relevance_low': 'Relevance (Lowest to Highest)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
            updateNoveltyButtonText();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
            updateImpactButtonText();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for relevance checkboxes to sync between mobile and desktop
            document.getElementById('mobile-relevance-highly').addEventListener('change', () => {
                document.getElementById('desktop-relevance-highly').checked = 
                    document.getElementById('mobile-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-moderately').addEventListener('change', () => {
                document.getElementById('desktop-relevance-moderately').checked = 
                    document.getElementById('mobile-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('desktop-relevance-tangentially').checked = 
                    document.getElementById('mobile-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-not').addEventListener('change', () => {
                document.getElementById('desktop-relevance-not').checked = 
                    document.getElementById('mobile-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-highly').addEventListener('change', () => {
                document.getElementById('mobile-relevance-highly').checked = 
                    document.getElementById('desktop-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-moderately').addEventListener('change', () => {
                document.getElementById('mobile-relevance-moderately').checked = 
                    document.getElementById('desktop-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('mobile-relevance-tangentially').checked = 
                    document.getElementById('desktop-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-not').addEventListener('change', () => {
                document.getElementById('mobile-relevance-not').checked = 
                    document.getElementById('desktop-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function resetPendingScoringFilters() {
            pendingScoringFilters = { ...currentScoringFilters };
            syncPendingScoringUI();
            updateScoringButtonText();
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function resetPendingRecommendationFilters() {
            pendingRecommendationFilters = { ...currentRecommendationFilters };
            syncPendingRecommendationUI();
            updateRecommendationButtonText();
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
            updateTopicButtonText();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // RELEVANCE FILTER FUNCTIONS  
        // ============================================================================
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = pendingRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = pendingRelevanceFilters.notRelevant;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = currentRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = currentRelevanceFilters.notRelevant;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            // Read the current checkbox states from the UI (like H-Index filter does)
            const highlyRelevantChecked = document.getElementById('mobile-relevance-highly').checked;
            const moderatelyRelevantChecked = document.getElementById('mobile-relevance-moderately').checked;
            const tangentiallyRelevantChecked = document.getElementById('mobile-relevance-tangentially').checked;
            const notRelevantChecked = document.getElementById('mobile-relevance-not').checked;
            
            const selectedOptions = [];
            if (highlyRelevantChecked) selectedOptions.push("Highly Relevant");
            if (moderatelyRelevantChecked) selectedOptions.push("Moderately Relevant");
            if (tangentiallyRelevantChecked) selectedOptions.push("Tangentially Relevant");
            if (notRelevantChecked) selectedOptions.push("Not Relevant");
            
            const selectionText = selectedOptions.length === 4 ? "All Selected" : 
                                selectedOptions.length === 0 ? "None Selected" : 
                                `${selectedOptions.length} Selected`;
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            // Get values from mobile (primary source)
            const mobileHighly = document.getElementById('mobile-relevance-highly');
            const mobileModerately = document.getElementById('mobile-relevance-moderately');
            const mobileTangentially = document.getElementById('mobile-relevance-tangentially');
            const mobileNot = document.getElementById('mobile-relevance-not');
            
            // Update pending filters from mobile if available, otherwise from desktop
            pendingRelevanceFilters.highlyRelevant = mobileHighly ? mobileHighly.checked : document.getElementById('desktop-relevance-highly').checked;
            pendingRelevanceFilters.moderatelyRelevant = mobileModerately ? mobileModerately.checked : document.getElementById('desktop-relevance-moderately').checked;
            pendingRelevanceFilters.tangentiallyRelevant = mobileTangentially ? mobileTangentially.checked : document.getElementById('desktop-relevance-tangentially').checked;
            pendingRelevanceFilters.notRelevant = mobileNot ? mobileNot.checked : document.getElementById('desktop-relevance-not').checked;
        }
        
        function resetPendingRelevanceFilters() {
            pendingRelevanceFilters = { ...currentRelevanceFilters };
            syncPendingRelevanceUI();
            updateRelevanceButtonText();
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_highly', currentRelevanceFilters.highlyRelevant.toString());
            params.set('relevance_moderately', currentRelevanceFilters.moderatelyRelevant.toString());
            params.set('relevance_tangentially', currentRelevanceFilters.tangentiallyRelevant.toString());
            params.set('relevance_not', currentRelevanceFilters.notRelevant.toString());
            window.history.replaceState({}, '', `${window.location.pathname}?${params.toString()}`);
        }
        
        function loadRelevanceFiltersFromURL() {
            const highlyRelevant = getUrlParameter('relevance_highly');
            const moderatelyRelevant = getUrlParameter('relevance_moderately');
            const tangentiallyRelevant = getUrlParameter('relevance_tangentially');
            const notRelevant = getUrlParameter('relevance_not');
            
            if (highlyRelevant !== null) {
                currentRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
                pendingRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
            }
            if (moderatelyRelevant !== null) {
                currentRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
                pendingRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
            }
            if (tangentiallyRelevant !== null) {
                currentRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
                pendingRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
            }
            if (notRelevant !== null) {
                currentRelevanceFilters.notRelevant = notRelevant === 'true';
                pendingRelevanceFilters.notRelevant = notRelevant === 'true';
            }
            
            syncRelevanceUI();
        }
        
        function passesRelevanceFilter(paper) {
            // Get selected topics
            const selectedTopics = [];
            if (currentTopicFilters.rlhf) selectedTopics.push('rlhf');
            if (currentTopicFilters.weakSupervision) selectedTopics.push('weak_supervision');
            if (currentTopicFilters.diffusionReasoning) selectedTopics.push('diffusion_reasoning');
            if (currentTopicFilters.distributedTraining) selectedTopics.push('distributed_training');
            if (currentTopicFilters.datasets) selectedTopics.push('datasets');
            
            // If no topics selected, skip relevance filtering
            if (selectedTopics.length === 0) return true;
            
            // Get selected relevance levels
            const selectedRelevanceLevels = [];
            if (currentRelevanceFilters.highlyRelevant) selectedRelevanceLevels.push('Highly Relevant');
            if (currentRelevanceFilters.moderatelyRelevant) selectedRelevanceLevels.push('Moderately Relevant');
            if (currentRelevanceFilters.tangentiallyRelevant) selectedRelevanceLevels.push('Tangentially Relevant');
            if (currentRelevanceFilters.notRelevant) selectedRelevanceLevels.push('Not Relevant');
            
            // If no relevance levels selected, show 0 papers
            if (selectedRelevanceLevels.length === 0) return false;
            
            // Check each selected topic
            for (let topic of selectedTopics) {
                const relevanceField = `${topic}_relevance`;
                const paperRelevance = paper[relevanceField];
                
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = paperRelevance === "not_validated" ? "Not Relevant" : paperRelevance;
                
                // If this topic's relevance matches any selected relevance level, paper passes
                if (selectedRelevanceLevels.includes(normalizedRelevance)) {
                    return true; // At least one topic matches
                }
            }
            
            // No selected topics had matching relevance levels
            return false;
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
            
            // Update Relevance
            updateDropdownDisabledState('mobile-relevance-btn', 'mobile-relevance-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-relevance-btn', 'desktop-relevance-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            // Check if there are no papers to display
            if (filteredSortedPapers.length === 0) {
                showNoPapersMessage();
                hidePaginationSections();
                return;
            }
            
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            showPaginationSections();
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        function showNoPapersMessage() {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const noPapersHTML = '<div class="flex items-center justify-center min-h-screen"><h2 class="font-heading text-2xl text-neutral-600">No papers to show</h2></div>';
            
            if (mobileContainer) {
                mobileContainer.innerHTML = noPapersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = noPapersHTML;
            }
        }

        function hidePaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = 'none';
                }
            });
        }

        function showPaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = '';
                }
            });
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            // Find the similarity container in the currently visible layout
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            // Find the justification container in the currently visible layout
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            // Find the normalized similarity container in the currently visible layout
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // QUICK FILTER FUNCTIONS
        // ============================================================================


        function applyQuickFilter(filterType) {
            // Build URL parameters based on filter type
            const url = new URL(window.location.href);
            
            // Clear all existing parameters
            url.search = '';
            
            // Set sort to recommendation best first for all filters
            url.searchParams.set('sort', 'recommend_best');
            
            switch(filterType) {
                case 'must-read':
                    // Recommendation: Only Must Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'true');
                    url.searchParams.set('recommendation_should', 'false');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'should-read':
                    // Recommendation: Only Should Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'false');
                    url.searchParams.set('recommendation_should', 'true');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'rlhf':
                    // Topic: Only RLHF + Relevance: Exclude Not Relevant
                    url.searchParams.set('topic_rlhf', 'true');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'weak-supervision':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'true');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'diffusion-reasoning':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'true');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'distributed-training':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'true');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'datasets':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'true');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'reset':
                    // Clear all parameters, which will reset everything to defaults
                    break;
                    
                default:
                    console.warn('Unknown quick filter type:', filterType);
                    return;
            }
            
            // Navigate to new URL, which will trigger existing URL parsing logic
            window.location.href = url.toString();
        }



        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Load Relevance filters from URL
            loadRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-relevance-dropdown'), button: document.getElementById('mobile-relevance-btn') },
                { dropdown: document.getElementById('desktop-relevance-dropdown'), button: document.getElementById('desktop-relevance-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('relevance')) {
                            resetPendingRelevanceFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        } else if (dropdown.id.includes('recommendation')) {
                            resetPendingRecommendationFilters();
                        } else if (dropdown.id.includes('scoring')) {
                            resetPendingScoringFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
