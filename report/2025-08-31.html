<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 31 August 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: 'rgba(107, 106, 101, 0.7)' // #6B6A65 with 70% opacity
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 31 August 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100 min-h-screen">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="mobile-quick-must-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="mobile-quick-should-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="mobile-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="mobile-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="mobile-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="mobile-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="mobile-quick-datasets" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="mobile-quick-reset" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-highly" checked>
                                            <label for="mobile-relevance-highly"></label>
                                        </div>
                                        <label for="mobile-relevance-highly" class="text-neutral-10 text-xl font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-moderately" checked>
                                            <label for="mobile-relevance-moderately"></label>
                                        </div>
                                        <label for="mobile-relevance-moderately" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-tangentially" checked>
                                            <label for="mobile-relevance-tangentially"></label>
                                        </div>
                                        <label for="mobile-relevance-tangentially" class="text-neutral-10 text-xl font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-not" checked>
                                            <label for="mobile-relevance-not"></label>
                                        </div>
                                        <label for="mobile-relevance-not" class="text-neutral-10 text-xl font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="desktop-quick-must-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="desktop-quick-should-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="desktop-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="desktop-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="desktop-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="desktop-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="desktop-quick-datasets" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="desktop-quick-reset" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-highly" checked>
                                            <label for="desktop-relevance-highly"></label>
                                        </div>
                                        <label for="desktop-relevance-highly" class="text-neutral-10 text-lg font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-moderately" checked>
                                            <label for="desktop-relevance-moderately"></label>
                                        </div>
                                        <label for="desktop-relevance-moderately" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-tangentially" checked>
                                            <label for="desktop-relevance-tangentially"></label>
                                        </div>
                                        <label for="desktop-relevance-tangentially" class="text-neutral-10 text-lg font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-not" checked>
                                            <label for="desktop-relevance-not"></label>
                                        </div>
                                        <label for="desktop-relevance-not" class="text-neutral-10 text-lg font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 31 August 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl min-h-screen">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2509.00641",
      "title": "AMCR: A Framework for Assessing and Mitigating Copyright Risks in\n  Generative Models",
      "authors": [
        "Zhipeng Yin",
        "Zichong Wang",
        "Avash Palikhe",
        "Zhen Liu",
        "Jun Liu",
        "Wenbin Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CR (Cryptography and Security)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Generative models have achieved impressive results in text to image tasks,\nsignificantly advancing visual content creation. However, this progress comes\nat a cost, as such models rely heavily on large-scale training data and may\nunintentionally replicate copyrighted elements, creating serious legal and\nethical challenges for real-world deployment. To address these concerns,\nresearchers have proposed various strategies to mitigate copyright risks, most\nof which are prompt based methods that filter or rewrite user inputs to prevent\nexplicit infringement. While effective in handling obvious cases, these\napproaches often fall short in more subtle situations, where seemingly benign\nprompts can still lead to infringing outputs. To address these limitations,\nthis paper introduces Assessing and Mitigating Copyright Risks (AMCR), a\ncomprehensive framework which i) builds upon prompt-based strategies by\nsystematically restructuring risky prompts into safe and non-sensitive forms,\nii) detects partial infringements through attention-based similarity analysis,\nand iii) adaptively mitigates risks during generation to reduce copyright\nviolations without compromising image quality. Extensive experiments validate\nthe effectiveness of AMCR in revealing and mitigating latent copyright risks,\noffering practical insights and benchmarks for the safer deployment of\ngenerative models.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00641v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00641v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.487,
      "distributed_training_score": 0.347,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a framework for assessing and mitigating copyright risks in generative models using prompt sanitization, attention-based analysis, and diffusion processes. It does not involve training a reward model on human-ranked data or using reinforcement learning to fine-tune models based on human feedback, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes diffusion models for image generation and risk mitigation, including iterative processes to steer outputs away from copyright issues. However, it does not adapt diffusion for multi-step logical reasoning or treat a 'Chain-of-Thought' as an entity for holistic correction, focusing instead on visual content creation rather than complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00646",
      "title": "RAG-PRISM: A Personalized, Rapid, and Immersive Skill Mastery Framework\n  with Adaptive Retrieval-Augmented Tutoring",
      "authors": [
        "Gaurangi Raul",
        "Yu-Zheng Lin",
        "Karan Patel",
        "Bono Po-Jen Shih",
        "Matthew W. Redondo",
        "Banafsheh Saber Latibari",
        "Jesus Pacheco",
        "Soheil Salehi",
        "Pratik Satam"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rapid digital transformation of Fourth Industrial Revolution (4IR)\nsystems is reshaping workforce needs, widening skill gaps, especially for older\nworkers. With growing emphasis on STEM skills such as robotics, automation,\nartificial intelligence (AI), and security, large-scale re-skilling and\nup-skilling are required. Training programs must address diverse backgrounds,\nlearning styles, and motivations to improve persistence and success, while\nensuring rapid, cost-effective workforce development through experiential\nlearning. To meet these challenges, we present an adaptive tutoring framework\nthat combines generative AI with Retrieval-Augmented Generation (RAG) to\ndeliver personalized training. The framework leverages document hit rate and\nMean Reciprocal Rank (MRR) to optimize content for each learner, and is\nbenchmarked against human-generated training for alignment and relevance. We\ndemonstrate the framework in 4IR cybersecurity learning by creating a synthetic\nQA dataset emulating trainee behavior, while RAG is tuned on curated\ncybersecurity materials. Evaluation compares its generated training with\nmanually curated queries representing realistic student interactions. Responses\nare produced using large language models (LLMs) including GPT-3.5 and GPT-4,\nassessed for faithfulness and content alignment. GPT-4 achieves the best\nperformance with 87% relevancy and 100% alignment. Results show this dual-mode\napproach enables the adaptive tutor to act as both a personalized topic\nrecommender and content generator, offering a scalable solution for rapid,\ntailored learning in 4IR education and workforce development.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00646v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00646v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.458,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.405,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using pre-existing LLMs like GPT-3.5 and GPT-4 for personalized tutoring via RAG, without any mention of training or fine-tuning models using human feedback, reward models, or reinforcement learning techniques. It evaluates generated responses for faithfulness and relevancy but does not involve aligning AI with human preferences through RLHF.",
      "weak_supervision_justification": "The paper mentions creating a synthetic QA dataset to emulate trainee behavior, which involves programmatically generated data for evaluation, potentially resembling weak supervision. However, it does not focus on training models with noisy or imprecise labels as a core method; instead, it emphasizes RAG and LLMs for content generation and personalization.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for multi-step logical reasoning. It uses RAG and LLMs for content retrieval and generation, with no mention of treating reasoning paths as entities for holistic correction, as required for diffusion-based reasoning.",
      "distributed_training_justification": "The paper discusses the application of RAG and LLMs in a tutoring framework but does not address distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes. It focuses on personalization and evaluation metrics, not on accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00647",
      "title": "LLM-HyPZ: Hardware Vulnerability Discovery using an LLM-Assisted Hybrid\n  Platform for Zero-Shot Knowledge Extraction and Refinement",
      "authors": [
        "Yu-Zheng Lin",
        "Sujan Ghimire",
        "Abhiram Nandimandalam",
        "Jonah Michael Camacho",
        "Unnati Tripathi",
        "Rony Macwan",
        "Sicong Shao",
        "Setareh Rafatirad",
        "Rozhin Yasaei",
        "Pratik Satam",
        "Soheil Salehi"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rapid growth of hardware vulnerabilities has created an urgent need for\nsystematic and scalable analysis methods. Unlike software flaws, which are\noften patchable post-deployment, hardware weaknesses remain embedded across\nproduct lifecycles, posing persistent risks to processors, embedded devices,\nand IoT platforms. Existing efforts such as the MITRE CWE Hardware List (2021)\nrelied on expert-driven Delphi surveys, which lack statistical rigor and\nintroduce subjective bias, while large-scale data-driven foundations for\nhardware weaknesses have been largely absent. In this work, we propose\nLLM-HyPZ, an LLM-assisted hybrid framework for zero-shot knowledge extraction\nand refinement from vulnerability corpora. Our approach integrates zero-shot\nLLM classification, contextualized embeddings, unsupervised clustering, and\nprompt-driven summarization to mine hardware-related CVEs at scale. Applying\nLLM-HyPZ to the 2021-2024 CVE corpus (114,836 entries), we identified 1,742\nhardware-related vulnerabilities. We distilled them into five recurring themes,\nincluding privilege escalation via firmware and BIOS, memory corruption in\nmobile and IoT systems, and physical access exploits. Benchmarking across seven\nLLMs shows that LLaMA 3.3 70B achieves near-perfect classification accuracy\n(99.5%) on a curated validation set. Beyond methodological contributions, our\nframework directly supported the MITRE CWE Most Important Hardware Weaknesses\n(MIHW) 2025 update by narrowing the candidate search space. Specifically, our\npipeline surfaced 411 of the 1,026 CVEs used for downstream MIHW analysis,\nthereby reducing expert workload and accelerating evidence gathering. These\nresults establish LLM-HyPZ as the first data-driven, scalable approach for\nsystematically discovering hardware vulnerabilities, thereby bridging the gap\nbetween expert knowledge and real-world vulnerability evidence.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00647v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00647v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.435,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.407,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on zero-shot LLM classification, embeddings, clustering, and summarization for hardware vulnerability discovery, without any mention of human feedback, reward models, or reinforcement learning to align AI models. It relies solely on pre-trained LLMs in a zero-shot manner, which does not involve RLHF.",
      "weak_supervision_justification": "The paper uses prompt-engineered LLMs to programmatically generate classifications and summaries from CVE data, which aligns with weak supervision by deriving labels from high-level, noisy sources without hand-labeled data. However, it does not explicitly train a new model using these labels, focusing instead on direct application of pre-trained LLMs, making it only moderately relevant.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper does not discuss distributed training, parallel computing, or multi-node strategies for accelerating model training; it only applies pre-trained LLMs for classification and analysis, with no reference to partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces LLM-HyPZ, a novel framework that leverages large language models for zero-shot classification, contextualized embeddings, unsupervised clustering, and prompt-driven summarization to systematically extract and refine hardware-related vulnerabilities from a large CVE corpus. By applying this hybrid approach to over 114,000 CVEs from 2021-2024, the authors identified 1,742 hardware vulnerabilities distilled into five key themes, demonstrated high classification accuracy with models like LLaMA 3.3 70B, and contributed to updating MITRE's hardware weakness list, establishing a scalable, data-driven method for hardware security analysis.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly innovative combination of LLMs with unsupervised techniques for zero-shot hardware vulnerability discovery, significantly advancing beyond expert-driven methods by providing a scalable, data-driven alternative.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence hardware security research and practical applications like vulnerability databases by offering a scalable framework, though its impact may be confined to specific subfields rather than broader domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality contribution with practical implications for cybersecurity, making it essential for researchers in hardware security to be aware of its methods and findings.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8e8848acb7bcd8d7b3207427f49111da65b5b139",
      "total_authors": 11,
      "authors_found": 11,
      "highest_h_index": 24,
      "average_h_index": 5.090909090909091,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Yu-Zheng Lin",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2267151334"
        },
        {
          "name": "Sujan Ghimire",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2302322223"
        },
        {
          "name": "Abhiram Nandimandalam",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378701823"
        },
        {
          "name": "Jonah Michael Camacho",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378694361"
        },
        {
          "name": "Unnati Tripathi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378705385"
        },
        {
          "name": "Rony Macwan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378702424"
        },
        {
          "name": "Sicong Shao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2267042634"
        },
        {
          "name": "S. Rafatirad",
          "h_index": 24,
          "profile_url": "https://www.semanticscholar.org/author/2951001"
        },
        {
          "name": "Rozhin Yasaei",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2029363250"
        },
        {
          "name": "Pratik Satam",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2200998"
        },
        {
          "name": "Soheil Salehi",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2276023224"
        }
      ]
    },
    {
      "id": "2509.00649",
      "title": "MV-SSM: Multi-View State Space Modeling for 3D Human Pose Estimation",
      "authors": [
        "Aviral Chharia",
        "Wenbo Gou",
        "Haoye Dong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "While significant progress has been made in single-view 3D human pose\nestimation, multi-view 3D human pose estimation remains challenging,\nparticularly in terms of generalizing to new camera configurations. Existing\nattention-based transformers often struggle to accurately model the spatial\narrangement of keypoints, especially in occluded scenarios. Additionally, they\ntend to overfit specific camera arrangements and visual scenes from training\ndata, resulting in substantial performance drops in new settings. In this\nstudy, we introduce a novel Multi-View State Space Modeling framework, named\nMV-SSM, for robustly estimating 3D human keypoints. We explicitly model the\njoint spatial sequence at two distinct levels: the feature level from\nmulti-view images and the person keypoint level. We propose a Projective State\nSpace (PSS) block to learn a generalized representation of joint spatial\narrangements using state space modeling. Moreover, we modify Mamba's\ntraditional scanning into an effective Grid Token-guided Bidirectional Scanning\n(GTBS), which is integral to the PSS block. Multiple experiments demonstrate\nthat MV-SSM achieves strong generalization, outperforming state-of-the-art\nmethods: +10.8 on AP25 (+24%) on the challenging three-camera setting in CMU\nPanoptic, +7.0 on AP25 (+13%) on varying camera arrangements, and +15.3 PCP\n(+38%) on Campus A1 in cross-dataset evaluations. Project Website:\nhttps://aviralchharia.github.io/MV-SSM",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00649v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00649v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.335,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00653",
      "title": "IndiaWeatherBench: A Dataset and Benchmark for Data-Driven Regional\n  Weather Forecasting over India",
      "authors": [
        "Tung Nguyen",
        "Harkanwar Singh",
        "Nilay Naharas",
        "Lucas Bandarkar",
        "Aditya Grover"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Regional weather forecasting is a critical problem for localized climate\nadaptation, disaster mitigation, and sustainable development. While machine\nlearning has shown impressive progress in global weather forecasting, regional\nforecasting remains comparatively underexplored. Existing efforts often use\ndifferent datasets and experimental setups, limiting fair comparison and\nreproducibility. We introduce IndiaWeatherBench, a comprehensive benchmark for\ndata-driven regional weather forecasting focused on the Indian subcontinent.\nIndiaWeatherBench provides a curated dataset built from high-resolution\nregional reanalysis products, along with a suite of deterministic and\nprobabilistic metrics to facilitate consistent training and evaluation. To\nestablish strong baselines, we implement and evaluate a range of models across\ndiverse architectures, including UNets, Transformers, and Graph-based networks,\nas well as different boundary conditioning strategies and training objectives.\nWhile focused on India, IndiaWeatherBench is easily extensible to other\ngeographic regions. We open-source all raw and preprocessed datasets, model\nimplementations, and evaluation pipelines to promote accessibility and future\ndevelopment. We hope IndiaWeatherBench will serve as a foundation for advancing\nregional weather forecasting research. Code is available at\nhttps://github.com/tung-nd/IndiaWeatherBench.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00653v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00653v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.373,
      "datasets_score": 0.465,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of IndiaWeatherBench, a new dataset for machine learning-based regional weather forecasting over India. It involves creating and curating a high-resolution dataset from regional reanalysis products, providing standardized train-validation-test splits, and establishing benchmarks with metrics for evaluation. This directly aligns with research on creating, analyzing, benchmarking, and evaluating datasets for AI applications, as it offers a comprehensive framework for ML model development and assessment.",
      "llm_score_status": "completed",
      "summary": "IndiaWeatherBench introduces a comprehensive benchmark for data-driven regional weather forecasting over the Indian subcontinent, addressing the lack of standardized datasets and evaluation protocols in this area by curating a high-resolution dataset from the IMDAA regional reanalysis, implementing various machine learning models like UNets, Transformers, and Graph-based networks as baselines, and providing deterministic and probabilistic metrics for consistent training and evaluation. The paper establishes strong baselines, highlights the dataset's extensibility to other regions, and open-sources all resources to foster community-driven advancements in regional weather forecasting, aiming to improve climate adaptation and disaster mitigation in India.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a specialized benchmark for regional weather forecasting in India, building on existing global benchmarks like WeatherBench to address a specific underexplored area, though it does not introduce entirely new techniques or architectures. This clever adaptation of established methods to a new regional context advances the state-of-the-art in localized forecasting without being groundbreaking.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of regional weather forecasting, particularly for India and potentially other regions, due to its open-source nature and provision of standardized datasets and baselines. However, its influence may be limited to specific geographic areas and not broadly transformative across all weather forecasting research.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution by providing a much-needed benchmark for regional weather forecasting, making it essential for researchers focused on machine learning applications in climate science. While not exceptional enough to be a must-read for all, it offers high-quality insights and resources that advance the field in a practical way.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/fca4628656be47aa8342fbc2ac9ec9e6e12253e7",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 9,
      "average_h_index": 4.2,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Tung Nguyen",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2175303602"
        },
        {
          "name": "Harkanwar Singh",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2274100424"
        },
        {
          "name": "Nilay Naharas",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2197483945"
        },
        {
          "name": "Lucas Bandarkar",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2091417351"
        },
        {
          "name": "Aditya Grover",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2259896660"
        }
      ]
    },
    {
      "id": "2509.00654",
      "title": "The Name-Free Gap: Policy-Aware Stylistic Control in Music Generation",
      "authors": [
        "Ashwin Nagarajan",
        "Hao-Wen Dong"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.MM (Multimedia)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Text-to-music models capture broad attributes such as instrumentation or\nmood, but fine-grained stylistic control remains an open challenge. Existing\nstylization methods typically require retraining or specialized conditioning,\nwhich complicates reproducibility and limits policy compliance when artist\nnames are restricted. We study whether lightweight, human-readable modifiers\nsampled from a large language model can provide a policy-robust alternative for\nstylistic control. Using MusicGen-small, we evaluate two artists: Billie Eilish\n(vocal pop) and Ludovico Einaudi (instrumental piano). For each artist, we use\nfifteen reference excerpts and evaluate matched seeds under three conditions:\nbaseline prompts, artist-name prompts, and five descriptor sets. All prompts\nare generated using a large language model. Evaluation uses both VGGish and\nCLAP embeddings with distributional and per-clip similarity measures, including\na new min-distance attribution metric. Results show that artist names are the\nstrongest control signal across both artists, while name-free descriptors\nrecover much of this effect. This highlights that existing safeguards such as\nthe restriction of artist names in music generation prompts may not fully\nprevent style imitation. Cross-artist transfers reduce alignment, showing that\ndescriptors encode targeted stylistic cues. We also present a descriptor table\nacross ten contemporary artists to illustrate the breadth of the tokens.\nTogether these findings define the name-free gap, the controllability\ndifference between artist-name prompts and policy-compliant descriptors, shown\nthrough a reproducible evaluation protocol for prompt-level controllability.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00654v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00654v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.394,
      "distributed_training_score": 0.298,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is on text-to-music generation and stylistic control using language model descriptors, without any involvement of reinforcement learning, human feedback, or training a reward model based on human-ranked data. It evaluates prompts and embeddings for music style, which does not align with RLHF concepts.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00658",
      "title": "Face4FairShifts: A Large Image Benchmark for Fairness and Robust\n  Learning across Visual Domains",
      "authors": [
        "Yumeng Lin",
        "Dong Li",
        "Xintao Wu",
        "Minglai Shao",
        "Xujiang Zhao",
        "Zhong Chen",
        "Chen Zhao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CY (Computers and Society)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Ensuring fairness and robustness in machine learning models remains a\nchallenge, particularly under domain shifts. We present Face4FairShifts, a\nlarge-scale facial image benchmark designed to systematically evaluate\nfairness-aware learning and domain generalization. The dataset includes 100,000\nimages across four visually distinct domains with 39 annotations within 14\nattributes covering demographic and facial features. Through extensive\nexperiments, we analyze model performance under distribution shifts and\nidentify significant gaps. Our findings emphasize the limitations of existing\nrelated datasets and the need for more effective fairness-aware domain\nadaptation techniques. Face4FairShifts provides a comprehensive testbed for\nadvancing equitable and reliable AI systems. The dataset is available online at\nhttps://meviuslab.github.io/Face4FairShifts/.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00658v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00658v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.311,
      "distributed_training_score": 0.381,
      "datasets_score": 0.496,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new dataset, Face4FairShifts, which includes 100,000 facial images across multiple domains, along with detailed descriptions of its creation, annotation processes (e.g., using 66 annotators and quality control), and benchmarking through experiments. This directly aligns with research on creating, analyzing, benchmarking, and evaluating datasets for machine learning and AI, as it addresses dataset curation methodologies, introduces a new benchmark for fairness and robustness, and performs comparative analysis against existing datasets.",
      "llm_score_status": "completed",
      "summary": "The paper introduces Face4FairShifts, a large-scale benchmark dataset with 100,000 facial images across four distinct visual domains (Photo, Art, Cartoon, and Sketch), designed to evaluate fairness and robustness in machine learning under distribution shifts, including covariate, semantic, demographic, concept, and correlation shifts. Through extensive experiments on fairness learning, out-of-distribution generalization, detection, and fairness-aware generalization, the authors demonstrate the dataset's ability to reveal performance gaps in existing methods, emphasizing the need for improved fairness-aware techniques while providing high-quality annotations via human annotators.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark dataset specifically tailored for fairness-aware out-of-distribution generalization, addressing a critical gap in existing resources and significantly advancing research in fair and robust AI.",
      "impact_score": "High",
      "impact_justification": "This work is likely to influence a wide range of future research and applications in computer vision and machine learning by offering a comprehensive testbed for developing fairness-aware models under real-world domain shifts.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality, significant contribution by introducing a valuable benchmark for fair AI research, making it essential for specialists in the field but not mandatory for a general audience.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/10e8551b6e52bacb791692188bd8a0d839b8899e",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 4,
      "average_h_index": 1.7142857142857142,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yumeng Lin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378727699"
        },
        {
          "name": "Dong Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372250203"
        },
        {
          "name": "Xintao Wu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2282586484"
        },
        {
          "name": "Minglai Shao",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2244624817"
        },
        {
          "name": "Xujiang Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378977603"
        },
        {
          "name": "Zhong Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379341142"
        },
        {
          "name": "Chen Zhao",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2283044442"
        }
      ]
    },
    {
      "id": "2509.00661",
      "title": "Automatic Identification and Description of Jewelry Through Computer\n  Vision and Neural Networks for Translators and Interpreters",
      "authors": [
        "Jose Manuel Alcalde-Llergo",
        "Aurora Ruiz-Mezcua",
        "Rocio Avila-Ramirez",
        "Andrea Zingoni",
        "Juri Taborri",
        "Enrique Yeguas-Bolivar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Identifying jewelry pieces presents a significant challenge due to the wide\nrange of styles and designs. Currently, precise descriptions are typically\nlimited to industry experts. However, translators and interpreters often\nrequire a comprehensive understanding of these items. In this study, we\nintroduce an innovative approach to automatically identify and describe jewelry\nusing neural networks. This method enables translators and interpreters to\nquickly access accurate information, aiding in resolving queries and gaining\nessential knowledge about jewelry. Our model operates at three distinct levels\nof description, employing computer vision techniques and image captioning to\nemulate expert analysis of accessories. The key innovation involves generating\nnatural language descriptions of jewelry across three hierarchical levels,\ncapturing nuanced details of each piece. Different image captioning\narchitectures are utilized to detect jewels in images and generate descriptions\nwith varying levels of detail. To demonstrate the effectiveness of our approach\nin recognizing diverse types of jewelry, we assembled a comprehensive database\nof accessory images. The evaluation process involved comparing various image\ncaptioning architectures, focusing particularly on the encoder decoder model,\ncrucial for generating descriptive captions. After thorough evaluation, our\nfinal model achieved a captioning accuracy exceeding 90 per cent.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00661v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00661v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.309,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00664",
      "title": "Fusion to Enhance: Fusion Visual Encoder to Enhance Multimodal Language\n  Model",
      "authors": [
        "Yifei She",
        "Huangxuan Wu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have made significant progress in\nbridging visual perception with high-level textual reasoning. However, they\nface a fundamental contradiction: while excelling at complex semantic\nunderstanding, these models often fail at basic visual tasks that require\nprecise detail perception. This deficiency primarily stems from the prevalent\narchitectural reliance on a single vision encoder optimized for high-level\nsemantic alignment, which inherently sacrifices the ability to capture\nfine-grained visual information. To address this issue, we introduce Fusion to\nEnhance (FtZ), a novel vision tower framework. FtZ moves beyond the\nsingle-encoder design by innovatively composing a semantically powerful anchor\nencoder with a perception-rich augmenting encoder via a lightweight Multi-Head\nCross-Attention mechanism. Experimental results demonstrate that on several\nchallenging benchmarks demanding fine-grained visual understanding, such as\nTextVQA, POPE, MMMU, MME and MM-Vet, our FtZ model significantly outperforms\nbaselines that use only a single encoder or existing feature fusion methods.\nThis work proves that composing heterogeneous expert encoders is an efficient\nand effective path to overcoming the visual perception bottleneck in current\nMLLMs, offering a new design paradigm for building next-generation AI systems\nwith stronger perceptual capabilities.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00664v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00664v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.474,
      "distributed_training_score": 0.376,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing Multimodal Large Language Models (MLLMs) by fusing multiple vision encoders to improve fine-grained visual perception, using mechanisms like Multi-Head Cross-Attention. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning or Chain-of-Thought correction. Therefore, there is no connection to the topic of Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00665",
      "title": "ER-LoRA: Effective-Rank Guided Adaptation for Weather-Generalized Depth\n  Estimation",
      "authors": [
        "Weilong Yan",
        "Xin Zhang",
        "Robby T. Tan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Monocular depth estimation under adverse weather conditions (e.g.\\ rain, fog,\nsnow, and nighttime) remains highly challenging due to the lack of reliable\nground truth and the difficulty of learning from unlabeled real-world data.\nExisting methods often rely on synthetic adverse data with pseudo-labels, which\nsuffer from domain gaps, or employ self-supervised learning, which violates\nphotometric assumptions in adverse scenarios. In this work, we propose to\nachieve weather-generalized depth estimation by Parameter-Efficient Fine-Tuning\n(PEFT) of Vision Foundation Models (VFMs), using only a small amount of\nhigh-visibility (normal) data. While PEFT has shown strong performance in\nsemantic tasks such as segmentation, it remains underexplored for geometry --\ncentric tasks like depth estimation -- especially in terms of balancing\neffective adaptation with the preservation of pretrained knowledge. To this\nend, we introduce the Selecting-Tuning-Maintaining (STM) strategy, which\nstructurally decomposes the pretrained weights of VFMs based on two kinds of\neffective ranks (entropy-rank and stable-rank). In the tuning phase, we\nadaptively select the proper rank number as well as the task-aware singular\ndirections for initialization, based on the entropy-rank and full-tuned weight;\nwhile in the maintaining stage, we enforce a principal direction regularization\nbased on the stable-rank. This design guarantees flexible task adaptation while\npreserving the strong generalization capability of the pretrained VFM.\nExtensive experiments on four real-world benchmarks across diverse weather\nconditions demonstrate that STM not only outperforms existing PEFT methods and\nfull fine-tuning but also surpasses methods trained with adverse synthetic\ndata, and even the depth foundation model",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00665v2",
      "pdf_url": "http://arxiv.org/pdf/2509.00665v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.342,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00673",
      "title": "Confident, Calibrated, or Complicit: Probing the Trade-offs between\n  Safety Alignment and Ideological Bias in Language Models in Detecting Hate\n  Speech",
      "authors": [
        "Sanjeeevan Selvaganapathy",
        "Mehwish Nasim"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "We investigate the efficacy of Large Language Models (LLMs) in detecting\nimplicit and explicit hate speech, examining whether models with minimal safety\nalignment (uncensored) might provide more objective classification capabilities\ncompared to their heavily-aligned (censored) counterparts. While uncensored\nmodels theoretically offer a less constrained perspective free from moral\nguardrails that could bias classification decisions, our results reveal a\nsurprising trade-off: censored models significantly outperform their uncensored\ncounterparts in both accuracy and robustness, achieving 78.7% versus 64.1%\nstrict accuracy. However, this enhanced performance comes with its own\nlimitation -- the safety alignment acts as a strong ideological anchor, making\ncensored models resistant to persona-based influence, while uncensored models\nprove highly malleable to ideological framing. Furthermore, we identify\ncritical failures across all models in understanding nuanced language such as\nirony. We also find alarming fairness disparities in performance across\ndifferent targeted groups and systemic overconfidence that renders\nself-reported certainty unreliable. These findings challenge the notion of LLMs\nas objective arbiters and highlight the need for more sophisticated auditing\nframeworks that account for fairness, calibration, and ideological consistency.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00673v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00673v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.501,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.343,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses RLHF as a method used for safety alignment in censored models, noting its potential to introduce biases in hate speech detection. While it examines the effects of such alignment, the main contribution is evaluating trade-offs in model performance rather than advancing or deeply analyzing RLHF itself, making it relevant but not central.",
      "weak_supervision_justification": "The paper does not mention or involve weak supervision techniques, such as programmatically generating labels for training. Its focus is on evaluating LLMs for hate speech detection and the impacts of safety alignment, with no reference to alternative supervision methods.",
      "diffusion_reasoning_justification": "The paper does not discuss diffusion-based reasoning, iterative refinement for logical tasks, or any related multi-step reasoning processes. Its content centers on hate speech detection in LLMs and the effects of alignment, without incorporating diffusion models or similar approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper investigates the trade-offs between safety alignment and ideological bias in Large Language Models (LLMs) for detecting implicit and explicit hate speech, comparing censored (heavily aligned) and uncensored (minimally aligned) models to assess their accuracy, robustness, and susceptibility to persona-based influences. The study finds that censored models outperform uncensored ones in accuracy (78.7% vs. 64.1%) and robustness but are less malleable to ideological framing, while both exhibit issues like poor handling of nuanced language, fairness disparities across targeted groups, and overconfidence, ultimately calling for more sophisticated auditing frameworks to ensure fairness and calibration.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by systematically examining the interaction between safety alignment and ideological bias in hate speech detection, combining existing ideas on model alignment and bias in a new way to reveal trade-offs. However, it does not introduce a entirely new problem or technique, building instead on prior work in LLM bias and classification.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI safety and hate speech detection due to its insights on alignment trade-offs and fairness issues, potentially influencing model development practices. Nonetheless, its applicability is somewhat limited to specific areas of computational language and AI ethics rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable insights into the complexities of LLM alignment and bias in hate speech detection, making it a strong contribution for researchers in AI ethics and language processing. While not essential for all, it is highly relevant for those working on model safety and fairness.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f28ca937f4f4b70309f3f5279ac3adc9331b8e8a",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Sanjeeevan Selvaganapathy",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378711793"
        },
        {
          "name": "Mehwish Nasim",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378687995"
        }
      ]
    },
    {
      "id": "2509.00676",
      "title": "LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model",
      "authors": [
        "Xiyao Wang",
        "Chunyuan Li",
        "Jianwei Yang",
        "Kai Zhang",
        "Bo Liu",
        "Tianyi Xiong",
        "Furong Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In vision-language modeling, critic models are typically trained to evaluate\noutputs -- assigning scalar scores or pairwise preferences -- rather than to\ngenerate responses. This separation from policy models, which produce the\nresponses, is so entrenched that critics are rarely considered for direct\npolicy use. In this work, we challenge this convention. We propose to\nreorganize preference-labeled critic datasets into verifiable training signals\nand perform reinforcement learning directly on a base generative model,\nproducing LLaVA-Critic-R1, a multimodal critic trained to optimize preference\njudgments while retaining full generation ability. Surprisingly,\nLLaVA-Critic-R1 emerges not only as a top-performing critic but also as a\ncompetitive policy model -- matching or surpassing specialized reasoning VLMs\ntrained with in-domain data across 26 visual reasoning and understanding\nbenchmarks, with an average gain of +5.7% over its base model (Qwen-2.5-VL-7B).\nExtending this approach to existing strong reasoning VLMs yields\nLLaVA-Critic-R1+, which further advances policy performance without sacrificing\ncritic quality, achieving a SoTA performance of 71.9 on MMMU at the 7B scale.\nFinally, we show that the enhanced critic ability benefits inference: applying\nself-critique at test time yields an average +13.8% improvement on five\nrepresentative reasoning tasks without additional training. Our results reveal\nthat RL training on critic data can produce a unified model excelling at both\nevaluation and generation, offering a simple path toward scalable,\nself-improving multimodal systems.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00676v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00676v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.498,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.36,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves training a critic model using reinforcement learning on preference-labeled datasets, which are derived from human or reference model judgments. This directly aligns with RLHF, as it uses human preference data to fine-tune a base model, enhancing both evaluation and generation capabilities.",
      "weak_supervision_justification": "The paper reorganizes critic datasets with preference labels, which may include noisy sources like reference models, resembling weak supervision. However, the primary focus is on reinforcement learning rather than programmatically generating labels for training, making it only loosely connected.",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for vision-language models and does not involve diffusion-based methods, iterative refinement, or multi-step logical reasoning processes as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper challenges the conventional separation between critic and policy models in vision-language modeling by proposing a reinforcement learning approach on preference-labeled datasets to train LLaVA-Critic-R1, a model that optimizes for preference judgments while retaining generative capabilities. Key findings reveal that this model not only outperforms as a critic but also as a policy model, achieving an average +5.7% improvement over its base model on 26 visual reasoning benchmarks, with extensions like LLaVA-Critic-R1+ further enhancing performance and enabling effective self-critique for additional gains without extra training.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel training paradigm using reinforcement learning on critic data to create a unified model for both evaluation and generation, significantly advancing the state-of-the-art in vision-language models by challenging established conventions. This approach represents a true innovation by demonstrating that critic models can secretly excel as policy models, offering a new pathway for multimodal system development.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a broad range of future research and applications in vision-language models by enabling more efficient, scalable systems that combine evaluation and generation capabilities. Its demonstrated improvements on benchmarks and test-time scaling suggest it could lead to advancements in areas like multimodal reasoning and self-improving AI.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality contribution with surprising empirical findings and practical implications for VLM development, making it a valuable read for researchers in computer vision and machine learning. While insightful, it may not be essential for all audiences beyond those focused on multimodal systems.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/56ddff358894ddf3144de57d25a101aa3ccccdcf",
      "total_authors": 7,
      "authors_found": 6,
      "highest_h_index": 11,
      "average_h_index": 2.3333333333333335,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Xiyao Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2324073811"
        },
        {
          "name": "Chunyuan Li",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2324068274"
        },
        {
          "name": "Jianwei Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378605395"
        },
        {
          "name": "Kai Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379041070"
        },
        {
          "name": "Bo Liu (Benjamin Liu)",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2156640188"
        },
        {
          "name": "Tianyi Xiong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378663686"
        },
        {
          "name": "Furong Huang",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2509.00677",
      "title": "CSFMamba: Cross State Fusion Mamba Operator for Multimodal Remote\n  Sensing Image Classification",
      "authors": [
        "Qingyu Wang",
        "Xue Jiang",
        "Guozheng Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal fusion has made great progress in the field of remote sensing\nimage classification due to its ability to exploit the complementary\nspatial-spectral information. Deep learning methods such as CNN and Transformer\nhave been widely used in these domains. State Space Models recently highlighted\nthat prior methods suffer from quadratic computational complexity. As a result,\nmodeling longer-range dependencies of spatial-spectral features imposes an\noverwhelming burden on the network. Mamba solves this problem by incorporating\ntime-varying parameters into ordinary SSM and performing hardware optimization,\nbut it cannot perform feature fusion directly. In order to make full use of\nMamba's low computational burden and explore the potential of internal\nstructure in multimodal feature fusion, we propose Cross State Fusion Mamba\n(CSFMamba) Network. Specifically, we first design the preprocessing module of\nremote sensing image information for the needs of Mamba structure, and combine\nit with CNN to extract multi-layer features. Secondly, a cross-state module\nbased on Mamba operator is creatively designed to fully fuse the feature of the\ntwo modalities. The advantages of Mamba and CNN are combined by designing a\nmore powerful backbone. We capture the fusion relationship between HSI and\nLiDAR modalities with stronger full-image understanding. The experimental\nresults on two datasets of MUUFL and Houston2018 show that the proposed method\noutperforms the experimental results of Transformer under the premise of\nreducing the network training burden.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00677v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00677v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.349,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00684",
      "title": "Valid Property-Enhanced Contrastive Learning for Targeted Optimization &\n  Resampling for Novel Drug Design",
      "authors": [
        "Amartya Banerjee",
        "Somnath Kar",
        "Anirban Pal",
        "Debabrata Maiti"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Efficiently steering generative models toward pharmacologically relevant\nregions of chemical space remains a major obstacle in molecular drug discovery\nunder low-data regimes. We present VECTOR+: Valid-property-Enhanced Contrastive\nLearning for Targeted Optimization and Resampling, a framework that couples\nproperty-guided representation learning with controllable molecule generation.\nVECTOR+ applies to both regression and classification tasks and enables\ninterpretable, data-efficient exploration of functional chemical space. We\nevaluate on two datasets: a curated PD-L1 inhibitor set (296 compounds with\nexperimental $IC_{50}$ values) and a receptor kinase inhibitor set (2,056\nmolecules by binding mode). Despite limited training data, VECTOR+ generates\nnovel, synthetically tractable candidates. Against PD-L1 (PDB 5J89), 100 of\n8,374 generated molecules surpass a docking threshold of $-15.0$ kcal/mol, with\nthe best scoring $-17.6$ kcal/mol compared to the top reference inhibitor\n($-15.4$ kcal/mol). The best-performing molecules retain the conserved biphenyl\npharmacophore while introducing novel motifs. Molecular dynamics (250 ns)\nconfirm binding stability (ligand RMSD < $2.5$ angstroms). VECTOR+ generalizes\nto kinase inhibitors, producing compounds with stronger docking scores than\nestablished drugs such as brigatinib and sorafenib. Benchmarking against JT-VAE\nand MolGPT across docking, novelty, uniqueness, and Tanimoto similarity\nhighlights the superior performance of our method. These results position our\nwork as a robust, extensible approach for property-conditioned molecular design\nin low-data settings, bridging contrastive learning and generative modeling for\nreproducible, AI-accelerated discovery.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00684v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00684v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.359,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework called VECTOR+ for molecular drug design using contrastive learning and Gaussian Mixture Models to generate novel compounds in low-data regimes. While the related work section briefly mentions diffusion-based models like GeoDiff and CMD-GEN for molecular generation, the paper does not incorporate diffusion models into its methodology or apply them for multi-step logical reasoning tasks. The topic specifically requires adaptation of diffusion for complex logical tasks, such as chain-of-thought reasoning, which is absent here. Thus, there is no clear component aligning with diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00692",
      "title": "CascadeFormer: A Family of Two-stage Cascading Transformers for\n  Skeleton-based Human Action Recognition",
      "authors": [
        "Yusen Peng",
        "Alper Yilmaz"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Skeleton-based human action recognition leverages sequences of human joint\ncoordinates to identify actions performed in videos. Owing to the intrinsic\nspatiotemporal structure of skeleton data, Graph Convolutional Networks (GCNs)\nhave been the dominant architecture in this field. However, recent advances in\ntransformer models and masked pretraining frameworks open new avenues for\nrepresentation learning. In this work, we propose CascadeFormer, a family of\ntwo-stage cascading transformers for skeleton-based human action recognition.\nOur framework consists of a masked pretraining stage to learn generalizable\nskeleton representations, followed by a cascading fine-tuning stage tailored\nfor discriminative action classification. We evaluate CascadeFormer across\nthree benchmark datasets (Penn Action N-UCLA, and NTU RGB+D 60), achieving\ncompetitive performance on all tasks. To promote reproducibility, we release\nour code and model checkpoints.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00692v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00692v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.356,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00693",
      "title": "DELTA: Variational Disentangled Learning for Privacy-Preserving Data\n  Reprogramming",
      "authors": [
        "Arun Vignesh Malarkkan",
        "Haoyue Bai",
        "Anjali Kaushik",
        "Yanjie Fu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In real-world applications, domain data often contains identifiable or\nsensitive attributes, is subject to strict regulations (e.g., HIPAA, GDPR), and\nrequires explicit data feature engineering for interpretability and\ntransparency. Existing feature engineering primarily focuses on advancing\ndownstream task performance, often risking privacy leakage. We generalize this\nlearning task under such new requirements as Privacy-Preserving Data\nReprogramming (PPDR): given a dataset, transforming features to maximize target\nattribute prediction accuracy while minimizing sensitive attribute prediction\naccuracy. PPDR poses challenges for existing systems: 1) generating\nhigh-utility feature transformations without being overwhelmed by a large\nsearch space, and 2) disentangling and eliminating sensitive information from\nutility-oriented features to reduce privacy inferability. To tackle these\nchallenges, we propose DELTA, a two-phase variational disentangled generative\nlearning framework. Phase I uses policy-guided reinforcement learning to\ndiscover feature transformations with downstream task utility, without any\nregard to privacy inferability. Phase II employs a variational LSTM seq2seq\nencoder-decoder with a utility-privacy disentangled latent space design and\nadversarial-causal disentanglement regularization to suppress privacy signals\nduring feature generation. Experiments on eight datasets show DELTA improves\npredictive performance by ~9.3% and reduces privacy leakage by ~35%,\ndemonstrating robust, privacy-aware data transformation.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00693v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00693v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.391,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning in Phase I to discover feature transformations based on utility scores from prediction accuracy, but it does not involve human feedback, a reward model trained on human-ranked data, or any alignment with human preferences. Thus, it does not align with RLHF.",
      "weak_supervision_justification": "The paper focuses on privacy-preserving feature transformation and generative learning, without any mention of programmatically generating noisy or imprecise labels for model training. It relies on standard data and evaluation metrics, not weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper employs variational autoencoders and LSTMs for feature generation and disentanglement, but it does not use diffusion models or iterative refinement processes for multi-step logical reasoning or chain-of-thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00696",
      "title": "Queuing for Civility: Regulating Emotions and Reducing Toxicity in\n  Digital Discourse",
      "authors": [
        "Akriti Verma",
        "Shama Islam",
        "Valeh Moghaddam",
        "Adnan Anwar"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)",
        "cs.LG (Machine Learning)",
        "cs.SI (Social and Information Networks)"
      ],
      "abstract": "The pervasiveness of online toxicity, including hate speech and trolling,\ndisrupts digital interactions and online well-being. Previous research has\nmainly focused on post-hoc moderation, overlooking the real-time emotional\ndynamics of online conversations and the impact of users' emotions on others.\nThis paper presents a graph-based framework to identify the need for emotion\nregulation within online conversations. This framework promotes self-reflection\nto manage emotional responses and encourage responsible behaviour in real time.\nAdditionally, a comment queuing mechanism is proposed to address intentional\ntrolls who exploit emotions to inflame conversations. This mechanism introduces\na delay in publishing comments, giving users time to self-regulate before\nfurther engaging in the conversation and helping maintain emotional balance.\nAnalysis of social media data from Twitter and Reddit demonstrates that the\ngraph-based framework reduced toxicity by 12%, while the comment queuing\nmechanism decreased the spread of anger by 15%, with only 4% of comments being\ntemporarily held on average. These findings indicate that combining real-time\nemotion regulation with delayed moderation can significantly improve well-being\nin online environments.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00696v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00696v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.29,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.261,
      "datasets_score": 0.277,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00700",
      "title": "Prompt the Unseen: Evaluating Visual-Language Alignment Beyond\n  Supervision",
      "authors": [
        "Raehyuk Jung",
        "Seungjun Yu",
        "Hyunjung Shim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision-Language Models (VLMs) combine a vision encoder and a large language\nmodel (LLM) through alignment training, showing strong performance on\nmultimodal tasks. A central component in this architecture is the projection\nlayer, which maps visual features into the LLM's embedding space. Despite its\nimportance, its ability to generalize to unseen visual concepts has not been\nsystematically evaluated. To address this, we propose a benchmark for\nevaluating projection-layer generalization. We adapt object detection datasets\n(rich in fine-grained annotations) into a prompting format and design\ntrain/test splits with disjoint label sets, enabling precise control over seen\nand unseen concept separation. Experimental results show that the projection\nlayer retains about 79 to 88 percent of the performance on unseen classes\ncompared to seen ones across various settings, suggesting a non-trivial level\nof generalization even without explicit alignment supervision on those\nconcepts. We further analyze this behavior through a mechanistic\ninterpretability lens. Our findings indicate that the feed-forward network in\nthe projection layer functions like a key-value memory, processing seen and\nunseen tokens in similar ways. This study introduces a new evaluation framework\nfor alignment generalization and highlights the potential for efficient VLM\ntraining with limited aligned data.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00700v2",
      "pdf_url": "http://arxiv.org/pdf/2509.00700v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.42,
      "weak_supervision_score": 0.426,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.356,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating the generalization of projection layers in Vision-Language Models using benchmarks and mechanistic analysis, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper utilizes pre-existing, fine-grained annotations from object detection datasets like Visual Genome, adapting them into prompts for training, which relies on structured labeled data rather than programmatically generated, noisy, or imprecise labels characteristic of weak supervision.",
      "diffusion_reasoning_justification": "The paper examines VLM projection layer generalization and mechanistic interpretability, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00701",
      "title": "Unsupervised Dataset Cleaning Framework for Encrypted Traffic\n  Classification",
      "authors": [
        "Kun Qiu",
        "Ying Wang",
        "Baoqian Li",
        "Wenjun Zhu"
      ],
      "categories": [
        "cs.NI (Networking and Internet Architecture)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Traffic classification, a technique for assigning network flows to predefined\ncategories, has been widely deployed in enterprise and carrier networks. With\nthe massive adoption of mobile devices, encryption is increasingly used in\nmobile applications to address privacy concerns. Consequently, traditional\nmethods such as Deep Packet Inspection (DPI) fail to distinguish encrypted\ntraffic. To tackle this challenge, Artificial Intelligence (AI), in particular\nMachine Learning (ML), has emerged as a promising solution for encrypted\ntraffic classification. A crucial prerequisite for any ML-based approach is\ntraffic data cleaning, which removes flows that are not useful for training\n(e.g., irrelevant protocols, background activity, control-plane messages, and\nlong-lived sessions). Existing cleaning solutions depend on manual inspection\nof every captured packet, making the process both costly and time-consuming. In\nthis poster, we present an unsupervised framework that automatically cleans\nencrypted mobile traffic. Evaluation on real-world datasets shows that our\nframework incurs only a 2%~2.5% reduction in classification accuracy compared\nwith manual cleaning. These results demonstrate that our method offers an\nefficient and effective preprocessing step for ML-based encrypted traffic\nclassification.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00701v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00701v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.419,
      "diffusion_reasoning_score": 0.294,
      "distributed_training_score": 0.315,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper presents an unsupervised framework for cleaning encrypted traffic datasets, which automates the removal of irrelevant flows to improve ML training efficiency. While this reduces the need for manual intervention, similar to weak supervision's goal of minimizing hand-labeling, the paper does not involve programmatically generating labels from noisy sources. Instead, it focuses on data preprocessing through unsupervised clustering, making it only loosely connected to weak supervision concepts.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00704",
      "title": "Why Pool When You Can Flow? Active Learning with GFlowNets",
      "authors": [
        "Renfei Zhang",
        "Mohit Pandey",
        "Artem Cherkasov",
        "Martin Ester"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The scalability of pool-based active learning is limited by the computational\ncost of evaluating large unlabeled datasets, a challenge that is particularly\nacute in virtual screening for drug discovery. While active learning strategies\nsuch as Bayesian Active Learning by Disagreement (BALD) prioritize informative\nsamples, it remains computationally intensive when scaled to libraries\ncontaining billions samples. In this work, we introduce BALD-GFlowNet, a\ngenerative active learning framework that circumvents this issue. Our method\nleverages Generative Flow Networks (GFlowNets) to directly sample objects in\nproportion to the BALD reward. By replacing traditional pool-based acquisition\nwith generative sampling, BALD-GFlowNet achieves scalability that is\nindependent of the size of the unlabeled pool. In our virtual screening\nexperiment, we show that BALD-GFlowNet achieves a performance comparable to\nthat of standard BALD baseline while generating more structurally diverse\nmolecules, offering a promising direction for efficient and scalable molecular\ndiscovery.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00704v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00704v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.411,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.404,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on active learning using GFlowNets to sample based on BALD rewards, which involve computational oracles for labeling in drug discovery. It does not involve human feedback, such as ranking data or fine-tuning models with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "The paper employs active learning with oracles to query and obtain labels, likely accurate ones from computational simulations in virtual screening. It does not rely on programmatically generated noisy or imprecise labels, which is the core of weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper addresses scalability in active learning by using generative sampling to avoid evaluating large pools, which indirectly relates to reducing computational overhead in large-scale tasks. However, it does not specifically discuss distributed training, parallel computing, or partitioning across nodes, focusing instead on generative methods.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00707",
      "title": "Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics\n  in Masked Diffusion LLMs",
      "authors": [
        "Daehoon Gwak",
        "Minseo Jung",
        "Junwoo Park",
        "Minho Park",
        "ChaeHun Park",
        "Junha Hyung",
        "Jaegul Choo"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Masked diffusion models (MDMs) offer a promising non-autoregressive\nalternative for large language modeling. Standard decoding methods for MDMs,\nsuch as confidence-based sampling, select tokens independently based on\nindividual token confidences at each diffusion step. However, we observe that\nthis independent token selection often results in generation orders resembling\nsequential autoregressive processes, limiting the advantages of\nnon-autoregressive modeling. To mitigate this pheonomenon, we propose\nReward-Weighted Sampling (RWS), a novel decoding strategy that leverages an\nexternal reward model to provide a principled global signal during the\niterative diffusion process. Specifically, at each diffusion step, RWS\nevaluates the quality of the entire intermediate sequence and scales token\nlogits accordingly, guiding token selection by integrating global\nsequence-level coherence. This method selectively increases the confidence of\ntokens that initially have lower scores, thereby promoting a more\nnon-autoregressive generation order. Furthermore, we provide theoretical\njustification showing that reward-weighted logit scaling induces beneficial\nrank reversals in token selection and consistently improves expected reward.\nExperiments demonstrate that RWS significantly promotes non-autoregressive\ngeneration orders, leading to improvements across multiple evaluation metrics.\nThese results highlight the effectiveness of integrating global signals in\nenhancing both the non-autoregressive properties and overall performance of\nMDMs.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00707v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00707v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.479,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.576,
      "distributed_training_score": 0.399,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses an external reward model for decoding in diffusion-based language models but does not involve training a model with human-ranked data or applying reinforcement learning for fine-tuning. There is no mention of human feedback, making it unrelated to RLHF.",
      "weak_supervision_justification": "The paper focuses on a decoding strategy for pre-trained models and does not involve training models with programmatically generated, noisy, or imprecise labels, which is the core of weak supervision.",
      "diffusion_reasoning_justification": "The paper employs diffusion models with iterative refinement for language generation, which shares some conceptual overlap with diffusion processes, but it does not specifically address multi-step logical reasoning or treat a Chain-of-Thought as a holistic entity for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00710",
      "title": "On Verifiable Legal Reasoning: A Multi-Agent Framework with Formalized\n  Knowledge Representations",
      "authors": [
        "Albert Sadowski",
        "Jarosław A. Chudziak"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Legal reasoning requires both precise interpretation of statutory language\nand consistent application of complex rules, presenting significant challenges\nfor AI systems. This paper introduces a modular multi-agent framework that\ndecomposes legal reasoning into distinct knowledge acquisition and application\nstages. In the first stage, specialized agents extract legal concepts and\nformalize rules to create verifiable intermediate representations of statutes.\nThe second stage applies this knowledge to specific cases through three steps:\nanalyzing queries to map case facts onto the ontology schema, performing\nsymbolic inference to derive logically entailed conclusions, and generating\nfinal answers using a programmatic implementation that operationalizes the\nontological knowledge. This bridging of natural language understanding with\nsymbolic reasoning provides explicit and verifiable inspection points,\nsignificantly enhancing transparency compared to end-to-end approaches.\nEvaluation on statutory tax calculation tasks demonstrates substantial\nimprovements, with foundational models achieving 76.4\\% accuracy compared to\n18.8\\% baseline performance, effectively narrowing the performance gap between\nreasoning and foundational models. These findings suggest that modular\narchitectures with formalized knowledge representations can make sophisticated\nlegal reasoning more accessible through computationally efficient models while\nenhancing consistency and explainability in AI legal reasoning, establishing a\nfoundation for future research into more transparent, trustworthy, and\neffective AI systems for legal domain.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00710v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00710v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.282,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a multi-agent framework for legal reasoning using formalized knowledge representations, symbolic inference, and ontological schemas, but it does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion techniques for multi-step logical tasks. The reasoning process described is sequential and symbolic, without treating a 'Chain-of-Thought' as a holistically corrected entity over iterations.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00713",
      "title": "It's-A-Me, Quantum Mario: Scalable Quantum Reinforcement Learning with\n  Multi-Chip Ensembles",
      "authors": [
        "Junghoon Justin Park",
        "Huan-Hsin Tseng",
        "Shinjae Yoo",
        "Samuel Yen-Chi Chen",
        "Jiook Cha"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Quantum reinforcement learning (QRL) promises compact function approximators\nwith access to vast Hilbert spaces, but its practical progress is slowed by\nNISQ-era constraints such as limited qubits and noise accumulation. We\nintroduce a multi-chip ensemble framework using multiple small Quantum\nConvolutional Neural Networks (QCNNs) to overcome these constraints. Our\napproach partitions complex, high-dimensional observations from the Super Mario\nBros environment across independent quantum circuits, then classically\naggregates their outputs within a Double Deep Q-Network (DDQN) framework. This\nmodular architecture enables QRL in complex environments previously\ninaccessible to quantum agents, achieving superior performance and learning\nstability compared to classical baselines and single-chip quantum models. The\nmulti-chip ensemble demonstrates enhanced scalability by reducing information\nloss from dimensionality reduction while remaining implementable on near-term\nquantum hardware, providing a practical pathway for applying QRL to real-world\nproblems.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00713v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00713v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.422,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution involves a multi-chip ensemble framework for Quantum Reinforcement Learning (QRL), where observation data is partitioned across multiple quantum circuits, processed in parallel, and aggregated classically. This directly aligns with distributed training concepts, as it partitions computation across multiple processors (quantum chips) to accelerate and scale model training, addressing NISQ constraints and enabling handling of high-dimensional environments like Super Mario Bros.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces a scalable quantum reinforcement learning (QRL) framework that utilizes multi-chip ensembles of small Quantum Convolutional Neural Networks (QCNNs) to address the limitations of NISQ devices in handling high-dimensional environments, such as Super Mario Bros. By partitioning complex observations across independent quantum circuits and aggregating their outputs within a Double Deep Q-Network (DDQN), the methodology achieves superior performance, learning stability, and scalability compared to classical baselines and single-chip quantum models, demonstrating a practical pathway for applying QRL to real-world problems.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing distributed quantum techniques with QRL to enable scalability in high-dimensional environments, though it builds on prior ideas like ensemble learning and variational quantum circuits rather than introducing a entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future QRL research by providing a scalable framework for complex environments, potentially leading to advancements in quantum AI applications, though its impact may be confined to specific subfields like quantum computing and reinforcement learning.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution to quantum reinforcement learning with practical implications for scalability, making it essential for researchers in quantum AI to be aware of its methods and findings.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/cdeccc216ee72d9c2c2af7de25e02cd96a4fb874",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 25,
      "average_h_index": 9.0,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Junghoon Park",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2258674420"
        },
        {
          "name": "Huan-Hsin Tseng",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2238634054"
        },
        {
          "name": "Shinjae Yoo",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2238633095"
        },
        {
          "name": "Samuel Yen-Chi Chen",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2261362529"
        },
        {
          "name": "Jiook Cha",
          "h_index": 25,
          "profile_url": "https://www.semanticscholar.org/author/40209044"
        }
      ]
    },
    {
      "id": "2509.00718",
      "title": "Exam Readiness Index (ERI): A Theoretical Framework for a Composite,\n  Explainable Index",
      "authors": [
        "Ananda Prakash Verma"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "We present a theoretical framework for an Exam Readiness Index (ERI): a\ncomposite, blueprint-aware score R in [0,100] that summarizes a learner's\nreadiness for a high-stakes exam while remaining interpretable and actionable.\nThe ERI aggregates six signals -- Mastery (M), Coverage (C), Retention (R),\nPace (P), Volatility (V), and Endurance (E) -- each derived from a stream of\npractice and mock-test interactions. We formalize axioms for component maps and\nthe composite, prove monotonicity, Lipschitz stability, and bounded drift under\nblueprint re-weighting, and show existence and uniqueness of the optimal linear\ncomposite under convex design constraints. We further characterize confidence\nbands via blueprint-weighted concentration and prove compatibility with\nprerequisite-admissible curricula (knowledge spaces / learning spaces). The\npaper focuses on theory; empirical study is left to future work.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00718v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00718v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.289,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00723",
      "title": "OmniDPO: A Preference Optimization Framework to Address Omni-Modal\n  Hallucination",
      "authors": [
        "Junzhe Chen",
        "Tianshu Zhang",
        "Shiyu Huang",
        "Yuwei Niu",
        "Chao Sun",
        "Rongzhou Zhang",
        "Guanyu Zhou",
        "Lijie Wen",
        "Xuming Hu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Recently, Omni-modal large language models (OLLMs) have sparked a new wave of\nresearch, achieving impressive results in tasks such as audio-video\nunderstanding and real-time environment perception. However, hallucination\nissues still persist. Similar to the bimodal setting, the priors from the text\nmodality tend to dominate, leading OLLMs to rely more heavily on textual cues\nwhile neglecting visual and audio information. In addition, fully multimodal\nscenarios introduce new challenges. Most existing models align visual or\nauditory modalities with text independently during training, while ignoring the\nintrinsic correlations between video and its corresponding audio. This\noversight results in hallucinations when reasoning requires interpreting hidden\naudio cues embedded in video content. To address these challenges, we propose\nOmniDPO, a preference-alignment framework designed to mitigate hallucinations\nin OLLMs. Specifically, OmniDPO incorporates two strategies: (1) constructing\ntext-preference sample pairs to enhance the model's understanding of\naudio-video interactions; and (2) constructing multimodal-preference sample\npairs to strengthen the model's attention to visual and auditory information.\nBy tackling both challenges, OmniDPO effectively improves multimodal grounding\nand reduces hallucination. Experiments conducted on two OLLMs demonstrate that\nOmniDPO not only effectively mitigates multimodal hallucinations but also\nsignificantly enhances the models' reasoning capabilities across modalities.\nAll code and datasets will be released upon paper acceptance.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00723v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00723v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.46,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.374,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper extends Direct Preference Optimization (DPO), which is related to preference alignment methods derived from RLHF concepts, but it does not involve training a separate reward model on human-ranked data or explicitly use human feedback. Instead, it constructs synthetic preference pairs using models like Qwen2-Audio, making it only loosely connected to the strict definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on preference optimization for mitigating hallucinations in omni-modal models and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00735",
      "title": "Task-Aware Adaptive Modulation: A Replay-Free and Resource-Efficient\n  Approach For Continual Graph Learning",
      "authors": [
        "Jingtao Liu",
        "Xinming Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Continual Graph Learning(CGL)focuses on acquiring new knowledge while\nretaining previously learned information, essential for real-world graph\napplications. Current methods grapple with two main issues:1) The\nStability-Plasticity Dilemma: Replay-based methods often create an imbalance\nbetween the Dilemma, while incurring significant storage costs.2) The\nResource-Heavy Pre-training: Leading replay-free methods critically depend on\nextensively pre-trained backbones, this reliance imposes a substantial resource\nburden.In this paper, we argue that the key to overcoming these challenges lies\nnot in replaying data or fine-tuning the entire network, but in dynamically\nmodulating the internal computational flow of a frozen backbone. We posit that\nlightweight, task-specific modules can effectively steer a GNN's reasoning\nprocess. Motivated by this insight, we propose Task-Aware Adaptive\nModulation(TAAM), a replay-free, resource-efficient approach that charts a new\npath for navigating the stability-plasticity dilemma. TAAM's core is its Neural\nSynapse Modulators(NSM), which are trained and then frozen for each task to\nstore expert knowledge. A pivotal prototype-guided strategy governs these\nmodulators: 1) For training, it initializes a new NSM by deep-copying from a\nsimilar past modulator to boost knowledge transfer. 2) For inference, it\nselects the most relevant frozen NSM for each task. These NSMs insert into a\nfrozen GNN backbone to perform fine-grained, node-attentive modulation of its\ninternal flow-different from the static perturbations of prior methods.\nExtensive experiments show that TAAM comprehensively outperforms\nstate-of-the-art methods across six GCIL benchmark datasets. The code will be\nreleased upon acceptance of the paper.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00735v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00735v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.426,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on continual graph learning using task-aware adaptive modulation with Neural Synapse Modulators in GNNs. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as defined by the topic.",
      "distributed_training_justification": "The paper addresses resource-efficient continual graph learning but does not discuss distributed training, parallel computing, or multi-node machine learning techniques for accelerating model training across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00740",
      "title": "Efficient Graph Understanding with LLMs via Structured Context Injection",
      "authors": [
        "Govind Waghmare",
        "Sumedh BG",
        "Sonia Gupta",
        "Srikanta Bedathur"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large Language Models (LLMs) have shown strong capabilities in solving\nproblems across domains, including graph-related tasks traditionally addressed\nby symbolic or algorithmic methods. In this work, we present a framework for\nstructured context injection, where task-specific information is systematically\nembedded in the input to guide LLMs in solving a wide range of graph problems.\nOur method does not require fine-tuning of LLMs, making it cost-efficient and\nlightweight. We observe that certain graph reasoning tasks remain challenging\nfor LLMs unless they are mapped to conceptually grounded representations.\nHowever, achieving such mappings through fine-tuning or repeated multi-step\nquerying can be expensive and inefficient. Our approach offers a practical\nalternative by injecting structured context directly into the input, enabling\nthe LLM to implicitly align the task with grounded conceptual spaces. We\nevaluate the approach on multiple graph tasks using both lightweight and large\nmodels, highlighting the trade-offs between accuracy and computational cost.\nThe results demonstrate consistent performance improvements, showing that\nstructured input context can rival or surpass more complex approaches. Our\nfindings underscore the value of structured context injection as an effective\nand scalable strategy for graph understanding with LLMs.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00740v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00740v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.395,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.502,
      "distributed_training_score": 0.355,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a framework for structured context injection in LLMs for graph tasks, which does not involve training models using programmatically generated labels or any form of weak supervision. It focuses on enhancing pre-trained models via input modifications, without addressing label generation, noisy data, or training processes.",
      "diffusion_reasoning_justification": "The paper does not incorporate diffusion-based processes, iterative refinement, or multi-step logical reasoning paths for correction. Instead, it proposes a single-step structured context injection method for LLMs on graph tasks, with no mention of treating reasoning as a holistic entity for refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00744",
      "title": "Quantum Causality: Resolving Simpson's Paradox with\n  $\\mathcal{DO}$-Calculus",
      "authors": [
        "Pilsung Kang"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Distinguishing correlation from causation is a fundamental challenge in\nmachine intelligence, often representing a critical barrier to building robust\nand trustworthy systems. While Pearl's $\\mathcal{DO}$-calculus provides a\nrigorous framework for causal inference, a parallel challenge lies in its\nphysical implementation. Here, we apply and experimentally validate a quantum\nalgorithmic framework for performing causal interventions. Our approach maps\ncausal networks onto quantum circuits where probabilistic links are encoded by\ncontrolled-rotation gates, and interventions are realized by a structural\nremodeling of the circuit -- a physical analogue to Pearl's ``graph surgery''.\nWe demonstrate the method's efficacy by resolving Simpson's Paradox in a\n3-qubit model, and show its scalability by quantifying confounding bias in a\n10-qubit healthcare simulation. Critically, we provide a proof-of-principle\nexperimental validation on an IonQ Aria quantum computer, successfully\nreproducing the paradox and its resolution in the presence of real-world noise.\nThis work establishes a practical pathway for quantum causal inference,\noffering a new computational tool to address deep-rooted challenges in\nalgorithmic fairness and explainable AI (XAI).",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00744v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00744v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.283,
      "weak_supervision_score": 0.297,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.308,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00745",
      "title": "Enhancing Fairness in Skin Lesion Classification for Medical Diagnosis\n  Using Prune Learning",
      "authors": [
        "Kuniko Paxton",
        "Koorosh Aslansefat",
        "Dhavalkumar Thakker",
        "Yiannis Papadopoulos",
        "Tanaya Maslekar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent advances in deep learning have significantly improved the accuracy of\nskin lesion classification models, supporting medical diagnoses and promoting\nequitable healthcare. However, concerns remain about potential biases related\nto skin color, which can impact diagnostic outcomes. Ensuring fairness is\nchallenging due to difficulties in classifying skin tones, high computational\ndemands, and the complexity of objectively verifying fairness. To address these\nchallenges, we propose a fairness algorithm for skin lesion classification that\novercomes the challenges associated with achieving diagnostic fairness across\nvarying skin tones. By calculating the skewness of the feature map in the\nconvolution layer of the VGG (Visual Geometry Group) network and the patches\nand the heads of the Vision Transformer, our method reduces unnecessary\nchannels related to skin tone, focusing instead on the lesion area. This\napproach lowers computational costs and mitigates bias without relying on\nconventional statistical methods. It potentially reduces model size while\nmaintaining fairness, making it more practical for real-world applications.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00745v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00745v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.376,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00749",
      "title": "Causal Interpretation of Sparse Autoencoder Features in Vision",
      "authors": [
        "Sangyu Han",
        "Yearim Kim",
        "Nojun Kwak"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Understanding what sparse auto-encoder (SAE) features in vision transformers\ntruly represent is usually done by inspecting the patches where a feature's\nactivation is highest. However, self-attention mixes information across the\nentire image, so an activated patch often co-occurs with-but does not cause-the\nfeature's firing. We propose Causal Feature Explanation (CaFE), which leverages\nEffective Receptive Field (ERF). We consider each activation of an SAE feature\nto be a target and apply input-attribution methods to identify the image\npatches that causally drive that activation. Across CLIP-ViT features, ERF maps\nfrequently diverge from naive activation maps, revealing hidden context\ndependencies (e.g., a \"roaring face\" feature that requires the co-occurrence of\neyes and nose, rather than merely an open mouth). Patch insertion tests confirm\nthat CaFE more effectively recovers or suppresses feature activations than\nactivation-ranked patches. Our results show that CaFE yields more faithful and\nsemantically precise explanations of vision-SAE features, highlighting the risk\nof misinterpretation when relying solely on activation location.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00749v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00749v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.304,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on causal interpretation of sparse autoencoder features in vision transformers, using methods like Effective Receptive Field and input attribution to understand feature activations. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00751",
      "title": "EVENT-Retriever: Event-Aware Multimodal Image Retrieval for Realistic\n  Captions",
      "authors": [
        "Dinh-Khoi Vo",
        "Van-Loc Nguyen",
        "Minh-Triet Tran",
        "Trung-Nghia Le"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Event-based image retrieval from free-form captions presents a significant\nchallenge: models must understand not only visual features but also latent\nevent semantics, context, and real-world knowledge. Conventional\nvision-language retrieval approaches often fall short when captions describe\nabstract events, implicit causality, temporal context, or contain long, complex\nnarratives. To tackle these issues, we introduce a multi-stage retrieval\nframework combining dense article retrieval, event-aware language model\nreranking, and efficient image collection, followed by caption-guided semantic\nmatching and rank-aware selection. We leverage Qwen3 for article search,\nQwen3-Reranker for contextual alignment, and Qwen2-VL for precise image\nscoring. To further enhance performance and robustness, we fuse outputs from\nmultiple configurations using Reciprocal Rank Fusion (RRF). Our system achieves\nthe top-1 score on the private test set of Track 2 in the EVENTA 2025 Grand\nChallenge, demonstrating the effectiveness of combining language-based\nreasoning and multimodal retrieval for complex, real-world image understanding.\nThe code is available at https://github.com/vdkhoi20/EVENT-Retriever.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00751v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00751v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.338,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a multi-stage framework for event-aware image retrieval using models like Qwen3 and Qwen2-VL, focusing on retrieval, reranking, and multimodal alignment. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00752",
      "title": "Multi-Level CLS Token Fusion for Contrastive Learning in Endoscopy Image\n  Classification",
      "authors": [
        "Y Hop Nguyen",
        "Doan Anh Phan Huu",
        "Trung Thai Tran",
        "Nhat Nam Mai",
        "Van Toi Giap",
        "Thao Thi Phuong Dao",
        "Trung-Nghia Le"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present a unified vision-language framework tailored for ENT endoscopy\nimage analysis that simultaneously tackles three clinically-relevant tasks:\nimage classification, image-to-image retrieval, and text-to-image retrieval.\nUnlike conventional CNN-based pipelines that struggle to capture cross-modal\nsemantics, our approach leverages the CLIP ViT-B/16 backbone and enhances it\nthrough Low-Rank Adaptation, multi-level CLS token aggregation, and spherical\nfeature interpolation. These components collectively enable efficient\nfine-tuning on limited medical data while improving representation diversity\nand semantic alignment across modalities. To bridge the gap between visual\ninputs and textual diagnostic context, we introduce class-specific natural\nlanguage prompts that guide the image encoder through a joint training\nobjective combining supervised classification with contrastive learning. We\nvalidated our framework through participation in the ACM MM'25 ENTRep Grand\nChallenge, achieving 95% accuracy and F1-score in classification, Recall@1 of\n0.93 and 0.92 for image-to-image and text-to-image retrieval respectively, and\nMRR scores of 0.97 and 0.96. Ablation studies demonstrated the incremental\nbenefits of each architectural component, validating the effectiveness of our\ndesign for robust multimodal medical understanding in low-resource clinical\nsettings.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00752v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00752v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.394,
      "distributed_training_score": 0.327,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00757",
      "title": "MarkSplatter: Generalizable Watermarking for 3D Gaussian Splatting Model\n  via Splatter Image Structure",
      "authors": [
        "Xiufeng Huang",
        "Ziyuan Luo",
        "Qi Song",
        "Ruofei Wang",
        "Renjie Wan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The growing popularity of 3D Gaussian Splatting (3DGS) has intensified the\nneed for effective copyright protection. Current 3DGS watermarking methods rely\non computationally expensive fine-tuning procedures for each predefined\nmessage. We propose the first generalizable watermarking framework that enables\nefficient protection of Splatter Image-based 3DGS models through a single\nforward pass. We introduce GaussianBridge that transforms unstructured 3D\nGaussians into Splatter Image format, enabling direct neural processing for\narbitrary message embedding. To ensure imperceptibility, we design a\nGaussian-Uncertainty-Perceptual heatmap prediction strategy for preserving\nvisual quality. For robust message recovery, we develop a dense\nsegmentation-based extraction mechanism that maintains reliable extraction even\nwhen watermarked objects occupy minimal regions in rendered views. Project\npage: https://kevinhuangxf.github.io/marksplatter.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00757v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00757v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.244,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.285,
      "datasets_score": 0.249,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00760",
      "title": "No More Sibling Rivalry: Debiasing Human-Object Interaction Detection",
      "authors": [
        "Bin Yang",
        "Yulin Zhang",
        "Hong-Yu Zhou",
        "Sibei Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Detection transformers have been applied to human-object interaction (HOI)\ndetection, enhancing the localization and recognition of human-action-object\ntriplets in images. Despite remarkable progress, this study identifies a\ncritical issue-\"Toxic Siblings\" bias-which hinders the interaction decoder's\nlearning, as numerous similar yet distinct HOI triplets interfere with and even\ncompete against each other both input side and output side to the interaction\ndecoder. This bias arises from high confusion among sibling\ntriplets/categories, where increased similarity paradoxically reduces\nprecision, as one's gain comes at the expense of its toxic sibling's decline.\nTo address this, we propose two novel debiasing learning\nobjectives-\"contrastive-then-calibration\" and \"merge-then-split\"-targeting the\ninput and output perspectives, respectively. The former samples sibling-like\nincorrect HOI triplets and reconstructs them into correct ones, guided by\nstrong positional priors. The latter first learns shared features among sibling\ncategories to distinguish them from other groups, then explicitly refines\nintra-group differentiation to preserve uniqueness. Experiments show that we\nsignificantly outperform both the baseline (+9.18% mAP on HICO-Det) and the\nstate-of-the-art (+3.59% mAP) across various settings.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00760v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00760v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.346,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on debiasing techniques for human-object interaction detection in computer vision, specifically addressing \"Toxic Siblings\" bias through novel learning objectives like \"contrastive-then-calibration\" and \"merge-then-split\". It does not involve reinforcement learning, human feedback, reward models, or any mechanism for aligning AI with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00761",
      "title": "L-MARS: Legal Multi-Agent Workflow with Orchestrated Reasoning and\n  Agentic Search",
      "authors": [
        "Ziqi Wang",
        "Boqin Yuan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "We present L-MARS (Legal Multi-Agent Workflow with Orchestrated Reasoning and\nAgentic Search), a system that reduces hallucination and uncertainty in legal\nquestion answering through coordinated multi-agent reasoning and retrieval.\nUnlike single-pass retrieval-augmented generation (RAG), L-MARS decomposes\nqueries into subproblems, issues targeted searches across heterogeneous sources\n(Serper web, local RAG, CourtListener case law), and employs a Judge Agent to\nverify sufficiency, jurisdiction, and temporal validity before answer\nsynthesis. This iterative reasoning-search-verification loop maintains\ncoherence, filters noisy evidence, and grounds answers in authoritative law. We\nevaluated L-MARS on LegalSearchQA, a new benchmark of 200 up-to-date multiple\nchoice legal questions in 2025. Results show that L-MARS substantially improves\nfactual accuracy, reduces uncertainty, and achieves higher preference scores\nfrom both human experts and LLM-based judges. Our work demonstrates that\nmulti-agent reasoning with agentic search offers a scalable and reproducible\nblueprint for deploying LLMs in high-stakes domains requiring precise legal\nretrieval and deliberation.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00761v2",
      "pdf_url": "http://arxiv.org/pdf/2509.00761v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.451,
      "distributed_training_score": 0.317,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces L-MARS, a multi-agent system for legal question answering that uses iterative reasoning, search, and verification loops. While it involves multi-step logical reasoning and refinement, it does not adapt the iterative refinement process of diffusion models, nor does it treat the Chain-of-Thought as a single entity for holistic correction using a diffusion model. The core mechanisms rely on agent-based workflows and retrieval-augmented generation, with no mention of diffusion-based techniques. Thus, the paper's contributions do not align with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00764",
      "title": "Low Power Approximate Multiplier Architecture for Deep Neural Networks",
      "authors": [
        "Pragun Jaswal",
        "L. Hemanth Krishna",
        "B. Srinivasu"
      ],
      "categories": [
        "cs.AR (Hardware Architecture)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper proposes an low power approximate multiplier architecture for deep\nneural network (DNN) applications. A 4:2 compressor, introducing only a single\ncombination error, is designed and integrated into an 8x8 unsigned multiplier.\nThis integration significantly reduces the usage of exact compressors while\npreserving low error rates. The proposed multiplier is employed within a custom\nconvolution layer and evaluated on neural network tasks, including image\nrecognition and denoising. Hardware evaluation demonstrates that the proposed\ndesign achieves up to 30.24% energy savings compared to the best among existing\nmultipliers. In image denoising, the custom approximate convolution layer\nachieves improved Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity\nIndex Measure (SSIM) compared to other approximate designs. Additionally, when\napplied to handwritten digit recognition, the model maintains high\nclassification accuracy. These results demonstrate that the proposed\narchitecture offers a favorable balance between energy efficiency and\ncomputational precision, making it suitable for low-power AI hardware\nimplementations.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00764v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00764v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.435,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on designing a low-power approximate multiplier for DNN hardware, emphasizing energy efficiency in arithmetic units for tasks like image recognition and denoising. It does not discuss distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation in multi-processor environments. The contributions are centered on single-device hardware optimizations, with no relation to distributed systems or accelerating training via multi-node setups.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00767",
      "title": "InterPose: Learning to Generate Human-Object Interactions from\n  Large-Scale Web Videos",
      "authors": [
        "Yangsong Zhang",
        "Abdul Ahad Butt",
        "Gül Varol",
        "Ivan Laptev"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Human motion generation has shown great advances thanks to the recent\ndiffusion models trained on large-scale motion capture data. Most of existing\nworks, however, currently target animation of isolated people in empty scenes.\nMeanwhile, synthesizing realistic human-object interactions in complex 3D\nscenes remains a critical challenge in computer graphics and robotics. One\nobstacle towards generating versatile high-fidelity human-object interactions\nis the lack of large-scale datasets with diverse object manipulations. Indeed,\nexisting motion capture data is typically restricted to single people and\nmanipulations of limited sets of objects. To address this issue, we propose an\nautomatic motion extraction pipeline and use it to collect interaction-rich\nhuman motions. Our new dataset InterPose contains 73.8K sequences of 3D human\nmotions and corresponding text captions automatically obtained from 45.8K\nvideos with human-object interactions. We perform extensive experiments and\ndemonstrate InterPose to bring significant improvements to state-of-the-art\nmethods for human motion generation. Moreover, using InterPose we develop an\nLLM-based agent enabling zero-shot animation of people interacting with diverse\nobjects and scenes.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00767v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00767v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.393,
      "distributed_training_score": 0.349,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00768",
      "title": "Aligning Reasoning LLMs for Materials Discovery with Physics-aware\n  Rejection Sampling",
      "authors": [
        "Lee Hyun",
        "Sohee Yoon",
        "Jinwoo Park",
        "Sue In Chae",
        "Seongeon Park",
        "Jooyeon Ahn",
        "Yebin Jung",
        "Youjung Chung",
        "Hogeun Chang",
        "Myeonginn Kang",
        "Jina Kim",
        "Ho-Gyeong Kim",
        "Myeonghun Jeong"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "AI-driven materials discovery that couples automated experimentation with\nalgorithmic decision-making requires process aware recipe to property\npredictors that are accurate, calibrated, and physically admissible. We\napproach this as a reasoning problem with large reasoning models (LRMs). To\ninstill reasoning capability into language models, we curate reasoning traces\nfrom a teacher model to train a student model. However, most training pipelines\nselect reasoning traces using binary correctness or learned preference signals\nthat poorly reflect physical admissibility. We introduce Physics-aware\nRejection Sampling (PaRS), a training-time trace selection scheme that favors\ntraces consistent with fundamental physics and numerically close to targets,\nwith lightweight halting to control compute. We instantiate our framework with\na large student model fine-tuned on traces synthesized by a larger teacher\nmodel, and evaluate under matched token budgets against various rejection\nsampling baselines. Our method improves accuracy and calibration, reduces\nphysics-violation rates, and lowers sampling cost relative to baselines. These\nresults indicate that modest, domain-aware constraints combined with\ntrace-level selection provide a practical path toward reliable, efficient LRMs\nfor process-aware property prediction and closed-loop materials design.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00768v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00768v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.51,
      "distributed_training_score": 0.395,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Physics-aware Rejection Sampling for training LRMs using physics-based criteria, not human feedback. While it mentions learned reward models in prior work, the method itself relies on automated, domain-specific filtering without involving human-ranked data or reinforcement learning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs rejection sampling for selecting reasoning traces but does not involve diffusion models or iterative refinement processes for logical tasks. There is no mention of treating Chain-of-Thought as a holistic entity for multi-step correction using diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00777",
      "title": "IntrinsicReal: Adapting IntrinsicAnything from Synthetic to Real Objects",
      "authors": [
        "Xiaokang Wei",
        "Zizheng Yan",
        "Zhangyang Xiong",
        "Yiming Hao",
        "Yipeng Qin",
        "Xiaoguang Han"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Estimating albedo (a.k.a., intrinsic image decomposition) from single RGB\nimages captured in real-world environments (e.g., the MVImgNet dataset)\npresents a significant challenge due to the absence of paired images and their\nground truth albedos. Therefore, while recent methods (e.g., IntrinsicAnything)\nhave achieved breakthroughs by harnessing powerful diffusion priors, they\nremain predominantly trained on large-scale synthetic datasets (e.g.,\nObjaverse) and applied directly to real-world RGB images, which ignores the\nlarge domain gap between synthetic and real-world data and leads to suboptimal\ngeneralization performance. In this work, we address this gap by proposing\nIntrinsicReal, a novel domain adaptation framework that bridges the\nabove-mentioned domain gap for real-world intrinsic image decomposition.\nSpecifically, our IntrinsicReal adapts IntrinsicAnything to the real domain by\nfine-tuning it using its high-quality output albedos selected by a novel dual\npseudo-labeling strategy: i) pseudo-labeling with an absolute confidence\nthreshold on classifier predictions, and ii) pseudo-labeling using the relative\npreference ranking of classifier predictions for individual input objects. This\nstrategy is inspired by human evaluation, where identifying the highest-quality\noutputs is straightforward, but absolute scores become less reliable for\nsub-optimal cases. In these situations, relative comparisons of outputs become\nmore accurate. To implement this, we propose a novel two-phase pipeline that\nsequentially applies these pseudo-labeling techniques to effectively adapt\nIntrinsicAnything to the real domain. Experimental results show that our\nIntrinsicReal significantly outperforms existing methods, achieving\nstate-of-the-art results for albedo estimation on both synthetic and real-world\ndatasets.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00777v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00777v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.333,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes diffusion models, such as in IntrinsicAnything and Diffusion-DPO, for iterative refinement in generating and adapting albedo images from synthetic to real domains. However, this application is focused on visual tasks like image decomposition and domain adaptation, not on solving complex logical tasks or treating a 'Chain-of-Thought' as a holistic entity for multi-step reasoning. Thus, while iterative refinement is present, it does not align with the topic's emphasis on logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00778",
      "title": "Energy Efficient Exact and Approximate Systolic Array Architecture for\n  Matrix Multiplication",
      "authors": [
        "Pragun Jaswal",
        "L. Hemanth Krishna",
        "B. Srinivasu"
      ],
      "categories": [
        "cs.AR (Hardware Architecture)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep Neural Networks (DNNs) require highly efficient matrix multiplication\nengines for complex computations. This paper presents a systolic array\narchitecture incorporating novel exact and approximate processing elements\n(PEs), designed using energy-efficient positive partial product and negative\npartial product cells, termed as PPC and NPPC, respectively. The proposed 8-bit\nexact and approximate PE designs are employed in a 8x8 systolic array, which\nachieves a energy savings of 22% and 32%, respectively, compared to the\nexisting design. To demonstrate their effectiveness, the proposed PEs are\nintegrated into a systolic array (SA) for Discrete Cosine Transform (DCT)\ncomputation, achieving high output quality with a PSNR of 38.21,dB.\nFurthermore, in an edge detection application using convolution, the\napproximate PE achieves a PSNR of 30.45,dB. These results highlight the\npotential of the proposed design to deliver significant energy efficiency while\nmaintaining competitive output quality, making it well-suited for\nerror-resilient image and vision processing applications.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00778v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00778v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.255,
      "weak_supervision_score": 0.271,
      "diffusion_reasoning_score": 0.313,
      "distributed_training_score": 0.406,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on energy-efficient systolic array architectures for matrix multiplication, which involves hardware-level parallel computing for AI workloads like DNNs. While systolic arrays enable parallelism that could indirectly support distributed training systems (e.g., in accelerators like TPUs), the paper does not address distributed training concepts such as multi-node data partitioning, model parallelism across processors, or algorithms for accelerating training over networks. Thus, it is only tangentially related through the broader theme of parallel computation in machine learning.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00781",
      "title": "Secure and Scalable Face Retrieval via Cancelable Product Quantization",
      "authors": [
        "Haomiao Tang",
        "Wenjie Li",
        "Yixiang Qiu",
        "Genping Wang",
        "Shu-Tao Xia"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Despite the ubiquity of modern face retrieval systems, their retrieval stage\nis often outsourced to third-party entities, posing significant risks to user\nportrait privacy. Although homomorphic encryption (HE) offers strong security\nguarantees by enabling arithmetic computations in the cipher space, its high\ncomputational inefficiency makes it unsuitable for real-time, real-world\napplications. To address this issue, we propose Cancelable Product\nQuantization, a highly efficient framework for secure face representation\nretrieval. Our hierarchical two-stage framework comprises: (i) a\nhigh-throughput cancelable PQ indexing module for fast candidate filtering, and\n(ii) a fine-grained cipher-space retrieval module for final precise face\nranking. A tailored protection mechanism is designed to secure the indexing\nmodule for cancelable biometric authentication while ensuring efficiency.\nExperiments on benchmark datasets demonstrate that our method achieves an\ndecent balance between effectiveness, efficiency and security.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00781v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00781v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.266,
      "diffusion_reasoning_score": 0.275,
      "distributed_training_score": 0.329,
      "datasets_score": 0.26,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00783",
      "title": "LegalChainReasoner: A Legal Chain-guided Framework for Criminal Judicial\n  Opinion Generation",
      "authors": [
        "Weizhe Shi",
        "Qiqi Wang",
        "Yihong Pan",
        "Qian Liu",
        "Kaiqi Zhao"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "A criminal judicial opinion represents the judge's disposition of a case,\nincluding the decision rationale and sentencing. Automatically generating such\nopinions can assist in analyzing sentencing consistency and provide judges with\nreferences to similar past cases. However, current research typically\napproaches this task by dividing it into two isolated subtasks: legal reasoning\nand sentencing prediction. This separation often leads to inconsistency between\nthe reasoning and predictions, failing to meet real-world judicial\nrequirements. Furthermore, prior studies rely on manually curated knowledge to\nenhance applicability, yet such methods remain limited in practical deployment.\nTo address these limitations and better align with legal practice, we propose a\nnew LegalAI task: Judicial Opinion Generation, which simultaneously produces\nboth legal reasoning and sentencing decisions. To achieve this, we introduce\nLegalChainReasoner, a framework that applies structured legal chains to guide\nthe model through comprehensive case assessments. By integrating factual\npremises, composite legal conditions, and sentencing conclusions, our approach\nensures flexible knowledge injection and end-to-end opinion generation.\nExperiments on two real-world and open-source Chinese legal case datasets\ndemonstrate that our method outperforms baseline models.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00783v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00783v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.494,
      "distributed_training_score": 0.28,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces the LegalChainReasoner framework, which uses structured legal chains (premise-situation-conclusion triplets) and Chain-Aware encoding for judicial opinion generation. It focuses on integrating legal reasoning and sentencing prediction but does not mention or utilize diffusion models, iterative refinement processes, or any mechanism for holistically correcting a chain-of-thought over multiple steps. Therefore, it lacks the core elements of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00786",
      "title": "Aligned Anchor Groups Guided Line Segment Detector",
      "authors": [
        "Zeyu Li",
        "Annan Shu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper introduces a novel line segment detector, the Aligned Anchor\nGroups guided Line Segment Detector (AAGLSD), designed to detect line segments\nfrom images with high precision and completeness. The algorithm employs a\nhierarchical approach to extract candidate pixels with different saliency\nlevels, including regular anchors and aligned anchor groups. AAGLSD initiates\nfrom these aligned anchor groups, sequentially linking anchors and updating the\ncurrently predicted line segment simultaneously. The final predictions are\nderived through straightforward validation and merging of adjacent line\nsegments, avoiding complex refinement strategies. AAGLSD is evaluated on\nvarious datasets and quantitative experiments demonstrate that the proposed\nmethod can effectively extract complete line segments from input images\ncompared to other advanced line segment detectors. The implementation is\navailable at https://github.com/LLiDaBao/AAGLSD.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00786v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00786v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.304,
      "distributed_training_score": 0.289,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00787",
      "title": "Diffusion-Based Image-to-Brain Signal Generation with Cross-Attention\n  Mechanisms for Visual Prostheses",
      "authors": [
        "Ganxi Xu",
        "Jinyi Long",
        "Jia Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Visual prostheses have shown great potential in restoring vision for blind\nindividuals. However, while researchers have successfully utilized M/EEG\nsignals to evoke visual perceptions during the brain decoding stage of visual\nprostheses, the complementary process-converting images to M/EEG signals in the\nbrain encoding stage-remains largely unexplored. Thus, we present the first\nimage-to-brain signal (M/EEG) framework based on denoising diffusion\nprobabilistic models enhanced with cross-attention mechanisms. Our framework\nconsists of two key architectural components: a pre-trained CLIP visual encoder\nthat extracts rich semantic representations from input images, and a\ncross-attention enhanced U-Net diffusion model that learns to reconstruct\nbiologically plausible brain signals through iterative denoising. Unlike\nconventional generative models that rely on simple concatenation for\nconditioning, our cross-attention modules enable dynamic interaction between\nvisual features and brain signal representations, facilitating fine-grained\nalignment during the generation process. Furthermore, we evaluate our framework\non two multimodal datasets (THINGS-EEG2 and THINGS-MEG) to demonstrate its\neffectiveness in generating biologically plausible brain signals. Additionally,\nwe pioneer the visualization of M/EEG topographies across all subjects in both\ndatasets, providing intuitive demonstrations of intra-subject and inter-subject\nvariations in brain signals.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00787v2",
      "pdf_url": "http://arxiv.org/pdf/2509.00787v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.528,
      "distributed_training_score": 0.32,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for generating brain signals from images in the context of visual prostheses, emphasizing generative tasks like denoising and signal reconstruction. It does not involve adapting diffusion models for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks. Therefore, there is no clear component aligning with the topic's definition.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00789",
      "title": "OmniReason: A Temporal-Guided Vision-Language-Action Framework for\n  Autonomous Driving",
      "authors": [
        "Pei Liu",
        "Qingtian Ning",
        "Xinyan Lu",
        "Haipeng Liu",
        "Weiliang Ma",
        "Dangen She",
        "Peng Jia",
        "Xianpeng Lang",
        "Jun Ma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in vision-language models (VLMs) have demonstrated impressive\nspatial reasoning capabilities for autonomous driving, yet existing methods\npredominantly focus on static scene understanding while neglecting the\nessential temporal dimension of real-world driving scenarios. To address this\ncritical limitation, we propose the OmniReason framework, which establishes\nrobust spatiotemporal reasoning by jointly modeling dynamic 3D environments and\ntheir underlying decision-making processes. Our work makes two fundamental\nadvances: (1) We introduce OmniReason-Data, two large-scale\nvision-language-action (VLA) datasets with dense spatiotemporal annotations and\nnatural language explanations, generated through a novel\nhallucination-mitigated auto-labeling pipeline that ensures both physical\nplausibility and temporal coherence; (2) We develop the OmniReason-Agent\narchitecture, which integrates a sparse temporal memory module for persistent\nscene context modeling and an explanation generator that produces\nhuman-interpretable decision rationales, facilitated by our spatiotemporal\nknowledge distillation approach that effectively captures spatiotemporal causal\nreasoning patterns. Comprehensive experiments demonstrate state-of-the-art\nperformance, where OmniReason-Agent achieves significant improvements in both\nopen-loop planning tasks and visual question answering (VQA) benchmarks, while\nestablishing new capabilities for interpretable, temporally-aware autonomous\nvehicles operating in complex, dynamic environments.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00789v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00789v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.45,
      "distributed_training_score": 0.356,
      "datasets_score": 0.413,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a vision-language-action framework for autonomous driving, emphasizing temporal reasoning, knowledge distillation, and dataset creation. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contributions include introducing two new large-scale datasets (OmniReason-nuScenes and OmniReason-Bench2Drive) with spatiotemporal annotations, detailing their creation via a hallucination-mitigated auto-labeling pipeline, and evaluating their use in benchmarks for autonomous driving. This directly aligns with research on creating, analyzing, and benchmarking datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The OmniReason framework addresses the limitations of existing vision-language models in autonomous driving by incorporating temporal reasoning for dynamic 3D environments. It introduces two large-scale datasets, OmniReason-Data, with dense spatiotemporal annotations generated via a hallucination-mitigated auto-labeling pipeline, and develops the OmniReason-Agent architecture, which integrates temporal memory and knowledge distillation for improved scene understanding, decision-making, and interpretability, achieving state-of-the-art performance in planning tasks and visual question answering benchmarks.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel framework and datasets that address the underexplored temporal dimension in vision-language models for autonomous driving, significantly advancing the state-of-the-art in spatiotemporal reasoning and interpretable decision-making.",
      "impact_score": "High",
      "impact_justification": "This work could broadly influence autonomous driving research and applications by enhancing safety, interpretability, and temporal awareness, potentially leading to widespread adoption in real-world systems and further innovations in AI-driven vehicles.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "As a high-quality paper with innovative contributions to autonomous driving, it offers valuable insights for researchers in computer vision and AI, making it essential for those working on dynamic scene understanding and decision-making.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/80c5e8dd70de11acfbdfb46e9256c3cf067b7759",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 10,
      "average_h_index": 2.3333333333333335,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Pei Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359687414"
        },
        {
          "name": "Qingtian Ning",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378708095"
        },
        {
          "name": "Xinyan Lu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2337666205"
        },
        {
          "name": "Haipeng Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2325230652"
        },
        {
          "name": "Weiliang Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374469647"
        },
        {
          "name": "Dangen She",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374155341"
        },
        {
          "name": "Peng Jia",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2277497559"
        },
        {
          "name": "Xianpeng Lang",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2277446558"
        },
        {
          "name": "Jun Ma",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2349362645"
        }
      ]
    },
    {
      "id": "2509.00793",
      "title": "Sharpe Ratio Optimization in Markov Decision Processes",
      "authors": [
        "Shuai Ma",
        "Guangwu Liu",
        "Li Xia"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Sharpe ratio (also known as reward-to-variability ratio) is a widely-used\nmetric in finance, which measures the additional return at the cost of per unit\nof increased risk (standard deviation of return). However, the optimization of\nSharpe ratio in Markov decision processes (MDPs) is challenging, because there\nexist two difficulties hindering the application of dynamic programming. One is\nthat dynamic programming does not work for fractional objectives, and the other\nis that dynamic programming is invalid for risk metrics. In this paper, we\nstudy the Sharpe ratio optimization in infinite-horizon MDPs, considering both\nthe long-run average and discounted settings. We address the first challenge\nwith the Dinkelbachs transform, which converts the Sharpe ratio objective to a\nmean-squared-variance (M2V) objective. It is shown that the M2V optimization\nand the original Sharpe ratio optimization share the same optimal policy when\nthe risk-sensitive parameter is equal to the optimal Sharpe ratio. For the\nsecond challenge, we develop an iterative algorithm to solve the M2V\noptimization which is similar to a mean-variance optimization in MDPs. We\niteratively solve the M2V problem and obtain the associated Sharpe ratio that\nis used to update the risk-sensitive parameter in the next iteration of M2V\nproblems. We show that such a sequence of Sharpe ratios derived is\nmonotonically increasing and converges to the optimal Sharpe ratio. For both\naverage and discounted MDP settings, we develop a policy iteration procedure\nand prove its convergence to the optimum. Numerical experiments are conducted\nfor validation. To the best of our knowledge, our approach is the first that\nsolves the Sharpe ratio optimization in MDPs with dynamic programming type\nalgorithms. We believe that the proposed algorithm can shed light on solving\nMDPs with other fractional objectives.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00793v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00793v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.268,
      "diffusion_reasoning_score": 0.296,
      "distributed_training_score": 0.297,
      "datasets_score": 0.202,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00797",
      "title": "ProCause: Generating Counterfactual Outcomes to Evaluate Prescriptive\n  Process Monitoring Methods",
      "authors": [
        "Jakob De Moor",
        "Hans Weytjens",
        "Johannes De Smedt"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ME (Methodology)"
      ],
      "abstract": "Prescriptive Process Monitoring (PresPM) is the subfield of Process Mining\nthat focuses on optimizing processes through real-time interventions based on\nevent log data. Evaluating PresPM methods is challenging due to the lack of\nground-truth outcomes for all intervention actions in datasets. A generative\ndeep learning approach from the field of Causal Inference (CI), RealCause, has\nbeen commonly used to estimate the outcomes for proposed intervention actions\nto evaluate a new policy. However, RealCause overlooks the temporal\ndependencies in process data, and relies on a single CI model architecture,\nTARNet, limiting its effectiveness. To address both shortcomings, we introduce\nProCause, a generative approach that supports both sequential (e.g., LSTMs) and\nnon-sequential models while integrating multiple CI architectures (S-Learner,\nT-Learner, TARNet, and an ensemble). Our research using a simulator with known\nground truths reveals that TARNet is not always the best choice; instead, an\nensemble of models offers more consistent reliability, and leveraging LSTMs\nshows potential for improved evaluations when temporal dependencies are\npresent. We further validate ProCause's practical effectiveness through a\nreal-world data analysis, ensuring a more reliable evaluation of PresPM\nmethods.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00797v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00797v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.285,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00798",
      "title": "Multimodal Iterative RAG for Knowledge Visual Question Answering",
      "authors": [
        "Changin Choi",
        "Wonseok Lee",
        "Jungmin Ko",
        "Wonjong Rhee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While Multimodal Large Language Models (MLLMs) have significantly advanced\nmultimodal understanding, their performance remains limited on\nknowledge-intensive visual questions that require external knowledge beyond the\nimage. Retrieval-Augmented Generation (RAG) has become a promising solution for\nproviding models with external knowledge, its conventional single-pass\nframework often fails to gather sufficient knowledge. To overcome this\nlimitation, we propose MI-RAG, a Multimodal Iterative RAG framework that\nleverages reasoning to enhance retrieval and update reasoning over newly\nretrieved knowledge across modalities. At each iteration, MI-RAG leverages an\naccumulated reasoning record to dynamically formulate a multi-query. These\nqueries then drive a joint search across heterogeneous knowledge bases\ncontaining both visually-grounded and textual knowledge. The newly acquired\nknowledge is synthesized into the reasoning record, progressively refining\nunderstanding across iterations. Experiments on challenging benchmarks,\nincluding Encyclopedic VQA, InfoSeek, and OK-VQA, show that MI-RAG\nsignificantly improves both retrieval recall and answer accuracy, establishing\na scalable approach for compositional reasoning in knowledge-intensive VQA.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00798v2",
      "pdf_url": "http://arxiv.org/pdf/2509.00798v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.477,
      "distributed_training_score": 0.291,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces MI-RAG, an iterative framework for multimodal retrieval-augmented generation in visual question answering, focusing on cycles of retrieval and reasoning to enhance knowledge gathering. However, it does not involve diffusion models or adapt the iterative refinement process of diffusion for logical tasks. The paper's approach is based on RAG and LLM reasoning, with no mention of treating a 'Chain-of-Thought' as a holistic entity via diffusion, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00800",
      "title": "SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting",
      "authors": [
        "Zhuodong Jiang",
        "Haoran Wang",
        "Guoxi Huang",
        "Brett Seymour",
        "Nantheera Anantrasirichai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate 3D reconstruction in underwater environments remains a complex\nchallenge due to issues such as light distortion, turbidity, and limited\nvisibility. AI-based techniques have been applied to address these issues,\nhowever, existing methods have yet to fully exploit the potential of AI,\nparticularly in integrating language models with visual processing. In this\npaper, we propose a novel framework that leverages multimodal cross-knowledge\nto create semantic-guided 3D Gaussian Splatting for robust and high-fidelity\ndeep-sea scene reconstruction. By embedding an extra semantic feature into each\nGaussian primitive and supervised by the CLIP extracted semantic feature, our\nmethod enforces semantic and structural awareness throughout the training. The\ndedicated semantic consistency loss ensures alignment with high-level scene\nunderstanding. Besides, we propose a novel stage-wise training strategy,\ncombining coarse-to-fine learning with late-stage parameter refinement, to\nfurther enhance both stability and reconstruction quality. Extensive results\nshow that our approach consistently outperforms state-of-the-art methods on\nSeaThru-NeRF and Submerged3D datasets across three metrics, with an improvement\nof up to 3.09 dB on average in terms of PSNR, making it a strong candidate for\napplications in underwater exploration and marine perception.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00800v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00800v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.293,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.328,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper uses CLIP-extracted semantic features to supervise the training of Gaussian primitives, which involves high-level guidance from a pre-trained model. This is somewhat related to weak supervision as it relies on indirectly derived labels rather than hand-annotated data, but it does not explicitly involve programmatically generating noisy or imprecise labels, making it only tangentially relevant.",
      "diffusion_reasoning_justification": "The paper focuses on semantic-guided Gaussian Splatting for 3D reconstruction and does not mention or utilize diffusion models, iterative refinement for logical tasks, or any multi-step reasoning processes, so it has no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00806",
      "title": "CaresAI at BioCreative IX Track 1 -- LLM for Biomedical QA",
      "authors": [
        "Reem Abdel-Salam",
        "Mary Adewunmi",
        "Modinat A. Abayomi"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large language models (LLMs) are increasingly evident for accurate question\nanswering across various domains. However, rigorous evaluation of their\nperformance on complex question-answering (QA) capabilities is essential before\ndeployment in real-world biomedical and healthcare applications. This paper\npresents our approach to the MedHopQA track of the BioCreative IX shared task,\nwhich focuses on multi-hop biomedical question answering involving diseases,\ngenes, and chemicals. We adopt a supervised fine-tuning strategy leveraging\nLLaMA 3 8B, enhanced with a curated biomedical question-answer dataset compiled\nfrom external sources including BioASQ, MedQuAD, and TREC. Three experimental\nsetups are explored: fine-tuning on combined short and long answers, short\nanswers only, and long answers only. While our models demonstrate strong domain\nunderstanding, achieving concept-level accuracy scores of up to 0.8, their\nExact Match (EM) scores remain significantly lower, particularly in the test\nphase. We introduce a two-stage inference pipeline for precise short-answer\nextraction to mitigate verbosity and improve alignment with evaluation metrics.\nDespite partial improvements, challenges persist in generating strictly\nformatted outputs. Our findings highlight the gap between semantic\nunderstanding and exact answer evaluation in biomedical LLM applications,\nmotivating further research in output control and post-processing strategies.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00806v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00806v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.319,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes supervised fine-tuning of LLMs, such as LLaMA 3 8B, for multi-hop biomedical question answering, focusing on datasets and inference pipelines. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity, as required for this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00808",
      "title": "Adaptive Contrast Adjustment Module: A Clinically-Inspired Plug-and-Play\n  Approach for Enhanced Fetal Plane Classification",
      "authors": [
        "Yang Chen",
        "Sanglin Zhao",
        "Baoyu Chen",
        "Mans Gustaf"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Fetal ultrasound standard plane classification is essential for reliable\nprenatal diagnosis but faces inherent challenges, including low tissue\ncontrast, boundary ambiguity, and operator-dependent image quality variations.\nTo overcome these limitations, we propose a plug-and-play adaptive contrast\nadjustment module (ACAM), whose core design is inspired by the clinical\npractice of doctors adjusting image contrast to obtain clearer and more\ndiscriminative structural information. The module employs a shallow\ntexture-sensitive network to predict clinically plausible contrast parameters,\ntransforms input images into multiple contrast-enhanced views through\ndifferentiable mapping, and fuses them within downstream classifiers. Validated\non a multi-center dataset of 12,400 images across six anatomical categories,\nthe module consistently improves performance across diverse models, with\naccuracy of lightweight models increasing by 2.02 percent, accuracy of\ntraditional models increasing by 1.29 percent, and accuracy of state-of-the-art\nmodels increasing by 1.15 percent. The innovation of the module lies in its\ncontent-aware adaptation capability, replacing random preprocessing with\nphysics-informed transformations that align with sonographer workflows while\nimproving robustness to imaging heterogeneity through multi-view fusion. This\napproach effectively bridges low-level image features with high-level\nsemantics, establishing a new paradigm for medical image analysis under\nreal-world image quality variations.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00808v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00808v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.338,
      "distributed_training_score": 0.304,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00813",
      "title": "AImoclips: A Benchmark for Evaluating Emotion Conveyance in\n  Text-to-Music Generation",
      "authors": [
        "Gyehun Go",
        "Satbyul Han",
        "Ahyeon Choi",
        "Eunjin Choi",
        "Juhan Nam",
        "Jeong Mi Park"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Recent advances in text-to-music (TTM) generation have enabled controllable\nand expressive music creation using natural language prompts. However, the\nemotional fidelity of TTM systems remains largely underexplored compared to\nhuman preference or text alignment. In this study, we introduce AImoclips, a\nbenchmark for evaluating how well TTM systems convey intended emotions to human\nlisteners, covering both open-source and commercial models. We selected 12\nemotion intents spanning four quadrants of the valence-arousal space, and used\nsix state-of-the-art TTM systems to generate over 1,000 music clips. A total of\n111 participants rated the perceived valence and arousal of each clip on a\n9-point Likert scale. Our results show that commercial systems tend to produce\nmusic perceived as more pleasant than intended, while open-source systems tend\nto perform the opposite. Emotions are more accurately conveyed under\nhigh-arousal conditions across all models. Additionally, all systems exhibit a\nbias toward emotional neutrality, highlighting a key limitation in affective\ncontrollability. This benchmark offers valuable insights into model-specific\nemotion rendering characteristics and supports future development of\nemotionally aligned TTM systems.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00813v2",
      "pdf_url": "http://arxiv.org/pdf/2509.00813v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.29,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.272,
      "datasets_score": 0.404,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on creating a benchmark for evaluating emotion conveyance in text-to-music systems using human ratings, but it does not involve training a reward model or using reinforcement learning to fine-tune models based on human feedback. There is no mention of RLHF processes, such as aligning models with human preferences through ranked data and RL algorithms.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction and analysis of the AImoclips benchmark dataset, which includes AI-generated music clips annotated with human ratings for valence and arousal. This directly aligns with research on creating, benchmarking, and evaluating datasets for AI applications, as it involves dataset curation, human annotation, and performance analysis.",
      "llm_score_status": "completed",
      "summary": "This paper introduces AImoclips, a benchmark designed to evaluate how effectively text-to-music (TTM) generation systems convey intended emotions, using 12 emotion intents across the valence-arousal space and generating over 1,000 music clips with six state-of-the-art models. Through ratings from 111 participants on perceived valence and arousal, the study reveals that commercial systems produce music perceived as more pleasant than intended, open-source systems do the opposite, emotions are better conveyed in high-arousal conditions, and all systems exhibit a bias toward emotional neutrality, highlighting limitations and guiding future improvements in affective TTM systems.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new benchmark for evaluating emotional conveyance in TTM systems, which cleverly combines existing emotion assessment techniques with music generation to address an underexplored aspect. While it doesn't introduce a entirely new problem, it advances the field by providing a practical tool for future research.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI music generation and affective computing due to its open-source dataset and insights into emotional accuracy. However, its influence may remain confined to specific areas rather than broadly affecting general AI or commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a valuable and practical contribution by establishing a benchmark for emotion evaluation in TTM systems, making it essential for researchers focused on AI-generated music to understand its findings and limitations. It represents a strong advancement in a niche area without being groundbreaking across the entire field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/fbdfe05ec5de0ed3f658b6074cb469c0d5b929cb",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 2,
      "average_h_index": 0.6666666666666666,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Gyehun Go",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376506798"
        },
        {
          "name": "Satbyul Han",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378727359"
        },
        {
          "name": "Ahyeon Choi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378620733"
        },
        {
          "name": "Eunjin Choi",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2146892830"
        },
        {
          "name": "Juhan Nam",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2278114494"
        },
        {
          "name": "Jeong Mi Park",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2324666376"
        }
      ]
    },
    {
      "id": "2509.00826",
      "title": "Sequential Difference Maximization: Generating Adversarial Examples via\n  Multi-Stage Optimization",
      "authors": [
        "Xinlei Liu",
        "Tao Hu",
        "Peng Yi",
        "Weitao Han",
        "Jichao Xie",
        "Baolin Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Efficient adversarial attack methods are critical for assessing the\nrobustness of computer vision models. In this paper, we reconstruct the\noptimization objective for generating adversarial examples as \"maximizing the\ndifference between the non-true labels' probability upper bound and the true\nlabel's probability,\" and propose a gradient-based attack method termed\nSequential Difference Maximization (SDM). SDM establishes a three-layer\noptimization framework of \"cycle-stage-step.\" The processes between cycles and\nbetween iterative steps are respectively identical, while optimization stages\ndiffer in terms of loss functions: in the initial stage, the negative\nprobability of the true label is used as the loss function to compress the\nsolution space; in subsequent stages, we introduce the Directional Probability\nDifference Ratio (DPDR) loss function to gradually increase the non-true\nlabels' probability upper bound by compressing the irrelevant labels'\nprobabilities. Experiments demonstrate that compared with previous SOTA\nmethods, SDM not only exhibits stronger attack performance but also achieves\nhigher attack cost-effectiveness. Additionally, SDM can be combined with\nadversarial training methods to enhance their defensive effects. The code is\navailable at https://github.com/X-L-Liu/SDM.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00826v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00826v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.354,
      "datasets_score": 0.272,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00827",
      "title": "Surface Defect Detection with Gabor Filter Using Reconstruction-Based\n  Blurring U-Net-ViT",
      "authors": [
        "Jongwook Si",
        "Sungyoung Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper proposes a novel approach to enhance the accuracy and reliability\nof texture-based surface defect detection using Gabor filters and a blurring\nU-Net-ViT model. By combining the local feature training of U-Net with the\nglobal processing of the Vision Transformer(ViT), the model effectively detects\ndefects across various textures. A Gaussian filter-based loss function removes\nbackground noise and highlights defect patterns, while Salt-and-Pepper(SP)\nmasking in the training process reinforces texture-defect boundaries, ensuring\nrobust performance in noisy environments. Gabor filters are applied in\npost-processing to emphasize defect orientation and frequency characteristics.\nParameter optimization, including filter size, sigma, wavelength, gamma, and\norientation, maximizes performance across datasets like MVTec-AD, Surface Crack\nDetection, and Marble Surface Anomaly Dataset, achieving an average Area Under\nthe Curve(AUC) of 0.939. The ablation studies validate that the optimal filter\nsize and noise probability significantly enhance defect detection performance.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00827v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00827v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.243,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.267,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00831",
      "title": "UPGS: Unified Pose-aware Gaussian Splatting for Dynamic Scene Deblurring",
      "authors": [
        "Zhijing Wu",
        "Longguang Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Reconstructing dynamic 3D scenes from monocular video has broad applications\nin AR/VR, robotics, and autonomous navigation, but often fails due to severe\nmotion blur caused by camera and object motion. Existing methods commonly\nfollow a two-step pipeline, where camera poses are first estimated and then 3D\nGaussians are optimized. Since blurring artifacts usually undermine pose\nestimation, pose errors could be accumulated to produce inferior reconstruction\nresults. To address this issue, we introduce a unified optimization framework\nby incorporating camera poses as learnable parameters complementary to 3DGS\nattributes for end-to-end optimization. Specifically, we recast camera and\nobject motion as per-primitive SE(3) affine transformations on 3D Gaussians and\nformulate a unified optimization objective. For stable optimization, we\nintroduce a three-stage training schedule that optimizes camera poses and\nGaussians alternatively. Particularly, 3D Gaussians are first trained with\nposes being fixed, and then poses are optimized with 3D Gaussians being\nuntouched. Finally, all learnable parameters are optimized together. Extensive\nexperiments on the Stereo Blur dataset and challenging real-world sequences\ndemonstrate that our method achieves significant gains in reconstruction\nquality and pose estimation accuracy over prior dynamic deblurring methods.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00831v2",
      "pdf_url": "http://arxiv.org/pdf/2509.00831v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.274,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.333,
      "datasets_score": 0.235,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00833",
      "title": "SegDINO: An Efficient Design for Medical and Natural Image Segmentation\n  with DINO-V3",
      "authors": [
        "Sicheng Yang",
        "Hongqiu Wang",
        "Zhaohu Xing",
        "Sixiang Chen",
        "Lei Zhu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The DINO family of self-supervised vision models has shown remarkable\ntransferability, yet effectively adapting their representations for\nsegmentation remains challenging. Existing approaches often rely on heavy\ndecoders with multi-scale fusion or complex upsampling, which introduce\nsubstantial parameter overhead and computational cost. In this work, we propose\nSegDINO, an efficient segmentation framework that couples a frozen DINOv3\nbackbone with a lightweight decoder. SegDINO extracts multi-level features from\nthe pretrained encoder, aligns them to a common resolution and channel width,\nand utilizes a lightweight MLP head to directly predict segmentation masks.\nThis design minimizes trainable parameters while preserving the\nrepresentational power of foundation features. Extensive experiments across six\nbenchmarks, including three medical datasets (TN3K, Kvasir-SEG, ISIC) and three\nnatural image datasets (MSD, VMD-D, ViSha), demonstrate that SegDINO\nconsistently achieves state-of-the-art performance compared to existing\nmethods. Code is available at https://github.com/script-Yang/SegDINO.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00833v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00833v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.274,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.38,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00834",
      "title": "Neuro-Symbolic Predictive Process Monitoring",
      "authors": [
        "Axel Mezini",
        "Elena Umili",
        "Ivan Donadello",
        "Fabrizio Maria Maggi",
        "Matteo Mancanelli",
        "Fabio Patrizi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.FL (Formal Languages and Automata Theory)",
        "cs.LG (Machine Learning)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "This paper addresses the problem of suffix prediction in Business Process\nManagement (BPM) by proposing a Neuro-Symbolic Predictive Process Monitoring\n(PPM) approach that integrates data-driven learning with temporal logic-based\nprior knowledge. While recent approaches leverage deep learning models for\nsuffix prediction, they often fail to satisfy even basic logical constraints\ndue to the absence of explicit integration of domain knowledge during training.\nWe propose a novel method to incorporate Linear Temporal Logic over finite\ntraces (LTLf) into the training process of autoregressive sequence predictors.\nOur approach introduces a differentiable logical loss function, defined using a\nsoft approximation of LTLf semantics and the Gumbel-Softmax trick, which can be\ncombined with standard predictive losses. This ensures the model learns to\ngenerate suffixes that are both accurate and logically consistent. Experimental\nevaluation on three real-world datasets shows that our method improves suffix\nprediction accuracy and compliance with temporal constraints. We also introduce\ntwo variants of the logic loss (local and global) and demonstrate their\neffectiveness under noisy and realistic settings. While developed in the\ncontext of BPM, our framework is applicable to any symbolic sequence generation\ntask and contributes toward advancing Neuro-Symbolic AI.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00834v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00834v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.323,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a Neuro-Symbolic approach for Predictive Process Monitoring using Linear Temporal Logic (LTLf) and a differentiable logical loss with the Gumbel-Softmax trick for training autoregressive models. It does not involve diffusion models, iterative refinement processes, or treating a Chain-of-Thought as a single entity for logical tasks. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00835",
      "title": "Satellite Image Utilization for Dehazing with Swin Transformer-Hybrid\n  U-Net and Watershed loss",
      "authors": [
        "Jongwook Si",
        "Sungyoung Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Satellite imagery plays a crucial role in various fields; however,\natmospheric interference and haze significantly degrade image clarity and\nreduce the accuracy of information extraction. To address these challenges,\nthis paper proposes a hybrid dehazing framework that integrates Swin\nTransformer and U-Net to balance global context learning and local detail\nrestoration, called SUFERNOBWA. The proposed network employs SwinRRDB, a Swin\nTransformer-based Residual-in-Residual Dense Block, in both the encoder and\ndecoder to effectively extract features. This module enables the joint learning\nof global contextual information and fine spatial structures, which is crucial\nfor structural preservation in satellite image. Furthermore, we introduce a\ncomposite loss function that combines L2 loss, guided loss, and a novel\nwatershed loss, which enhances structural boundary preservation and ensures\npixel-level accuracy. This architecture enables robust dehazing under diverse\natmospheric conditions while maintaining structural consistency across restored\nimages. Experimental results demonstrate that the proposed method outperforms\nstate-of-the-art models on both the RICE and SateHaze1K datasets. Specifically,\non the RICE dataset, the proposed approach achieved a PSNR of 33.24 dB and an\nSSIM of 0.967, which is a significant improvement over existing method. This\nstudy provides an effective solution for mitigating atmospheric interference in\nsatellite imagery and highlights its potential applicability across diverse\nremote sensing applications.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00835v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00835v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.272,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.299,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00839",
      "title": "Adaptive Vehicle Speed Classification via BMCNN with Reinforcement\n  Learning-Enhanced Acoustic Processing",
      "authors": [
        "Yuli Zhang",
        "Pengfei Fan",
        "Ruiyuan Jiang",
        "Hankang Gu",
        "Dongyao Jia",
        "Xinheng Wang"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Traffic congestion remains a pressing urban challenge, requiring intelligent\ntransportation systems for real-time management. We present a hybrid framework\nthat combines deep learning and reinforcement learning for acoustic vehicle\nspeed classification. A dual-branch BMCNN processes MFCC and wavelet features\nto capture complementary frequency patterns. An attention-enhanced DQN\nadaptively selects the minimal number of audio frames and triggers early\ndecisions once confidence thresholds are reached. Evaluations on IDMT-Traffic\nand our SZUR-Acoustic (Suzhou) datasets show 95.99% and 92.3% accuracy, with up\nto 1.63x faster average processing via early termination. Compared with A3C,\nDDDQN, SA2C, PPO, and TD3, the method provides a superior accuracy-efficiency\ntrade-off and is suitable for real-time ITS deployment in heterogeneous urban\nenvironments.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00839v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00839v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.361,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00843",
      "title": "Look Beyond: Two-Stage Scene View Generation via Panorama and Video\n  Diffusion",
      "authors": [
        "Xueyang Kang",
        "Zhengkang Xiang",
        "Zezheng Zhang",
        "Kourosh Khoshelham"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Novel view synthesis (NVS) from a single image is highly ill-posed due to\nlarge unobserved regions, especially for views that deviate significantly from\nthe input. While existing methods focus on consistency between the source and\ngenerated views, they often fail to maintain coherence and correct view\nalignment across long-range or looped trajectories. We propose a model that\naddresses this by decomposing single-view NVS into a 360-degree scene\nextrapolation followed by novel view interpolation. This design ensures\nlong-term view and scene consistency by conditioning on keyframes extracted and\nwarped from a generated panoramic representation. In the first stage, a\npanorama diffusion model learns the scene prior from the input perspective\nimage. Perspective keyframes are then sampled and warped from the panorama and\nused as anchor frames in a pre-trained video diffusion model, which generates\nnovel views through a proposed spatial noise diffusion process. Compared to\nprior work, our method produces globally consistent novel views -- even in loop\nclosure scenarios -- while enabling flexible camera control. Experiments on\ndiverse scene datasets demonstrate that our approach outperforms existing\nmethods in generating coherent views along user-defined trajectories. Our\nimplementation is available at https://github.com/YiGuYT/LookBeyond.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00843v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00843v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.284,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.526,
      "distributed_training_score": 0.309,
      "datasets_score": 0.265,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for generating panoramic scenes and novel views in computer vision, specifically for novel view synthesis from a single image. While it employs iterative refinement in diffusion processes for visual generation, it does not involve adapting diffusion for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks. The core contributions are in scene extrapolation and view interpolation, not in reasoning applications.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00846",
      "title": "Causal SHAP: Feature Attribution with Dependency Awareness through\n  Causal Discovery",
      "authors": [
        "Woon Yee Ng",
        "Li Rong Wang",
        "Siyuan Liu",
        "Xiuyi Fan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ME (Methodology)"
      ],
      "abstract": "Explaining machine learning (ML) predictions has become crucial as ML models\nare increasingly deployed in high-stakes domains such as healthcare. While\nSHapley Additive exPlanations (SHAP) is widely used for model interpretability,\nit fails to differentiate between causality and correlation, often\nmisattributing feature importance when features are highly correlated. We\npropose Causal SHAP, a novel framework that integrates causal relationships\ninto feature attribution while preserving many desirable properties of SHAP. By\ncombining the Peter-Clark (PC) algorithm for causal discovery and the\nIntervention Calculus when the DAG is Absent (IDA) algorithm for causal\nstrength quantification, our approach addresses the weakness of SHAP.\nSpecifically, Causal SHAP reduces attribution scores for features that are\nmerely correlated with the target, as validated through experiments on both\nsynthetic and real-world datasets. This study contributes to the field of\nExplainable AI (XAI) by providing a practical framework for causal-aware model\nexplanations. Our approach is particularly valuable in domains such as\nhealthcare, where understanding true causal relationships is critical for\ninformed decision-making.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00846v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00846v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.295,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for causal feature attribution in machine learning using SHAP integrated with causal discovery algorithms (PC and IDA), focusing on Explainable AI. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as defined by the topic. Therefore, there is no overlap or relation to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00852",
      "title": "Why it is worth making an effort with GenAI",
      "authors": [
        "Yvonne Rogers"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Students routinely use ChatGPT and the like now to help them with their\nhomework, such as writing an essay. It takes less effort to complete and is\neasier to do than by hand. It can even produce as good if not better output\nthan the student's own work. However, there is a growing concern that\nover-reliance on using GenAI in this way will stifle the development of\nlearning writing and critical thinking skills. How might this trend be\nreversed? What if students were required to make more effort when using GenAI\nto do their homework? It might be more challenging, but the additional effort\ninvolved could result in them learning more and having a greater sense of\nachievement. This tension can be viewed as a form of effort paradox; where\neffort is both viewed as something to be avoided but at the same time is\nvalued. Is it possible to let students learn sometimes with less and other\ntimes more effort? Students are already adept at the former but what about the\nlatter? Could we design new kinds of AI tools that deliberately require more\neffort to use to deepen the learning experience? In this paper, I begin to\noutline what form these might take, for example, asking students to use a\ncombination of GenAI tools with traditional learning approaches (e.g.\nnote-taking while reading). I also discuss how else to design tools to think\nwith that augments human cognition; where students learn more the skills of\nmetacognition and reflection.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00852v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00852v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.285,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on the educational use of GenAI, emphasizing effort in learning and designing tools for metacognition, but it does not discuss diffusion models, iterative refinement for reasoning, or any multi-step logical reasoning processes. There is no connection to the specified topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00859",
      "title": "Quantization Meets OOD: Generalizable Quantization-aware Training from a\n  Flatness Perspective",
      "authors": [
        "Jiacheng Jiang",
        "Yuan Meng",
        "Chen Tang",
        "Han Yu",
        "Qun Li",
        "Zhi Wang",
        "Wenwu Zhu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Current quantization-aware training (QAT) methods primarily focus on\nenhancing the performance of quantized models on in-distribution (I.D) data,\nwhile overlooking the potential performance degradation on out-of-distribution\n(OOD) data. In this paper, we first substantiate this problem through rigorous\nexperiment, showing that QAT can lead to a significant OOD generalization\nperformance degradation. Further, we find the contradiction between the\nperspective that flatness of loss landscape gives rise to superior OOD\ngeneralization and the phenomenon that QAT lead to a sharp loss landscape, can\ncause the above problem. Therefore, we propose a flatness-oriented QAT method,\nFQAT, to achieve generalizable QAT. Specifically, i) FQAT introduces a\nlayer-wise freezing mechanism to mitigate the gradient conflict issue between\ndual optimization objectives (i.e., vanilla QAT and flatness). ii) FQAT\nproposes an disorder-guided adaptive freezing algorithm to dynamically\ndetermines which layers to freeze at each training step, effectively addressing\nthe challenges caused by interference between layers. A gradient disorder\nmetric is designed to help the algorithm identify unstable layers during\ntraining. Extensive experiments on influential OOD benchmark demonstrate the\nsuperiority of our method over state-of-the-art baselines under both I.D and\nOOD image classification tasks.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00859v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00859v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.415,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is on quantization-aware training (QAT) for neural networks, specifically addressing out-of-distribution (OOD) generalization by incorporating flatness optimization and adaptive freezing mechanisms. It does not involve distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors, as its focus is solely on model quantization and optimization techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00862",
      "title": "Speech Command Recognition Using LogNNet Reservoir Computing for\n  Embedded Systems",
      "authors": [
        "Yuriy Izotov",
        "Andrei Velichko"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "This paper presents a low-resource speech-command recognizer combining\nenergy-based voice activity detection (VAD), an optimized Mel-Frequency\nCepstral Coefficients (MFCC) pipeline, and the LogNNet reservoir-computing\nclassifier. Using four commands from the Speech Commands da-taset downsampled\nto 8 kHz, we evaluate four MFCC aggregation schemes and find that adaptive\nbinning (64-dimensional feature vector) offers the best accuracy-to-compactness\ntrade-off. The LogNNet classifier with architecture 64:33:9:4 reaches 92.04%\naccuracy under speaker-independent evaluation, while requiring significantly\nfewer parameters than conventional deep learn-ing models. Hardware\nimplementation on Arduino Nano 33 IoT (ARM Cor-tex-M0+, 48 MHz, 32 KB RAM)\nvalidates the practical feasibility, achieving ~90% real-time recognition\naccuracy while consuming only 18 KB RAM (55% utilization). The complete\npipeline (VAD -> MFCC -> LogNNet) thus enables reliable on-device\nspeech-command recognition under strict memory and compute limits, making it\nsuitable for battery-powered IoT nodes, wire-less sensor networks, and\nhands-free control interfaces.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00862v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00862v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.276,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.285,
      "distributed_training_score": 0.306,
      "datasets_score": 0.267,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00866",
      "title": "Can General-Purpose Omnimodels Compete with Specialists? A Case Study in\n  Medical Image Segmentation",
      "authors": [
        "Yizhe Zhang",
        "Qiang Chen",
        "Tao Zhou"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The emergence of powerful, general-purpose omnimodels capable of processing\ndiverse data modalities has raised a critical question: can these\n``jack-of-all-trades'' systems perform on par with highly specialized models in\nknowledge-intensive domains? This work investigates this question within the\nhigh-stakes field of medical image segmentation. We conduct a comparative study\nanalyzing the zero-shot performance of a state-of-the-art omnimodel (Gemini 2.5\nPro, the ``Nano Banana'' model) against domain-specific deep learning models on\nthree distinct tasks: polyp (endoscopy), retinal vessel (fundus), and breast\ntumor segmentation (ultrasound). Our study focuses on performance at the\nextremes by curating subsets of the ``easiest'' and ``hardest'' cases based on\nthe specialist models' accuracy. Our findings reveal a nuanced and\ntask-dependent landscape. For polyp and breast tumor segmentation, specialist\nmodels excel on easy samples, but the omnimodel demonstrates greater robustness\non hard samples where specialists fail catastrophically. Conversely, for the\nfine-grained task of retinal vessel segmentation, the specialist model\nmaintains superior performance across both easy and hard cases. Intriguingly,\nqualitative analysis suggests omnimodels may possess higher sensitivity,\nidentifying subtle anatomical features missed by human annotators. Our results\nindicate that while current omnimodels are not yet a universal replacement for\nspecialists, their unique strengths suggest a potential complementary role with\nspecialist models, particularly in enhancing robustness on challenging edge\ncases.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00866v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00866v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.377,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00872",
      "title": "Pose as Clinical Prior: Learning Dual Representations for Scoliosis\n  Screening",
      "authors": [
        "Zirui Zhou",
        "Zizhao Peng",
        "Dongyang Jin",
        "Chao Fan",
        "Fengwei An",
        "Shiqi Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent AI-based scoliosis screening methods primarily rely on large-scale\nsilhouette datasets, often neglecting clinically relevant postural\nasymmetries-key indicators in traditional screening. In contrast, pose data\nprovide an intuitive skeletal representation, enhancing clinical\ninterpretability across various medical applications. However, pose-based\nscoliosis screening remains underexplored due to two main challenges: (1) the\nscarcity of large-scale, annotated pose datasets; and (2) the discrete and\nnoise-sensitive nature of raw pose coordinates, which hinders the modeling of\nsubtle asymmetries. To address these limitations, we introduce\nScoliosis1K-Pose, a 2D human pose annotation set that extends the original\nScoliosis1K dataset, comprising 447,900 frames of 2D keypoints from 1,050\nadolescents. Building on this dataset, we introduce the Dual Representation\nFramework (DRF), which integrates a continuous skeleton map to preserve spatial\nstructure with a discrete Postural Asymmetry Vector (PAV) that encodes\nclinically relevant asymmetry descriptors. A novel PAV-Guided Attention (PGA)\nmodule further uses the PAV as clinical prior to direct feature extraction from\nthe skeleton map, focusing on clinically meaningful asymmetries. Extensive\nexperiments demonstrate that DRF achieves state-of-the-art performance.\nVisualizations further confirm that the model leverages clinical asymmetry cues\nto guide feature extraction and promote synergy between its dual\nrepresentations. The dataset and code are publicly available at\nhttps://zhouzi180.github.io/Scoliosis1K/.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00872v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00872v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.313,
      "distributed_training_score": 0.352,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00883",
      "title": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors",
      "authors": [
        "Denis Los",
        "Igor Petushkov"
      ],
      "categories": [
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00883v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00883v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.484,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on using AI-powered tools for fine-grained parallelization of latency-critical applications on SMT processors, which involves parallel computing techniques. However, it does not address distributed training of machine learning models, such as partitioning data or computation across multiple nodes for model training. The use of LLMs in the paper is for parallelization advice, not for accelerating ML training itself, making the connection indirect.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00884",
      "title": "An Explainable Gaussian Process Auto-encoder for Tabular Data",
      "authors": [
        "Wei Zhang",
        "Brian Barr",
        "John Paisley"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Explainable machine learning has attracted much interest in the community\nwhere the stakes are high. Counterfactual explanations methods have become an\nimportant tool in explaining a black-box model. The recent advances have\nleveraged the power of generative models such as an autoencoder. In this paper,\nwe propose a novel method using a Gaussian process to construct the\nauto-encoder architecture for generating counterfactual samples. The resulting\nmodel requires fewer learnable parameters and thus is less prone to\noverfitting. We also introduce a novel density estimator that allows for\nsearching for in-distribution samples. Furthermore, we introduce an algorithm\nfor selecting the optimal regularization rate on density estimator while\nsearching for counterfactuals. We experiment with our method in several\nlarge-scale tabular datasets and compare with other auto-encoder-based methods.\nThe results show that our method is capable of generating diversified and\nin-distribution counterfactual samples.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00884v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00884v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.322,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using Gaussian processes in auto-encoders for generating counterfactual explanations in tabular data, emphasizing explainable AI and model interpretability. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. There is no mention of treating a Chain-of-Thought as an entity or any diffusion-based components, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00891",
      "title": "ChatCLIDS: Simulating Persuasive AI Dialogues to Promote Closed-Loop\n  Insulin Adoption in Type 1 Diabetes Care",
      "authors": [
        "Zonghai Yao",
        "Talha Chafekar",
        "Junda Wang",
        "Shuo Han",
        "Feiyun Ouyang",
        "Junhui Qian",
        "Lingxi Li",
        "Hong Yu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Real-world adoption of closed-loop insulin delivery systems (CLIDS) in type 1\ndiabetes remains low, driven not by technical failure, but by diverse\nbehavioral, psychosocial, and social barriers. We introduce ChatCLIDS, the\nfirst benchmark to rigorously evaluate LLM-driven persuasive dialogue for\nhealth behavior change. Our framework features a library of expert-validated\nvirtual patients, each with clinically grounded, heterogeneous profiles and\nrealistic adoption barriers, and simulates multi-turn interactions with nurse\nagents equipped with a diverse set of evidence-based persuasive strategies.\nChatCLIDS uniquely supports longitudinal counseling and adversarial social\ninfluence scenarios, enabling robust, multi-dimensional evaluation. Our\nfindings reveal that while larger and more reflective LLMs adapt strategies\nover time, all models struggle to overcome resistance, especially under\nrealistic social pressure. These results highlight critical limitations of\ncurrent LLMs for behavior change, and offer a high-fidelity, scalable testbed\nfor advancing trustworthy persuasive AI in healthcare and beyond.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00891v2",
      "pdf_url": "http://arxiv.org/pdf/2509.00891v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.435,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.343,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces ChatCLIDS, a benchmark for evaluating LLM-driven persuasive dialogues in healthcare, focusing on simulation and assessment of AI interactions for behavior change. It does not involve training or fine-tuning AI models using reinforcement learning from human feedback (RLHF), such as creating a reward model based on human-ranked data. While the evaluation includes expert annotations, this is for assessment purposes and not for RLHF-based model alignment or training. Thus, the paper's contributions are unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00900",
      "title": "Towards Early Detection: AI-Based Five-Year Forecasting of Breast Cancer\n  Risk Using Digital Breast Tomosynthesis Imaging",
      "authors": [
        "Manon A. Dorster",
        "Felix J. Dorfner",
        "Mason C. Cleveland",
        "Melisa S. Guelen",
        "Jay Patel",
        "Dania Daye",
        "Jean-Philippe Thiran",
        "Albert E. Kim",
        "Christopher P. Bridge"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "As early detection of breast cancer strongly favors successful therapeutic\noutcomes, there is major commercial interest in optimizing breast cancer\nscreening. However, current risk prediction models achieve modest performance\nand do not incorporate digital breast tomosynthesis (DBT) imaging, which was\nFDA-approved for breast cancer screening in 2011. To address this unmet need,\nwe present a deep learning (DL)-based framework capable of forecasting an\nindividual patient's 5-year breast cancer risk directly from screening DBT.\nUsing an unparalleled dataset of 161,753 DBT examinations from 50,590 patients,\nwe trained a risk predictor based on features extracted using the Meta AI\nDINOv2 image encoder, combined with a cumulative hazard layer, to assess a\npatient's likelihood of developing breast cancer over five years. On a held-out\ntest set, our best-performing model achieved an AUROC of 0.80 on predictions\nwithin 5 years. These findings reveal the high potential of DBT-based DL\napproaches to complement traditional risk assessment tools, and serve as a\npromising basis for additional investigation to validate and enhance our work.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00900v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00900v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.341,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00905",
      "title": "Spotlighter: Revisiting Prompt Tuning from a Representative Mining View",
      "authors": [
        "Yutong Gao",
        "Maoyuan Shao",
        "Xinyang Huang",
        "Chuang Zhu",
        "Lijuan Sun",
        "Yu Weng",
        "Xuan Liu",
        "Guoshun Nan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "CLIP's success has demonstrated that prompt tuning can achieve robust\ncross-modal semantic alignment for tasks ranging from open-domain recognition\nto fine-grained classification. However, redundant or weakly relevant feature\ncomponents introduce noise and incur unnecessary computational costs. In this\nwork, we propose Spotlighter, a lightweight token-selection framework that\nsimultaneously enhances accuracy and efficiency in prompt tuning. Spotlighter\nevaluates each visual token's activation from both sample-wise and\nsemantic-wise perspectives and retains only the top-scoring tokens for\ndownstream prediction. A class-specific semantic memory bank of learned\nprototypes refines this selection, ensuring semantic representativeness and\ncompensating for discarded features. To further prioritize informative signals,\nwe introduce a two-level ranking mechanism that dynamically weights\ntoken--prototype interactions. Across 11 few-shot benchmarks, Spotlighter\noutperforms CLIP by up to 11.19\\% in harmonic mean accuracy and achieves up to\n0.8K additional FPS, with only 21 extra parameters. These results establish\nSpotlighter as an effective and scalable baseline for prompt tuning. Code for\nour method will be available at\nhttps://github.com/greatest-gourmet/Spotlighter.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00905v2",
      "pdf_url": "http://arxiv.org/pdf/2509.00905v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.427,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.388,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is on improving prompt tuning in vision-language models like CLIP through token selection and semantic memory banks to enhance accuracy and efficiency. It does not involve reinforcement learning, human feedback, reward models, or any mechanism for aligning AI with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00911",
      "title": "GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing\n  Redundant Sorting while Preserving Rasterization Efficiency",
      "authors": [
        "Joongho Jo",
        "Jongsun Park"
      ],
      "categories": [
        "cs.AR (Hardware Architecture)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D Gaussian Splatting (3D-GS) has emerged as a promising alternative to\nneural radiance fields (NeRF) as it offers high speed as well as high image\nquality in novel view synthesis. Despite these advancements, 3D-GS still\nstruggles to meet the frames per second (FPS) demands of real-time\napplications. In this paper, we introduce GS-TG, a tile-grouping-based\naccelerator that enhances 3D-GS rendering speed by reducing redundant sorting\noperations and preserving rasterization efficiency. GS-TG addresses a critical\ntrade-off issue in 3D-GS rendering: increasing the tile size effectively\nreduces redundant sorting operations, but it concurrently increases unnecessary\nrasterization computations. So, during sorting of the proposed approach, GS-TG\ngroups small tiles (for making large tiles) to share sorting operations across\ntiles within each group, significantly reducing redundant computations. During\nrasterization, a bitmask assigned to each Gaussian identifies relevant small\ntiles, to enable efficient sharing of sorting results. Consequently, GS-TG\nenables sorting to be performed as if a large tile size is used by grouping\ntiles during the sorting stage, while allowing rasterization to proceed with\nthe original small tiles by using bitmasks in the rasterization stage. GS-TG is\na lossless method requiring no retraining or fine-tuning and it can be\nseamlessly integrated with previous 3D-GS optimization techniques. Experimental\nresults show that GS-TG achieves an average speed-up of 1.54 times over\nstate-of-the-art 3D-GS accelerators.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00911v2",
      "pdf_url": "http://arxiv.org/pdf/2509.00911v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.251,
      "weak_supervision_score": 0.259,
      "diffusion_reasoning_score": 0.315,
      "distributed_training_score": 0.393,
      "datasets_score": 0.252,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00914",
      "title": "TinyMusician: On-Device Music Generation with Knowledge Distillation and\n  Mixed Precision Quantization",
      "authors": [
        "Hainan Wang",
        "Mehdi Hosseinzadeh",
        "Reza Rawassizadeh"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "The success of the generative model has gained unprecedented attention in the\nmusic generation area. Transformer-based architectures have set new benchmarks\nfor model performance. However, their practical adoption is hindered by some\ncritical challenges: the demand for massive computational resources and\ninference time, due to their large number of parameters. These obstacles make\nthem infeasible to deploy on edge devices, such as smartphones and wearables,\nwith limited computational resources. In this work, we present TinyMusician, a\nlightweight music generation model distilled from MusicGen (a State-of-the-art\nmusic generation model). TinyMusician integrates two innovations: (i)\nStage-mixed Bidirectional and Skewed KL-Divergence and (ii) Adaptive\nMixed-Precision Quantization. The experimental results demonstrate that\nTinyMusician retains 93% of the MusicGen-Small performance with 55% less model\nsize. TinyMusician is the first mobile-deployable music generation model that\neliminates cloud dependency while maintaining high audio fidelity and efficient\nresource usage",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00914v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00914v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.394,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00917",
      "title": "DarkVRAI: Capture-Condition Conditioning and Burst-Order Selective Scan\n  for Low-light RAW Video Denoising",
      "authors": [
        "Youngjin Oh",
        "Junhyeong Kwon",
        "Junyoung Park",
        "Nam Ik Cho"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Low-light RAW video denoising is a fundamentally challenging task due to\nsevere signal degradation caused by high sensor gain and short exposure times,\nwhich are inherently limited by video frame rate requirements. To address this,\nwe propose DarkVRAI, a novel framework that achieved first place in the AIM\n2025 Low-light RAW Video Denoising Challenge. Our method introduces two primary\ncontributions: (1) a successful application of a conditioning scheme for image\ndenoising, which explicitly leverages capture metadata, to video denoising to\nguide the alignment and denoising processes, and (2) a Burst-Order Selective\nScan (BOSS) mechanism that effectively models long-range temporal dependencies\nwithin the noisy video sequence. By synergistically combining these components,\nDarkVRAI demonstrates state-of-the-art performance on a rigorous and realistic\nbenchmark dataset, setting a new standard for low-light video denoising.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00917v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00917v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.323,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00923",
      "title": "Robust Deep Monte Carlo Counterfactual Regret Minimization: Addressing\n  Theoretical Risks in Neural Fictitious Self-Play",
      "authors": [
        "Zakaria El Jaafari"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.GT (Computer Science and Game Theory)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Monte Carlo Counterfactual Regret Minimization (MCCFR) has emerged as a\ncornerstone algorithm for solving extensive-form games, but its integration\nwith deep neural networks introduces scale-dependent challenges that manifest\ndifferently across game complexities. This paper presents a comprehensive\nanalysis of how neural MCCFR component effectiveness varies with game scale and\nproposes an adaptive framework for selective component deployment. We identify\nthat theoretical risks such as nonstationary target distribution shifts, action\nsupport collapse, variance explosion, and warm-starting bias have\nscale-dependent manifestation patterns, requiring different mitigation\nstrategies for small versus large games. Our proposed Robust Deep MCCFR\nframework incorporates target networks with delayed updates, uniform\nexploration mixing, variance-aware training objectives, and comprehensive\ndiagnostic monitoring. Through systematic ablation studies on Kuhn and Leduc\nPoker, we demonstrate scale-dependent component effectiveness and identify\ncritical component interactions. The best configuration achieves final\nexploitability of 0.0628 on Kuhn Poker, representing a 60% improvement over the\nclassical framework (0.156). On the more complex Leduc Poker domain, selective\ncomponent usage achieves exploitability of 0.2386, a 23.5% improvement over the\nclassical framework (0.3703) and highlighting the importance of careful\ncomponent selection over comprehensive mitigation. Our contributions include:\n(1) a formal theoretical analysis of risks in neural MCCFR, (2) a principled\nmitigation framework with convergence guarantees, (3) comprehensive multi-scale\nexperimental validation revealing scale-dependent component interactions, and\n(4) practical guidelines for deployment in larger games.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00923v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00923v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.439,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.396,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper addresses improvements to Monte Carlo Counterfactual Regret Minimization using deep neural networks for game theory and extensive-form games, focusing on algorithmic stability and performance in scenarios like poker. It does not involve human feedback, such as ranking data or reward models based on human preferences, which are core to RLHF. Thus, the paper's contributions are unrelated to aligning AI models with human values.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00928",
      "title": "Superposition in Graph Neural Networks",
      "authors": [
        "Lukas Pertl",
        "Han Xuanyuan",
        "Pietro Liò"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Interpreting graph neural networks (GNNs) is difficult because message\npassing mixes signals and internal channels rarely align with human concepts.\nWe study superposition, the sharing of directions by multiple features,\ndirectly in the latent space of GNNs. Using controlled experiments with\nunambiguous graph concepts, we extract features as (i) class-conditional\ncentroids at the graph level and (ii) linear-probe directions at the node\nlevel, and then analyze their geometry with simple basis-invariant diagnostics.\nAcross GCN/GIN/GAT we find: increasing width produces a phase pattern in\noverlap; topology imprints overlap onto node-level features that pooling\npartially remixes into task-aligned graph axes; sharper pooling increases axis\nalignment and reduces channel sharing; and shallow models can settle into\nmetastable low-rank embeddings. These results connect representational geometry\nwith concrete design choices (width, pooling, and final-layer activations) and\nsuggest practical approaches for more interpretable GNNs.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00928v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00928v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.401,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines superposition and interpretability in Graph Neural Networks (GNNs), focusing on feature geometry and architectural choices like width and pooling. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper's main contribution is analyzing internal representations in GNNs, such as superposition and feature extraction, without any discussion of distributed training, parallel computing, multi-node systems, or strategies for accelerating training across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00930",
      "title": "SATQuest: A Verifier for Logical Reasoning Evaluation and Reinforcement\n  Fine-Tuning of LLMs",
      "authors": [
        "Yanxiao Zhao",
        "Yaqian Li",
        "Zihao Bo",
        "Rinyoichi Takezoe",
        "Haojia Hui",
        "Mo Guang",
        "Lei Ren",
        "Xiaolin Qin",
        "Kaiwen Long"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated remarkable\ngeneral reasoning capabilities. However, systematically evaluating and\nenhancing these reasoning capabilities is challenging due to the lack of\ncontrollable and scalable tools for fine-grained analysis. Existing benchmarks\nand datasets often lack the necessary variable control for multi-dimensional,\nsystematic analysis and training, or have narrow problem types and formats. To\naddress these limitations, we introduce SATQuest, a systematic verifier\ndesigned to evaluate and enhance logical reasoning in LLMs by generating\ndiverse, Satisfiability-based logical reasoning problems directly from\nConjunctive Normal Form (CNF) instances. SATQuest structures these problems\nalong three orthogonal dimensions: instance scale, problem type, and question\nformat, employing randomized, SAT-based problem generation and objective answer\nverification via PySAT. This design mitigates memorization issues, allows for\nnuanced insights into reasoning performance, and enables effective\nreinforcement fine-tuning. Our extensive evaluation of various LLMs using\nSATQuest identified significant limitations in their logical reasoning,\nparticularly in generalizing beyond familiar mathematical formats. Furthermore,\nwe show that reinforcement fine-tuning with SATQuest rewards substantially\nimproves targeted task performance and generalizes to more complex instances,\nwhile highlighting remaining challenges in cross-format adaptation. Through\nthese demonstrations, we showcase SATQuest's potential as a foundational tool\nand a valuable starting point for advancing LLM logical reasoning.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00930v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00930v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.438,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.522,
      "distributed_training_score": 0.374,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses Reinforcement Learning from Verifiable Rewards (RLVR) using automated rewards from SATQuest, an AI-based verifier, rather than human-ranked data to train a reward model. Since RLHF specifically requires human feedback, this approach does not qualify.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on SAT-based problem generation and verification for logical reasoning in LLMs, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for multi-step correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00934",
      "title": "MedCOD: Enhancing English-to-Spanish Medical Translation of Large\n  Language Models Using Enriched Chain-of-Dictionary Framework",
      "authors": [
        "Md Shahidul Salim",
        "Lian Fu",
        "Arav Adikesh Ramakrishnan",
        "Zonghai Yao",
        "Hong Yu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present MedCOD (Medical Chain-of-Dictionary), a hybrid framework designed\nto improve English-to-Spanish medical translation by integrating\ndomain-specific structured knowledge into large language models (LLMs). MedCOD\nintegrates domain-specific knowledge from both the Unified Medical Language\nSystem (UMLS) and the LLM-as-Knowledge-Base (LLM-KB) paradigm to enhance\nstructured prompting and fine-tuning. We constructed a parallel corpus of 2,999\nEnglish-Spanish MedlinePlus articles and a 100-sentence test set annotated with\nstructured medical contexts. Four open-source LLMs (Phi-4, Qwen2.5-14B,\nQwen2.5-7B, and LLaMA-3.1-8B) were evaluated using structured prompts that\nincorporated multilingual variants, medical synonyms, and UMLS-derived\ndefinitions, combined with LoRA-based fine-tuning. Experimental results\ndemonstrate that MedCOD significantly improves translation quality across all\nmodels. For example, Phi-4 with MedCOD and fine-tuning achieved BLEU 44.23,\nchrF++ 28.91, and COMET 0.863, surpassing strong baseline models like GPT-4o\nand GPT-4o-mini. Ablation studies confirm that both MedCOD prompting and model\nadaptation independently contribute to performance gains, with their\ncombination yielding the highest improvements. These findings highlight the\npotential of structured knowledge integration to enhance LLMs for medical\ntranslation tasks.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00934v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00934v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.33,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00935",
      "title": "SCOUT: Toward Sub-Quadratic Attention via Segment Compression for\n  Optimized Utility in Transformers",
      "authors": [
        "Aref Jafari",
        "Yuhe Fan",
        "Benyamin Jamialahmadi",
        "Parsa Farinneya",
        "Boxing Chen",
        "Marzieh S. Tahaei"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Transformers have demonstrated strong performance across a wide range of\nsequence modeling tasks, but their quadratic attention complexity limits\nscalability to long sequences. Linear models such as Mamba and sliding-window\nattention (SWA) address this by mixing tokens through recurrent or localized\noperations with fixed-size memory, achieving efficient inference. However,\nthese methods risk degrading performance on long sequences due to their\ninability to retain detailed information from distant tokens. We propose SCOUT\n(Segment Compression for Optimized Utility in Transformers), a hybrid\narchitecture that compresses tokens locally within fixed-size segments and\napplies attention only over these compressed representations. Each token\nembedding is first enriched via a linear local mixer, Mamba or SWA, that\nintegrates recent context. Then, instead of attending to all previous tokens,\neach token sparsely attends to a small number of compressed checkpoint tokens\nthat summarize the input history. This design retains much of the expressivity\nof full attention while substantially reducing the computational and memory\ncost. By attending to compressed history rather than all previous tokens, SCOUT\nincurs slightly higher memory than purely linear models, but its growth rate\nremains sub-quadratic and far more scalable than that of full Transformers. We\nanalyze SCOUT's computational and memory efficiency and evaluate it empirically\non long-context language modeling and reasoning tasks. SCOUT with both Mamba\nand SWA mixers outperforms strong long-sequence baselines under the same\ncomputational budget, matches full-attention Transformers on language modeling\nand common-sense reasoning tasks at 400M and 1.3B scales. Moreover, our SCOUT\nachieves higher end-to-end throughput than SOTA models, while delivering\ncomparable results on long sequence benchmarks.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00935v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00935v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.431,
      "distributed_training_score": 0.409,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces SCOUT, a hybrid architecture for efficient Transformers focusing on attention mechanisms and sequence compression. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. There is no mention of adapting diffusion for tasks like Chain-of-Thought correction.",
      "distributed_training_justification": "The paper discusses improving computational efficiency and throughput in Transformers through architectural changes like segment compression, but it does not address distributed training, parallel computing across multiple nodes, or algorithms for partitioning data/models. The focus is on single-model scalability, not multi-node systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00936",
      "title": "UrbanInsight: A Distributed Edge Computing Framework with LLM-Powered\n  Data Filtering for Smart City Digital Twins",
      "authors": [
        "Kishor Datta Gupta",
        "Md Manjurul Ahsan",
        "Mohd Ariful Haque",
        "Roy George",
        "Azmine Toushik Wasi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Cities today generate enormous streams of data from sensors, cameras, and\nconnected infrastructure. While this information offers unprecedented\nopportunities to improve urban life, most existing systems struggle with scale,\nlatency, and fragmented insights. This work introduces a framework that blends\nphysics-informed machine learning, multimodal data fusion, and knowledge graph\nrepresentation with adaptive, rule-based intelligence powered by large language\nmodels (LLMs). Physics-informed methods ground learning in real-world\nconstraints, ensuring predictions remain meaningful and consistent with\nphysical dynamics. Knowledge graphs act as the semantic backbone, integrating\nheterogeneous sensor data into a connected, queryable structure. At the edge,\nLLMs generate context-aware rules that adapt filtering and decision-making in\nreal time, enabling efficient operation even under constrained resources.\nTogether, these elements form a foundation for digital twin systems that go\nbeyond passive monitoring to provide actionable insights. By uniting\nphysics-based reasoning, semantic data fusion, and adaptive rule generation,\nthis approach opens new possibilities for creating responsive, trustworthy, and\nsustainable smart infrastructures.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00936v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00936v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.437,
      "datasets_score": 0.403,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "Not Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a framework using physics-informed ML, LLMs for rule generation, and knowledge graphs for smart city applications, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. There is no mention of treating a Chain-of-Thought as a single entity for correction.",
      "distributed_training_justification": "The paper describes a distributed edge computing framework for data processing and filtering, which involves parallel processing at edge nodes to reduce latency. However, it does not focus on distributed training of models, such as partitioning data or computation for accelerating ML training across nodes, making it only loosely related.",
      "datasets_justification": "The paper discusses handling urban data streams from sensors and infrastructure but does not involve creating, analyzing, benchmarking, or evaluating datasets for ML or AI applications. It focuses on framework design rather than dataset-related research.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00943",
      "title": "Protocol for Clustering 4DSTEM Data for Phase Differentiation in Glasses",
      "authors": [
        "Mridul Kumar",
        "Yevgeny Rakita"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Phase-change materials (PCMs) such as Ge-Sb-Te alloys are widely used in\nnon-volatile memory applications due to their rapid and reversible switching\nbetween amorphous and crystalline states. However, their functional properties\nare strongly governed by nanoscale variations in composition and structure,\nwhich are challenging to resolve using conventional techniques. Here, we apply\nunsupervised machine learning to 4-dimensional scanning transmission electron\nmicroscopy (4D-STEM) data to identify compositional and structural\nheterogeneity in Ge-Sb-Te. After preprocessing and dimensionality reduction\nwith principal component analysis (PCA), cluster validation was performed with\nt-SNE and UMAP, followed by k-means clustering optimized through silhouette\nscoring. Four distinct clusters were identified which were mapped back to the\ndiffraction data. Elemental intensity histograms revealed chemical signatures\nchange across clusters, oxygen and germanium enrichment in Cluster 1, tellurium\nin Cluster 2, antimony in Cluster 3, and germanium again in Cluster 4.\nFurthermore, averaged diffraction patterns from these clusters confirmed\nstructural variations. Together, these findings demonstrate that clustering\nanalysis can provide a powerful framework for correlating local chemical and\nstructural features in PCMs, offering deeper insights into their intrinsic\nheterogeneity.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00943v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00943v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.232,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.332,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00946",
      "title": "Ultrasound-based detection and malignancy prediction of breast lesions\n  eligible for biopsy: A multi-center clinical-scenario study using nomograms,\n  large language models, and radiologist evaluation",
      "authors": [
        "Ali Abbasian Ardakani",
        "Afshin Mohammadi",
        "Taha Yusuf Kuzan",
        "Beyza Nur Kuzan",
        "Hamid Khorshidi",
        "Ashkan Ghorbani",
        "Alisa Mohebbi",
        "Fariborz Faeghi",
        "Sepideh Hatamikia",
        "U Rajendra Acharya"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "To develop and externally validate integrated ultrasound nomograms combining\nBIRADS features and quantitative morphometric characteristics, and to compare\ntheir performance with expert radiologists and state of the art large language\nmodels in biopsy recommendation and malignancy prediction for breast lesions.\nIn this retrospective multicenter, multinational study, 1747 women with\npathologically confirmed breast lesions underwent ultrasound across three\ncenters in Iran and Turkey. A total of 10 BIRADS and 26 morphological features\nwere extracted from each lesion. A BIRADS, morphometric, and fused nomogram\nintegrating both feature sets was constructed via logistic regression. Three\nradiologists (one senior, two general) and two ChatGPT variants independently\ninterpreted deidentified breast lesion images. Diagnostic performance for\nbiopsy recommendation (BIRADS 4,5) and malignancy prediction was assessed in\ninternal and two external validation cohorts. In pooled analysis, the fused\nnomogram achieved the highest accuracy for biopsy recommendation (83.0%) and\nmalignancy prediction (83.8%), outperforming the morphometric nomogram, three\nradiologists and both ChatGPT models. Its AUCs were 0.901 and 0.853 for the two\ntasks, respectively. In addition, the performance of the BIRADS nomogram was\nsignificantly higher than the morphometric nomogram, three radiologists and\nboth ChatGPT models for biopsy recommendation and malignancy prediction.\nExternal validation confirmed the robust generalizability across different\nultrasound platforms and populations. An integrated BIRADS morphometric\nnomogram consistently outperforms standalone models, LLMs, and radiologists in\nguiding biopsy decisions and predicting malignancy. These interpretable,\nexternally validated tools have the potential to reduce unnecessary biopsies\nand enhance personalized decision making in breast imaging.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00946v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00946v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.287,
      "weak_supervision_score": 0.307,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.259,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00949",
      "title": "Structure and Destructure: Dual Forces in the Making of Knowledge\n  Engines",
      "authors": [
        "Yihong Chen"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The making of knowledge engines in natural language processing has been\nshaped by two seemingly distinct paradigms: one grounded in structure, the\nother driven by massively available unstructured data. The structured paradigm\nleverages predefined symbolic interactions, such as knowledge graphs, as priors\nand designs models to capture them. In contrast, the unstructured paradigm\ncenters on scaling transformer architectures with increasingly vast data and\nmodel sizes, as seen in modern large language models. Despite their divergence,\nthis thesis seeks to establish conceptual connections bridging these paradigms.\nTwo complementary forces, structure and destructure, emerge across both\nparadigms: structure organizes seen symbolic interactions, while destructure,\nthrough periodic embedding resets, improves model plasticity and generalization\nto unseen scenarios. These connections form a new recipe for developing general\nknowledge engines that can support transparent, controllable, and adaptable\nintelligent systems.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00949v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00949v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.475,
      "distributed_training_score": 0.38,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on bridging structured and unstructured paradigms in NLP, emphasizing concepts like knowledge graphs and embedding resets for generalization, but it does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. There is no component related to treating a Chain-of-Thought as an entity for holistic correction, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00955",
      "title": "ART: Adaptive Resampling-based Training for Imbalanced Classification",
      "authors": [
        "Arjun Basandrai",
        "Shourya Jain",
        "K. Ilanthenral"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Traditional resampling methods for handling class imbalance typically uses\nfixed distributions, undersampling the majority or oversampling the minority.\nThese static strategies ignore changes in class-wise learning difficulty, which\ncan limit the overall performance of the model.\n  This paper proposes an Adaptive Resampling-based Training (ART) method that\nperiodically updates the distribution of the training data based on the\nclass-wise performance of the model. Specifically, ART uses class-wise macro F1\nscores, computed at fixed intervals, to determine the degree of resampling to\nbe performed.\n  Unlike instance-level difficulty modeling, which is noisy and\noutlier-sensitive, ART adapts at the class level. This allows the model to\nincrementally shift its attention towards underperforming classes in a way that\nbetter aligns with the optimization objective.\n  Results on diverse benchmarks, including Pima Indians Diabetes and Yeast\ndataset demonstrate that ART consistently outperforms both resampling-based and\nalgorithm-level methods, including Synthetic Minority Oversampling Technique\n(SMOTE), NearMiss Undersampling, and Cost-sensitive Learning on binary as well\nas multi-class classification tasks with varying degrees of imbalance.\n  In most settings, these improvements are statistically significant. On\ntabular datasets, gains are significant under paired t-tests and Wilcoxon tests\n(p < 0.05), while results on text and image tasks remain favorable. Compared to\ntraining on the original imbalanced data, ART improves macro F1 by an average\nof 2.64 percentage points across all tested tabular datasets. Unlike existing\nmethods, whose performance varies by task, ART consistently delivers the\nstrongest macro F1, making it a reliable choice for imbalanced classification.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00955v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00955v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.287,
      "distributed_training_score": 0.375,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00958",
      "title": "A Hybrid Ai Framework For Strategic Patent Portfolio Pruning:\n  Integrating Learning To-Rank And Market Need Analysis For Technology Transfer\n  Optimization",
      "authors": [
        "Manish Verma",
        "Vivek Sharma",
        "Vishal Singh"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "This paper introduces a novel, multi stage hybrid intelligence framework for\npruning patent portfolios to identify high value assets for technology\ntransfer. Current patent valuation methods often rely on retrospective\nindicators or manual, time intensive analysis. Our framework automates and\ndeepens this process by combining a Learning to Rank (LTR) model, which\nevaluates patents against over 30 legal and commercial parameters, with a\nunique \"Need-Seed\" agent-based system. The \"Need Agent\" uses Natural Language\nProcessing (NLP) to mine unstructured market and industry data, identifying\nexplicit technological needs. Concurrently, the \"Seed Agent\" employs fine tuned\nLarge Language Models (LLMs) to analyze patent claims and map their\ntechnological capabilities. The system generates a \"Core Ontology Framework\"\nthat matches high potential patents (Seeds) to documented market demands\n(Needs), providing a strategic rationale for divestment decisions. We detail\nthe architecture, including a dynamic parameter weighting system and a crucial\nHuman in the-Loop (HITL) validation protocol, to ensure both adaptability and\nreal-world credibility.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00958v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00958v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.436,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.343,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a hybrid AI framework for patent portfolio pruning, using Learning to Rank (LTR), NLP, LLMs, and a Human-in-the-Loop (HITL) validation protocol for decision-making. While it mentions HITL for validation, this is for human oversight in outputs, not for training a reward model on human-ranked data and fine-tuning via reinforcement learning, as required for RLHF. Thus, the paper does not involve or relate to RLHF techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00961",
      "title": "Ultra Strong Machine Learning: Teaching Humans Active Learning\n  Strategies via Automated AI Explanations",
      "authors": [
        "Lun Ai",
        "Johannes Langer",
        "Ute Schmid",
        "Stephen Muggleton"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Ultra Strong Machine Learning (USML) refers to symbolic learning systems that\nnot only improve their own performance but can also teach their acquired\nknowledge to quantifiably improve human performance. In this work, we present\nLENS (Logic Programming Explanation via Neural Summarisation), a neuro-symbolic\nmethod that combines symbolic program synthesis with large language models\n(LLMs) to automate the explanation of machine-learned logic programs in natural\nlanguage. LENS addresses a key limitation of prior USML approaches by replacing\nhand-crafted explanation templates with scalable automated generation. Through\nsystematic evaluation using multiple LLM judges and human validation, we\ndemonstrate that LENS generates superior explanations compared to direct LLM\nprompting and hand-crafted templates. To investigate whether LENS can teach\ntransferable active learning strategies, we carried out a human learning\nexperiment across three related domains. Our results show no significant human\nperformance improvements, suggesting that comprehensive LLM responses may\noverwhelm users for simpler problems rather than providing learning support.\nOur work provides a solid foundation for building effective USML systems to\nsupport human learning. The source code is available on:\nhttps://github.com/lun-ai/LENS.git.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00961v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00961v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.47,
      "weak_supervision_score": 0.491,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.353,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using Inductive Logic Programming (ILP) and large language models (LLMs) to generate explanations for teaching humans, but it does not involve training an AI model with human feedback, a reward model, or reinforcement learning techniques. There is no alignment of AI with human preferences through RLHF.",
      "weak_supervision_justification": "The paper describes learning logic programs via ILP from data and domain knowledge, but it does not emphasize programmatically generating noisy or imprecise labels for training. There is no discussion of weak supervision as a primary approach, relying instead on standard data sources for ILP.",
      "diffusion_reasoning_justification": "The paper uses LLMs for generating and scoring explanations of logic programs, but it does not employ diffusion models or an iterative refinement process for multi-step logical reasoning. There is no component adapting diffusion techniques to handle a 'Chain-of-Thought' entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00963",
      "title": "Who Gets Left Behind? Auditing Disability Inclusivity in Large Language\n  Models",
      "authors": [
        "Deepika Dash",
        "Yeshil Bangera",
        "Mithil Bangera",
        "Gouthami Vadithya",
        "Srikant Panda"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used for accessibility\nguidance, yet many disability groups remain underserved by their advice. To\naddress this gap, we present taxonomy aligned benchmark1 of human validated,\ngeneral purpose accessibility questions, designed to systematically audit\ninclusivity across disabilities. Our benchmark evaluates models along three\ndimensions: Question-Level Coverage (breadth within answers), Disability-Level\nCoverage (balance across nine disability categories), and Depth (specificity of\nsupport). Applying this framework to 17 proprietary and open-weight models\nreveals persistent inclusivity gaps: Vision, Hearing, and Mobility are\nfrequently addressed, while Speech, Genetic/Developmental, Sensory-Cognitive,\nand Mental Health remain under served. Depth is similarly concentrated in a few\ncategories but sparse elsewhere. These findings reveal who gets left behind in\ncurrent LLM accessibility guidance and highlight actionable levers:\ntaxonomy-aware prompting/training and evaluations that jointly audit breadth,\nbalance, and depth.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00963v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00963v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.385,
      "datasets_score": 0.426,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on creating a benchmark for auditing disability inclusivity in LLMs and evaluating models on coverage and depth, but it does not involve training models using human feedback, reward models, or reinforcement learning techniques. There is no mention of RLHF in the methodology or contributions.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the development of a taxonomy-aligned benchmark with human-validated accessibility questions, which is used for evaluating and analyzing LLMs. This directly aligns with research on creating, benchmarking, and evaluating datasets for AI applications, as it introduces a new dataset for inclusivity assessments.",
      "llm_score_status": "completed",
      "summary": "This paper introduces a benchmark to systematically audit the inclusivity of Large Language Models (LLMs) in providing accessibility guidance across nine disability categories, evaluating models on dimensions such as question-level coverage, disability-level coverage, and depth of support. By testing 17 proprietary and open-weight models with human-validated questions, the authors reveal significant gaps in coverage—particularly for underrepresented categories like Speech, Genetic/Developmental, Sensory-Cognitive, and Mental Health—and propose actionable interventions, such as taxonomy-aware prompting, to enhance balanced and comprehensive guidance in LLMs.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by developing a new benchmark and framework for auditing disability inclusivity in LLMs, building on existing fairness research with a clever combination of metrics for breadth, balance, and depth. While it advances the state-of-the-art in AI accessibility, it primarily extends rather than introduces entirely new problems or techniques.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in AI ethics and accessibility subfields, as it highlights critical inclusivity gaps and suggests practical interventions for improving LLMs. However, its influence may be limited to specific areas like NLP fairness rather than broadly transforming commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers high-quality insights into an important and underexplored aspect of AI fairness, providing a valuable benchmark and recommendations that researchers in AI and accessibility should be aware of. It is a strong contribution but not essential for those outside the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b0ee300fdcda68069cf11c65bbe2e01f5f6bce8e",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 2,
      "average_h_index": 0.6,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Deepika Dash",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378710517"
        },
        {
          "name": "Yeshil Bangera",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2349538763"
        },
        {
          "name": "Mithil Bangera",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2349540665"
        },
        {
          "name": "Gouthami Vadithya",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378705313"
        },
        {
          "name": "Srikant Panda",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378573851"
        }
      ]
    },
    {
      "id": "2509.00969",
      "title": "Seeing More, Saying More: Lightweight Language Experts are Dynamic Video\n  Token Compressors",
      "authors": [
        "Xiangchen Wang",
        "Jinrui Zhang",
        "Teng Wang",
        "Haigang Zhang",
        "Feng Zheng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advancements in large video-language models have revolutionized video\nunderstanding tasks. However, their efficiency is significantly constrained by\nprocessing high volumes of visual tokens. Existing token compression strategies\napply a fixed compression ratio, ignoring the variability in semantic density\namong different video clips. Consequently, this lead to inadequate\nrepresentation of information-rich clips due to insufficient tokens and\nunnecessary computation on static or content-poor ones. To address this, we\npropose LangDC, a Language-aware Dynamic Token Compressor. LangDC leverages a\nlightweight language model to describe video clips, converting them into soft\ncaption tokens as visual representations. Trained with our proposed semantic\ndensity-aware supervision, LangDC aims to 1) cover key visual cues necessary\nfor downstream task reasoning and 2) dynamically adjust compression ratios\nbased on scene richness, reflected by descriptions length. Our design mimics\nhow humans dynamically express what they see: complex scenes (seeing more)\nelicit more detailed language to convey nuances (saying more), whereas simpler\nscenes are described with fewer words. Experimental results show that our\nmethod reduces FLOPs by 49% compared to VideoGPT+ while maintaining competitive\nperformance. Furthermore, qualitative results demonstrate our approach\nadaptively adjusts the token compression ratio based on video segment richness.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00969v2",
      "pdf_url": "http://arxiv.org/pdf/2509.00969v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.375,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a language-aware dynamic token compressor for video-language models, focusing on efficiency in visual token processing and semantic density adaptation. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00971",
      "title": "CoreThink: A Symbolic Reasoning Layer to reason over Long Horizon Tasks\n  with LLMs",
      "authors": [
        "Jay Vaghasiya",
        "Omkar Ghugarkar",
        "Vishvesh Bhat",
        "Vipul Dholaria",
        "Julian McAuley"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce CoreThink, a state-of-the-art Reasoning Layer built upon a novel\nreasoning method called General Symbolics. This approach diverges from\nreasoning paradigms such as test-time scaling, Supervised Fine-Tuning (SFT),\nand Reinforcement Learning with Verifiable Rewards (RLVR). CoreThink General\nSymbolic Reasoner (GSR) is specifically structured around three key use cases:\ntool-calling, code generation, and planning, demonstrating exemplary\nperformance across a total of seven benchmarks in their respective areas.\nNotably, we are achieving SOTA scores of 66.66% on Livecodebench v6, 89% on\nInstruction-Following Evals, and 24.4% on ARC-AGI-2. We also present an agentic\ncoding IDE, developed using the principles of General Symbolics, which achieves\na state-of-the-art accuracy of 62.3% on SWE-Bench Lite. We are able to achieve\nthese improvements without any fine-tuning or training costs. Our Reasoning\nLayer is designed to provide a pure performance uplift, ensuring that a model's\naccuracy on reasoning tasks is never negatively impacted. We argue that\nincumbent methods will eventually lead to diminishing returns in LLM\nperformance, necessitating the development of new reasoning techniques. This\ntechnical report details our approach at a high level and the availability of\nthe CoreThink models for reasoning-intensive use cases.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00971v2",
      "pdf_url": "http://arxiv.org/pdf/2509.00971v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.503,
      "distributed_training_score": 0.359,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces CoreThink, a symbolic reasoning layer based on General Symbolics, which focuses on logical structures, tool-calling, code generation, and planning without any reference to diffusion models or iterative refinement processes. It does not adapt diffusion techniques for multi-step logical reasoning or treat Chain-of-Thought as a holistically corrected entity, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00973",
      "title": "Clone What You Can't Steal: Black-Box LLM Replication via Logit Leakage\n  and Distillation",
      "authors": [
        "Kanchon Gharami",
        "Hansaka Aluvihare",
        "Shafika Showkat Moni",
        "Berker Peköz"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in mission-critical\nsystems, facilitating tasks such as satellite operations, command-and-control,\nmilitary decision support, and cyber defense. Many of these systems are\naccessed through application programming interfaces (APIs). When such APIs lack\nrobust access controls, they can expose full or top-k logits, creating a\nsignificant and often overlooked attack surface. Prior art has mainly focused\non reconstructing the output projection layer or distilling surface-level\nbehaviors. However, regenerating a black-box model under tight query\nconstraints remains underexplored. We address that gap by introducing a\nconstrained replication pipeline that transforms partial logit leakage into a\nfunctional deployable substitute model clone. Our two-stage approach (i)\nreconstructs the output projection matrix by collecting top-k logits from under\n10k black-box queries via singular value decomposition (SVD) over the logits,\nthen (ii) distills the remaining architecture into compact student models with\nvarying transformer depths, trained on an open source dataset. A 6-layer\nstudent recreates 97.6% of the 6-layer teacher model's hidden-state geometry,\nwith only a 7.31% perplexity increase, and a 7.58 Negative Log-Likelihood\n(NLL). A 4-layer variant achieves 17.1% faster inference and 18.1% parameter\nreduction with comparable performance. The entire attack completes in under 24\ngraphics processing unit (GPU) hours and avoids triggering API rate-limit\ndefenses. These results demonstrate how quickly a cost-limited adversary can\nclone an LLM, underscoring the urgent need for hardened inference APIs and\nsecure on-premise defense deployments.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00973v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00973v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.403,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on black-box LLM replication using logit leakage and distillation, with no mention of human feedback, reward models, or reinforcement learning techniques for model alignment. Its main contribution is on model cloning and security, not RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical reasoning, or multi-step processes for chain-of-thought correction. It centers on reconstructing model components via SVD and distillation, without any diffusion-based elements.",
      "distributed_training_justification": "The paper mentions GPU hours for the attack but does not discuss algorithms, systems, or techniques for distributed training, parallel computing, or partitioning data/models across nodes. Its focus is on model replication, not distributed training methods.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00975",
      "title": "Self-Exploring Language Models for Explainable Link Forecasting on\n  Temporal Graphs via Reinforcement Learning",
      "authors": [
        "Zifeng Ding",
        "Shenyang Huang",
        "Zeyu Cao",
        "Emma Kondrup",
        "Zachary Yang",
        "Xingyue Huang",
        "Yuan Sui",
        "Zhangdie Yuan",
        "Yuqicheng Zhu",
        "Xianglong Hu",
        "Yuan He",
        "Farimah Poursafaei",
        "Michael Bronstein",
        "Andreas Vlachos"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Forecasting future links is a central task in temporal graph (TG) reasoning,\nrequiring models to leverage historical interactions to predict upcoming ones.\nTraditional neural approaches, such as temporal graph neural networks, achieve\nstrong performance but lack explainability and cannot be applied to unseen\ngraphs without retraining. Recent studies have begun to explore using large\nlanguage models (LLMs) for graph reasoning, but most of them are constrained to\nstatic graphs or small synthetic TGs and lack the evaluation of the quality of\nreasoning traces generated by LLMs. In this work, we present Reasoning-Enhanced\nLearning for Temporal Graphs (ReaL-TG), a reinforcement learning framework that\nfine-tunes LLMs to perform explainable link forecasting on real-world TGs.\nReaL-TG uses outcome-based reward to encourage models to self-explore reasoning\nstrategies from graph structure and to produce explanations that directly\njustify their predictions. To enable evaluation on LLM-generated reasoning\ntraces, we propose a new evaluation protocol combining ranking metrics with an\nLLM-as-a-Judge system that assesses both the quality of reasoning and the\nimpact of hallucinations. Experiments with ReaL-TG-4B, obtained by fine-tuning\nQwen3-4B under our framework, show that it outperforms much larger frontier\nLLMs, including GPT-5 mini, on ranking metrics, while producing high-quality\nexplanations confirmed by both the LLM judge and human evaluation.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00975v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00975v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.477,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.542,
      "distributed_training_score": 0.381,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with an outcome-based reward for fine-tuning LLMs on temporal graph tasks, but it does not involve human feedback, such as training a reward model on human-ranked data. Instead, rewards are derived from task performance outcomes, which does not align with the definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for explainable link forecasting on temporal graphs and does not mention or utilize diffusion models or their iterative refinement processes for multi-step logical reasoning. There is no component involving holistic correction of reasoning paths as described in diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00987",
      "title": "Causal MAS: A Survey of Large Language Model Architectures for Discovery\n  and Effect Estimation",
      "authors": [
        "Adib Bazgir",
        "Amir Habibdoust",
        "Yuwen Zhang",
        "Xing Song"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious reasoning and generation tasks. However, their proficiency in complex\ncausal reasoning, discovery, and estimation remains an area of active\ndevelopment, often hindered by issues like hallucination, reliance on spurious\ncorrelations, and difficulties in handling nuanced, domain-specific, or\npersonalized causal relationships. Multi-agent systems, leveraging the\ncollaborative or specialized abilities of multiple LLM-based agents, are\nemerging as a powerful paradigm to address these limitations. This review paper\nexplores the burgeoning field of causal multi-agent LLMs. We examine how these\nsystems are designed to tackle different facets of causality, including causal\nreasoning and counterfactual analysis, causal discovery from data, and the\nestimation of causal effects. We delve into the diverse architectural patterns\nand interaction protocols employed, from pipeline-based processing and debate\nframeworks to simulation environments and iterative refinement loops.\nFurthermore, we discuss the evaluation methodologies, benchmarks, and diverse\napplication domains where causal multi-agent LLMs are making an impact,\nincluding scientific discovery, healthcare, fact-checking, and personalized\nsystems. Finally, we highlight the persistent challenges, open research\nquestions, and promising future directions in this synergistic field, aiming to\nprovide a comprehensive overview of its current state and potential trajectory.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00987v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00987v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.49,
      "distributed_training_score": 0.371,
      "datasets_score": 0.399,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper is a survey on multi-agent systems using LLMs for causal reasoning and does not mention reinforcement learning, human feedback, reward models, or any alignment processes involving human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses iterative refinement loops in multi-agent LLMs for causal tasks but does not reference diffusion models, multi-step logical reasoning via diffusion processes, or adaptations of diffusion for holistic Chain-of-Thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00989",
      "title": "Towards Integrating Multi-Spectral Imaging with Gaussian Splatting",
      "authors": [
        "Josef Grün",
        "Lukas Meyer",
        "Maximilian Weiherer",
        "Bernhard Egger",
        "Marc Stamminger",
        "Linus Franke"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present a study of how to integrate color (RGB) and multi-spectral imagery\n(red, green, red-edge, and near-infrared) into the 3D Gaussian Splatting (3DGS)\nframework, a state-of-the-art explicit radiance-field-based method for fast and\nhigh-fidelity 3D reconstruction from multi-view images. While 3DGS excels on\nRGB data, naive per-band optimization of additional spectra yields poor\nreconstructions due to inconsistently appearing geometry in the spectral\ndomain. This problem is prominent, even though the actual geometry is the same,\nregardless of spectral modality. To investigate this, we evaluate three\nstrategies: 1) Separate per-band reconstruction with no shared structure. 2)\nSplitting optimization, in which we first optimize RGB geometry, copy it, and\nthen fit each new band to the model by optimizing both geometry and band\nrepresentation. 3) Joint, in which the modalities are jointly optimized,\noptionally with an initial RGB-only phase. We showcase through quantitative\nmetrics and qualitative novel-view renderings on multi-spectral datasets the\neffectiveness of our dedicated optimized Joint strategy, increasing overall\nspectral reconstruction as well as enhancing RGB results through spectral\ncross-talk. We therefore suggest integrating multi-spectral data directly into\nthe spherical harmonics color components to compactly model each Gaussian's\nmulti-spectral reflectance. Moreover, our analysis reveals several key\ntrade-offs in when and how to introduce spectral bands during optimization,\noffering practical insights for robust multi-modal 3DGS reconstruction.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00989v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00989v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.247,
      "weak_supervision_score": 0.292,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.326,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00992",
      "title": "Online Decentralized Federated Multi-task Learning With Trustworthiness\n  in Cyber-Physical Systems",
      "authors": [
        "Olusola Odeyomi",
        "Sofiat Olaosebikan",
        "Ajibuwa Opeyemi",
        "Oluwadoyinsola Ige"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multi-task learning is an effective way to address the challenge of model\npersonalization caused by high data heterogeneity in federated learning.\nHowever, extending multi-task learning to the online decentralized federated\nlearning setting is yet to be explored. The online decentralized federated\nlearning setting considers many real-world applications of federated learning,\nsuch as autonomous systems, where clients communicate peer-to-peer and the data\ndistribution of each client is time-varying. A more serious problem in\nreal-world applications of federated learning is the presence of Byzantine\nclients. Byzantine-resilient approaches used in federated learning work only\nwhen the number of Byzantine clients is less than one-half the total number of\nclients. Yet, it is difficult to put a limit on the number of Byzantine clients\nwithin a system in reality. However, recent work in robotics shows that it is\npossible to exploit cyber-physical properties of a system to predict clients'\nbehavior and assign a trust probability to received signals. This can help to\nachieve resiliency in the presence of a dominating number of Byzantine clients.\nTherefore, in this paper, we develop an online decentralized federated\nmulti-task learning algorithm to provide model personalization and resiliency\nwhen the number of Byzantine clients dominates the number of honest clients.\nOur proposed algorithm leverages cyber-physical properties, such as the\nreceived signal strength in wireless systems or side information, to assign a\ntrust probability to local models received from neighbors in each iteration.\nOur simulation results show that the proposed algorithm performs close to a\nByzantine-free setting.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00992v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00992v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.312,
      "distributed_training_score": 0.458,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution involves developing an online decentralized federated multi-task learning algorithm, which is a form of distributed training. It utilizes multiple clients (nodes) to collaboratively train models without a central server, addressing data partitioning across nodes and handling communication for model updates. This directly aligns with distributed training concepts, such as parallel computing and multi-node machine learning, by strategically managing computation and data across processors to enhance efficiency and resilience in real-world scenarios like cyber-physical systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of model personalization and Byzantine attacks in online decentralized federated multi-task learning for cyber-physical systems, proposing an algorithm that leverages cyber-physical properties, such as received signal strength, to assign trust probabilities to local models from neighboring clients. The methodology involves modeling the problem as a constrained optimization solved via regularized Lagrangian optimization, enabling resilience even when Byzantine clients dominate, with simulation results demonstrating sublinear regret and constraint violation performance comparable to Byzantine-free scenarios.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel extension of multi-task learning to the online decentralized federated setting with Byzantine resilience using trust probabilities from cyber-physical properties, which has not been explored before and significantly advances the state-of-the-art in handling time-varying data and malicious clients.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in federated learning for cyber-physical systems by providing a robust method against dominant Byzantine attacks, potentially leading to citations and developments in areas like autonomous and wireless systems, though its applicability is somewhat niche.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to resilient federated learning, making it important for researchers focused on machine learning security and cyber-physical systems, as it addresses critical real-world challenges effectively.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6ff88b4e95be4deab3af165eef17bfbbc81a5ef9",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 5,
      "average_h_index": 2.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "O. Odeyomi",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/1605963270"
        },
        {
          "name": "Sofiat Olaosebikan",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/41019965"
        },
        {
          "name": "A. Opeyemi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2079254782"
        },
        {
          "name": "Oluwadoyinsola Ige",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2334029925"
        }
      ]
    },
    {
      "id": "2509.00996",
      "title": "MEPT: Mixture of Expert Prompt Tuning as a Manifold Mapper",
      "authors": [
        "Runjia Zeng",
        "Guangyan Sun",
        "Qifan Wang",
        "Tong Geng",
        "Sohail Dianat",
        "Xiaotian Han",
        "Raghuveer Rao",
        "Xueling Zhang",
        "Cheng Han",
        "Lifu Huang",
        "Dongfang Liu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Considering deep neural networks as manifold mappers, the\npretrain-then-fine-tune paradigm can be interpreted as a two-stage process:\npretrain establishes a broad knowledge base, and fine-tune adjusts the model\nparameters to activate specific neural pathways to align with the target\nmanifold. Although prior fine-tuning approaches demonstrate success, their\nrigid parameter space limits their ability to dynamically activate appropriate\nneural pathways, rendering them ill-equipped to adapt flexibly to the diverse\nand evolving data distributions. In light of this view, we propose a novel\napproach, Mixture of Expert Prompt Tuning (MEPT), as an effective and efficient\nmanifold-mapping framework. MEPT leverages the Mixture of Experts architecture\nby integrating multiple prompt experts to adaptively learn diverse and\nnon-stationary data distributions. Empirical evaluations demonstrate that MEPT\noutperforms several state-of-the-art parameter efficient baselines on\nSuperGLUE, achieving notable improvements in mean accuracy (e.g., 1.94%) while\nsignificantly reducing activated prompts by 79.25%. The effectiveness of MEPT\nis further supported by theoretical insights from manifold learning and\nvalidated through neural activation pathway visualization results. Our code is\navaliable at https://runjia.tech/emnlp_mept/.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00996v2",
      "pdf_url": "http://arxiv.org/pdf/2509.00996v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.45,
      "distributed_training_score": 0.399,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Mixture of Expert Prompt Tuning (MEPT) for parameter-efficient fine-tuning of language models, emphasizing manifold mapping and neural pathway activation. It does not involve reinforcement learning, human feedback, reward models, or any alignment with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses MEPT as a framework for adapting to data distributions via Mixture of Experts, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. It centers on prompt tuning and manifold learning instead.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00997",
      "title": "Supporting Our AI Overlords: Redesigning Data Systems to be Agent-First",
      "authors": [
        "Shu Liu",
        "Soujanya Ponnapalli",
        "Shreya Shankar",
        "Sepanta Zeighami",
        "Alan Zhu",
        "Shubham Agarwal",
        "Ruiqi Chen",
        "Samion Suwito",
        "Shuo Yuan",
        "Ion Stoica",
        "Matei Zaharia",
        "Alvin Cheung",
        "Natacha Crooks",
        "Joseph E. Gonzalez",
        "Aditya G. Parameswaran"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.DB (Databases)"
      ],
      "abstract": "Large Language Model (LLM) agents, acting on their users' behalf to\nmanipulate and analyze data, are likely to become the dominant workload for\ndata systems in the future. When working with data, agents employ a\nhigh-throughput process of exploration and solution formulation for the given\ntask, one we call agentic speculation. The sheer volume and inefficiencies of\nagentic speculation can pose challenges for present-day data systems. We argue\nthat data systems need to adapt to more natively support agentic workloads. We\ntake advantage of the characteristics of agentic speculation that we identify,\ni.e., scale, heterogeneity, redundancy, and steerability - to outline a number\nof new research opportunities for a new agent-first data systems architecture,\nranging from new query interfaces, to new query processing techniques, to new\nagentic memory stores.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.00997v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00997v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.434,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.419,
      "datasets_score": 0.404,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "Not Relevant",
      "rlhf_justification": "The paper focuses on redesigning data systems for LLM agents and agentic speculation, without any discussion of training AI models, human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper does not address machine learning training, label generation, or using noisy sources for supervision; it instead proposes architectural changes for data systems to support agentic workloads.",
      "diffusion_reasoning_justification": "The paper discusses LLM agents and exploratory querying but does not mention diffusion models, iterative refinement for logical reasoning, or multi-step Chain-of-Thought processes.",
      "distributed_training_justification": "The paper mentions high-throughput and potentially parallel requests by agents, which could relate to parallel computing in data systems, but it does not focus on distributed training of models, multi-node machine learning, or accelerating model training.",
      "datasets_justification": "The paper does not involve creating, analyzing, benchmarking, or evaluating datasets; it is a vision paper on data systems architecture for agents, with no mention of dataset-related research.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01013",
      "title": "Weather-Dependent Variations in Driver Gaze Behavior: A Case Study in\n  Rainy Conditions",
      "authors": [
        "Ghazal Farhani",
        "Taufiq Rahman",
        "Dominique Charlebois"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Rainy weather significantly increases the risk of road accidents due to\nreduced visibility and vehicle traction. Understanding how experienced drivers\nadapt their visual perception through gaze behavior under such conditions is\ncritical for designing robust driver monitoring systems (DMS) and for informing\nadvanced driver assistance systems (ADAS). This case study investigates the eye\ngaze behavior of a driver operating the same highway route under both clear and\nrainy conditions. To this end, gaze behavior was analyzed by a two-step\nclustering approach: first, clustering gaze points within 10-second intervals,\nand then aggregating cluster centroids into meta-clusters. This, along with\nMarkov transition matrices and metrics such as fixation duration, gaze\nelevation, and azimuth distributions, reveals meaningful behavioral shifts.\nWhile the overall gaze behavior focused on the road with occasional mirror\nchecks remains consistent, rainy conditions lead to more frequent dashboard\nglances, longer fixation durations, and higher gaze elevation, indicating\nincreased cognitive focus. These findings offer valuable insight into visual\nattention patterns under adverse conditions and highlight the potential of\nleveraging gaze modeling to aid in the design of more robust ADAS and DMS.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.01013v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01013v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.29,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.295,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01016",
      "title": "Analysis of Error Sources in LLM-based Hypothesis Search for Few-Shot\n  Rule Induction",
      "authors": [
        "Aishni Parab",
        "Hongjing Lu",
        "Ying Nian Wu",
        "Sumit Gulwani"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)",
        "cs.NE (Neural and Evolutionary Computing)"
      ],
      "abstract": "Inductive reasoning enables humans to infer abstract rules from limited\nexamples and apply them to novel situations. In this work, we compare an\nLLM-based hypothesis search framework with direct program generation approaches\non few-shot rule induction tasks. Our findings show that hypothesis search\nachieves performance comparable to humans, while direct program generation\nfalls notably behind. An error analysis reveals key bottlenecks in hypothesis\ngeneration and suggests directions for advancing program induction methods.\nOverall, this paper underscores the potential of LLM-based hypothesis search\nfor modeling inductive reasoning and the challenges in building more efficient\nsystems.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.01016v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01016v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.458,
      "weak_supervision_score": 0.455,
      "diffusion_reasoning_score": 0.506,
      "distributed_training_score": 0.341,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on LLM-based hypothesis search and program generation for inductive reasoning, with no mention of human feedback, reward models, or reinforcement learning techniques. It involves iterative refinement based on data testing, not alignment with human preferences.",
      "weak_supervision_justification": "The paper evaluates LLM frameworks for few-shot rule induction using direct examples, but does not involve programmatically generating training labels from noisy or imprecise sources. It relies on predefined tasks and human-comparable benchmarks rather than weak supervision methods.",
      "diffusion_reasoning_justification": "The paper describes an iterative hypothesis search and refinement process using LLMs, but does not adapt diffusion models or treat the reasoning path as a holistically corrected entity over steps. There is no clear component involving diffusion-based mechanisms for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01019",
      "title": "AI-driven Dispensing of Coral Reseeding Devices for Broad-scale\n  Restoration of the Great Barrier Reef",
      "authors": [
        "Scarlett Raine",
        "Benjamin Moshirian",
        "Tobias Fischer"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Coral reefs are on the brink of collapse, with climate change, ocean\nacidification, and pollution leading to a projected 70-90% loss of coral\nspecies within the next decade. Restoration efforts are crucial, but their\nsuccess hinges on introducing automation to upscale efforts. We present\nautomated deployment of coral re-seeding devices powered by artificial\nintelligence, computer vision, and robotics. Specifically, we perform automated\nsubstrate classification, enabling detection of areas of the seafloor suitable\nfor coral growth, thus significantly reducing reliance on human experts and\nincreasing the range and efficiency of restoration. Real-world testing of the\nalgorithms on the Great Barrier Reef leads to deployment accuracy of 77.8%,\nsub-image patch classification of 89.1%, and real-time model inference at 5.5\nframes per second. Further, we present and publicly contribute a large\ncollection of annotated substrate image data to foster future research in this\narea.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.01019v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01019v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.372,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01021",
      "title": "Quantum-like Coherence Derived from the Interaction between Chemical\n  Reaction and Its Environment",
      "authors": [
        "Yukio-Pegio Gunji",
        "Andrew Adamatzky",
        "Panagiotis Mougkogiannis",
        "Andrei Khrenikov"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "nlin.AO (Adaptation and Self-Organizing Systems)"
      ],
      "abstract": "By uncovering the contrast between Artificial Intelligence and Natural-born\nIntelligence as a computational process, we define closed computing and open\ncomputing, and implement open computing within chemical reactions. This\ninvolves forming a mixture and invalidation of the computational process and\nthe execution environment, which are logically distinct, and coalescing both to\ncreate a system that adjusts fluctuations. We model chemical reactions by\nconsidering the computation as the chemical reaction and the execution\nenvironment as the degree of aggregation of molecules that interact with the\nreactive environment. This results in a chemical reaction that progresses while\nrepeatedly clustering and de-clustering, where concentration no longer holds\nsignificant meaning. Open computing is segmented into Token computing, which\nfocuses on the individual behavior of chemical molecules, and Type computing,\nwhich focuses on normative behavior. Ultimately, both are constructed as an\ninterplay between the two. In this system, Token computing demonstrates\nself-organizing critical phenomena, while Type computing exhibits quantum\nlogic. Through their interplay, the recruitment of fluctuations is realized,\ngiving rise to interactions between quantum logical subspaces corresponding to\nquantum coherence across different Hilbert spaces. As a result, spike waves are\nformed, enabling signal transmission. This occurrence may be termed\nquantum-like coherence, implying the source of enzymes responsible for\ncontrolling spike waves and biochemical rhythms.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.01021v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01021v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.249,
      "weak_supervision_score": 0.249,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.267,
      "datasets_score": 0.205,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01022",
      "title": "Symbolic Planning and Multi-Agent Path Finding in Extremely Dense\n  Environments with Movable Obstacles",
      "authors": [
        "Bo Fu",
        "Zhe Chen",
        "Rahul Chandan",
        "Alex Barbosa",
        "Michael Caldara",
        "Joey Durham",
        "Federico Pecora"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)",
        "cs.RO (Robotics)"
      ],
      "abstract": "We introduce the Block Rearrangement Problem (BRaP), a challenging component\nof large warehouse management which involves rearranging storage blocks within\ndense grids to achieve a target state. We formally define the BRaP as a graph\nsearch problem. Building on intuitions from sliding puzzle problems, we propose\nfive search-based solution algorithms, leveraging joint configuration space\nsearch, classical planning, multi-agent pathfinding, and expert heuristics. We\nevaluate the five approaches empirically for plan quality and scalability.\nDespite the exponential relation between search space size and block number,\nour methods demonstrate efficiency in creating rearrangement plans for deeply\nburied blocks in up to 80x80 grids.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.01022v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01022v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.254,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.323,
      "datasets_score": 0.247,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01028",
      "title": "CompSlider: Compositional Slider for Disentangled Multiple-Attribute\n  Image Generation",
      "authors": [
        "Zixin Zhu",
        "Kevin Duarte",
        "Mamshad Nayeem Rizve",
        "Chengyuan Xu",
        "Ratheesh Kalarot",
        "Junsong Yuan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In text-to-image (T2I) generation, achieving fine-grained control over\nattributes - such as age or smile - remains challenging, even with detailed\ntext prompts. Slider-based methods offer a solution for precise control of\nimage attributes. Existing approaches typically train individual adapter for\neach attribute separately, overlooking the entanglement among multiple\nattributes. As a result, interference occurs among different attributes,\npreventing precise control of multiple attributes together. To address this\nchallenge, we aim to disentangle multiple attributes in slider-based generation\nto enbale more reliable and independent attribute manipulation. Our approach,\nCompSlider, can generate a conditional prior for the T2I foundation model to\ncontrol multiple attributes simultaneously. Furthermore, we introduce novel\ndisentanglement and structure losses to compose multiple attribute changes\nwhile maintaining structural consistency within the image. Since CompSlider\noperates in the latent space of the conditional prior and does not require\nretraining the foundation model, it reduces the computational burden for both\ntraining and inference. We evaluate our approach on a variety of image\nattributes and highlight its generality by extending to video generation.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.01028v2",
      "pdf_url": "http://arxiv.org/pdf/2509.01028v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.318,
      "datasets_score": 0.287,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a method for fine-grained control of attributes in text-to-image generation using diffusion-based models, focusing on disentangling attributes in image synthesis. It does not adapt the iterative refinement process of diffusion for solving complex logical tasks or handle chain-of-thought reasoning as a holistic entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01031",
      "title": "Reinforcement Learning Driven Generalizable Feature Representation for\n  Cross-User Activity Recognition",
      "authors": [
        "Xiaozhou Ye",
        "Kevin I-Kai Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Human Activity Recognition (HAR) using wearable sensors is crucial for\nhealthcare, fitness tracking, and smart environments, yet cross-user\nvariability -- stemming from diverse motion patterns, sensor placements, and\nphysiological traits -- hampers generalization in real-world settings.\nConventional supervised learning methods often overfit to user-specific\npatterns, leading to poor performance on unseen users. Existing domain\ngeneralization approaches, while promising, frequently overlook temporal\ndependencies or depend on impractical domain-specific labels. We propose\nTemporal-Preserving Reinforcement Learning Domain Generalization (TPRL-DG), a\nnovel framework that redefines feature extraction as a sequential\ndecision-making process driven by reinforcement learning. TPRL-DG leverages a\nTransformer-based autoregressive generator to produce temporal tokens that\ncapture user-invariant activity dynamics, optimized via a multi-objective\nreward function balancing class discrimination and cross-user invariance. Key\ninnovations include: (1) an RL-driven approach for domain generalization, (2)\nautoregressive tokenization to preserve temporal coherence, and (3) a\nlabel-free reward design eliminating the need for target user annotations.\nEvaluations on the DSADS and PAMAP2 datasets show that TPRL-DG surpasses\nstate-of-the-art methods in cross-user generalization, achieving superior\naccuracy without per-user calibration. By learning robust, user-invariant\ntemporal patterns, TPRL-DG enables scalable HAR systems, facilitating\nadvancements in personalized healthcare, adaptive fitness tracking, and\ncontext-aware environments.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.01031v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01031v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.439,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.39,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper employs reinforcement learning (RL) to optimize feature extraction for cross-user activity recognition, using a multi-objective reward function focused on class discrimination and user invariance. However, it does not involve human feedback, such as human-ranked data or a reward model trained on human preferences, which are core to RLHF. Instead, the reward design is label-free and automated, making the paper related to RL in general but not specifically to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02615",
      "title": "Radio Astronomy in the Era of Vision-Language Models: Prompt Sensitivity\n  and Adaptation",
      "authors": [
        "Mariia Drozdova",
        "Erica Lastufka",
        "Vitaliy Kinakh",
        "Taras Holotyak",
        "Daniel Schaerer",
        "Slava Voloshynovskiy"
      ],
      "categories": [
        "astro-ph.IM (Instrumentation and Methods for Astrophysics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Vision-Language Models (VLMs), such as recent Qwen and Gemini models, are\npositioned as general-purpose AI systems capable of reasoning across domains.\nYet their capabilities in scientific imaging, especially on unfamiliar and\npotentially previously unseen data distributions, remain poorly understood. In\nthis work, we assess whether generic VLMs, presumed to lack exposure to\nastronomical corpora, can perform morphology-based classification of radio\ngalaxies using the MiraBest FR-I/FR-II dataset. We explore prompting strategies\nusing natural language and schematic diagrams, and, to the best of our\nknowledge, we are the first to introduce visual in-context examples within\nprompts in astronomy. Additionally, we evaluate lightweight supervised\nadaptation via LoRA fine-tuning. Our findings reveal three trends: (i) even\nprompt-based approaches can achieve good performance, suggesting that VLMs\nencode useful priors for unfamiliar scientific domains; (ii) however, outputs\nare highly unstable, i.e. varying sharply with superficial prompt changes such\nas layout, ordering, or decoding temperature, even when semantic content is\nheld constant; and (iii) with just 15M trainable parameters and no\nastronomy-specific pretraining, fine-tuned Qwen-VL achieves near\nstate-of-the-art performance (3% Error rate), rivaling domain-specific models.\nThese results suggest that the apparent \"reasoning\" of VLMs often reflects\nprompt sensitivity rather than genuine inference, raising caution for their use\nin scientific domains. At the same time, with minimal adaptation, generic VLMs\ncan rival specialized models, offering a promising but fragile tool for\nscientific discovery.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.02615v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02615v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.316,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines Vision-Language Models for radio astronomy image classification, focusing on prompting strategies and fine-tuning, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. Thus, it does not align with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03540",
      "title": "Improving Factuality in LLMs via Inference-Time Knowledge Graph\n  Construction",
      "authors": [
        "Shanglin Wu",
        "Lihui Liu",
        "Jinho D. Choi",
        "Kai Shu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) often struggle with producing factually\nconsistent answers due to limitations in their parametric memory.\nRetrieval-Augmented Generation (RAG) methods address this issue by\nincorporating external knowledge from trusted sources at inference time.\nHowever, such methods typically treat knowledge as unstructured text, which\nlimits their ability to support compositional reasoning and identify factual\ninconsistencies. To overcome these limitations, we propose a novel framework\nthat dynamically constructs and expands knowledge graphs (KGs) during\ninference, integrating both internal knowledge extracted from LLMs and external\ninformation retrieved from external sources. Our method begins by extracting a\nseed KG from the question via prompting, followed by iterative expansion using\nthe LLM's latent knowledge. The graph is then selectively refined through\nexternal retrieval, enhancing factual coverage and correcting inaccuracies. We\nevaluate our approach on three diverse factual QA benchmarks, demonstrating\nconsistent improvements in factual accuracy, answer precision, and\ninterpretability over baseline prompting and static KG-augmented methods. Our\nfindings suggest that inference-time KG construction is a promising direction\nfor enhancing LLM factuality in a structured, interpretable, and scalable\nmanner.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.03540v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03540v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.426,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.49,
      "distributed_training_score": 0.315,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on dynamic knowledge graph construction for improving LLM factuality at inference time, without any mention of human feedback, reward models, or reinforcement learning techniques. It does not involve aligning models with human preferences through ranked data or fine-tuning via RL.",
      "weak_supervision_justification": "The paper's method involves extracting and expanding knowledge graphs from LLMs and external sources via prompting, but it does not address training models with programmatically generated labels from noisy sources. It is centered on inference-time enhancements rather than the weak supervision paradigm for label generation during training.",
      "diffusion_reasoning_justification": "The paper describes iterative expansion of knowledge graphs for reasoning, but it does not adapt diffusion models or their iterative refinement processes for logical tasks. There is no mention of treating a Chain-of-Thought as a holistic entity for multi-step correction using diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04481",
      "title": "Narrative-to-Scene Generation: An LLM-Driven Pipeline for 2D Game\n  Environments",
      "authors": [
        "Yi-Chun Chen",
        "Arnav Jhala"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Recent advances in large language models(LLMs) enable compelling story\ngeneration, but connecting narrative text to playable visual environments\nremains an open challenge in procedural content generation(PCG). We present a\nlightweight pipeline that transforms short narrative prompts into a sequence of\n2D tile-based game scenes, reflecting the temporal structure of stories. Given\nan LLM-generated narrative, our system identifies three key time frames,\nextracts spatial predicates in the form of \"Object-Relation-Object\" triples,\nand retrieves visual assets using affordance-aware semantic embeddings from the\nGameTileNet dataset. A layered terrain is generated using Cellular Automata,\nand objects are placed using spatial rules grounded in the predicate structure.\nWe evaluated our system in ten diverse stories, analyzing tile-object matching,\naffordance-layer alignment, and spatial constraint satisfaction across frames.\nThis prototype offers a scalable approach to narrative-driven scene generation\nand lays the foundation for future work on multi-frame continuity, symbolic\ntracking, and multi-agent coordination in story-centered PCG.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.04481v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04481v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.298,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a pipeline using LLMs for narrative-to-scene generation in 2D games, involving predicate extraction, asset retrieval, and Cellular Automata for terrain. It does not mention or utilize diffusion models, iterative refinement for logical tasks, or any adaptation of diffusion processes for Chain-of-Thought reasoning. Thus, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04482",
      "title": "Energy Landscapes Enable Reliable Abstention in Retrieval-Augmented\n  Large Language Models for Healthcare",
      "authors": [
        "Ravi Shankar",
        "Sheng Wong",
        "Lin Li",
        "Magdalena Bachmann",
        "Alex Silverthorne",
        "Beth Albert",
        "Gabriel Davis Jones"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reliable abstention is critical for retrieval-augmented generation (RAG)\nsystems, particularly in safety-critical domains such as women's health, where\nincorrect answers can lead to harm. We present an energy-based model (EBM) that\nlearns a smooth energy landscape over a dense semantic corpus of 2.6M\nguideline-derived questions, enabling the system to decide when to generate or\nabstain. We benchmark the EBM against a calibrated softmax baseline and a\nk-nearest neighbour (kNN) density heuristic across both easy and hard\nabstention splits, where hard cases are semantically challenging\nnear-distribution queries. The EBM achieves superior abstention performance\nabstention on semantically hard cases, reaching AUROC 0.961 versus 0.950 for\nsoftmax, while also reducing FPR@95 (0.235 vs 0.331). On easy negatives,\nperformance is comparable across methods, but the EBM's advantage becomes most\npronounced in safety-critical hard distributions. A comprehensive ablation with\ncontrolled negative sampling and fair data exposure shows that robustness stems\nprimarily from the energy scoring head, while the inclusion or exclusion of\nspecific negative types (hard, easy, mixed) sharpens decision boundaries but is\nnot essential for generalisation to hard cases. These results demonstrate that\nenergy-based abstention scoring offers a more reliable confidence signal than\nprobability-based softmax confidence, providing a scalable and interpretable\nfoundation for safe RAG systems.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.04482v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04482v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.445,
      "distributed_training_score": 0.361,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on energy-based models for abstention in RAG systems for healthcare, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "The paper involves controlled negative sampling and programmatic generation of training data for the energy-based model, which could loosely relate to weak supervision through noisy or imprecise labels, but this is not the main contribution or explicitly framed as weak supervision.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical reasoning, or any multi-step reasoning processes; it centers on energy-based abstention in RAG systems.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04483",
      "title": "DecMetrics: Structured Claim Decomposition Scoring for Factually\n  Consistent LLM Outputs",
      "authors": [
        "Minghui Huang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Claim decomposition plays a crucial role in the fact-checking process by\nbreaking down complex claims into simpler atomic components and identifying\ntheir unfactual elements. Despite its importance, current research primarily\nfocuses on generative methods for decomposition, with insufficient emphasis on\nevaluating the quality of these decomposed atomic claims. To bridge this gap,\nwe introduce \\textbf{DecMetrics}, which comprises three new metrics:\n\\texttt{COMPLETENESS}, \\texttt{CORRECTNESS}, and \\texttt{SEMANTIC ENTROPY},\ndesigned to automatically assess the quality of claims produced by\ndecomposition models. Utilizing these metrics, we develop a lightweight claim\ndecomposition model, optimizing its performance through the integration of\nthese metrics as a reward function. Through automatic evaluation, our approach\naims to set a benchmark for claim decomposition, enhancing both the reliability\nand effectiveness of fact-checking systems.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.04483v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04483v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.314,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of metrics (COMPLETENESS, CORRECTNESS, SEMANTIC ENTROPY) for evaluating claim decomposition in fact-checking, along with a reinforcement learning-based model and benchmark. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for holistic correction, as required for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04484",
      "title": "The Good, the Bad and the Constructive: Automatically Measuring Peer\n  Review's Utility for Authors",
      "authors": [
        "Abdelrahman Sadallah",
        "Tim Baumgärtner",
        "Iryna Gurevych",
        "Ted Briscoe"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Providing constructive feedback to paper authors is a core component of peer\nreview. With reviewers increasingly having less time to perform reviews,\nautomated support systems are required to ensure high reviewing quality, thus\nmaking the feedback in reviews useful for authors. To this end, we identify\nfour key aspects of review comments (individual points in weakness sections of\nreviews) that drive the utility for authors: Actionability, Grounding &\nSpecificity, Verifiability, and Helpfulness. To enable evaluation and\ndevelopment of models assessing review comments, we introduce the RevUtil\ndataset. We collect 1,430 human-labeled review comments and scale our data with\n10k synthetically labeled comments for training purposes. The synthetic data\nadditionally contains rationales, i.e., explanations for the aspect score of a\nreview comment. Employing the RevUtil dataset, we benchmark fine-tuned models\nfor assessing review comments on these aspects and generating rationales. Our\nexperiments demonstrate that these fine-tuned models achieve agreement levels\nwith humans comparable to, and in some cases exceeding, those of powerful\nclosed models like GPT-4o. Our analysis further reveals that machine-generated\nreviews generally underperform human reviews on our four aspects.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.04484v2",
      "pdf_url": "http://arxiv.org/pdf/2509.04484v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.429,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.319,
      "datasets_score": 0.412,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on fine-tuning models using human-labeled and synthetic data for peer review assessment, but it does not involve creating a reward model or using reinforcement learning to align AI with human preferences. It is primarily supervised learning, not RLHF.",
      "weak_supervision_justification": "The paper uses programmatically generated synthetic labels (10k comments) to scale training data, which aligns with weak supervision by relying on noisy or automated sources rather than perfect hand-labeling. However, it also includes direct human annotations, so weak supervision is not the primary focus.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes introducing the RevUtil dataset, collecting and annotating 1,430 human-labeled comments plus 10k synthetic ones, and benchmarking models on it, directly aligning with research on creating, analyzing, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges in peer review by defining four key aspects—Actionability, Grounding & Specificity, Verifiability, and Helpfulness—that enhance the utility of review comments for authors, and introduces the RevUtil dataset comprising 1,430 human-labeled and 10,000 synthetic comments to train and evaluate models for assessing these aspects. The methodology involves collecting and annotating data, fine-tuning models, and benchmarking them against human judgments and closed models like GPT-4o, with key findings showing that fine-tuned models achieve comparable or superior agreement with humans and that machine-generated reviews generally underperform human ones in these aspects.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new dataset and specific aspects for evaluating review comments, building on existing peer review challenges with a clever combination of human and synthetic labeling techniques. However, it does not introduce a entirely new problem, as automated review assessment has been explored before.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI-assisted peer review and computational linguistics, given its practical application in improving review quality. Nonetheless, its influence may be limited to specific academic and conference settings rather than broader commercial or interdisciplinary areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution through its dataset and model benchmarks, making it important for researchers in AI and peer review systems to be aware of. While not essential for all, it provides useful insights for those focused on improving academic processes.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/669c45bd83238b7682dee607dcb1df6018c45305",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 11,
      "average_h_index": 5.25,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "A. Sadallah",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2233498337"
        },
        {
          "name": "Tim Baumgärtner",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/51190347"
        },
        {
          "name": "Iryna Gurevych",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2260340390"
        },
        {
          "name": "Ted Briscoe",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2320311842"
        }
      ]
    },
    {
      "id": "2509.04485",
      "title": "ASCENDgpt: A Phenotype-Aware Transformer Model for Cardiovascular Risk\n  Prediction from Electronic Health Records",
      "authors": [
        "Chris Sainsbury",
        "Andreas Karwath"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present ASCENDgpt, a transformer-based model specifically designed for\ncardiovascular risk prediction from longitudinal electronic health records\n(EHRs). Our approach introduces a novel phenotype-aware tokenization scheme\nthat maps 47,155 raw ICD codes to 176 clinically meaningful phenotype tokens,\nachieving 99.6\\% consolidation of diagnosis codes while preserving semantic\ninformation. This phenotype mapping contributes to a total vocabulary of 10,442\ntokens - a 77.9\\% reduction when compared with using raw ICD codes directly. We\npretrain ASCENDgpt on sequences derived from 19402 unique individuals using a\nmasked language modeling objective, then fine-tune for time-to-event prediction\nof five cardiovascular outcomes: myocardial infarction (MI), stroke, major\nadverse cardiovascular events (MACE), cardiovascular death, and all-cause\nmortality. Our model achieves excellent discrimination on the held-out test set\nwith an average C-index of 0.816, demonstrating strong performance across all\noutcomes (MI: 0.792, stroke: 0.824, MACE: 0.800, cardiovascular death: 0.842,\nall-cause mortality: 0.824). The phenotype-based approach enables clinically\ninterpretable predictions while maintaining computational efficiency. Our work\ndemonstrates the effectiveness of domain-specific tokenization and pretraining\nfor EHR-based risk prediction tasks.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.04485v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04485v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.29,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.326,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05327",
      "title": "Layer-Wise Anomaly Detection in Directed Energy Deposition using\n  High-Fidelity Fringe Projection Profilometry",
      "authors": [
        "Guanzhong Hu",
        "Wenpan Li",
        "Rujing Zha",
        "Ping Guo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Directed energy deposition (DED), a metal additive manufacturing process, is\nhighly susceptible to process-induced defects such as geometric deviations,\nlack of fusion, and poor surface finish. This work presents a\nbuild-height-synchronized fringe projection system for in-situ, layer-wise\nsurface reconstruction of laser-DED components, achieving a reconstruction\naccuracy of ${\\pm}$46 ${\\mu}$m. From the reconstructed 3D morphology, two\ncomplementary geometry-based point cloud metrics are introduced: local point\ndensity, which highlights poor surface finish, and normal-change rate, which\nidentifies lack-of-fusion features. These methods enable automated,\nannotation-free identification of common deposition anomalies directly from\nreconstructed surfaces, without the need for manual labeling. By directly\nlinking geometric deviation to defect formation, the approach enables precise\nanomaly localization and advances the feasibility of closed-loop process\ncontrol. This work establishes fringe projection as a practical tool for\nmicrometer-scale monitoring in DED, bridging the gap between process signatures\nand part geometry for certifiable additive manufacturing.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.05327v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05327v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.329,
      "distributed_training_score": 0.345,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05328",
      "title": "Feed Two Birds with One Scone: Exploiting Function-Space Regularization\n  for Both OOD Robustness and ID Fine-Tuning Performance",
      "authors": [
        "Xiang Yuan",
        "Jun Shu",
        "Deyu meng",
        "Zongben Xu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Robust fine-tuning aims to achieve competitive in-distribution (ID)\nperformance while maintaining the out-of-distribution (OOD) robustness of a\npre-trained model when transferring it to a downstream task. To remedy this,\nmost robust fine-tuning methods aim to preserve the pretrained weights,\nfeatures, or logits. However, we find that these methods cannot always improve\nOOD robustness for different model architectures. This is due to the OOD\nrobustness requiring the model function to produce stable prediction for input\ninformation of downstream tasks, while existing methods might serve as a poor\nproxy for the optimization in the function space. Based on this finding, we\npropose a novel regularization that constrains the distance of fine-tuning and\npre-trained model in the function space with the simulated OOD samples, aiming\nto preserve the OOD robustness of the pre-trained model. Besides, to further\nenhance the OOD robustness capability of the fine-tuning model, we introduce an\nadditional consistency regularization to promote stable predictions of\nperturbed samples. Extensive experiments demonstrate our approach could\nconsistently improve both downstream task ID fine-tuning performance and OOD\nrobustness across a variety of CLIP backbones, outperforming existing\nregularization-based robust fine-tuning methods.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.05328v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05328v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.406,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on robust fine-tuning methods for pre-trained models like CLIP, emphasizing regularization in function space for OOD robustness and ID performance. It does not involve reinforcement learning, human feedback, reward models, or aligning AI with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper addresses optimization techniques for fine-tuning models, such as functional regularization, but does not discuss distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05329",
      "title": "Optical Music Recognition of Jazz Lead Sheets",
      "authors": [
        "Juan Carlos Martinez-Sevilla",
        "Francesco Foscarin",
        "Patricia Garcia-Iasci",
        "David Rizo",
        "Jorge Calvo-Zaragoza",
        "Gerhard Widmer"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this paper, we address the challenge of Optical Music Recognition (OMR)\nfor handwritten jazz lead sheets, a widely used musical score type that encodes\nmelody and chords. The task is challenging due to the presence of chords, a\nscore component not handled by existing OMR systems, and the high variability\nand quality issues associated with handwritten images. Our contribution is\ntwo-fold. We present a novel dataset consisting of 293 handwritten jazz lead\nsheets of 163 unique pieces, amounting to 2021 total staves aligned with\nHumdrum **kern and MusicXML ground truth scores. We also supply synthetic score\nimages generated from the ground truth. The second contribution is the\ndevelopment of an OMR model for jazz lead sheets. We discuss specific\ntokenisation choices related to our kind of data, and the advantages of using\nsynthetic scores and pretrained models. We publicly release all code, data, and\nmodels.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.05329v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05329v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.294,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.319,
      "distributed_training_score": 0.269,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05330",
      "title": "MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition\n  Dataset",
      "authors": [
        "Seyed Muhammad Hossein Mousavi",
        "Atiye Ilanloo"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Automatic emotion recognition has become increasingly important with the rise\nof AI, especially in fields like healthcare, education, and automotive systems.\nHowever, there is a lack of multimodal datasets, particularly involving body\nmotion and physiological signals, which limits progress in the field. To\naddress this, the MVRS dataset is introduced, featuring synchronized recordings\nfrom 13 participants aged 12 to 60 exposed to VR based emotional stimuli\n(relaxation, fear, stress, sadness, joy). Data were collected using eye\ntracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR\nsignals (Arduino UNO), all timestamp aligned. Participants followed a unified\nprotocol with consent and questionnaires. Features from each modality were\nextracted, fused using early and late fusion techniques, and evaluated with\nclassifiers to confirm the datasets quality and emotion separability, making\nMVRS a valuable contribution to multimodal affective computing.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.05330v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05330v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.258,
      "diffusion_reasoning_score": 0.252,
      "distributed_training_score": 0.241,
      "datasets_score": 0.427,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of the MVRS dataset, a new multimodal dataset for emotion recognition in AI applications. It details the dataset's creation, including data collection from various modalities (e.g., eye tracking, body motion, physiological signals), synchronization methods, participant protocols, and evaluation through feature extraction, fusion techniques, and classifier assessments. This directly aligns with research on creating, analyzing, and evaluating datasets for machine learning, making it a strong fit for the topic.",
      "llm_score_status": "completed",
      "summary": "The MVRS dataset is introduced to address the lack of multimodal datasets for emotion recognition, featuring synchronized data from eye tracking, body motion, and physiological signals collected from 13 participants exposed to VR-based stimuli for emotions like relaxation, fear, stress, sadness, and joy. The methodology involves data collection using devices such as a VR headset, Kinect v2, and Arduino UNO, followed by feature extraction, fusion techniques, and classifier evaluation to confirm emotion separability, thereby contributing to advancements in affective computing.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new multimodal dataset that integrates VR stimuli with body motion and physiological signals, addressing a known gap in emotion recognition research. However, it builds on existing techniques rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of multimodal affective computing, as it provides a new dataset for applications in healthcare and education. Nonetheless, its influence may be limited to specific research areas rather than broadly across AI.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable new dataset that advances emotion recognition studies, making it essential for researchers in affective computing to be aware of. While not groundbreaking, its practical contributions warrant attention from relevant specialists.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f1ece7a0d44ad1af6aa242fe757dea8a10716c69",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 2,
      "average_h_index": 2.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Seyed Muhammad Hossein Mousavi",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2243966292"
        },
        {
          "name": "Atiye Ilanloo",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2162969496"
        }
      ]
    },
    {
      "id": "2509.05331",
      "title": "ForensicsData: A Digital Forensics Dataset for Large Language Models",
      "authors": [
        "Youssef Chakir",
        "Iyad Lahsen-Cherif"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "The growing complexity of cyber incidents presents significant challenges for\ndigital forensic investigators, especially in evidence collection and analysis.\nPublic resources are still limited because of ethical, legal, and privacy\nconcerns, even though realistic datasets are necessary to support research and\ntool developments. To address this gap, we introduce ForensicsData, an\nextensive Question-Context-Answer (Q-C-A) dataset sourced from actual malware\nanalysis reports. It consists of more than 5,000 Q-C-A triplets. A unique\nworkflow was used to create the dataset, which extracts structured data, uses\nlarge language models (LLMs) to transform it into Q-C-A format, and then uses a\nspecialized evaluation process to confirm its quality. Among the models\nevaluated, Gemini 2 Flash demonstrated the best performance in aligning\ngenerated content with forensic terminology. ForensicsData aims to advance\ndigital forensics by enabling reproducible experiments and fostering\ncollaboration within the research community.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.05331v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05331v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.371,
      "datasets_score": 0.524,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of ForensicsData, a new dataset for digital forensics applications, which directly aligns with research on creating datasets for machine learning and AI. It details dataset curation methodologies using LLMs to generate and annotate Q-C-A triplets, evaluates dataset quality through specialized validation processes, and addresses benchmarking by comparing LLM performances, making it a core example of dataset creation and evaluation in AI contexts.",
      "llm_score_status": "completed",
      "summary": "The paper introduces ForensicsData, a comprehensive dataset of over 5,000 Question-Context-Answer triplets derived from real malware analysis reports, to address the scarcity of public digital forensics resources amid ethical and privacy constraints. It outlines a novel workflow using large language models (LLMs) like Gemini 2 Flash to transform structured data into this format, evaluates the dataset's quality through specialized validation processes, and highlights its potential to enable reproducible experiments and advance research in digital forensics by providing a structured resource for training and testing forensic tools.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing LLM techniques for data generation applied to a new context in digital forensics, creating a structured dataset that improves upon the lack of public resources, though it does not introduce a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the digital forensics and AI security subfields by providing a valuable dataset for research and tool development, but its influence may be limited to specialized applications rather than broader commercial or interdisciplinary advancements.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, practical contribution to digital forensics research through its innovative dataset and methodology, making it essential for researchers in AI and security to be aware of, though not universally groundbreaking.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/841de6652215f570b5fb8decccdf6b45f8a458f7",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Youssef Chakir",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379666031"
        },
        {
          "name": "Iyad Lahsen-Cherif",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379667020"
        }
      ]
    },
    {
      "id": "2509.05332",
      "title": "Integrated Simulation Framework for Adversarial Attacks on Autonomous\n  Vehicles",
      "authors": [
        "Christos Anagnostopoulos",
        "Ioulia Kapsali",
        "Alexandros Gkillas",
        "Nikos Piperigkos",
        "Aris S. Lalos"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Autonomous vehicles (AVs) rely on complex perception and communication\nsystems, making them vulnerable to adversarial attacks that can compromise\nsafety. While simulation offers a scalable and safe environment for robustness\ntesting, existing frameworks typically lack comprehensive supportfor modeling\nmulti-domain adversarial scenarios. This paper introduces a novel, open-source\nintegrated simulation framework designed to generate adversarial attacks\ntargeting both perception and communication layers of AVs. The framework\nprovides high-fidelity modeling of physical environments, traffic dynamics, and\nV2X networking, orchestrating these components through a unified core that\nsynchronizes multiple simulators based on a single configuration file. Our\nimplementation supports diverse perception-level attacks on LiDAR sensor data,\nalong with communication-level threats such as V2X message manipulation and GPS\nspoofing. Furthermore, ROS 2 integration ensures seamless compatibility with\nthird-party AV software stacks. We demonstrate the framework's effectiveness by\nevaluating the impact of generated adversarial scenarios on a state-of-the-art\n3D object detector, revealing significant performance degradation under\nrealistic conditions.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.05332v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05332v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.329,
      "distributed_training_score": 0.341,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.06979",
      "title": "Exploring Over-stationarization in Deep Learning-based Bus/Tram Arrival\n  Time Prediction: Analysis and Non-stationary Effect Recovery",
      "authors": [
        "Zirui Li",
        "Bin Yang",
        "Meng Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Arrival time prediction (ATP) of public transport vehicles is essential in\nimproving passenger experience and supporting traffic management. Deep learning\nhas demonstrated outstanding performance in ATP due to its ability to model\nnon-linear and temporal dynamics. In the multi-step ATP, non-stationary data\nwill degrade the model performance due to the variation in variables' joint\ndistribution along the temporal direction. Previous studies mainly applied\nnormalization to eliminate the non-stationarity in time series, thereby\nachieving better predictability. However, the normalization may obscure useful\ncharacteristics inherent in non-stationarity, which is known as the\nover-stationarization. In this work, to trade off predictability and\nnon-stationarity, a new approach for multi-step ATP, named non-stationary ATP (\nNSATP), is proposed. The method consists of two stages: series stationarization\nand non-stationarity effect recovery. The first stage aims at improving the\npredictability. As for the latter, NSATP extends a state-of-the-art method from\none-dimensional to two dimensional based models to capture the hidden\nperiodicity in time series and designs a compensation module of\nover-stationarization by learning scaling and shifting factors from raw data.\n125 days' public transport operational data of Dresden is collected for\nvalidation. Experimental results show that compared to baseline methods, the\nproposed NSATP can reduce RMSE, MAE, and MAPE by 2.37%, 1.22%, and 2.26% for\ntrams and by 1.72%, 0.60%, and 1.17% for buses, respectively.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.06979v1",
      "pdf_url": "http://arxiv.org/pdf/2509.06979v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.273,
      "weak_supervision_score": 0.286,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.351,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.06980",
      "title": "RLFactory: A Plug-and-Play Reinforcement Learning Post-Training\n  Framework for LLM Multi-Turn Tool-Use",
      "authors": [
        "Jiajun Chai",
        "Guojun Yin",
        "Zekun Xu",
        "Chuhuai Yue",
        "Yi Jia",
        "Siyu Xia",
        "Xiaohan Wang",
        "Jiwen Jiang",
        "Xiaoguang Li",
        "Chengqi Dong",
        "Hang He",
        "Wei Lin"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models excel at basic reasoning but struggle with tasks that\nrequire interaction with external tools. We present RLFactory, a plug-and-play\nreinforcement learning post-training framework for multi-round tool use.\nRLFactory tackles (i) tool-call stability and adaptability amid tool\nheterogeneity and interface issues via an asyncio-based asynchronous caller and\na decoupled tool/training architecture, and (ii) diverse evaluation needs via a\nreward layer supporting rule-based, model-judgment, and tool-verification\nsignals. It reconstructs the MDP by introducing observation markers from tool\nfeedback, closing the loop among model, tools, and environment, and implements\na generate-parse-invoke-update workflow for dynamic policy optimization. On\nSearch-R1 with Qwen3-4B, RLFactory achieves a 0.486 test score on the Natural\nQuestions (NQ) dataset, surpassing larger models trained with similar\ntechniques (e.g., Qwen2.5-7B-Instruct-GRPO at 0.473), and increases training\nthroughput by 6.8x. RLFactory provides a low-barrier, highly adaptable\nframework for strengthening multi-round tool use of LLMs in real-world\nscenarios. Code: https://github.com/Simple-Efficient/RL-Factory.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.06980v1",
      "pdf_url": "http://arxiv.org/pdf/2509.06980v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.529,
      "weak_supervision_score": 0.415,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.406,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a reinforcement learning framework using rewards from rule-based, model-judgment, and tool-verification signals, with no mention of human feedback, human-ranked data, or a separate reward model based on human preferences. Thus, it does not align with RLHF.",
      "weak_supervision_justification": "The paper does not involve programmatically generating labels from noisy or imprecise sources for training. Instead, it centers on reinforcement learning for tool interactions, without any discussion of weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper's main contribution is a reinforcement learning framework for tool use, with no reference to diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. It focuses on generate-parse-invoke-update workflows, not diffusion-based approaches.",
      "distributed_training_justification": "The paper mentions increasing training throughput by 6.8x through asynchronous tool calling via asyncio, which involves parallelization for efficiency, but it does not specifically address distributed training across multiple nodes, data partitioning, or multi-node machine learning as core elements.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10490",
      "title": "Distributed Gossip-GAN for Low-overhead CSI Feedback Training in FDD\n  mMIMO-OFDM Systems",
      "authors": [
        "Yuwen Cao",
        "Guijun Liu",
        "Tomoaki Ohtsuki",
        "Howard H. Yang",
        "Tony Q. S. Quek"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.IT (Information Theory)",
        "math.IT (Information Theory)"
      ],
      "abstract": "The deep autoencoder (DAE) framework has turned out to be efficient in\nreducing the channel state information (CSI) feedback overhead in massive\nmultiple-input multipleoutput (mMIMO) systems. However, these DAE approaches\npresented in prior works rely heavily on large-scale data collected through the\nbase station (BS) for model training, thus rendering excessive bandwidth usage\nand data privacy issues, particularly for mMIMO systems. When considering\nusers' mobility and encountering new channel environments, the existing CSI\nfeedback models may often need to be retrained. Returning back to previous\nenvironments, however, will make these models perform poorly and face the risk\nof catastrophic forgetting. To solve the above challenging problems, we propose\na novel gossiping generative adversarial network (Gossip-GAN)-aided CSI\nfeedback training framework. Notably, Gossip-GAN enables the CSI feedback\ntraining with low-overhead while preserving users' privacy. Specially, each\nuser collects a small amount of data to train a GAN model. Meanwhile, a fully\ndistributed gossip-learning strategy is exploited to avoid model overfitting,\nand to accelerate the model training as well. Simulation results demonstrate\nthat Gossip-GAN can i) achieve a similar CSI feedback accuracy as centralized\ntraining with real-world datasets, ii) address catastrophic forgetting\nchallenges in mobile scenarios, and iii) greatly reduce the uplink bandwidth\nusage. Besides, our results show that the proposed approach possesses an\ninherent robustness.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.10490v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10490v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.323,
      "distributed_training_score": 0.405,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a distributed Gossip-GAN framework that employs a fully decentralized gossip-learning strategy for training a generative adversarial network (GAN) model across multiple users in mMIMO systems. This directly aligns with distributed training concepts, as it partitions model training across nodes (user equipments) without a central server, reduces communication overhead, and accelerates training while preserving privacy. The approach addresses key aspects like multi-node cooperation and low-overhead distributed learning, making it a strong fit for the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of high overhead and data privacy in CSI feedback for FDD mMIMO-OFDM systems by proposing a distributed Gossip-GAN framework, where users train GAN models locally using a gossip learning strategy to collaborate without a central server, thereby reducing uplink bandwidth usage and mitigating catastrophic forgetting in mobile scenarios. The methodology involves each user collecting minimal data for GAN training, employing consistency regularization to enhance model performance, and using the trained GAN to assist in adapting to new environments, with simulations on the DeepMIMO dataset demonstrating comparable accuracy to centralized methods, improved generalization, and inherent robustness.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of GAN and gossip learning to address CSI feedback issues in a distributed manner, offering a notable improvement over existing centralized methods by enhancing privacy and efficiency. While it builds on established techniques, it applies them innovatively to a specific problem in wireless communications.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of wireless communications and distributed AI, particularly for mMIMO systems in 6G, due to its practical solutions for bandwidth reduction and privacy. However, its influence may be limited to specific applications rather than broadly transformative across all AI or signal processing domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to CSI feedback in mMIMO systems by introducing an efficient distributed framework that tackles real-world issues like privacy and mobility, making it essential for researchers in wireless AI. While not groundbreaking for all audiences, it provides insights that could inspire further advancements in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c3b7298c40ad06464eb3d4dec301dbe3d8f70261",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 15,
      "average_h_index": 7.2,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Yuwen Cao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2316521374"
        },
        {
          "name": "Guijun Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2358873776"
        },
        {
          "name": "Tomoaki Ohtsuki",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2239901353"
        },
        {
          "name": "Howard H. Yang",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/2143390992"
        },
        {
          "name": "Tony Q. S. Quek",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2257289359"
        }
      ]
    },
    {
      "id": "2509.10493",
      "title": "Online Learning Based Efficient Resource Allocation for LoRaWAN Network",
      "authors": [
        "Ruiqi Wang",
        "Wenjun Li",
        "Jing Ren",
        "Tongyu Song",
        "Xiong Wang",
        "Sheng Wang",
        "Shizhong Xu"
      ],
      "categories": [
        "cs.NI (Networking and Internet Architecture)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The deployment of large-scale LoRaWAN networks requires jointly optimizing\nconflicting metrics like Packet Delivery Ratio (PDR) and Energy Efficiency (EE)\nby dynamically allocating transmission parameters, including Carrier Frequency,\nSpreading Factor, and Transmission Power. Existing methods often oversimplify\nthis challenge, focusing on a single metric or lacking the adaptability needed\nfor dynamic channel environments, leading to suboptimal performance. To address\nthis, we propose two online learning-based resource allocation frameworks that\nintelligently navigate the PDR-EE trade-off. Our foundational proposal, D-LoRa,\nis a fully distributed framework that models the problem as a Combinatorial\nMulti-Armed Bandit. By decomposing the joint parameter selection and employing\nspecialized, disaggregated reward functions, D-LoRa dramatically reduces\nlearning complexity and enables nodes to autonomously adapt to network\ndynamics. To further enhance performance in LoRaWAN networks, we introduce\nCD-LoRa, a hybrid framework that integrates a lightweight, centralized\ninitialization phase to perform a one-time, quasi-optimal channel assignment\nand action space pruning, thereby accelerating subsequent distributed learning.\nExtensive simulations and real-world field experiments demonstrate the\nsuperiority of our frameworks, showing that D-LoRa excels in non-stationary\nenvironments while CD-LoRa achieves the fastest convergence in stationary\nconditions. In physical deployments, our methods outperform state-of-the-art\nbaselines, improving PDR by up to 10.8% and EE by 26.1%, confirming their\npractical effectiveness for scalable and efficient LoRaWAN networks.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.10493v2",
      "pdf_url": "http://arxiv.org/pdf/2509.10493v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.278,
      "distributed_training_score": 0.406,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper introduces D-LoRa, a fully distributed framework using Combinatorial Multi-Armed Bandit (CMAB) for resource allocation in LoRaWAN networks, where each device learns independently. This involves distributed decision-making and learning across nodes, which aligns with aspects of distributed training, such as multi-node machine learning and parallel computing for optimization. However, the focus is on online resource allocation rather than traditional distributed training of machine learning models (e.g., gradient synchronization or data partitioning for model training), making it moderately relevant rather than highly so.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of resource allocation in large-scale LoRaWAN networks by proposing two online learning-based frameworks, D-LoRa and CD-LoRa, to jointly optimize Packet Delivery Ratio (PDR) and Energy Efficiency (EE) through dynamic selection of Carrier Frequency, Spreading Factor, and Transmission Power. D-LoRa employs a fully distributed Combinatorial Multi-Armed Bandit approach for autonomous device adaptation, while CD-LoRa integrates a lightweight centralized initialization to accelerate learning and reduce complexity; experimental results from simulations and real-world deployments show these frameworks outperform baselines, achieving up to 10.8% improvement in PDR and 26.1% in EE.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting Combinatorial Multi-Armed Bandit for distributed resource allocation in LoRaWAN, cleverly combining existing online learning techniques to handle dynamic environments more effectively, though it does not introduce an entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research and applications in LoRaWAN and IoT networking by providing practical frameworks for better resource management, but its applicability is primarily confined to specific subfields like wireless communications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a strong, practical contribution to resource allocation in IoT networks, making it valuable for researchers in networking and AI, though it may not be essential for those outside these areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1b69e1da6e32789d2541f581cd8baf385bad3679",
      "total_authors": 7,
      "authors_found": 6,
      "highest_h_index": 17,
      "average_h_index": 6.833333333333333,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Ruiqi Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2341650566"
        },
        {
          "name": "Wenjun Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2365435612"
        },
        {
          "name": "Jing Ren",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Tongyu Song",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/1515235097"
        },
        {
          "name": "Xiong Wang",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/48630713"
        },
        {
          "name": "Sheng Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2255485466"
        },
        {
          "name": "Shizhong Xu",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/145311464"
        }
      ]
    },
    {
      "id": "2509.12213",
      "title": "Scaling Up Data Parallelism in Decentralized Deep Learning",
      "authors": [
        "Bing Xie",
        "Junqi Yin",
        "Zhenyu Zhou",
        "Sarp Oral",
        "Feiyi Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Although it has been extensively explored in theory, decentralized learning\nis not yet green-lighted for production use, largely due to a lack of\nstability, scalability, and generality in large scale DNN training. To shed\nlight on the production use of decentralized learning, this work studies\ndecentralized data parallel training at scale. To this end, we introduce a\nbenchmarking framework, namely DBench, to host both centralized and\ndecentralized DNN training. Building upon DBench, we introduce a benchmarking\nmethodology to uncover the correlations between model accuracy and the\nvariances of parameter tensors by varying communication graphs and training\nscales. Based on the benchmarking results, we observe that, (1) Similar to\ncentralized learning, decentralized data parallel training also presents the\nissues of scalability and generality when the training scales up; (2) The model\naccuracy of decentralized learning is correlated to the number of connections\nin a communication graph; (3) The model accuracy of decentralized learning is\nsurprisingly sensitive to the variance of parameter tensors across model\nreplicas. Built upon the observations, we propose Ada, a decentralized adaptive\napproach that performs large scale DNN training following a decentralized SGD\nmethod and adapting the communication graph in use dynamically throughout\ntraining iterations. We apply Ada on large scale training and observe that Ada\ncan obtain the best convergence rates consistently in decentralized DNN\ntraining, and delivers equally or comparably good model accuracy for all sample\napplications as centralized learning does, even when training ResNet50 for\nImageNet-1K on the scale of 1008 GPUs.",
      "published_date": "2025-08-31",
      "arxiv_url": "http://arxiv.org/abs/2509.12213v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12213v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.571,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution centers on decentralized data parallel training for large-scale DNNs, which directly aligns with distributed training concepts. It introduces DBench for benchmarking centralized and decentralized approaches, analyzes scalability issues across multiple nodes, and proposes Ada, an adaptive algorithm that dynamically adjusts communication graphs to enhance convergence. These elements strategically address partitioning data and computation across processors, accelerating training, and improving efficiency in parallel computing environments.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of scalability and stability in decentralized deep learning by introducing DBench, a benchmarking framework for evaluating both centralized and decentralized DNN training, and Ada, an adaptive approach that dynamically adjusts communication graphs during training. Through controlled experiments using DBench, the authors uncover correlations between model accuracy and parameter tensor variances, revealing that decentralized learning faces similar scalability issues as centralized methods, and propose Ada to achieve superior convergence rates and comparable accuracy, even at scales up to 1008 GPUs for applications like ResNet50 on ImageNet-1K.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing DBench for benchmarking and Ada for adaptive communication graphs, which cleverly combine existing decentralized learning concepts to enhance scalability and stability in large-scale DNN training. While not introducing an entirely new problem, it advances practical application through these tools, making it a clever adaptation rather than a groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of distributed deep learning, as it provides actionable insights and a new adaptive method for improving training efficiency at scale. However, its influence may be limited to specific applications in decentralized systems rather than broadly across all AI research.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution by offering practical tools and insights for decentralized deep learning, which are essential for researchers focused on scalable AI training. While not essential for all readers, it provides high-quality advancements that warrant attention in the distributed machine learning community.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ef65437e15a8c25518306e4c0127302659b1d9b8",
      "total_authors": 5,
      "authors_found": 3,
      "highest_h_index": 3,
      "average_h_index": 1.3333333333333333,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Bing Xie",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2291509639"
        },
        {
          "name": "Junqi Yin",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhenyu Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380866859"
        },
        {
          "name": "Sarp Oral",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2262454007"
        },
        {
          "name": "Feiyi Wang",
          "h_index": null,
          "profile_url": null
        }
      ]
    }
  ],
  "total_papers": 121,
  "date": "2025-08-31"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Relevance Filter State Management
        let currentRelevanceFilters = {
            highlyRelevant: true,
            moderatelyRelevant: true,
            tangentiallyRelevant: true,
            notRelevant: true
        };
        
        let pendingRelevanceFilters = { ...currentRelevanceFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function calculateRelevanceScore(paper) {
            let score = 0;
            
            // Only consider topics that are currently selected/enabled in the topic filter
            const topicsToConsider = [];
            if (currentTopicFilters.rlhf) topicsToConsider.push('rlhf_relevance');
            if (currentTopicFilters.weakSupervision) topicsToConsider.push('weak_supervision_relevance');
            if (currentTopicFilters.diffusionReasoning) topicsToConsider.push('diffusion_reasoning_relevance');
            if (currentTopicFilters.distributedTraining) topicsToConsider.push('distributed_training_relevance');
            if (currentTopicFilters.datasets) topicsToConsider.push('datasets_relevance');
            
            // If no topics are selected, return 0
            if (topicsToConsider.length === 0) return 0;
            
            // Weighted scoring system
            const relevanceWeights = {
                'Highly Relevant': 4,
                'Moderately Relevant': 3,
                'Tangentially Relevant': 2,
                'Not Relevant': 1
            };
            
            // Sum up scores for selected topics only
            for (let topicField of topicsToConsider) {
                const relevance = paper[topicField];
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = relevance === "not_validated" ? "Not Relevant" : relevance;
                score += relevanceWeights[normalizedRelevance] || 1; // Default to 1 if unknown
            }
            
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'relevance_high':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(b) - calculateRelevanceScore(a));
                    break;
                case 'relevance_low':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(a) - calculateRelevanceScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'relevance_high': 'Relevance (Highest to Lowest)',
                'relevance_low': 'Relevance (Lowest to Highest)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
            updateNoveltyButtonText();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
            updateImpactButtonText();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for relevance checkboxes to sync between mobile and desktop
            document.getElementById('mobile-relevance-highly').addEventListener('change', () => {
                document.getElementById('desktop-relevance-highly').checked = 
                    document.getElementById('mobile-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-moderately').addEventListener('change', () => {
                document.getElementById('desktop-relevance-moderately').checked = 
                    document.getElementById('mobile-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('desktop-relevance-tangentially').checked = 
                    document.getElementById('mobile-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-not').addEventListener('change', () => {
                document.getElementById('desktop-relevance-not').checked = 
                    document.getElementById('mobile-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-highly').addEventListener('change', () => {
                document.getElementById('mobile-relevance-highly').checked = 
                    document.getElementById('desktop-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-moderately').addEventListener('change', () => {
                document.getElementById('mobile-relevance-moderately').checked = 
                    document.getElementById('desktop-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('mobile-relevance-tangentially').checked = 
                    document.getElementById('desktop-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-not').addEventListener('change', () => {
                document.getElementById('mobile-relevance-not').checked = 
                    document.getElementById('desktop-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function resetPendingScoringFilters() {
            pendingScoringFilters = { ...currentScoringFilters };
            syncPendingScoringUI();
            updateScoringButtonText();
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function resetPendingRecommendationFilters() {
            pendingRecommendationFilters = { ...currentRecommendationFilters };
            syncPendingRecommendationUI();
            updateRecommendationButtonText();
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
            updateTopicButtonText();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // RELEVANCE FILTER FUNCTIONS  
        // ============================================================================
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = pendingRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = pendingRelevanceFilters.notRelevant;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = currentRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = currentRelevanceFilters.notRelevant;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            // Read the current checkbox states from the UI (like H-Index filter does)
            const highlyRelevantChecked = document.getElementById('mobile-relevance-highly').checked;
            const moderatelyRelevantChecked = document.getElementById('mobile-relevance-moderately').checked;
            const tangentiallyRelevantChecked = document.getElementById('mobile-relevance-tangentially').checked;
            const notRelevantChecked = document.getElementById('mobile-relevance-not').checked;
            
            const selectedOptions = [];
            if (highlyRelevantChecked) selectedOptions.push("Highly Relevant");
            if (moderatelyRelevantChecked) selectedOptions.push("Moderately Relevant");
            if (tangentiallyRelevantChecked) selectedOptions.push("Tangentially Relevant");
            if (notRelevantChecked) selectedOptions.push("Not Relevant");
            
            const selectionText = selectedOptions.length === 4 ? "All Selected" : 
                                selectedOptions.length === 0 ? "None Selected" : 
                                `${selectedOptions.length} Selected`;
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            // Get values from mobile (primary source)
            const mobileHighly = document.getElementById('mobile-relevance-highly');
            const mobileModerately = document.getElementById('mobile-relevance-moderately');
            const mobileTangentially = document.getElementById('mobile-relevance-tangentially');
            const mobileNot = document.getElementById('mobile-relevance-not');
            
            // Update pending filters from mobile if available, otherwise from desktop
            pendingRelevanceFilters.highlyRelevant = mobileHighly ? mobileHighly.checked : document.getElementById('desktop-relevance-highly').checked;
            pendingRelevanceFilters.moderatelyRelevant = mobileModerately ? mobileModerately.checked : document.getElementById('desktop-relevance-moderately').checked;
            pendingRelevanceFilters.tangentiallyRelevant = mobileTangentially ? mobileTangentially.checked : document.getElementById('desktop-relevance-tangentially').checked;
            pendingRelevanceFilters.notRelevant = mobileNot ? mobileNot.checked : document.getElementById('desktop-relevance-not').checked;
        }
        
        function resetPendingRelevanceFilters() {
            pendingRelevanceFilters = { ...currentRelevanceFilters };
            syncPendingRelevanceUI();
            updateRelevanceButtonText();
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_highly', currentRelevanceFilters.highlyRelevant.toString());
            params.set('relevance_moderately', currentRelevanceFilters.moderatelyRelevant.toString());
            params.set('relevance_tangentially', currentRelevanceFilters.tangentiallyRelevant.toString());
            params.set('relevance_not', currentRelevanceFilters.notRelevant.toString());
            window.history.replaceState({}, '', `${window.location.pathname}?${params.toString()}`);
        }
        
        function loadRelevanceFiltersFromURL() {
            const highlyRelevant = getUrlParameter('relevance_highly');
            const moderatelyRelevant = getUrlParameter('relevance_moderately');
            const tangentiallyRelevant = getUrlParameter('relevance_tangentially');
            const notRelevant = getUrlParameter('relevance_not');
            
            if (highlyRelevant !== null) {
                currentRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
                pendingRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
            }
            if (moderatelyRelevant !== null) {
                currentRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
                pendingRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
            }
            if (tangentiallyRelevant !== null) {
                currentRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
                pendingRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
            }
            if (notRelevant !== null) {
                currentRelevanceFilters.notRelevant = notRelevant === 'true';
                pendingRelevanceFilters.notRelevant = notRelevant === 'true';
            }
            
            syncRelevanceUI();
        }
        
        function passesRelevanceFilter(paper) {
            // Get selected topics
            const selectedTopics = [];
            if (currentTopicFilters.rlhf) selectedTopics.push('rlhf');
            if (currentTopicFilters.weakSupervision) selectedTopics.push('weak_supervision');
            if (currentTopicFilters.diffusionReasoning) selectedTopics.push('diffusion_reasoning');
            if (currentTopicFilters.distributedTraining) selectedTopics.push('distributed_training');
            if (currentTopicFilters.datasets) selectedTopics.push('datasets');
            
            // If no topics selected, skip relevance filtering
            if (selectedTopics.length === 0) return true;
            
            // Get selected relevance levels
            const selectedRelevanceLevels = [];
            if (currentRelevanceFilters.highlyRelevant) selectedRelevanceLevels.push('Highly Relevant');
            if (currentRelevanceFilters.moderatelyRelevant) selectedRelevanceLevels.push('Moderately Relevant');
            if (currentRelevanceFilters.tangentiallyRelevant) selectedRelevanceLevels.push('Tangentially Relevant');
            if (currentRelevanceFilters.notRelevant) selectedRelevanceLevels.push('Not Relevant');
            
            // If no relevance levels selected, show 0 papers
            if (selectedRelevanceLevels.length === 0) return false;
            
            // Check each selected topic
            for (let topic of selectedTopics) {
                const relevanceField = `${topic}_relevance`;
                const paperRelevance = paper[relevanceField];
                
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = paperRelevance === "not_validated" ? "Not Relevant" : paperRelevance;
                
                // If this topic's relevance matches any selected relevance level, paper passes
                if (selectedRelevanceLevels.includes(normalizedRelevance)) {
                    return true; // At least one topic matches
                }
            }
            
            // No selected topics had matching relevance levels
            return false;
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
            
            // Update Relevance
            updateDropdownDisabledState('mobile-relevance-btn', 'mobile-relevance-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-relevance-btn', 'desktop-relevance-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            // Check if there are no papers to display
            if (filteredSortedPapers.length === 0) {
                showNoPapersMessage();
                hidePaginationSections();
                return;
            }
            
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            showPaginationSections();
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        function showNoPapersMessage() {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const noPapersHTML = '<div class="flex items-center justify-center min-h-screen"><h2 class="font-heading text-2xl text-neutral-600">No papers to show</h2></div>';
            
            if (mobileContainer) {
                mobileContainer.innerHTML = noPapersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = noPapersHTML;
            }
        }

        function hidePaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = 'none';
                }
            });
        }

        function showPaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = '';
                }
            });
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            // Find the similarity container in the currently visible layout
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            // Find the justification container in the currently visible layout
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            // Find the normalized similarity container in the currently visible layout
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // QUICK FILTER FUNCTIONS
        // ============================================================================


        function applyQuickFilter(filterType) {
            // Build URL parameters based on filter type
            const url = new URL(window.location.href);
            
            // Clear all existing parameters
            url.search = '';
            
            // Set sort to recommendation best first for all filters
            url.searchParams.set('sort', 'recommend_best');
            
            switch(filterType) {
                case 'must-read':
                    // Recommendation: Only Must Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'true');
                    url.searchParams.set('recommendation_should', 'false');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'should-read':
                    // Recommendation: Only Should Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'false');
                    url.searchParams.set('recommendation_should', 'true');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'rlhf':
                    // Topic: Only RLHF + Relevance: Exclude Not Relevant
                    url.searchParams.set('topic_rlhf', 'true');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'weak-supervision':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'true');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'diffusion-reasoning':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'true');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'distributed-training':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'true');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'datasets':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'true');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'reset':
                    // Clear all parameters, which will reset everything to defaults
                    break;
                    
                default:
                    console.warn('Unknown quick filter type:', filterType);
                    return;
            }
            
            // Navigate to new URL, which will trigger existing URL parsing logic
            window.location.href = url.toString();
        }



        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Load Relevance filters from URL
            loadRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-relevance-dropdown'), button: document.getElementById('mobile-relevance-btn') },
                { dropdown: document.getElementById('desktop-relevance-dropdown'), button: document.getElementById('desktop-relevance-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('relevance')) {
                            resetPendingRelevanceFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        } else if (dropdown.id.includes('recommendation')) {
                            resetPendingRecommendationFilters();
                        } else if (dropdown.id.includes('scoring')) {
                            resetPendingScoringFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
