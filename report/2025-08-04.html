<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 04 August 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: 'rgba(107, 106, 101, 0.7)' // #6B6A65 with 70% opacity
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 04 August 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100 min-h-screen">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="mobile-quick-must-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="mobile-quick-should-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="mobile-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="mobile-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="mobile-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="mobile-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="mobile-quick-datasets" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="mobile-quick-reset" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-highly" checked>
                                            <label for="mobile-relevance-highly"></label>
                                        </div>
                                        <label for="mobile-relevance-highly" class="text-neutral-10 text-xl font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-moderately" checked>
                                            <label for="mobile-relevance-moderately"></label>
                                        </div>
                                        <label for="mobile-relevance-moderately" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-tangentially" checked>
                                            <label for="mobile-relevance-tangentially"></label>
                                        </div>
                                        <label for="mobile-relevance-tangentially" class="text-neutral-10 text-xl font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-not" checked>
                                            <label for="mobile-relevance-not"></label>
                                        </div>
                                        <label for="mobile-relevance-not" class="text-neutral-10 text-xl font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="desktop-quick-must-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="desktop-quick-should-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="desktop-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="desktop-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="desktop-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="desktop-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="desktop-quick-datasets" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="desktop-quick-reset" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-highly" checked>
                                            <label for="desktop-relevance-highly"></label>
                                        </div>
                                        <label for="desktop-relevance-highly" class="text-neutral-10 text-lg font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-moderately" checked>
                                            <label for="desktop-relevance-moderately"></label>
                                        </div>
                                        <label for="desktop-relevance-moderately" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-tangentially" checked>
                                            <label for="desktop-relevance-tangentially"></label>
                                        </div>
                                        <label for="desktop-relevance-tangentially" class="text-neutral-10 text-lg font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-not" checked>
                                            <label for="desktop-relevance-not"></label>
                                        </div>
                                        <label for="desktop-relevance-not" class="text-neutral-10 text-lg font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 04 August 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl min-h-screen">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2508.01961",
      "title": "Kronecker-LoRA: hybrid Kronecker-LoRA adapters for scalable, sustainable\n  fine-tuning",
      "authors": [
        "Yixin Shen"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Fine-tuning massive pre-trained language models across many tasks demands\nadapters that are both parameter-efficient and highly expressive. We introduce\n\\textbf{Kron-LoRA}, a two-stage adapter that first factorizes each frozen\nlinear update as a Kronecker product \\[ \\Delta W = A \\otimes B \\] and then\ncompresses \\[ B \\in \\mathbb{R}^{d_{B2}\\times d_{B1}} \\] via an \\(r\\)-rank LoRA\ndecomposition \\(B \\approx B_{1}B_{2}\\). By leveraging \\[ \\mathrm{rank}(A\n\\otimes B) \\;=\\; \\mathrm{rank}(A)\\,\\mathrm{rank}(B), \\] Kron-LoRA retains the\nexpressivity of the update while using up to $4\\!\\times\\!$ fewer parameters\nthan a standard rank-8 LoRA adapter. Its compact adapter matrices also quantize\nto 8- or 4-bit with less accuracy degradation than LoRA, enabling further\nmemory and storage savings for on-device deployment. We benchmark on DistilBERT\nand Mistral-7B across five tasks (PIQA, HellaSwag, WinoGrande, ARC-Easy,\nARC-Challenge) over multiple epochs of adapter-only tuning: on DistilBERT, an\n840 K-parameter Kron-LoRA matches LoRA-16's performance, and on Mistral-7B, a\n5.7 M-parameter Kron-LoRA rivals LoRA-8 with modest memory savings and only a\n3-8\\% speed overhead. In sequential fine-tuning from ARC-Challenge to ARC-Easy,\nKron-LoRA retains 55.18\\% accuracy versus 53.17\\% for LoRA-8-despite using only\none-quarter of the adapter parameters-underscoring its competitive cross-task\ntransfer performance. By uniting Kronecker structure, low-rank compression,\nquantization-friendliness, and by providing transparent trade-off analysis,\nKron-LoRA offers a scalable, sustainable, and continual-learning-ready solution\nfor multi-task adaptation of large language models.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.01961v1",
      "pdf_url": "http://arxiv.org/pdf/2508.01961v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.44,
      "datasets_score": 0.26,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on parameter-efficient fine-tuning methods, specifically introducing Kron-LoRA for compressing and optimizing adapters in large language models. It discusses techniques like Kronecker products and low-rank decompositions to reduce parameters and improve efficiency, but does not address distributed training, parallel computing, or multi-node strategies for accelerating model training. There is no mention of partitioning data, architecture, or computation across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.01965",
      "title": "From Photons to Physics: Autonomous Indoor Drones and the Future of\n  Objective Property Assessment",
      "authors": [
        "Petteri Teikari",
        "Mike Jarrell",
        "Irene Bandera Moreno",
        "Harri Pesola"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The convergence of autonomous indoor drones with physics-aware sensing\ntechnologies promises to transform property assessment from subjective visual\ninspection to objective, quantitative measurement. This comprehensive review\nexamines the technical foundations enabling this paradigm shift across four\ncritical domains: (1) platform architectures optimized for indoor navigation,\nwhere weight constraints drive innovations in heterogeneous computing,\ncollision-tolerant design, and hierarchical control systems; (2) advanced\nsensing modalities that extend perception beyond human vision, including\nhyperspectral imaging for material identification, polarimetric sensing for\nsurface characterization, and computational imaging with metaphotonics enabling\nradical miniaturization; (3) intelligent autonomy through active reconstruction\nalgorithms, where drones equipped with 3D Gaussian Splatting make strategic\ndecisions about viewpoint selection to maximize information gain within battery\nconstraints; and (4) integration pathways with existing property workflows,\nincluding Building Information Modeling (BIM) systems and industry standards\nlike Uniform Appraisal Dataset (UAD) 3.6.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.01965v1",
      "pdf_url": "http://arxiv.org/pdf/2508.01965v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.318,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.01966",
      "title": "Self-Supervised YOLO: Leveraging Contrastive Learning for\n  Label-Efficient Object Detection",
      "authors": [
        "Manikanta Kotthapalli",
        "Reshma Bhatia",
        "Nainsi Jain"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "One-stage object detectors such as the YOLO family achieve state-of-the-art\nperformance in real-time vision applications but remain heavily reliant on\nlarge-scale labeled datasets for training. In this work, we present a\nsystematic study of contrastive self-supervised learning (SSL) as a means to\nreduce this dependency by pretraining YOLOv5 and YOLOv8 backbones on unlabeled\nimages using the SimCLR framework. Our approach introduces a simple yet\neffective pipeline that adapts YOLO's convolutional backbones as encoders,\nemploys global pooling and projection heads, and optimizes a contrastive loss\nusing augmentations of the COCO unlabeled dataset (120k images). The pretrained\nbackbones are then fine-tuned on a cyclist detection task with limited labeled\ndata. Experimental results show that SSL pretraining leads to consistently\nhigher mAP, faster convergence, and improved precision-recall performance,\nespecially in low-label regimes. For example, our SimCLR-pretrained YOLOv8\nachieves a mAP@50:95 of 0.7663, outperforming its supervised counterpart\ndespite using no annotations during pretraining. These findings establish a\nstrong baseline for applying contrastive SSL to one-stage detectors and\nhighlight the potential of unlabeled data as a scalable resource for\nlabel-efficient object detection.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.01966v1",
      "pdf_url": "http://arxiv.org/pdf/2508.01966v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.447,
      "diffusion_reasoning_score": 0.327,
      "distributed_training_score": 0.359,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on self-supervised learning (SSL) via contrastive methods like SimCLR to pretrain YOLO detectors on unlabeled data, aiming to reduce reliance on large labeled datasets. While this aligns with the broader goal of weak supervision—to minimize the need for perfectly hand-labeled data—the paper does not involve programmatically generating labels from high-level, noisy, or imprecise sources. Instead, it uses data augmentations to create supervisory signals directly from unlabeled images. This makes the paper related in spirit (label efficiency) but not a direct application of weak supervision techniques, hence moderately relevant.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper investigates the use of contrastive self-supervised learning (SSL) with the SimCLR framework to pretrain the backbones of YOLOv5 and YOLOv8 on unlabeled images from the COCO dataset, aiming to reduce reliance on large labeled datasets for object detection. The methodology involves adapting YOLO's convolutional backbones as encoders, applying global pooling and projection heads, and fine-tuning the pretrained models on a cyclist detection task with limited labeled data, resulting in higher mean Average Precision (mAP), faster convergence, and improved precision-recall performance compared to models trained from scratch.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying contrastive SSL to under-explored one-stage detectors like YOLOv5 and YOLOv8, combining existing SSL techniques with YOLO architectures in a new way to enhance label efficiency.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in label-efficient object detection within computer vision subfields, as it provides a strong baseline for applying SSL to popular detectors and could inspire further adaptations in real-world applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a high-quality, practical contribution to SSL for object detection, offering empirical evidence and insights that are valuable for researchers focused on efficient training methods in computer vision.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/4ba1d8e63efd7cfa154b0df6ea097973ac308d73",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Manikanta Kotthapalli",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374486628"
        },
        {
          "name": "Reshma Bhatia",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374487273"
        },
        {
          "name": "Nainsi Jain",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374488578"
        }
      ]
    },
    {
      "id": "2508.01969",
      "title": "Accelerating LLM Reasoning via Early Rejection with Partial Reward\n  Modeling",
      "authors": [
        "Seyyed Saeid Cheshmi",
        "Azal Ahmad Khan",
        "Xinran Wang",
        "Zirui Liu",
        "Ali Anwar"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly relied upon for solving complex\nreasoning tasks in domains such as mathematics, logic, and multi-step question\nanswering. A growing line of work seeks to improve reasoning quality by scaling\ninference time compute particularly through Process Reward Models (PRMs), used\nto reward the reasoning at intermediate steps. While effective, these methods\nintroduce substantial computational overhead, especially when generating large\nnumbers of solutions in parallel. In this paper, we investigate whether PRMs\ncan be used mid-generation to provide early signals that enable the rejection\nof suboptimal candidates before full generation of step is complete. We\nintroduce the hypothesis that PRMs are also Partial Reward Models, meaning that\nthe scores they assign to partially completed reasoning step are predictive of\nfinal output quality. This allows for principled early rejection based on\nintermediate token-level signals. We support this hypothesis both\ntheoretically, by proving that the risk of discarding optimal beams decreases\nexponentially with generation length and empirically, by demonstrating a strong\ncorrelation between partial and final rewards across multiple reward models. On\nmath reasoning benchmarks, our method achieves up to 1.4$\\times$-9$\\times$\nreduction in inference FLOPs without degrading final performance. These results\nsuggest that early rejection is a powerful mechanism for improving the\ncompute-efficiency of reasoning in LLMs.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.01969v1",
      "pdf_url": "http://arxiv.org/pdf/2508.01969v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.455,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.508,
      "distributed_training_score": 0.402,
      "datasets_score": 0.287,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper utilizes Process Reward Models (PRMs), which are similar to reward models in RLHF, as they evaluate intermediate reasoning steps to guide generation. However, the main contribution focuses on inference-time efficiency through early rejection, rather than training or fine-tuning models using human feedback-based reinforcement learning. Thus, it builds on RLHF concepts but does not directly advance RLHF methodologies.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for reasoning tasks. It focuses on using PRMs for early rejection in LLM generation, with no mention of treating chains-of-thought as entities for multi-step correction via diffusion mechanisms.",
      "distributed_training_justification": "The paper addresses inference-time compute efficiency for LLMs through early rejection, not distributed training, parallel computing, or multi-node strategies for accelerating model training. There is no discussion of partitioning data, architecture, or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper aims to enhance the computational efficiency of Large Language Models (LLMs) in reasoning tasks by introducing early rejection using Partial Reward Models, hypothesizing that Process Reward Models can evaluate partially generated sequences to predict final quality. The methodology involves using PRMs mid-generation to score intermediate outputs, allowing for the rejection of suboptimal reasoning paths, supported by theoretical proofs that the risk of discarding optimal trajectories decreases exponentially with partial length and empirical results showing up to 9x reduction in inference FLOPs on benchmarks like AIME and Math-500 without performance degradation.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting existing Process Reward Models for early rejection based on partial evaluations, offering a clever combination of techniques to enhance efficiency in LLM reasoning. While it builds on prior work, it introduces a new hypothesis and validation method that advances the state-of-the-art in a practical way.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to significantly influence future research and commercial applications by reducing computational costs for LLM reasoning, making it more accessible and efficient. Its empirical demonstrations of substantial FLOPs reductions suggest broad applicability in AI optimization.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a strong, valuable contribution with innovative efficiency techniques and solid empirical support, making it essential for researchers focused on LLM optimization and reasoning. It is a high-quality work that advances practical aspects of AI without being groundbreaking enough to be a must-read.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e0f2818afdb766c69659504bc77bff05fb348362",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 6,
      "average_h_index": 3.4,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Seyyed Saeid Cheshmi",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2225621684"
        },
        {
          "name": "A. Khan",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2163674593"
        },
        {
          "name": "Xinran Wang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2154500473"
        },
        {
          "name": "Zirui Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375344176"
        },
        {
          "name": "Ali Anwar",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2297772064"
        }
      ]
    },
    {
      "id": "2508.01977",
      "title": "TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought\n  Reasoning in Language Models",
      "authors": [
        "Fan Gao",
        "Cheng Huang",
        "Nyima Tashi",
        "Yutong Liu",
        "Xiangxiang Wang",
        "Thupten Tsering",
        "Ban Ma-bao",
        "Renzeg Duojie",
        "Gadeng Luosang",
        "Rinchen Dongrub",
        "Dorje Tashi",
        "Xiao Feng",
        "Hao Wang",
        "Yongbin Yu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "To address the severe data scarcity in Tibetan, a low-resource language\nspoken by over six million people, we introduce TIBSTC-CoT, the large-scale,\nmulti-domain Tibetan dataset automatically constructed via chain-of-thought\nprompting with large language models (LLMs). TIBSTC-CoT establishes a scalable\nand reproducible framework for dataset creation in low-resource settings,\ncovering diverse domains and reasoning patterns essential for language\nunderstanding and generation. Building on this dataset, we develop the\nSunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with\nchain-of-thought capabilities. Trained entirely on TIBSTC-CoT,\nSunshine-thinking has demonstrated strong reasoning and generation performance,\ncomparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a\nsignificant step toward inclusive AI by enabling high-quality Tibetan language\nprocessing through both resource creation and model innovation. All data are\navailable: https://github.com/Vicentvankor/sun-shine.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.01977v1",
      "pdf_url": "http://arxiv.org/pdf/2508.01977v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.445,
      "distributed_training_score": 0.383,
      "datasets_score": 0.458,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on creating a Tibetan instruction dataset using chain-of-thought prompting and developing LLMs with CoT capabilities, but it does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. There is no mention of treating reasoning paths as entities for holistic correction over steps, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction and construction of TIBSTC-CoT, a large-scale, multi-domain dataset for Tibetan language processing, including details on curation methodologies, domain coverage, and statistics. This directly aligns with research on creating, analyzing, and evaluating datasets for AI and machine learning applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces TIBSTC-CoT, a large-scale, multi-domain dataset for Tibetan language processing, generated through chain-of-thought prompting with large language models to tackle data scarcity in low-resource languages. By training the Sunshine-thinking family of LLMs on this dataset, the authors demonstrate that these models achieve strong reasoning and generation performance comparable to state-of-the-art multilingual LLMs, thereby providing a scalable framework for advancing NLP in underrepresented languages.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing chain-of-thought prompting techniques to create a dataset for a low-resource language like Tibetan, offering a notable improvement in addressing data scarcity rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work provides a replicable framework for dataset creation in low-resource languages and releases the dataset, likely leading to citations and further developments within the subfield of multilingual and low-resource NLP.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution by addressing a critical gap in low-resource language processing and providing practical resources, making it valuable for researchers in AI and NLP focused on inclusivity and underrepresented languages.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/02a571f2c38ec25ba1a6ff3c6bf2297cd640bced",
      "total_authors": 14,
      "authors_found": 14,
      "highest_h_index": 17,
      "average_h_index": 4.642857142857143,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Fan Gao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2340191210"
        },
        {
          "name": "Cheng Huang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2275607695"
        },
        {
          "name": "N. Tashi",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/4577369"
        },
        {
          "name": "Yutong Liu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2351808095"
        },
        {
          "name": "Xiangxiang Wang",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/50141063"
        },
        {
          "name": "Thupten Tsering",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/1395602706"
        },
        {
          "name": "Ma-bao Ban",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2350513105"
        },
        {
          "name": "Renzeg Duojie",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2350514280"
        },
        {
          "name": "Gadeng Luosang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2338177409"
        },
        {
          "name": "Rinchen Dongrub",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2195041814"
        },
        {
          "name": "Dorje Tashi",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2338177725"
        },
        {
          "name": "Xiao Feng",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2350493590"
        },
        {
          "name": "Hao Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2256769995"
        },
        {
          "name": "Yongbin Yu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2241970724"
        }
      ]
    },
    {
      "id": "2508.01980",
      "title": "On-the-Fly Object-aware Representative Point Selection in Point Cloud",
      "authors": [
        "Xiaoyu Zhang",
        "Ziwei Wang",
        "Hai Dong",
        "Zhifeng Bao",
        "Jiajun Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Point clouds are essential for object modeling and play a critical role in\nassisting driving tasks for autonomous vehicles (AVs). However, the significant\nvolume of data generated by AVs creates challenges for storage, bandwidth, and\nprocessing cost. To tackle these challenges, we propose a representative point\nselection framework for point cloud downsampling, which preserves critical\nobject-related information while effectively filtering out irrelevant\nbackground points. Our method involves two steps: (1) Object Presence\nDetection, where we introduce an unsupervised density peak-based classifier and\na supervised Na\\\"ive Bayes classifier to handle diverse scenarios, and (2)\nSampling Budget Allocation, where we propose a strategy that selects\nobject-relevant points while maintaining a high retention rate of object\ninformation. Extensive experiments on the KITTI and nuScenes datasets\ndemonstrate that our method consistently outperforms state-of-the-art baselines\nin both efficiency and effectiveness across varying sampling rates. As a\nmodel-agnostic solution, our approach integrates seamlessly with diverse\ndownstream models, making it a valuable and scalable addition to the 3D point\ncloud downsampling toolkit for AV applications.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.01980v1",
      "pdf_url": "http://arxiv.org/pdf/2508.01980v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.303,
      "distributed_training_score": 0.365,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.01984",
      "title": "IMoRe: Implicit Program-Guided Reasoning for Human Motion Q&A",
      "authors": [
        "Chen Li",
        "Chinthani Sugandhika",
        "Yeo Keat Ee",
        "Eric Peh",
        "Hao Zhang",
        "Hong Yang",
        "Deepu Rajan",
        "Basura Fernando"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Existing human motion Q\\&A methods rely on explicit program execution, where\nthe requirement for manually defined functional modules may limit the\nscalability and adaptability. To overcome this, we propose an implicit\nprogram-guided motion reasoning (IMoRe) framework that unifies reasoning across\nmultiple query types without manually designed modules. Unlike existing\nimplicit reasoning approaches that infer reasoning operations from question\nwords, our model directly conditions on structured program functions, ensuring\na more precise execution of reasoning steps. Additionally, we introduce a\nprogram-guided reading mechanism, which dynamically selects multi-level motion\nrepresentations from a pretrained motion Vision Transformer (ViT), capturing\nboth high-level semantics and fine-grained motion cues. The reasoning module\niteratively refines memory representations, leveraging structured program\nfunctions to extract relevant information for different query types. Our model\nachieves state-of-the-art performance on Babel-QA and generalizes to a newly\nconstructed motion Q\\&A dataset based on HuMMan, demonstrating its adaptability\nacross different motion reasoning datasets. Code and dataset are available at:\nhttps://github.com/LUNAProject22/IMoRe.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.01984v1",
      "pdf_url": "http://arxiv.org/pdf/2508.01984v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.515,
      "distributed_training_score": 0.278,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an implicit program-guided reasoning framework for human motion Q&A, which uses iterative memory refinement inspired by the MAC model. It does not involve diffusion models or adapt the iterative refinement process of diffusion for logical tasks. There is no mention of treating a Chain-of-Thought as a single entity for holistic correction, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.01987",
      "title": "Controllable and Stealthy Shilling Attacks via Dispersive Latent\n  Diffusion",
      "authors": [
        "Shutong Qiao",
        "Wei Yuan",
        "Junliang Yu",
        "Tong Chen",
        "Quoc Viet Hung Nguyen",
        "Hongzhi Yin"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Recommender systems (RSs) are now fundamental to various online platforms,\nbut their dependence on user-contributed data leaves them vulnerable to\nshilling attacks that can manipulate item rankings by injecting fake users.\nAlthough widely studied, most existing attack models fail to meet two critical\nobjectives simultaneously: achieving strong adversarial promotion of target\nitems while maintaining realistic behavior to evade detection. As a result, the\ntrue severity of shilling threats that manage to reconcile the two objectives\nremains underappreciated. To expose this overlooked vulnerability, we present\nDLDA, a diffusion-based attack framework that can generate highly effective yet\nindistinguishable fake users by enabling fine-grained control over target\npromotion. Specifically, DLDA operates in a pre-aligned collaborative embedding\nspace, where it employs a conditional latent diffusion process to iteratively\nsynthesize fake user profiles with precise target item control. To evade\ndetection, DLDA introduces a dispersive regularization mechanism that promotes\nvariability and realism in generated behavioral patterns. Extensive experiments\non three real-world datasets and five popular RS models demonstrate that,\ncompared to prior attacks, DLDA consistently achieves stronger item promotion\nwhile remaining harder to detect. These results highlight that modern RSs are\nmore vulnerable than previously recognized, underscoring the urgent need for\nmore robust defenses.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.01987v1",
      "pdf_url": "http://arxiv.org/pdf/2508.01987v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.475,
      "distributed_training_score": 0.341,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on shilling attacks in recommender systems using diffusion models to generate fake user profiles, with no mention of human feedback, reward models, or reinforcement learning techniques for aligning AI models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a diffusion model for iterative refinement in generating fake user profiles for attacks, but it does not apply this to multi-step logical reasoning or solving complex logical tasks; instead, it is for data synthesis in recommender system manipulation.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.01994",
      "title": "Deeply Dual Supervised learning for melanoma recognition",
      "authors": [
        "Rujosh Polma",
        "Krishnan Menon Iyer"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "As the application of deep learning in dermatology continues to grow, the\nrecognition of melanoma has garnered significant attention, demonstrating\npotential for improving diagnostic accuracy. Despite advancements in image\nclassification techniques, existing models still face challenges in identifying\nsubtle visual cues that differentiate melanoma from benign lesions. This paper\npresents a novel Deeply Dual Supervised Learning framework that integrates\nlocal and global feature extraction to enhance melanoma recognition. By\nemploying a dual-pathway structure, the model focuses on both fine-grained\nlocal features and broader contextual information, ensuring a comprehensive\nunderstanding of the image content. The framework utilizes a dual attention\nmechanism that dynamically emphasizes critical features, thereby reducing the\nrisk of overlooking subtle characteristics of melanoma. Additionally, we\nintroduce a multi-scale feature aggregation strategy to ensure robust\nperformance across varying image resolutions. Extensive experiments on\nbenchmark datasets demonstrate that our framework significantly outperforms\nstate-of-the-art methods in melanoma detection, achieving higher accuracy and\nbetter resilience against false positives. This work lays the foundation for\nfuture research in automated skin cancer recognition and highlights the\neffectiveness of dual supervised learning in medical image analysis.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.01994v1",
      "pdf_url": "http://arxiv.org/pdf/2508.01994v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.365,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a Deeply Dual Supervised Learning framework for melanoma recognition, focusing on integrating local and global features, dual attention mechanisms, and multi-scale aggregation using benchmark datasets. It relies on standard supervised learning with presumably hand-labeled or high-quality data, as there is no mention of programmatically generating labels from noisy or imprecise sources. Thus, it does not align with weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.01997",
      "title": "DIRF: A Framework for Digital Identity Protection and Clone Governance\n  in Agentic AI Systems",
      "authors": [
        "Hammad Atta",
        "Muhammad Zeeshan Baig",
        "Yasir Mehmood",
        "Nadeem Shahzad",
        "Ken Huang",
        "Muhammad Aziz Ul Haq",
        "Muhammad Awais",
        "Kamal Ahmed",
        "Anthony Green"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.ET (Emerging Technologies)"
      ],
      "abstract": "The rapid advancement and widespread adoption of generative artificial\nintelligence (AI) pose significant threats to the integrity of personal\nidentity, including digital cloning, sophisticated impersonation, and the\nunauthorized monetization of identity-related data. Mitigating these risks\nnecessitates the development of robust AI-generated content detection systems,\nenhanced legal frameworks, and ethical guidelines. This paper introduces the\nDigital Identity Rights Framework (DIRF), a structured security and governance\nmodel designed to protect behavioral, biometric, and personality-based digital\nlikeness attributes to address this critical need. Structured across nine\ndomains and 63 controls, DIRF integrates legal, technical, and hybrid\nenforcement mechanisms to secure digital identity consent, traceability, and\nmonetization. We present the architectural foundations, enforcement strategies,\nand key use cases supporting the need for a unified framework. This work aims\nto inform platform builders, legal entities, and regulators about the essential\ncontrols needed to enforce identity rights in AI-driven systems.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.01997v1",
      "pdf_url": "http://arxiv.org/pdf/2508.01997v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.439,
      "weak_supervision_score": 0.305,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.324,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces the Digital Identity Rights Framework (DIRF), which focuses on security, governance, and ethical controls for protecting digital identities in AI systems, including aspects like consent verification and clone detection. It does not discuss or involve reinforcement learning techniques, human feedback for training AI models, reward models, or any alignment processes related to RLHF. Therefore, the paper's main contribution is unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02000",
      "title": "Localizing Audio-Visual Deepfakes via Hierarchical Boundary Modeling",
      "authors": [
        "Xuanjun Chen",
        "Shih-Peng Cheng",
        "Jiawei Du",
        "Lin Zhang",
        "Xiaoxiao Miao",
        "Chung-Che Wang",
        "Haibin Wu",
        "Hung-yi Lee",
        "Jyh-Shing Roger Jang"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.AS (Audio and Speech Processing)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Audio-visual temporal deepfake localization under the content-driven partial\nmanipulation remains a highly challenging task. In this scenario, the deepfake\nregions are usually only spanning a few frames, with the majority of the rest\nremaining identical to the original. To tackle this, we propose a Hierarchical\nBoundary Modeling Network (HBMNet), which includes three modules: an\nAudio-Visual Feature Encoder that extracts discriminative frame-level\nrepresentations, a Coarse Proposal Generator that predicts candidate boundary\nregions, and a Fine-grained Probabilities Generator that refines these\nproposals using bidirectional boundary-content probabilities. From the modality\nperspective, we enhance audio-visual learning through dedicated encoding and\nfusion, reinforced by frame-level supervision to boost discriminability. From\nthe temporal perspective, HBMNet integrates multi-scale cues and bidirectional\nboundary-content relationships. Experiments show that encoding and fusion\nprimarily improve precision, while frame-level supervision boosts recall. Each\nmodule (audio-visual fusion, temporal scales, bi-directionality) contributes\ncomplementary benefits, collectively enhancing localization performance. HBMNet\noutperforms BA-TFD and UMMAFormer and shows improved potential scalability with\nmore training data.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02000v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02000v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.421,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.37,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on audio-visual deepfake localization using a hierarchical network, involving feature encoding, proposal generation, and fusion techniques. It does not involve reinforcement learning, human feedback, reward models, or any alignment with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a network for detecting and localizing deepfakes, emphasizing audio-visual fusion and boundary modeling, but it does not use diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes as described in diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02003",
      "title": "Fast and Memory-efficient Non-line-of-sight Imaging with Quasi-Fresnel\n  Transform",
      "authors": [
        "Yijun Wei",
        "Jianyu Wang",
        "Leping Xiao",
        "Zuoqiang Shi",
        "Xing Fu",
        "Lingyun Qiu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Non-line-of-sight (NLOS) imaging seeks to reconstruct hidden objects by\nanalyzing reflections from intermediary surfaces. Existing methods typically\nmodel both the measurement data and the hidden scene in three dimensions,\noverlooking the inherently two-dimensional nature of most hidden objects. This\noversight leads to high computational costs and substantial memory consumption,\nlimiting practical applications and making real-time, high-resolution NLOS\nimaging on lightweight devices challenging. In this paper, we introduce a novel\napproach that represents the hidden scene using two-dimensional functions and\nemploys a Quasi-Fresnel transform to establish a direct inversion formula\nbetween the measurement data and the hidden scene. This transformation\nleverages the two-dimensional characteristics of the problem to significantly\nreduce computational complexity and memory requirements. Our algorithm\nefficiently performs fast transformations between these two-dimensional\naggregated data, enabling rapid reconstruction of hidden objects with minimal\nmemory usage. Compared to existing methods, our approach reduces runtime and\nmemory demands by several orders of magnitude while maintaining imaging\nquality. The substantial reduction in memory usage not only enhances\ncomputational efficiency but also enables NLOS imaging on lightweight devices\nsuch as mobile and embedded systems. We anticipate that this method will\nfacilitate real-time, high-resolution NLOS imaging and broaden its\napplicability across a wider range of platforms.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02003v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02003v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.243,
      "weak_supervision_score": 0.266,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.291,
      "datasets_score": 0.189,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02004",
      "title": "Devil is in the Detail: Towards Injecting Fine Details of Image Prompt\n  in Image Generation via Conflict-free Guidance and Stratified Attention",
      "authors": [
        "Kyungmin Jo",
        "Jooyeol Yun",
        "Jaegul Choo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "While large-scale text-to-image diffusion models enable the generation of\nhigh-quality, diverse images from text prompts, these prompts struggle to\ncapture intricate details, such as textures, preventing the user intent from\nbeing reflected. This limitation has led to efforts to generate images\nconditioned on user-provided images, referred to as image prompts. Recent work\nmodifies the self-attention mechanism to impose image conditions in generated\nimages by replacing or concatenating the keys and values from the image prompt.\nThis enables the self-attention layer to work like a cross-attention layer,\ngenerally used to incorporate text prompts. In this paper, we identify two\ncommon issues in existing methods of modifying self-attention to generate\nimages that reflect the details of image prompts. First, existing approaches\nneglect the importance of image prompts in classifier-free guidance.\nSpecifically, current methods use image prompts as both desired and undesired\nconditions in classifier-free guidance, causing conflicting signals. To resolve\nthis, we propose conflict-free guidance by using image prompts only as desired\nconditions, ensuring that the generated image faithfully reflects the image\nprompt. In addition, we observe that the two most common self-attention\nmodifications involve a trade-off between the realism of the generated image\nand alignment with the image prompt. Specifically, selecting more keys and\nvalues from the image prompt improves alignment, while selecting more from the\ngenerated image enhances realism. To balance both, we propose an new\nself-attention modification method, Stratified Attention to jointly use keys\nand values from both images rather than selecting between them. Through\nextensive experiments across three image generation tasks, we show that the\nproposed method outperforms existing image-prompting models in faithfully\nreflecting the image prompt.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02004v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02004v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.491,
      "distributed_training_score": 0.306,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing text-to-image diffusion models for better image generation by addressing issues in self-attention and guidance mechanisms, specifically for incorporating image prompts. It does not involve adapting the diffusion process for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks, which are central to the topic. Therefore, there is no relevant component for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02016",
      "title": "Dynamic Context Adaptation for Consistent Role-Playing Agents with\n  Retrieval-Augmented Generations",
      "authors": [
        "Jeiyoon Park",
        "Yongshin Han",
        "Minseop Kim",
        "Kisu Yang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We propose AMADEUS, which is composed of Adaptive Context-aware Text Splitter\n(ACTS), Guided Selection (GS), and Attribute Extractor (AE). ACTS finds an\noptimal chunk length and hierarchical contexts for each character. AE\nidentifies a character's general attributes from the chunks retrieved by GS and\nuses these attributes as a final context to maintain robust persona consistency\neven when answering out of knowledge questions. To facilitate the development\nand evaluation of RAG-based RPAs, we construct CharacterRAG, a role-playing\ndataset that consists of persona documents for 15 distinct fictional characters\ntotaling 976K written characters, and 450 question and answer pairs. We find\nthat our framework effectively models not only the knowledge possessed by\ncharacters, but also various attributes such as personality.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02016v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02016v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.395,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.307,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution focuses on Retrieval-Augmented Generation (RAG) for role-playing agents, including components like Adaptive Context-aware Text Splitter, Guided Selection, and Attribute Extractor to maintain persona consistency. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. There is no mention of adapting diffusion techniques for complex logical tasks or treating Chain-of-Thought as a holistically refined entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02018",
      "title": "SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models",
      "authors": [
        "Wanqi Yang",
        "Yanda Li",
        "Yunchao Wei",
        "Meng Fang",
        "Ling Chen"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large audio-language models (LALMs) have achieved near-human performance in\nsentence-level transcription and emotion recognition. However, existing\nevaluations focus mainly on surface-level perception, leaving the capacity of\nmodels for contextual and inference-driven reasoning in speech-based scenarios\ninsufficiently examined. To address this gap, we introduce SpeechR, a unified\nbenchmark for evaluating reasoning over speech in large audio-language models.\nSpeechR evaluates models along three key dimensions: factual retrieval,\nprocedural inference, and normative judgment. It includes three distinct\nevaluation formats. The multiple-choice version measures answer selection\naccuracy. The generative version assesses the coherence and logical consistency\nof reasoning chains. The acoustic-feature version investigates whether\nvariations in stress and emotion affect reasoning performance. Evaluations on\neleven state-of-the-art LALMs reveal that high transcription accuracy does not\ntranslate into strong reasoning capabilities. SpeechR establishes a structured\nbenchmark for evaluating reasoning in spoken language, enabling more targeted\nanalysis of model capabilities across diverse dialogue-based tasks.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02018v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02018v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.534,
      "distributed_training_score": 0.324,
      "datasets_score": 0.399,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a benchmark for evaluating speech reasoning in large audio-language models but does not discuss training methods, human feedback, or reinforcement learning techniques. It focuses solely on assessment, not on aligning models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates reasoning capabilities in large audio-language models through benchmarks like multiple-choice and generative tasks, but it does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02028",
      "title": "Bench2ADVLM: A Closed-Loop Benchmark for Vision-language Models in\n  Autonomous Driving",
      "authors": [
        "Tianyuan Zhang",
        "Ting Jin",
        "Lu Wang",
        "Jiangfan Liu",
        "Siyuan Liang",
        "Mingchuan Zhang",
        "Aishan Liu",
        "Xianglong Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision-Language Models (VLMs) have recently emerged as a promising paradigm\nin autonomous driving (AD). However, current performance evaluation protocols\nfor VLM-based AD systems (ADVLMs) are predominantly confined to open-loop\nsettings with static inputs, neglecting the more realistic and informative\nclosed-loop setting that captures interactive behavior, feedback resilience,\nand real-world safety. To address this, we introduce Bench2ADVLM, a unified\nhierarchical closed-loop evaluation framework for real-time, interactive\nassessment of ADVLMs across both simulation and physical platforms. Inspired by\ndual-process theories of cognition, we first adapt diverse ADVLMs to simulation\nenvironments via a dual-system adaptation architecture. In this design,\nheterogeneous high-level driving commands generated by target ADVLMs (fast\nsystem) are interpreted by a general-purpose VLM (slow system) into\nstandardized mid-level control actions suitable for execution in simulation. To\nbridge the gap between simulation and reality, we design a physical control\nabstraction layer that translates these mid-level actions into low-level\nactuation signals, enabling, for the first time, closed-loop testing of ADVLMs\non physical vehicles. To enable more comprehensive evaluation, Bench2ADVLM\nintroduces a self-reflective scenario generation module that automatically\nexplores model behavior and uncovers potential failure modes for\nsafety-critical scenario generation. Overall, Bench2ADVLM establishes a\nhierarchical evaluation pipeline that seamlessly integrates high-level abstract\nreasoning, mid-level simulation actions, and low-level real-world execution.\nExperiments on diverse scenarios across multiple state-of-the-art ADVLMs and\nphysical platforms validate the diagnostic strength of our framework, revealing\nthat existing ADVLMs still exhibit limited performance under closed-loop\nconditions.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02028v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02028v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.384,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a benchmark for evaluating Vision-Language Models in autonomous driving through closed-loop testing and a dual-system architecture, but it does not involve any training or fine-tuning processes using human feedback, reward models, or reinforcement learning to align models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a dual-system adaptation for translating high-level commands but does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning for holistic correction of a Chain-of-Thought. It focuses on evaluation frameworks rather than diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02029",
      "title": "A Confidence-Diversity Framework for Calibrating AI Judgement in\n  Accessible Qualitative Coding Tasks",
      "authors": [
        "Zhilong Zhao",
        "Yindi Liu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "LLMs enable qualitative coding at large scale, but assessing reliability\nremains challenging where human experts seldom agree. We investigate\nconfidence-diversity calibration as a quality assessment framework for\naccessible coding tasks where LLMs already demonstrate strong performance but\nexhibit overconfidence. Analysing 5,680 coding decisions from eight\nstate-of-the-art LLMs across ten categories, we find that mean self-confidence\ntracks inter-model agreement closely (Pearson r=0.82). Adding model diversity\nquantified as normalised Shannon entropy produces a dual signal explaining\nagreement almost completely (R-squared=0.979), though this high predictive\npower likely reflects task simplicity for current LLMs. The framework enables a\nthree-tier workflow auto-accepting 35 percent of segments with less than 5\npercent error, cutting manual effort by 65 percent. Cross-domain validation\nconfirms transferability (kappa improvements of 0.20 to 0.78). While\nestablishing a methodological foundation for AI judgement calibration, the true\npotential likely lies in more challenging scenarios where LLMs may demonstrate\ncomparative advantages over human cognitive limitations.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02029v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02029v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.484,
      "weak_supervision_score": 0.477,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.372,
      "datasets_score": 0.43,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on calibrating AI judgements for qualitative coding using confidence and diversity measures, without any involvement in training models with human feedback or reinforcement learning techniques. It does not align with RLHF, which specifically involves fine-tuning models using a reward model based on human-ranked data.",
      "weak_supervision_justification": "The paper involves handling potentially noisy or uncertain labels through inter-model agreement and diversity, which could loosely relate to weak supervision's use of imprecise sources for labeling. However, it does not primarily focus on programmatically generating training labels or training models with weak supervision; instead, it emphasizes calibration for existing AI outputs.",
      "diffusion_reasoning_justification": "The paper discusses LLMs and chain-of-thought prompting for qualitative coding but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based methods. Its core is confidence-diversity calibration, not adapting diffusion for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper analyzes and evaluates datasets, such as 5,680 coding decisions from LLMs and six public datasets for cross-domain validation, which involves benchmarking and analysis. While this supports the paper's framework, the main contribution is methodological calibration rather than focused dataset creation, curation, or comprehensive benchmarking.",
      "llm_score_status": "completed",
      "summary": "This paper proposes a confidence-diversity framework to calibrate AI judgments in qualitative coding tasks, addressing the challenge of LLM overconfidence where human experts often disagree. By analyzing 5,680 coding decisions from eight state-of-the-art LLMs across ten categories, the authors demonstrate that mean self-confidence correlates strongly with inter-model agreement (Pearson r=0.82), and combining it with model diversity (normalized Shannon entropy) predicts agreement nearly perfectly (R-squared=0.979), enabling a workflow that auto-accepts 35% of segments with less than 5% error and reduces manual effort by 65%, with cross-domain validation confirming its transferability.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing ideas of model confidence and diversity into a new framework for qualitative coding, effectively addressing overconfidence in LLMs without introducing a entirely novel problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI-assisted qualitative research, as it provides a practical method to reduce manual effort and improve reliability, though its influence may be limited to accessible tasks rather than broader applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a high-quality contribution with practical implications for human-AI collaboration in qualitative coding, making it valuable for researchers in AI and social sciences to understand and potentially apply.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e7a3f689c1159f99933ec0cf6378771de00a28ee",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zhilong Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374769462"
        },
        {
          "name": "Yindi Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374498267"
        }
      ]
    },
    {
      "id": "2508.02034",
      "title": "Protego: User-Centric Pose-Invariant Privacy Protection Against Face\n  Recognition-Induced Digital Footprint Exposure",
      "authors": [
        "Ziling Wang",
        "Shuya Yang",
        "Jialin Lu",
        "Ka-Ho Chow"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Face recognition (FR) technologies are increasingly used to power large-scale\nimage retrieval systems, raising serious privacy concerns. Services like\nClearview AI and PimEyes allow anyone to upload a facial photo and retrieve a\nlarge amount of online content associated with that person. This not only\nenables identity inference but also exposes their digital footprint, such as\nsocial media activity, private photos, and news reports, often without their\nconsent. In response to this emerging threat, we propose Protego, a\nuser-centric privacy protection method that safeguards facial images from such\nretrieval-based privacy intrusions. Protego encapsulates a user's 3D facial\nsignatures into a pose-invariant 2D representation, which is dynamically\ndeformed into a natural-looking 3D mask tailored to the pose and expression of\nany facial image of the user, and applied prior to online sharing. Motivated by\na critical limitation of existing methods, Protego amplifies the sensitivity of\nFR models so that protected images cannot be matched even among themselves.\nExperiments show that Protego significantly reduces retrieval accuracy across a\nwide range of black-box FR models and performs at least 2x better than existing\nmethods. It also offers unprecedented visual coherence, particularly in video\nsettings where consistency and natural appearance are essential. Overall,\nProtego contributes to the fight against the misuse of FR for mass surveillance\nand unsolicited identity tracing.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02034v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02034v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.295,
      "distributed_training_score": 0.302,
      "datasets_score": 0.272,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02037",
      "title": "Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a\n  Time",
      "authors": [
        "Huihan Li",
        "You Chen",
        "Siyuan Wang",
        "Yixin He",
        "Ninareh Mehrabi",
        "Rahul Gupta",
        "Xiang Ren"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) perform well on reasoning benchmarks but often\nfail when inputs alter slightly, raising concerns about the extent to which\ntheir success relies on memorization. This issue is especially acute in\nChain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger\nintermediate errors that cascade into incorrect final answers. We introduce\nSTIM, a novel framework for Source-aware Token-level Identification of\nMemorization, which attributes each token in a reasoning chain to one of\nmultiple memorization sources - local, mid-range, or long-range - based on\ntheir statistical co-occurrence with the token in the pretraining corpus. Our\ntoken-level analysis across tasks and distributional settings reveals that\nmodels rely more on memorization in complex or long-tail cases, and that local\nmemorization is often the dominant driver of errors, leading to up to 67% of\nwrong tokens. We also show that memorization scores from STIM can be effective\nin predicting the wrong tokens in the wrong reasoning step. STIM offers a\npowerful tool for diagnosing and improving model reasoning and can generalize\nto other structured step-wise generation tasks.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02037v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02037v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.542,
      "distributed_training_score": 0.348,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework called STIM for identifying memorization in Chain-of-Thought reasoning at the token level, focusing on LLMs and their reliance on memorized patterns. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical tasks. While it discusses Chain-of-Thought, there is no component of multi-step logical reasoning using a diffusion model, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02043",
      "title": "Conditional Diffusion Model with Anatomical-Dose Dual Constraints for\n  End-to-End Multi-Tumor Dose Prediction",
      "authors": [
        "Hui Xie",
        "Haiqin Hu",
        "Lijuan Ding",
        "Qing Li",
        "Yue Sun",
        "Tao Tan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Radiotherapy treatment planning often relies on time-consuming,\ntrial-and-error adjustments that heavily depend on the expertise of\nspecialists, while existing deep learning methods face limitations in\ngeneralization, prediction accuracy, and clinical applicability. To tackle\nthese challenges, we propose ADDiff-Dose, an Anatomical-Dose Dual Constraints\nConditional Diffusion Model for end-to-end multi-tumor dose prediction. The\nmodel employs LightweightVAE3D to compress high-dimensional CT data and\nintegrates multimodal inputs, including target and organ-at-risk (OAR) masks\nand beam parameters, within a progressive noise addition and denoising\nframework. It incorporates conditional features via a multi-head attention\nmechanism and utilizes a composite loss function combining MSE, conditional\nterms, and KL divergence to ensure both dosimetric accuracy and compliance with\nclinical constraints. Evaluation on a large-scale public dataset (2,877 cases)\nand three external institutional cohorts (450 cases in total) demonstrates that\nADDiff-Dose significantly outperforms traditional baselines, achieving an MAE\nof 0.101-0.154 (compared to 0.316 for UNet and 0.169 for GAN models), a DICE\ncoefficient of 0.927 (a 6.8% improvement), and limiting spinal cord maximum\ndose error to within 0.1 Gy. The average plan generation time per case is\nreduced to 22 seconds. Ablation studies confirm that the structural encoder\nenhances compliance with clinical dose constraints by 28.5%. To our knowledge,\nthis is the first study to introduce a conditional diffusion model framework\nfor radiotherapy dose prediction, offering a generalizable and efficient\nsolution for automated treatment planning across diverse tumor sites, with the\npotential to substantially reduce planning time and improve clinical workflow\nefficiency.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02043v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02043v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.542,
      "distributed_training_score": 0.37,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the application of a conditional diffusion model for generating radiotherapy dose predictions, using iterative denoising to refine medical image data. This focuses on generative tasks in a clinical context, such as dose distribution optimization, and does not involve adapting diffusion models for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02044",
      "title": "Graph Unlearning via Embedding Reconstruction -- A Range-Null Space\n  Decomposition Approach",
      "authors": [
        "Hang Yin",
        "Zipeng Liu",
        "Xiaoyong Peng",
        "Liyao Xiang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.SI (Social and Information Networks)"
      ],
      "abstract": "Graph unlearning is tailored for GNNs to handle widespread and various graph\nstructure unlearning requests, which remain largely unexplored. The GIF (graph\ninfluence function) achieves validity under partial edge unlearning, but faces\nchallenges in dealing with more disturbing node unlearning. To avoid the\noverhead of retraining and realize the model utility of unlearning, we proposed\na novel node unlearning method to reverse the process of aggregation in GNN by\nembedding reconstruction and to adopt Range-Null Space Decomposition for the\nnodes' interaction learning. Experimental results on multiple representative\ndatasets demonstrate the SOTA performance of our proposed approach.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02044v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02044v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.369,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02047",
      "title": "Mapillary Vistas Validation for Fine-Grained Traffic Signs: A Benchmark\n  Revealing Vision-Language Model Limitations",
      "authors": [
        "Sparsh Garg",
        "Abhishek Aich"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Obtaining high-quality fine-grained annotations for traffic signs is critical\nfor accurate and safe decision-making in autonomous driving. Widely used\ndatasets, such as Mapillary, often provide only coarse-grained labels - without\ndistinguishing semantically important types such as stop signs or speed limit\nsigns. To this end, we present a new validation set for traffic signs derived\nfrom the Mapillary dataset called Mapillary Vistas Validation for Traffic Signs\n(MVV), where we decompose composite traffic signs into granular, semantically\nmeaningful categories. The dataset includes pixel-level instance masks and has\nbeen manually annotated by expert annotators to ensure label fidelity. Further,\nwe benchmark several state-of-the-art VLMs against the self-supervised DINOv2\nmodel on this dataset and show that DINOv2 consistently outperforms all VLM\nbaselines-not only on traffic sign recognition, but also on heavily represented\ncategories like vehicles and humans. Our analysis reveals significant\nlimitations in current vision-language models for fine-grained visual\nunderstanding and establishes DINOv2 as a strong baseline for dense semantic\nmatching in autonomous driving scenarios. This dataset and evaluation framework\npave the way for more reliable, interpretable, and scalable perception systems.\n  Code and data are available at: https://github.com/nec-labs-ma/relabeling",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02047v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02047v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.364,
      "datasets_score": 0.435,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves creating a new dataset (Mapillary Vistas Validation for Traffic Signs, or MVV) by relabeling and annotating traffic signs with fine-grained categories, which directly aligns with dataset creation and curation. It also includes benchmarking and evaluating models on this dataset, fitting the topic's focus on dataset introduction, curation methodologies, benchmark evaluation, and analysis for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces the Mapillary Vistas Validation for Traffic Signs (MVV) dataset, derived from the Mapillary dataset, featuring fine-grained annotations for traffic signs across 11 semantically meaningful categories to address the limitations of coarse-grained labels in autonomous driving applications. The authors benchmark state-of-the-art vision-language models (VLMs) like InternVL-3 and Gemma-3 against the self-supervised DINOv2 model on tasks such as traffic sign recognition, demonstrating that DINOv2 significantly outperforms VLMs in fine-grained visual understanding, thereby revealing critical shortcomings in VLMs for safety-critical perception and establishing a new benchmark for future research.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a new fine-grained dataset from an existing one and benchmarking models in a targeted way, combining established techniques to highlight VLM limitations, though it does not introduce a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of computer vision for autonomous driving, as it provides a new benchmark dataset and insights into VLM performance, potentially influencing research on fine-grained recognition.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution by introducing a valuable dataset and demonstrating key limitations in current models, making it important for researchers in autonomous driving and computer vision to be aware of for advancing perception systems.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/4a7524d7c0b3c3a184961bc7eaca179586a417d1",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 10,
      "average_h_index": 5.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Sparsh Garg",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374483754"
        },
        {
          "name": "Abhishek Aich",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2274692"
        }
      ]
    },
    {
      "id": "2508.02049",
      "title": "Epi$^2$-Net: Advancing Epidemic Dynamics Forecasting with\n  Physics-Inspired Neural Networks",
      "authors": [
        "Rui Sun",
        "Chenghua Gong",
        "Tianjun Gu",
        "Yuhao Zheng",
        "Jie Ding",
        "Juyuan Zhang",
        "Liming Pan",
        "Linyuan Lü"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Advancing epidemic dynamics forecasting is vital for targeted interventions\nand safeguarding public health. Current approaches mainly fall into two\ncategories: mechanism-based and data-driven models. Mechanism-based models are\nconstrained by predefined compartmental structures and oversimplified system\nassumptions, limiting their ability to model complex real-world dynamics, while\ndata-driven models focus solely on intrinsic data dependencies without physical\nor epidemiological constraints, risking biased or misleading representations.\nAlthough recent studies have attempted to integrate epidemiological knowledge\ninto neural architectures, most of them fail to reconcile explicit physical\npriors with neural representations. To overcome these obstacles, we introduce\nEpi$^2$-Net, a Epidemic Forecasting Framework built upon Physics-Inspired\nNeural Networks. Specifically, we propose reconceptualizing epidemic\ntransmission from the physical transport perspective, introducing the concept\nof neural epidemic transport. Further, we present a physic-inspired deep\nlearning framework, and integrate physical constraints with neural modules to\nmodel spatio-temporal patterns of epidemic dynamics. Experiments on real-world\ndatasets have demonstrated that Epi$^2$-Net outperforms state-of-the-art\nmethods in epidemic forecasting, providing a promising solution for future\nepidemic containment. The code is available at:\nhttps://anonymous.4open.science/r/Epi-2-Net-48CE.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02049v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02049v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.289,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.349,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02051",
      "title": "HCF: Hierarchical Cascade Framework for Distributed Multi-Stage Image\n  Compression",
      "authors": [
        "Junhao Cai",
        "Taegun An",
        "Chengjun Jin",
        "Sung Il Choi",
        "JuHyun Park",
        "Changhee Joo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Distributed multi-stage image compression -- where visual content traverses\nmultiple processing nodes under varying quality requirements -- poses\nchallenges. Progressive methods enable bitstream truncation but underutilize\navailable compute resources; successive compression repeats costly pixel-domain\noperations and suffers cumulative quality loss and inefficiency;\nfixed-parameter models lack post-encoding flexibility. In this work, we\ndeveloped the Hierarchical Cascade Framework (HCF) that achieves high\nrate-distortion performance and better computational efficiency through direct\nlatent-space transformations across network nodes in distributed multi-stage\nimage compression system. Under HCF, we introduced policy-driven quantization\ncontrol to optimize rate-distortion trade-offs, and established the edge\nquantization principle through differential entropy analysis. The configuration\nbased on this principle demonstrates up to 0.6dB PSNR gains over other\nconfigurations. When comprehensively evaluated on the Kodak, CLIC, and\nCLIC2020-mobile datasets, HCF outperforms successive-compression methods by up\nto 5.56% BD-Rate in PSNR on CLIC, while saving up to 97.8% FLOPs, 96.5% GPU\nmemory, and 90.0% execution time. It also outperforms state-of-the-art\nprogressive compression methods by up to 12.64% BD-Rate on Kodak and enables\nretraining-free cross-quality adaptation with 7.13-10.87% BD-Rate reductions on\nCLIC2020-mobile.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02051v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02051v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.29,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.455,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on a framework for distributed multi-stage image compression, involving computations across network nodes for efficiency in compression tasks. While it mentions distributed computational resources, it does not address distributed training of machine learning models, such as partitioning data or models for training acceleration. Thus, it is only loosely related through the general concept of distributed computing.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02054",
      "title": "Enhancement of Quantum Semi-Supervised Learning via Improved Laplacian\n  and Poisson Methods",
      "authors": [
        "Hamed Gholipour",
        "Farid Bozorgnia",
        "Hamzeh Mohammadigheymasi",
        "Kailash Hambarde",
        "Javier Mancilla",
        "Hugo Proenca",
        "Joao Neves",
        "Moharram Challenger"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper develops a hybrid quantum approach for graph-based semi-supervised\nlearning to enhance performance in scenarios where labeled data is scarce. We\nintroduce two enhanced quantum models, the Improved Laplacian Quantum\nSemi-Supervised Learning (ILQSSL) and the Improved Poisson Quantum\nSemi-Supervised Learning (IPQSSL), that incorporate advanced label propagation\nstrategies within variational quantum circuits. These models utilize QR\ndecomposition to embed graph structure directly into quantum states, thereby\nenabling more effective learning in low-label settings. We validate our methods\nacross four benchmark datasets like Iris, Wine, Heart Disease, and German\nCredit Card -- and show that both ILQSSL and IPQSSL consistently outperform\nleading classical semi-supervised learning algorithms, particularly under\nlimited supervision. Beyond standard performance metrics, we examine the effect\nof circuit depth and qubit count on learning quality by analyzing entanglement\nentropy and Randomized Benchmarking (RB). Our results suggest that while some\nlevel of entanglement improves the model's ability to generalize, increased\ncircuit complexity may introduce noise that undermines performance on current\nquantum hardware. Overall, the study highlights the potential of\nquantum-enhanced models for semi-supervised learning, offering practical\ninsights into how quantum circuits can be designed to balance expressivity and\nstability. These findings support the role of quantum machine learning in\nadvancing data-efficient classification, especially in applications constrained\nby label availability and hardware limitations.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02054v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02054v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.446,
      "diffusion_reasoning_score": 0.333,
      "distributed_training_score": 0.375,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on quantum-enhanced semi-supervised learning (SSL) with scarce labeled data, which shares the broad goal of weak supervision in dealing with limited or imperfect labeling. However, it primarily addresses label propagation through graph-based quantum methods rather than programmatically generating large quantities of noisy or imprecise labels, making the connection indirect.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02056",
      "title": "StarPose: 3D Human Pose Estimation via Spatial-Temporal Autoregressive\n  Diffusion",
      "authors": [
        "Haoxin Yang",
        "Weihong Chen",
        "Xuemiao Xu",
        "Cheng Xu",
        "Peng Xiao",
        "Cuifeng Sun",
        "Shaoyu Huang",
        "Shengfeng He"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Monocular 3D human pose estimation remains a challenging task due to inherent\ndepth ambiguities and occlusions. Compared to traditional methods based on\nTransformers or Convolutional Neural Networks (CNNs), recent diffusion-based\napproaches have shown superior performance, leveraging their probabilistic\nnature and high-fidelity generation capabilities. However, these methods often\nfail to account for the spatial and temporal correlations across predicted\nframes, resulting in limited temporal consistency and inferior accuracy in\npredicted 3D pose sequences. To address these shortcomings, this paper proposes\nStarPose, an autoregressive diffusion framework that effectively incorporates\nhistorical 3D pose predictions and spatial-temporal physical guidance to\nsignificantly enhance both the accuracy and temporal coherence of pose\npredictions. Unlike existing approaches, StarPose models the 2D-to-3D pose\nmapping as an autoregressive diffusion process. By synergically integrating\npreviously predicted 3D poses with 2D pose inputs via a Historical Pose\nIntegration Module (HPIM), the framework generates rich and informative\nhistorical pose embeddings that guide subsequent denoising steps, ensuring\ntemporally consistent predictions. In addition, a fully plug-and-play\nSpatial-Temporal Physical Guidance (STPG) mechanism is tailored to refine the\ndenoising process in an iterative manner, which further enforces spatial\nanatomical plausibility and temporal motion dynamics, rendering robust and\nrealistic pose estimates. Extensive experiments on benchmark datasets\ndemonstrate that StarPose outperforms state-of-the-art methods, achieving\nsuperior accuracy and temporal consistency in 3D human pose estimation. Code is\navailable at https://github.com/wileychan/StarPose.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02056v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02056v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.306,
      "diffusion_reasoning_score": 0.477,
      "distributed_training_score": 0.349,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion models for iterative refinement in 3D human pose estimation, incorporating historical poses and physical guidance to improve sequences over time. While this involves multi-step processes similar to diffusion's iterative nature, it focuses on generative modeling for spatial-temporal data in computer vision, not on solving complex logical tasks or chain-of-thought reasoning as defined. Thus, it shares a conceptual link through iterative refinement but lacks direct application to logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02062",
      "title": "RICL: Adding In-Context Adaptability to Pre-Trained\n  Vision-Language-Action Models",
      "authors": [
        "Kaustubh Sridhar",
        "Souradeep Dutta",
        "Dinesh Jayaraman",
        "Insup Lee"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multi-task ``vision-language-action'' (VLA) models have recently demonstrated\nincreasing promise as generalist foundation models for robotics, achieving\nnon-trivial performance out of the box on new tasks in new environments.\nHowever, for such models to be truly useful, an end user must have easy means\nto teach them to improve. For language and vision models, the emergent ability\nto perform in-context learning (ICL) has proven to be a versatile and highly\nuseful interface to easily teach new tasks with no parameter finetuning.\nUnfortunately, VLAs pre-trained with imitation learning objectives do not\nnaturally acquire ICL abilities. In this paper, we demonstrate that, with the\nright finetuning recipe and a small robot demonstration dataset, it is possible\nto inject in-context adaptability post hoc into such a VLA. After retraining\nfor in-context learning (RICL), our system permits an end user to provide a\nsmall number (10-20) of demonstrations for a new task. RICL then fetches the\nmost relevant portions of those demonstrations into the VLA context to exploit\nICL, performing the new task and boosting task performance. We apply RICL to\ninject ICL into the $\\pi_{0}$-FAST VLA, and show that it permits large\nin-context improvements for a variety of new manipulation tasks with only 20\ndemonstrations per task, without any parameter updates. When parameter updates\non the target task demonstrations is possible, RICL finetuning further boosts\nperformance. We release code and model weights for RICL-$\\pi_{0}$-FAST\nalongside the paper to enable, for the first time, a simple in-context learning\ninterface for new manipulation tasks. Website: https://ricl-vla.github.io.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02062v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02062v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.464,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.357,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on adding in-context learning to pre-trained Vision-Language-Action (VLA) models using imitation learning and retrieval-augmented techniques, without involving human feedback, reward models, or reinforcement learning for alignment. There is no mention of training a reward model or using RL to fine-tune based on human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper applies RICL to models like π₀-FAST, which may be related to diffusion-based models (e.g., π₀-diffusion is mentioned), but it does not focus on adapting diffusion for multi-step logical reasoning or chain-of-thought refinement. Instead, it emphasizes in-context learning for robotics tasks, so the connection is indirect and not central to the contribution.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02063",
      "title": "TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to\n  Training-Time Belief Sources in LLMs",
      "authors": [
        "Amitava Das",
        "Vinija Jain",
        "Aman Chadha"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) fine-tuned to align with human values often\nexhibit alignment drift, producing unsafe or policy-violating completions when\nexposed to adversarial prompts, decoding perturbations, or paraphrased\njailbreaks. While prior work has behaviorally characterized alignment failure,\nlittle is known about the training-time belief sources underlying these\nfailures. We introduce TraceAlign, a unified framework for tracing unsafe\ncompletions back to their root causes in the model's training corpus. Central\nto our approach is the Belief Conflict Index (BCI), which quantifies semantic\ninconsistency between generated spans and aligned policies, based on retrieved\ntraining documents using suffix-array matching. We propose three complementary\ninterventions: (i) TraceShield, an inference-time safety filter that refuses\ncompletions with high-BCI spans, (ii) Contrastive Belief Deconfliction Loss, a\ncontrastive fine-tuning objective penalizing high-BCI continuations during DPO,\nand (iii) Prov-Decode, a provenance-aware decoding strategy that vetoes beam\nexpansions predicted to yield high-BCI spans. Together, these defenses reduce\nalignment drift by up to 85% on our curated Alignment Drift Benchmark (ADB)\nwhile preserving utility on standard tasks, with delta less than 0.2 and\nimproved refusal quality. We further derive a theoretical upper bound on drift\nlikelihood via suffix-array span statistics, linking memorization frequency and\nlength to adversarial reactivation risk. TraceAlign thus provides the first\nscalable, traceable, and grounded toolkit for understanding and mitigating\nalignment failures at source. To encourage further exploration and development,\nwe open-source our implementation at:\nhttps://anonymous.4open.science/r/tracealign-2DA7",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02063v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02063v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.477,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.391,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper references RLHF as a method used to align LLMs, noting that models fine-tuned with RLHF can still experience alignment drift. It introduces a new fine-tuning objective, Contrastive Belief Deconfliction Loss, which is applied during DPO (a variant of RLHF), to mitigate issues. However, the primary contribution is on tracing and addressing alignment failures, not on advancing RLHF techniques themselves.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on tracing alignment failures in LLMs through tools like TraceAlign and Belief Conflict Index, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. Its contributions are centered on alignment drift and safety, not reasoning mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces TraceAlign, a framework designed to attribute alignment failures in large language models (LLMs) to specific training-time belief sources by using tools like OLMOTRACE and the Belief Conflict Index (BCI) to quantify semantic inconsistencies. It proposes three interventions—TraceShield for inference-time filtering, Contrastive Belief Deconfliction Loss for fine-tuning, and Prov-Decode for provenance-aware decoding—to reduce alignment drift by up to 85% on a curated benchmark while preserving model utility, and it provides a theoretical bound linking training data memorization to drift risk.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework and metric (TraceAlign and BCI) that advances the state-of-the-art by tracing alignment failures directly to training-time sources, moving beyond behavioral analysis to causal attribution.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research in LLM safety and alignment, as well as commercial applications by providing scalable tools to mitigate adversarial risks.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This is a high-quality paper with significant contributions to AI alignment and practical interventions, making it essential for researchers focused on LLM robustness and safety.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/9bb52bb4bb85fb20e6ba81cfe9f4d6cface7803c",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 11,
      "average_h_index": 9.0,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Amitava Das",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2258322706"
        },
        {
          "name": "Vinija Jain",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2212131028"
        },
        {
          "name": "Aman Chadha",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2275226689"
        }
      ]
    },
    {
      "id": "2508.02066",
      "title": "MolReasoner: Toward Effective and Interpretable Reasoning for Molecular\n  LLMs",
      "authors": [
        "Guojiang Zhao",
        "Sihang Li",
        "Zixiang Lu",
        "Zheng Cheng",
        "Haitao Lin",
        "Lirong Wu",
        "Hanchen Xia",
        "Hengxing Cai",
        "Wentao Guo",
        "Hongshuai Wang",
        "Mingjun Xu",
        "Siyu Zhu",
        "Guolin Ke",
        "Linfeng Zhang",
        "Zhifeng Gao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Large Language Models(LLMs) have demonstrated remarkable performance across\nvarious domains, yet their capabilities in molecular reasoning remain\ninsufficiently explored. Current approaches tend to rely heavily on\ngeneral-purpose prompting, which lacks domain-specific molecular semantics,\nwhile those that use fine-tuning strategies often face challenges with\ninterpretability and reasoning depth. To address these issues, we introduce\nMolReasoner, a two-stage framework designed to transition LLMs from\nmemorization towards chemical reasoning. First, we propose Mol-SFT, which\ninitializes the model's reasoning abilities via synthetic Chain-of-Thought(CoT)\nsamples generated by GPT-4o and verified for chemical accuracy. Subsequently,\nMol-RL applies reinforcement learning with specialized reward functions\ndesigned explicitly to align chemical structures with linguistic descriptions,\nthereby enhancing molecular reasoning capabilities. Our approach notably\nenhances interpretability, improving the model 's molecular understanding and\nenabling better generalization. Extensive experiments demonstrate that\nMolReasoner outperforms existing methods, and marking a significant shift from\nmemorization-based outputs to robust chemical reasoning.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02066v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02066v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.591,
      "distributed_training_score": 0.329,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces MolReasoner, a framework using Supervised Fine-Tuning (Mol-SFT) with Chain-of-Thought samples and Reinforcement Learning (Mol-RL) to enhance molecular reasoning in LLMs. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity over multiple steps, as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02067",
      "title": "YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection\n  Innovations and Challenges",
      "authors": [
        "Manikanta Kotthapalli",
        "Deepika Ravipati",
        "Reshma Bhatia"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Over the past decade, object detection has advanced significantly, with the\nYOLO (You Only Look Once) family of models transforming the landscape of\nreal-time vision applications through unified, end-to-end detection frameworks.\nFrom YOLOv1's pioneering regression-based detection to the latest YOLOv9, each\nversion has systematically enhanced the balance between speed, accuracy, and\ndeployment efficiency through continuous architectural and algorithmic\nadvancements.. Beyond core object detection, modern YOLO architectures have\nexpanded to support tasks such as instance segmentation, pose estimation,\nobject tracking, and domain-specific applications including medical imaging and\nindustrial automation. This paper offers a comprehensive review of the YOLO\nfamily, highlighting architectural innovations, performance benchmarks,\nextended capabilities, and real-world use cases. We critically analyze the\nevolution of YOLO models and discuss emerging research directions that extend\ntheir impact across diverse computer vision domains.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02067v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02067v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.283,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.338,
      "distributed_training_score": 0.332,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02069",
      "title": "SpikeSTAG: Spatial-Temporal Forecasting via GNN-SNN Collaboration",
      "authors": [
        "Bang Hu",
        "Changze Lv",
        "Mingjie Li",
        "Yunpeng Liu",
        "Xiaoqing Zheng",
        "Fengzhe Zhang",
        "Wei cao",
        "Fan Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Spiking neural networks (SNNs), inspired by the spiking behavior of\nbiological neurons, offer a distinctive approach for capturing the complexities\nof temporal data. However, their potential for spatial modeling in multivariate\ntime-series forecasting remains largely unexplored. To bridge this gap, we\nintroduce a brand new SNN architecture, which is among the first to seamlessly\nintegrate graph structural learning with spike-based temporal processing for\nmultivariate time-series forecasting. Specifically, we first embed time\nfeatures and an adaptive matrix, eliminating the need for predefined graph\nstructures. We then further learn sequence features through the Observation\n(OBS) Block. Building upon this, our Multi-Scale Spike Aggregation (MSSA)\nhierarchically aggregates neighborhood information through spiking SAGE layers,\nenabling multi-hop feature extraction while eliminating the need for\nfloating-point operations. Finally, we propose a Dual-Path Spike Fusion (DSF)\nBlock to integrate spatial graph features and temporal dynamics via a\nspike-gated mechanism, combining LSTM-processed sequences with spiking\nself-attention outputs, effectively improve the model accuracy of long sequence\ndatasets. Experiments show that our model surpasses the state-of-the-art\nSNN-based iSpikformer on all datasets and outperforms traditional temporal\nmodels at long horizons, thereby establishing a new paradigm for efficient\nspatial-temporal modeling.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02069v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02069v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.404,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on integrating GNNs and SNNs for spatial-temporal forecasting in multivariate time-series data, emphasizing spike-based processing and graph learning. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought correction, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper's main contribution is a novel SNN architecture for efficient spatial-temporal modeling, including energy-efficient operations like eliminating floating-point computations. However, it does not discuss distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors, so it is not relevant to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02073",
      "title": "Risk identification based on similar case retrieval enhancement,",
      "authors": [
        "Jiawei Li",
        "Chengye Yang",
        "Yaochen Zhang",
        "Weilin Sun",
        "Lei Meng",
        "Xiangxu Meng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The goal of construction site risk and hazard identification is to enhance\nsafety management through automation. Existing research based on large language\nmodels falls into two categories: image-text matching for collaborative\nreasoning, which struggles with complex hazard features, and instruction\nfine-tuning or dialogue guidance using professional datasets, which suffers\nfrom high training costs and poor generalization.To address this, we propose a\nhazard identification method using similar case retrieval enhancement. By\nintegrating external knowledge and retrieved case contexts via prompt\nfine-tuning, we mitigate misjudgments caused by limited domain knowledge and\nweak feature associations. Our method includes three modules: retrieval\nlibrary, image similarity retrieval, and large model retrieval enhancement,\nenabling efficient recognition without training. Experiments on real\nconstruction data show significant improvements. For instance, GLM-4V's\nrecognition accuracy increased to 50\\%, a 35.49\\% boost. The method enhances\naccuracy, context understanding, and stability, offering new theoretical and\ntechnical support for hazard detection.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02073v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02073v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.306,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a hazard identification method using similar case retrieval enhancement, prompt fine-tuning, and image similarity for large language models in construction safety. It does not involve human feedback, training a reward model on human-ranked data, or reinforcement learning for model alignment, which are core elements of RLHF. Thus, there is no connection to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02076",
      "title": "Everyone Contributes! Incentivizing Strategic Cooperation in Multi-LLM\n  Systems via Sequential Public Goods Games",
      "authors": [
        "Yunhao Liang",
        "Yuan Qu",
        "Jingyuan Yang",
        "Shaochong Lin",
        "Zuo-Jun Max Shen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.GT (Computer Science and Game Theory)"
      ],
      "abstract": "Coordinating multiple large language models (LLMs) to solve complex tasks\ncollaboratively poses a fundamental trade-off between the computation costs and\ncollective performance compared with individual model. We introduce a novel,\ngame-theoretically grounded reinforcement learning (RL) framework, the\nMulti-Agent Cooperation Sequential Public Goods Game (MAC-SPGG), to\nsystematically incentivize cooperation in multi-LLM ensembles. In MAC-SPGG, LLM\nagents move in sequence, observing predecessors' outputs and updating beliefs\nto condition their own contributions. By redesigning the public-goods reward,\neffortful contributions become the unique Subgame Perfect Nash Equilibrium\n(SPNE), which eliminates free-riding under traditional SPGG or PGG. Its\nsequential protocol replaces costly round-based information exchanges with a\nstreamlined decision flow, cutting communication overhead while retaining\nstrategic depth. We prove the existence and uniqueness of the SPNE under\nrealistic parameters, and empirically show that MAC-SPGG-trained ensembles\noutperform single-agent baselines, chain-of-thought prompting, and other\ncooperative methods, even achieving comparable performance to large-scale\nmodels across reasoning, math, code generation, and NLP tasks. Our results\nhighlight the power of structured, incentive-aligned MAC-SPGG cooperation for\nscalable and robust multi-agent language generation.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02076v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02076v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.437,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.415,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes a reinforcement learning framework based on game theory and public goods games to coordinate multiple LLMs, but it does not involve human feedback, human-ranked data, or a reward model trained on human preferences. Instead, rewards are derived from the game's structure, making it standard RL rather than RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses sequential decision-making and iterative refinement in multi-LLM coordination via public goods games, but it does not adapt diffusion models or their iterative processes for reasoning tasks. There is no mention of treating a chain-of-thought as a single entity for holistic correction through diffusion-like steps.",
      "distributed_training_justification": "The paper involves multi-agent LLM systems that coordinate sequentially to reduce communication overhead, which could indirectly relate to distributed computing concepts. However, its main focus is on game-theoretic cooperation for task-solving, not on algorithms or systems for accelerating model training across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02079",
      "title": "AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided\n  Decomposition and Riemannian-Geodesic Collision Regularization",
      "authors": [
        "Amitava Das",
        "Abhilekh Borah",
        "Vinija Jain",
        "Aman Chadha"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Low-rank adaptation (LoRA) has become a standard tool for efficiently\nfine-tuning large language models (LLMs). Yet, even minor LoRA updates can\ninduce alignment drift, weakening safety and behavioral constraints through\nentangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL),\na principled framework for preserving alignment during finetuning. AGL\nintroduces several key components: a primary task loss for supervision, Fisher\nInformation Matrix-based regularization to restrict updates in\nalignment-sensitive subspaces, and task-specific regularization to stabilize\nthe integration of new knowledge. We further introduce collision-aware\nregularization, blending Riemannian overlap -- which penalizes coordinate-wise\ninterference -- and geodesic separation -- which encourages disjoint update\ngeometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and\nunsafe prompts designed to quantify alignment drift and safety degradation.\nEmpirical evaluations show that AGL mitigates alignment drift by up to 50% on\nsafety-critical benchmarks without degrading downstream task performance.\nComprehensive ablation confirms that each component contributes distinctly to\npreserving latent safety behaviors. Finally, we derive and validate a scaling\nlaw for catastrophic forgetting, revealing that AGL flattens post-finetuning\nloss escalation while preserving adaptation dynamics. AGL is a structurally\ngrounded refinement of LoRA, ensuring alignment preservation with minimal\ntrade-offs. To encourage further exploration and development, we open-source\nour implementation.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02079v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02079v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.512,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.418,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses preserving alignment in LLMs during fine-tuning, which is often achieved through RLHF in related works, but it does not involve training a reward model, human-ranked data, or reinforcement learning. Instead, it focuses on regularization techniques like Fisher-guided decomposition to maintain existing alignment, making the connection indirect.",
      "weak_supervision_justification": "The paper's main contribution is a fine-tuning framework with regularizations to preserve alignment, and it does not address generating labels from noisy or programmatic sources. There is no mention of weak supervision techniques or reliance on imperfect labeling methods.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on LoRA-based fine-tuning with alignment-preserving regularizations and does not discuss parallel computing, multi-node setups, or strategies for partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02082",
      "title": "S-RRG-Bench: Structured Radiology Report Generation with Fine-Grained\n  Evaluation Framework",
      "authors": [
        "Yingshu Li",
        "Yunyi Liu",
        "Zhanyu Wang",
        "Xinyu Liang",
        "Lingqiao Liu",
        "Lei Wang",
        "Luping Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Radiology report generation (RRG) for diagnostic images, such as chest\nX-rays, plays a pivotal role in both clinical practice and AI. Traditional\nfree-text reports suffer from redundancy and inconsistent language,\ncomplicating the extraction of critical clinical details. Structured radiology\nreport generation (S-RRG) offers a promising solution by organizing information\ninto standardized, concise formats. However, existing approaches often rely on\nclassification or visual question answering (VQA) pipelines that require\npredefined label sets and produce only fragmented outputs. Template-based\napproaches, which generate reports by replacing keywords within fixed sentence\npatterns, further compromise expressiveness and often omit clinically important\ndetails. In this work, we present a novel approach to S-RRG that includes\ndataset construction, model training, and the introduction of a new evaluation\nframework. We first create a robust chest X-ray dataset (MIMIC-STRUC) that\nincludes disease names, severity levels, probabilities, and anatomical\nlocations, ensuring that the dataset is both clinically relevant and\nwell-structured. We train an LLM-based model to generate standardized,\nhigh-quality reports. To assess the generated reports, we propose a specialized\nevaluation metric (S-Score) that not only measures disease prediction accuracy\nbut also evaluates the precision of disease-specific details, thus offering a\nclinically meaningful metric for report quality that focuses on elements\ncritical to clinical decision-making and demonstrates a stronger alignment with\nhuman assessments. Our approach highlights the effectiveness of structured\nreports and the importance of a tailored evaluation metric for S-RRG, providing\na more clinically relevant measure of report quality.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02082v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02082v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.304,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on structured radiology report generation using large language models (LLMs), including dataset creation, model training for JSON outputs, and a new evaluation metric. It does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02084",
      "title": "SSBD Ontology: A Two-Tier Approach for Interoperable Bioimaging Metadata",
      "authors": [
        "Yuki Yamagata",
        "Koji Kyoda",
        "Hiroya Itoga",
        "Emi Fujisawa",
        "Shuichi Onami"
      ],
      "categories": [
        "cs.DL (Digital Libraries)",
        "cs.AI (Artificial Intelligence)",
        "cs.DB (Databases)"
      ],
      "abstract": "Advanced bioimaging technologies have enabled the large-scale acquisition of\nmultidimensional data, yet effective metadata management and interoperability\nremain significant challenges. To address these issues, we propose a new\nontology-driven framework for the Systems Science of Biological Dynamics\nDatabase (SSBD) that adopts a two-tier architecture. The core layer provides a\nclass-centric structure referencing existing biomedical ontologies, supporting\nboth SSBD:repository -- which focuses on rapid dataset publication with minimal\nmetadata -- and SSBD:database, which is enhanced with biological and\nimaging-related annotations. Meanwhile, the instance layer represents actual\nimaging dataset information as Resource Description Framework individuals that\nare explicitly linked to the core classes. This layered approach aligns\nflexible instance data with robust ontological classes, enabling seamless\nintegration and advanced semantic queries. By coupling flexibility with rigor,\nthe SSBD Ontology promotes interoperability, data reuse, and the discovery of\nnovel biological mechanisms. Moreover, our solution aligns with the Recommended\nMetadata for Biological Images guidelines and fosters compatibility.\nUltimately, our approach contributes to establishing a Findable, Accessible,\nInteroperable, and Reusable data ecosystem within the bioimaging community.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02084v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02084v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.225,
      "weak_supervision_score": 0.285,
      "diffusion_reasoning_score": 0.267,
      "distributed_training_score": 0.271,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02085",
      "title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning\n  with LLM-Based Agents",
      "authors": [
        "Jiaye Lin",
        "Yifu Guo",
        "Yuzhen Han",
        "Sen Hu",
        "Ziyi Ni",
        "Licheng Wang",
        "Mingguang Chen",
        "Hongzhang Liu",
        "Ronghao Chen",
        "Yangfan He",
        "Daxin Jiang",
        "Binxing Jiao",
        "Chen Hu",
        "Huacan Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Model (LLM)-based agents have recently shown impressive\ncapabilities in complex reasoning and tool use via multi-step interactions with\ntheir environments. While these agents have the potential to tackle complicated\ntasks, their problem-solving process, i.e., agents' interaction trajectory\nleading to task completion, remains underexploited. These trajectories contain\nrich feedback that can navigate agents toward the right directions for solving\nproblems correctly. Although prevailing approaches, such as Monte Carlo Tree\nSearch (MCTS), can effectively balance exploration and exploitation, they\nignore the interdependence among various trajectories and lack the diversity of\nsearch spaces, which leads to redundant reasoning and suboptimal outcomes. To\naddress these challenges, we propose SE-Agent, a Self-Evolution framework that\nenables Agents to optimize their reasoning processes iteratively. Our approach\nrevisits and enhances former pilot trajectories through three key operations:\nrevision, recombination, and refinement. This evolutionary mechanism enables\ntwo critical advantages: (1) it expands the search space beyond local optima by\nintelligently exploring diverse solution paths guided by previous trajectories,\nand (2) it leverages cross-trajectory inspiration to efficiently enhance\nperformance while mitigating the impact of suboptimal reasoning paths. Through\nthese mechanisms, SE-Agent achieves continuous self-evolution that\nincrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench\nVerified to resolve real-world GitHub issues. Experimental results across five\nstrong LLMs show that integrating SE-Agent delivers up to 55% relative\nimprovement, achieving state-of-the-art performance among all open-source\nagents on SWE-bench Verified. Our code and demonstration materials are publicly\navailable at https://github.com/JARVIS-Xs/SE-Agent.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02085v4",
      "pdf_url": "http://arxiv.org/pdf/2508.02085v4",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.512,
      "distributed_training_score": 0.349,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces SE-Agent, a framework for iteratively optimizing LLM agent trajectories through operations like revision, recombination, and refinement. It does not involve human feedback, a reward model trained on human-ranked data, or reinforcement learning techniques for alignment, focusing instead on autonomous trajectory evolution.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes an iterative process for refining agent trajectories in multi-step reasoning, but it does not adapt diffusion models or their refinement processes. There is no mention of treating Chain-of-Thought as a holistic entity for correction via diffusion-like mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02091",
      "title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest\n  Neighbor Search",
      "authors": [
        "Xiaoya Li",
        "Xiaofei Sun",
        "Albert Wang",
        "Chris Shum",
        "Jiwei Li"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.DB (Databases)"
      ],
      "abstract": "Approximate nearest-neighbor search (ANNS) algorithms have become\nincreasingly critical for recent AI applications, particularly in\nretrieval-augmented generation (RAG) and agent-based LLM applications. In this\npaper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS\noptimization as a reinforcement learning problem where execution speed serves\nas the reward signal. This approach enables the automatic generation of\nprogressively faster ANNS implementations while maintaining accuracy\nconstraints. Our experimental evaluation demonstrates CRINN's effectiveness\nacross six widely-used NNS benchmark datasets. When compared against\nstate-of-the-art open-source ANNS algorithms, CRINN achieves best performance\non three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and\nGloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean\nand GloVe-25-angular). The implications of CRINN's success reach well beyond\nANNS optimization: It validates that LLMs augmented with reinforcement learning\ncan function as an effective tool for automating sophisticated algorithmic\noptimizations that demand specialized knowledge and labor-intensive manual\nrefinement. Code can be found at https://github.com/deepreinforce-ai/CRINN",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02091v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02091v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.443,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.37,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is CRINN, a framework that uses contrastive reinforcement learning to optimize approximate nearest-neighbor search (ANNS) algorithms, with rewards based on execution speed metrics rather than human preferences. RLHF specifically requires training a reward model on human-ranked data to align AI models with human feedback, which is not present in this work. While the paper employs reinforcement learning, it relies on automated performance metrics, making it distinct from RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02092",
      "title": "FPEdit: Robust LLM Fingerprinting through Localized Knowledge Editing",
      "authors": [
        "Shida Wang",
        "Chaohu Liu",
        "Yubo Wang",
        "Linli Xu"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models represent significant investments in computation, data,\nand engineering expertise, making them extraordinarily valuable intellectual\nassets. Nevertheless, these AI assets remain vulnerable to unauthorized\nredistribution and commercial exploitation through fine-tuning or black-box\ndeployment. Current fingerprinting approaches face a fundamental trade-off:\nintrinsic methods require full parameter access, while backdoor-based\ntechniques employ statistically anomalous triggers easily detected and filtered\nby adversaries. To address these limitations, we introduce FPEdit, a novel\nknowledge-editing framework that injects semantically coherent natural language\nfingerprints by modifying a sparse subset of model weights. This ensures\nstealthy and precise ownership encoding without degrading the core\nfunctionality. Extensive experiments show that FPEdit achieves $95$-$100\\%$\nfingerprint retention under both full-parameter fine-tuning and\nparameter-efficient adaptation, while preserving performance on 24 downstream\nbenchmarks. Moreover, FPEdit remains robust under quantization, pruning, and\nstochastic decoding, and can embed 10 fingerprint pairs into LLaMA2-7B in under\n10 minutes using less than 32 GB of GPU memory, a $70\\%$ reduction in resource\nrequirements compared to existing techniques. These advances establish FPEdit\nas the first fingerprinting approach to simultaneously achieve robustness\nagainst adaptation, resistance to detection, and preservation of model utility,\nproviding a minimally invasive solution for reliable provenance verification of\nlarge language models in adversarial deployment scenarios.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02092v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02092v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.436,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.377,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a fingerprinting framework for LLMs using localized knowledge editing to protect intellectual property, with no mention of reinforcement learning, human feedback, reward models, or aligning models with human preferences. It focuses on techniques like fine-tuning and knowledge editing, which are unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02093",
      "title": "\"Stack It Up!\": 3D Stable Structure Generation from 2D Hand-drawn Sketch",
      "authors": [
        "Yiqing Xu",
        "Linfeng Li",
        "Cunjun Yu",
        "David Hsu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Imagine a child sketching the Eiffel Tower and asking a robot to bring it to\nlife. Today's robot manipulation systems can't act on such sketches\ndirectly-they require precise 3D block poses as goals, which in turn demand\nstructural analysis and expert tools like CAD. We present StackItUp, a system\nthat enables non-experts to specify complex 3D structures using only 2D\nfront-view hand-drawn sketches. StackItUp introduces an abstract relation graph\nto bridge the gap between rough sketches and accurate 3D block arrangements,\ncapturing the symbolic geometric relations (e.g., left-of) and stability\npatterns (e.g., two-pillar-bridge) while discarding noisy metric details from\nsketches. It then grounds this graph to 3D poses using compositional diffusion\nmodels and iteratively updates it by predicting hidden internal and rear\nsupports-critical for stability but absent from the sketch. Evaluated on\nsketches of iconic landmarks and modern house designs, StackItUp consistently\nproduces stable, multilevel 3D structures and outperforms all baselines in both\nstability and visual resemblance.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02093v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02093v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.307,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Moderately Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses compositional diffusion models for generating 3D poses from an abstract relation graph, involving iterative updates based on stability checks and predictions of hidden supports. This aligns somewhat with the iterative refinement process of diffusion models, as the system performs multi-step processes to achieve a stable arrangement. However, the diffusion models are primarily applied to generative tasks (e.g., pose generation) rather than directly solving complex logical tasks or treating a full Chain-of-Thought as a holistic entity for reasoning. Thus, while there is an iterative component, it does not fully embody multi-step logical reasoning as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "StackItUp is a system that enables non-experts to generate stable 3D structures for robot manipulation from simple 2D front-view hand-drawn sketches, addressing challenges like metric imprecision and missing supports by using an abstract relation graph to capture geometric relations and stability patterns. The methodology involves extracting the graph from sketches, grounding it to 3D poses via compositional diffusion models, and iteratively updating it to ensure stability, with evaluations showing superior performance in stability and visual resemblance compared to baselines on diverse architectural sketches.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel abstract relation graph combined with compositional diffusion models to bridge 2D sketches and 3D structures, significantly advancing state-of-the-art in robot manipulation from imprecise inputs.",
      "impact_score": "High",
      "impact_justification": "This work has the potential to influence a wide range of applications in robotics and AI, such as making robot systems more accessible for everyday users and creative tasks, likely leading to broader adoption and further research.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a strong, innovative contribution to AI and robotics that addresses practical challenges in human-robot interaction, making it valuable for researchers to read and consider for future developments.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e2b26e57433e077ac078f526927b4d974b81ba50",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 5,
      "average_h_index": 2.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yiqing Xu",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2188781980"
        },
        {
          "name": "Linfeng Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2299487259"
        },
        {
          "name": "Cunjun Yu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2322989065"
        },
        {
          "name": "David Hsu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2257179736"
        }
      ]
    },
    {
      "id": "2508.02095",
      "title": "VLM4D: Towards Spatiotemporal Awareness in Vision Language Models",
      "authors": [
        "Shijie Zhou",
        "Alexander Vilesov",
        "Xuehai He",
        "Ziyu Wan",
        "Shuwang Zhang",
        "Aditya Nagachandra",
        "Di Chang",
        "Dongdong Chen",
        "Xin Eric Wang",
        "Achuta Kadambi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Vision language models (VLMs) have shown remarkable capabilities in\nintegrating linguistic and visual reasoning but remain fundamentally limited in\nunderstanding dynamic spatiotemporal interactions. Humans effortlessly track\nand reason about object movements, rotations, and perspective shifts-abilities\nessential for robust dynamic real-world understanding yet notably lacking in\ncurrent VLMs. In this paper, we introduce VLM4D, the first benchmark\nspecifically designed to evaluate the spatiotemporal reasoning capabilities of\nVLMs. Our benchmark comprises diverse real-world and synthetic videos\naccompanied by carefully curated question-answer pairs emphasizing\ntranslational and rotational motions, perspective awareness, and motion\ncontinuity. Through comprehensive evaluations of state-of-the-art open and\nclosed-source VLMs, we identify significant performance gaps compared to human\nbaselines, highlighting fundamental deficiencies in existing models. Extensive\nanalysis reveals that VLMs struggle particularly with integrating multiple\nvisual cues and maintaining temporal coherence. We further explore promising\ndirections, such as leveraging 4D feature field reconstruction and targeted\nspatiotemporal supervised fine-tuning, demonstrating their effectiveness in\nenhancing spatiotemporal comprehension. Our work aims to encourage deeper\nexploration into improving VLMs' spatial and temporal grounding, paving the way\ntowards more capable and reliable visual intelligence for dynamic environments.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02095v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02095v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.338,
      "datasets_score": 0.409,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a benchmark for spatiotemporal reasoning in VLMs and analyzing their limitations, without any mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. It discusses 4D feature reconstruction and fine-tuning, but these are unrelated to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of the VLM4D benchmark, which involves creating and curating a new dataset of real-world and synthetic videos with spatiotemporal QA annotations. It details dataset composition, generation pipelines, and evaluations, directly aligning with research on dataset creation, analysis, and benchmarking for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces VLM4D, a novel benchmark designed to evaluate the spatiotemporal reasoning capabilities of Vision Language Models (VLMs) using a dataset of diverse real-world and synthetic videos with curated question-answer pairs focusing on motions, perspectives, and temporal coherence. Through comprehensive evaluations of state-of-the-art VLMs, it reveals significant performance gaps compared to humans, particularly in integrating visual cues and maintaining temporal awareness, and suggests promising improvements like 4D feature reconstruction and targeted fine-tuning to enhance dynamic scene understanding.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark for evaluating spatiotemporal reasoning in VLMs, which addresses a previously underexplored limitation and significantly advances the state-of-the-art in vision-language model assessment.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research in VLMs by providing a standardized benchmark and highlighting critical deficiencies, potentially leading to broader advancements in dynamic visual intelligence and commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with a new benchmark and insightful analysis that is valuable for researchers in computer vision and AI to understand and improve spatiotemporal reasoning in VLMs.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f4619d5918d7a25866dfe36749a22290636adf72",
      "total_authors": 10,
      "authors_found": 10,
      "highest_h_index": 26,
      "average_h_index": 4.5,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Shijie Zhou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2340931651"
        },
        {
          "name": "Alexander Vilesov",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2178674684"
        },
        {
          "name": "Xuehai He",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2149253467"
        },
        {
          "name": "Ziyu Wan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374483963"
        },
        {
          "name": "Shuwang Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2352211743"
        },
        {
          "name": "Aditya Nagachandra",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374482303"
        },
        {
          "name": "Di Chang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375275588"
        },
        {
          "name": "Dongdong Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375217108"
        },
        {
          "name": "Xin Eric Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2300489334"
        },
        {
          "name": "Achuta Kadambi",
          "h_index": 26,
          "profile_url": "https://www.semanticscholar.org/author/2425405"
        }
      ]
    },
    {
      "id": "2508.02096",
      "title": "Evaluating User Experience in Conversational Recommender Systems: A\n  Systematic Review Across Classical and LLM-Powered Approaches",
      "authors": [
        "Raj Mahmud",
        "Yufeng Wu",
        "Abdullah Bin Sawad",
        "Shlomo Berkovsky",
        "Mukesh Prasad",
        "A. Baki Kocaballi"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Conversational Recommender Systems (CRSs) are receiving growing research\nattention across domains, yet their user experience (UX) evaluation remains\nlimited. Existing reviews largely overlook empirical UX studies, particularly\nin adaptive and large language model (LLM)-based CRSs. To address this gap, we\nconducted a systematic review following PRISMA guidelines, synthesising 23\nempirical studies published between 2017 and 2025. We analysed how UX has been\nconceptualised, measured, and shaped by domain, adaptivity, and LLM. Our\nfindings reveal persistent limitations: post hoc surveys dominate, turn-level\naffective UX constructs are rarely assessed, and adaptive behaviours are seldom\nlinked to UX outcomes. LLM-based CRSs introduce further challenges, including\nepistemic opacity and verbosity, yet evaluations infrequently address these\nissues. We contribute a structured synthesis of UX metrics, a comparative\nanalysis of adaptive and nonadaptive systems, and a forward-looking agenda for\nLLM-aware UX evaluation. These findings support the development of more\ntransparent, engaging, and user-centred CRS evaluation practices.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02096v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02096v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.305,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.239,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02104",
      "title": "REACT-KD: Region-Aware Cross-modal Topological Knowledge Distillation\n  for Interpretable Medical Image Classification",
      "authors": [
        "Hongzhao Chen",
        "Hexiao Ding",
        "Yufeng Jiang",
        "Jing Lan",
        "Ka Chun Li",
        "Gerald W. Y. Cheng",
        "Sam Ng",
        "Chi Lai Ho",
        "Jing Cai",
        "Liang-ting Lin",
        "Jung Sun Yoo"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Reliable and interpretable tumor classification from clinical imaging remains\na core challenge due to heterogeneous modality quality, limited annotations,\nand the lack of structured anatomical guidance. We introduce REACT-KD, a\nRegion-Aware Cross-modal Topological Knowledge Distillation framework that\ntransfers rich supervision from high-fidelity multi-modal sources into a\nlightweight CT-based student model. The framework uses a dual teacher design:\none branch captures structure-function relationships using dual-tracer PET/CT,\nand the other models dose-aware features through synthetically degraded\nlow-dose CT data. These branches jointly guide the student model through two\ncomplementary objectives. The first focuses on semantic alignment via logits\ndistillation, while the second models anatomical topology using region graph\ndistillation. A shared CBAM-3D module is employed to maintain consistent\nattention across modalities. To improve reliability for deployment, REACT-KD\nintroduces modality dropout during training, allowing inference under partial\nor noisy inputs. The staging task for hepatocellular carcinoma (HCC) is\nconducted as a case study. REACT-KD achieves an average AUC of 93.4% on an\ninternal PET/CT cohort and maintains 76.6% to 81.5% AUC across varying dose\nlevels in external CT testing. Decision curve analysis shows that REACT-KD\nconsistently provides the highest clinical benefit across decision thresholds,\nsupporting its potential in real-world diagnostics. Code is available at\nhttps://github.com/Kinetics-JOJO/REACT-KD.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02104v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02104v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.365,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a knowledge distillation framework for medical image classification, focusing on cross-modal learning and anatomical topology. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02106",
      "title": "Towards Immersive Human-X Interaction: A Real-Time Framework for\n  Physically Plausible Motion Synthesis",
      "authors": [
        "Kaiyang Ji",
        "Ye Shi",
        "Zichen Jin",
        "Kangyi Chen",
        "Lan Xu",
        "Yuexin Ma",
        "Jingyi Yu",
        "Jingya Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Real-time synthesis of physically plausible human interactions remains a\ncritical challenge for immersive VR/AR systems and humanoid robotics. While\nexisting methods demonstrate progress in kinematic motion generation, they\noften fail to address the fundamental tension between real-time responsiveness,\nphysical feasibility, and safety requirements in dynamic human-machine\ninteractions. We introduce Human-X, a novel framework designed to enable\nimmersive and physically plausible human interactions across diverse entities,\nincluding human-avatar, human-humanoid, and human-robot systems. Unlike\nexisting approaches that focus on post-hoc alignment or simplified physics, our\nmethod jointly predicts actions and reactions in real-time using an\nauto-regressive reaction diffusion planner, ensuring seamless synchronization\nand context-aware responses. To enhance physical realism and safety, we\nintegrate an actor-aware motion tracking policy trained with reinforcement\nlearning, which dynamically adapts to interaction partners' movements while\navoiding artifacts like foot sliding and penetration. Extensive experiments on\nthe Inter-X and InterHuman datasets demonstrate significant improvements in\nmotion quality, interaction continuity, and physical plausibility over\nstate-of-the-art methods. Our framework is validated in real-world\napplications, including virtual reality interface for human-robot interaction,\nshowcasing its potential for advancing human-robot collaboration.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02106v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02106v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.332,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions training an actor-aware motion tracking policy using reinforcement learning, but it does not involve human feedback, such as training a reward model on human-ranked data. Instead, the RL is likely based on predefined rewards for physical feasibility and safety, which does not align with the core definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses an auto-regressive diffusion planner for motion synthesis, which involves iterative refinement, but it applies this to physical motion generation rather than multi-step logical reasoning or Chain-of-Thought processes as defined. While diffusion models are employed, they are not used for solving complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02107",
      "title": "AutoLoRA: Automatic LoRA Retrieval and Fine-Grained Gated Fusion for\n  Text-to-Image Generation",
      "authors": [
        "Zhiwen Li",
        "Zhongjie Duan",
        "Die Chen",
        "Cen Chen",
        "Daoyuan Chen",
        "Yaliang Li",
        "Yingda Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Despite recent advances in photorealistic image generation through\nlarge-scale models like FLUX and Stable Diffusion v3, the practical deployment\nof these architectures remains constrained by their inherent intractability to\nparameter fine-tuning. While low-rank adaptation (LoRA) have demonstrated\nefficacy in enabling model customization with minimal parameter overhead, the\neffective utilization of distributed open-source LoRA modules faces three\ncritical challenges: sparse metadata annotation, the requirement for zero-shot\nadaptation capabilities, and suboptimal fusion strategies for multi-LoRA fusion\nstrategies. To address these limitations, we introduce a novel framework that\nenables semantic-driven LoRA retrieval and dynamic aggregation through two key\ncomponents: (1) weight encoding-base LoRA retriever that establishes a shared\nsemantic space between LoRA parameter matrices and text prompts, eliminating\ndependence on original training data, and (2) fine-grained gated fusion\nmechanism that computes context-specific fusion weights across network layers\nand diffusion timesteps to optimally integrate multiple LoRA modules during\ngeneration. Our approach achieves significant improvement in image generation\nperfermance, thereby facilitating scalable and data-efficient enhancement of\nfoundational models. This work establishes a critical bridge between the\nfragmented landscape of community-developed LoRAs and practical deployment\nrequirements, enabling collaborative model evolution through standardized\nadapter integration.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02107v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02107v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.518,
      "distributed_training_score": 0.377,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework for LoRA retrieval and fusion in text-to-image generation, focusing on semantic-driven adapters and diffusion-based enhancements. It does not involve human feedback, reward models, or reinforcement learning for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper builds on diffusion models for image generation, including fusion across diffusion timesteps and layers, but it does not adapt the diffusion process for multi-step logical reasoning or holistic correction of Chain-of-Thought entities; it is centered on image synthesis rather than complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02110",
      "title": "Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious\n  Tools",
      "authors": [
        "Kanghua Mo",
        "Li Hu",
        "Yucheng Long",
        "Zhihao Li"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities\nin complex reasoning and decision-making by leveraging external tools. However,\nthis tool-centric paradigm introduces a previously underexplored attack\nsurface: adversaries can manipulate tool metadata -- such as names,\ndescriptions, and parameter schemas -- to influence agent behavior. We identify\nthis as a new and stealthy threat surface that allows malicious tools to be\npreferentially selected by LLM agents, without requiring prompt injection or\naccess to model internals. To demonstrate and exploit this vulnerability, we\npropose the Attractive Metadata Attack (AMA), a black-box in-context learning\nframework that generates highly attractive but syntactically and semantically\nvalid tool metadata through iterative optimization. Our attack integrates\nseamlessly into standard tool ecosystems and requires no modification to the\nagent's execution framework. Extensive experiments across ten realistic,\nsimulated tool-use scenarios and a range of popular LLM agents demonstrate\nconsistently high attack success rates (81\\%-95\\%) and significant privacy\nleakage, with negligible impact on primary task execution. Moreover, the attack\nremains effective even under prompt-level defenses and structured\ntool-selection protocols such as the Model Context Protocol, revealing systemic\nvulnerabilities in current agent architectures. These findings reveal that\nmetadata manipulation constitutes a potent and stealthy attack surface,\nhighlighting the need for execution-level security mechanisms that go beyond\nprompt-level defenses.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02110v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02110v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.306,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes an attack method using a state-action-value optimization framework for generating attractive tool metadata, which loosely draws from reinforcement learning principles. However, it does not involve human feedback, a reward model trained on human-ranked data, or fine-tuning an AI model to align with human preferences, which are core to RLHF. Thus, the connection is indirect and not central to the paper's contributions.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02111",
      "title": "Tackling Ill-posedness of Reversible Image Conversion with Well-posed\n  Invertible Network",
      "authors": [
        "Yuanfei Huang",
        "Hua Huang"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Reversible image conversion (RIC) suffers from ill-posedness issues due to\nits forward conversion process being considered an underdetermined system.\nDespite employing invertible neural networks (INN), existing RIC methods\nintrinsically remain ill-posed as inevitably introducing uncertainty by\nincorporating randomly sampled variables. To tackle the ill-posedness dilemma,\nwe focus on developing a reliable approximate left inverse for the\nunderdetermined system by constructing an overdetermined system with a non-zero\nGram determinant, thus ensuring a well-posed solution. Based on this principle,\nwe propose a well-posed invertible $1\\times1$ convolution (WIC), which\neliminates the reliance on random variable sampling and enables the development\nof well-posed invertible networks. Furthermore, we design two innovative\nnetworks, WIN-Na\\\"ive and WIN, with the latter incorporating advanced\nskip-connections to enhance long-term memory. Our methods are evaluated across\ndiverse RIC tasks, including reversible image hiding, image rescaling, and\nimage decolorization, consistently achieving state-of-the-art performance.\nExtensive experiments validate the effectiveness of our approach, demonstrating\nits ability to overcome the bottlenecks of existing RIC solutions and setting a\nnew benchmark in the field. Codes are available in\nhttps://github.com/BNU-ERC-ITEA/WIN.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02111v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02111v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.295,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.296,
      "datasets_score": 0.24,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02113",
      "title": "DeflareMamba: Hierarchical Vision Mamba for Contextually Consistent Lens\n  Flare Removal",
      "authors": [
        "Yihang Huang",
        "Yuanfei Huang",
        "Junhui Lin",
        "Hua Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Lens flare removal remains an information confusion challenge in the\nunderlying image background and the optical flares, due to the complex optical\ninteractions between light sources and camera lens. While recent solutions have\nshown promise in decoupling the flare corruption from image, they often fail to\nmaintain contextual consistency, leading to incomplete and inconsistent flare\nremoval. To eliminate this limitation, we propose DeflareMamba, which leverages\nthe efficient sequence modeling capabilities of state space models while\nmaintains the ability to capture local-global dependencies. Particularly, we\ndesign a hierarchical framework that establishes long-range pixel correlations\nthrough varied stride sampling patterns, and utilize local-enhanced state space\nmodels that simultaneously preserves local details. To the best of our\nknowledge, this is the first work that introduces state space models to the\nflare removal task. Extensive experiments demonstrate that our method\neffectively removes various types of flare artifacts, including scattering and\nreflective flares, while maintaining the natural appearance of non-flare\nregions. Further downstream applications demonstrate the capacity of our method\nto improve visual object recognition and cross-modal semantic understanding.\nCode is available at https://github.com/BNU-ERC-ITEA/DeflareMamba.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02113v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02113v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.325,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of DeflareMamba, a hierarchical vision model using State Space Models (SSMs) for lens flare removal in images. It focuses on image processing and maintaining contextual consistency, with no involvement of diffusion models, iterative refinement for logical tasks, or Chain-of-Thought reasoning. Therefore, it does not align with diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02115",
      "title": "Coward: Toward Practical Proactive Federated Backdoor Defense via\n  Collision-based Watermark",
      "authors": [
        "Wenjie Li",
        "Siying Gu",
        "Yiming Li",
        "Kangjie Chen",
        "Zhili Chen",
        "Tianwei Zhang",
        "Shu-Tao Xia",
        "Dacheng Tao"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Backdoor detection is currently the mainstream defense against backdoor\nattacks in federated learning (FL), where malicious clients upload poisoned\nupdates that compromise the global model and undermine the reliability of FL\ndeployments. Existing backdoor detection techniques fall into two categories,\nincluding passive and proactive ones, depending on whether the server\nproactively modifies the global model. However, both have inherent limitations\nin practice: passive defenses are vulnerable to common non-i.i.d. data\ndistributions and random participation of FL clients, whereas current proactive\ndefenses suffer inevitable out-of-distribution (OOD) bias because they rely on\nbackdoor co-existence effects. To address these issues, we introduce a new\nproactive defense, dubbed Coward, inspired by our discovery of multi-backdoor\ncollision effects, in which consecutively planted, distinct backdoors\nsignificantly suppress earlier ones. In general, we detect attackers by\nevaluating whether the server-injected, conflicting global watermark is erased\nduring local training rather than retained. Our method preserves the advantages\nof proactive defenses in handling data heterogeneity (\\ie, non-i.i.d. data)\nwhile mitigating the adverse impact of OOD bias through a revised detection\nmechanism. Extensive experiments on benchmark datasets confirm the\neffectiveness of Coward and its resilience to potential adaptive attacks. The\ncode for our method would be available at\nhttps://github.com/still2009/cowardFL.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02115v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02115v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.38,
      "datasets_score": 0.263,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02120",
      "title": "Don't Overthink It: A Survey of Efficient R1-style Large Reasoning\n  Models",
      "authors": [
        "Linan Yue",
        "Yichao Du",
        "Yizhi Wang",
        "Weibo Gao",
        "Fangzhou Yao",
        "Li Wang",
        "Ye Liu",
        "Ziyu Xu",
        "Qi Liu",
        "Shimin Di",
        "Min-Ling Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recently, Large Reasoning Models (LRMs) have gradually become a research\nhotspot due to their outstanding performance in handling complex tasks. Among\nthem, DeepSeek R1 has garnered significant attention for its exceptional\nperformance and open-source nature, driving advancements in the research of\nR1-style LRMs. Unlike traditional Large Language Models (LLMs), these models\nenhance logical deduction and decision-making capabilities during reasoning by\nincorporating mechanisms such as long chain-of-thought and self-reflection\nthrough reinforcement learning. However, with the widespread application of\nthese models, the problem of overthinking has gradually emerged. Specifically,\nwhen generating answers, these models often construct excessively long\nreasoning chains with redundant or repetitive steps, which leads to reduced\nreasoning efficiency and may affect the accuracy of the final answer. To this\nend, various efficient reasoning methods have been proposed, aiming to reduce\nthe length of reasoning paths without compromising model performance and\nreasoning capability. By reviewing the current research advancements in the\nfield of efficient reasoning methods systematically, we categorize existing\nworks into two main directions based on the lens of single-model optimization\nversus model collaboration: (1) Efficient Reasoning with Single Model, which\nfocuses on improving the reasoning efficiency of individual models; and (2)\nEfficient Reasoning with Model Collaboration, which explores optimizing\nreasoning paths through collaboration among multiple models. Besides, we\nmaintain a public GitHub repository that tracks the latest progress in\nefficient reasoning methods.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02120v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02120v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.557,
      "distributed_training_score": 0.419,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions self-reflection through reinforcement learning as a mechanism in LRMs for enhancing reasoning, but it does not specify the use of human feedback, a reward model trained on human-ranked data, or fine-tuning based on human preferences. Thus, while reinforcement learning is referenced, it does not align with the core definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on efficient reasoning methods for LRMs, including chain-of-thought and model collaboration, but does not mention diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction. There is no component related to multi-step logical reasoning via diffusion.",
      "distributed_training_justification": "The paper surveys efficient reasoning methods, including model collaboration for reasoning tasks, but does not discuss distributed training, parallel computing for accelerating model training, or strategies for partitioning data/computation across nodes. Its focus is on reasoning efficiency, not training processes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02121",
      "title": "A Survey on AgentOps: Categorization, Challenges, and Future Directions",
      "authors": [
        "Zexin Wang",
        "Jingjing Li",
        "Quan Zhou",
        "Haotian Si",
        "Yuanhao Liu",
        "Jianhui Li",
        "Gaogang Xie",
        "Fei Sun",
        "Dan Pei",
        "Changhua Pei"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "As the reasoning capabilities of Large Language Models (LLMs) continue to\nadvance, LLM-based agent systems offer advantages in flexibility and\ninterpretability over traditional systems, garnering increasing attention.\nHowever, despite the widespread research interest and industrial application of\nagent systems, these systems, like their traditional counterparts, frequently\nencounter anomalies. These anomalies lead to instability and insecurity,\nhindering their further development. Therefore, a comprehensive and systematic\napproach to the operation and maintenance of agent systems is urgently needed.\nUnfortunately, current research on the operations of agent systems is sparse.\nTo address this gap, we have undertaken a survey on agent system operations\nwith the aim of establishing a clear framework for the field, defining the\nchallenges, and facilitating further development. Specifically, this paper\nbegins by systematically defining anomalies within agent systems, categorizing\nthem into intra-agent anomalies and inter-agent anomalies. Next, we introduce a\nnovel and comprehensive operational framework for agent systems, dubbed Agent\nSystem Operations (AgentOps). We provide detailed definitions and explanations\nof its four key stages: monitoring, anomaly detection, root cause analysis, and\nresolution.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02121v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02121v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.352,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02124",
      "title": "Trainable Dynamic Mask Sparse Attention",
      "authors": [
        "Jingze Shi",
        "Yifan Wu",
        "Bingheng Wu",
        "Yiran Peng",
        "Liangdong Wang",
        "Guang Liu",
        "Yuyu Luo"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In large language models, the demand for modeling long contexts is constantly\nincreasing, but the quadratic complexity of the standard self-attention\nmechanism often becomes a bottleneck. Although existing sparse attention\nmechanisms have improved efficiency, they may still encounter issues such as\nstatic patterns or information loss. We introduce a trainable dynamic mask\nsparse attention mechanism, Dynamic Mask Attention, which effectively utilizes\ncontent-aware and position-aware sparsity. DMA achieves this through two key\ninnovations: First, it dynamically generates content-aware sparse masks from\nvalue representations, enabling the model to identify and focus on critical\ninformation adaptively. Second, it implements position-aware sparse attention\ncomputation that effectively skips unnecessary calculation regions. This\ndual-sparsity design allows the model to significantly reduce the computational\ncomplexity of important information while retaining complete information,\nachieving an excellent balance between information fidelity and computational\nefficiency. We have verified the performance of DMA through comprehensive\nexperiments. Comparative studies show that DMA outperforms multi-head\nattention, sliding window attention, multi-head latent attention, and native\nsparse attention in terms of perplexity under Chinchilla Scaling Law settings.\nMoreover, in challenging multi-query associative recall tasks, DMA also\ndemonstrates superior performance and efficiency compared to these methods.\nCrucially, in the evaluation of a 1.7B parameter model, DMA significantly\noutperforms multi-head attention in both standard benchmark performance and the\nchallenging needle-in-a-haystack task. These experimental results highlight its\ncapability to balance model efficiency and long-context modeling ability\neffectively.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02124v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02124v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.441,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on developing a sparse attention mechanism for large language models to improve efficiency in handling long contexts, with no mention of diffusion models, iterative refinement processes, or adapting chain-of-thought for multi-step logical reasoning. It does not involve generative diffusion-based techniques for reasoning tasks.",
      "distributed_training_justification": "The paper addresses computational efficiency in attention mechanisms through hardware optimizations like dynamic masking, but it does not discuss distributed training, parallel computing across multiple nodes, or strategies for partitioning data/model computation. The focus is on single-device efficiency, not multi-node machine learning systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02127",
      "title": "Beyond RGB and Events: Enhancing Object Detection under Adverse Lighting\n  with Monocular Normal Maps",
      "authors": [
        "Mingjie Liu",
        "Hanqing Liu",
        "Chuang Zhu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate object detection under adverse lighting conditions is critical for\nreal-world applications such as autonomous driving. Although neuromorphic event\ncameras have been introduced to handle these scenarios, adverse lighting often\ninduces distracting reflections from tunnel walls or road surfaces, which\nfrequently lead to false obstacle detections. However, neither RGB nor event\ndata alone is robust enough to address these complexities, and mitigating these\nissues without additional sensors remains underexplored. To overcome these\nchallenges, we propose leveraging normal maps, directly predicted from\nmonocular RGB images, as robust geometric cues to suppress false positives and\nenhance detection accuracy. We introduce NRE-Net, a novel multi-modal detection\nframework that effectively fuses three complementary modalities: monocularly\npredicted surface normal maps, RGB images, and event streams. To optimize the\nfusion process, our framework incorporates two key modules: the Adaptive\nDual-stream Fusion Module (ADFM), which integrates RGB and normal map features,\nand the Event-modality Aware Fusion Module (EAFM), which adapts to the high\ndynamic range characteristics of event data. Extensive evaluations on the\nDSEC-Det-sub and PKU-DAVIS-SOD datasets demonstrate that NRE-Net significantly\noutperforms state-of-the-art methods. Our approach achieves mAP50 improvements\nof 7.9% and 6.1% over frame-based approaches (e.g., YOLOX), while surpassing\nthe fusion-based SFNet by 2.7% on the DSEC-Det-sub dataset and SODFormer by\n7.1% on the PKU-DAVIS-SOD dataset.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02127v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02127v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.308,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02128",
      "title": "Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill\n  in Large Language Models",
      "authors": [
        "Tai An",
        "Ruwu Cai",
        "Yanzhe Zhang",
        "Yang Liu",
        "Hao Chen",
        "Pengcheng Xie",
        "Sheng Chang",
        "Yiwu Yao",
        "Gongyi Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In the era of large language models (LLMs), N:M sparsity has emerged as a\nstructured compression technique critical for accelerating inference. While\nprior work has primarily focused on weight sparsity, it often suffers from\nsignificant accuracy degradation. Activation sparsity, though promising, is\ntypically training-dependent and faces challenges in generalization. To address\nthese limitations, we introduce Amber Pruner, a training-free N:M activation\nsparsity method designed specifically for the prefill stage, targeting the\nacceleration of linear projection layers in LLMs. Extensive experiments across\nmultiple models and sparsity ratios (2:4, 4:8, and 8:16) demonstrate that Amber\nPruner can effectively sparsify and accelerate more than 55% of linear\ncomputations without requiring model retraining. To further enhance generality\nand efficiency, we propose Outstanding-sparse, a unified framework that\nintegrates Amber Pruner with post-training W8A8 quantization. Our approach\npreserves strong performance across a range of downstream tasks, with notable\nadvantages in generative tasks. This work pioneers a new frontier in activation\nsparsity, providing foundational insights that are poised to guide the\nco-evolution of algorithms and architectures in the design of next-generation\nAI systems.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02128v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02128v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.459,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is on optimizing inference in large language models through N:M activation sparsity, specifically for accelerating the prefill stage without retraining. It does not discuss distributed training, parallel computing across nodes, or strategies for partitioning data/computation during model training, making it unrelated to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02129",
      "title": "VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic\n  Urban Scenes Modeling",
      "authors": [
        "Yuru Xiao",
        "Zihan Lin",
        "Chao Lu",
        "Deming Zhai",
        "Kui Jiang",
        "Wenbo Zhao",
        "Wei Zhang",
        "Junjun Jiang",
        "Huanran Wang",
        "Xianming Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Dynamic urban scene modeling is a rapidly evolving area with broad\napplications. While current approaches leveraging neural radiance fields or\nGaussian Splatting have achieved fine-grained reconstruction and high-fidelity\nnovel view synthesis, they still face significant limitations. These often stem\nfrom a dependence on pre-calibrated object tracks or difficulties in accurately\nmodeling fast-moving objects from undersampled capture, particularly due to\nchallenges in handling temporal discontinuities. To overcome these issues, we\npropose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our\nkey insight is to distill robust, temporally consistent priors from a test-time\nadapted video diffusion model. To ensure precise pose alignment and effective\nintegration of this denoised content, we introduce two core innovations: a\njoint timestamp optimization strategy that refines interpolated frame poses,\nand an uncertainty distillation method that adaptively extracts target content\nwhile preserving well-reconstructed regions. Extensive experiments demonstrate\nthat our method significantly enhances dynamic modeling, especially for\nfast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view\nsynthesis over baseline approaches.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02129v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02129v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.271,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.49,
      "distributed_training_score": 0.334,
      "datasets_score": 0.277,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using video diffusion models to enhance 4D Gaussian Splatting for dynamic urban scene modeling, specifically for improving temporal consistency and novel view synthesis. It does not involve adapting diffusion models for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks; instead, it applies diffusion for generative video enhancement in computer vision contexts.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02130",
      "title": "The Complexity of Extreme Climate Events on the New Zealand's Kiwifruit\n  Industry",
      "authors": [
        "Boyuan Zheng",
        "Victor W. Chu",
        "Zhidong Li",
        "Evan Webster",
        "Ashley Rootsey"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Climate change has intensified the frequency and severity of extreme weather\nevents, presenting unprecedented challenges to the agricultural industry\nworldwide. In this investigation, we focus on kiwifruit farming in New Zealand.\nWe propose to examine the impacts of climate-induced extreme events,\nspecifically frost, drought, extreme rainfall, and heatwave, on kiwifruit\nharvest yields. These four events were selected due to their significant\nimpacts on crop productivity and their prevalence as recorded by climate\nmonitoring institutions in the country. We employed Isolation Forest, an\nunsupervised anomaly detection method, to analyse climate history and recorded\nextreme events, alongside with kiwifruit yields. Our analysis reveals\nconsiderable variability in how different types of extreme event affect\nkiwifruit yields underscoring notable discrepancies between climatic extremes\nand individual farm's yield outcomes. Additionally, our study highlights\ncritical limitations of current anomaly detection approaches, particularly in\naccurately identifying events such as frost. These findings emphasise the need\nfor integrating supplementary features like farm management strategies with\nclimate adaptation practices. Our further investigation will employ ensemble\nmethods that consolidate nearby farms' yield data and regional climate station\nfeatures to reduce variance, thereby enhancing the accuracy and reliability of\nextreme event detection and the formulation of response strategies.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02130v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02130v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.286,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.245,
      "distributed_training_score": 0.288,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02131",
      "title": "A Neural Quality Metric for BRDF Models",
      "authors": [
        "Behnaz Kavoosighafi",
        "Rafal K. Mantiuk",
        "Saghi Hajisharif",
        "Ehsan Miandji",
        "Jonas Unger"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurately evaluating the quality of bidirectional reflectance distribution\nfunction (BRDF) models is essential for photo-realistic rendering. Traditional\nBRDF-space metrics often employ numerical error measures that fail to capture\nperceptual differences evident in rendered images. In this paper, we introduce\nthe first perceptually informed neural quality metric for BRDF evaluation that\noperates directly in BRDF space, eliminating the need for rendering during\nquality assessment. Our metric is implemented as a compact multi-layer\nperceptron (MLP), trained on a dataset of measured BRDFs supplemented with\nsynthetically generated data and labelled using a perceptually validated\nimage-space metric. The network takes as input paired samples of reference and\napproximated BRDFs and predicts their perceptual quality in terms of\njust-objectionable-difference (JOD) scores. We show that our neural metric\nachieves significantly higher correlation with human judgments than existing\nBRDF-space metrics. While its performance as a loss function for BRDF fitting\nremains limited, the proposed metric offers a perceptually grounded alternative\nfor evaluating BRDF models.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02131v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02131v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.296,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.286,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02132",
      "title": "All Stories Are One Story: Emotional Arc Guided Procedural Game Level\n  Generation",
      "authors": [
        "Yunge Wen",
        "Chenliang Huang",
        "Hangyu Zhou",
        "Zhuo Zeng",
        "Chun Ming Louis Po",
        "Julian Togelius",
        "Timothy Merino",
        "Sam Earle"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The emotional arc is a universal narrative structure underlying stories\nacross cultures and media -- an idea central to structuralist narratology,\noften encapsulated in the phrase \"all stories are one story.\" We present a\nframework for procedural game narrative generation that incorporates emotional\narcs as a structural backbone for both story progression and gameplay dynamics.\nLeveraging established narratological theories and large-scale empirical\nanalyses, we focus on two core emotional patterns -- Rise and Fall -- to guide\nthe generation of branching story graphs. Each story node is automatically\npopulated with characters, items, and gameplay-relevant attributes (e.g.,\nhealth, attack), with difficulty adjusted according to the emotional\ntrajectory. Implemented in a prototype action role-playing game (ARPG), our\nsystem demonstrates how emotional arcs can be operationalized using large\nlanguage models (LLMs) and adaptive entity generation. Evaluation through\nplayer ratings, interviews, and sentiment analysis shows that emotional arc\nintegration significantly enhances engagement, narrative coherence, and\nemotional impact. These results highlight the potential of emotionally\nstructured procedural generation for advancing interactive storytelling for\ngames.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02132v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02132v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.245,
      "diffusion_reasoning_score": 0.394,
      "distributed_training_score": 0.235,
      "datasets_score": 0.273,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02134",
      "title": "Free-MoRef: Instantly Multiplexing Context Perception Capabilities of\n  Video-MLLMs within Single Inference",
      "authors": [
        "Kuo Wang",
        "Quanlong Zheng",
        "Junlin Xie",
        "Yanhao Zhang",
        "Jinguo Luo",
        "Haonan Lu",
        "Liang Lin",
        "Fan Zhou",
        "Guanbin Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Video Multimodal Large Language Models~(Video-MLLM) have achieved remarkable\nadvancements in video understanding tasks. However, constrained by the context\nlength limitation in the underlying LLMs, existing Video-MLLMs typically\nexhibit suboptimal performance on long video scenarios. To understand extended\ninput frames, common solutions span token compression and streaming inference\ntechniques, which sacrifice feature granularity or inference efficiency.\nDifferently, to efficiently achieve comprehensive understanding of longer frame\ninputs, we draw ideas from MoE and propose a training-free approach\n\\textbf{Free-MoRef}, which instantly multiplexes the context perception\ncapabilities of Video-MLLMs within one inference pass. Specifically, Free-MoRef\nreconstructs the vision tokens into several short sequences as\nmulti-references. Subsequently, we introduce MoRef-attention, which gathers\nclues from the multi-reference chunks in parallel to summarize unified query\nactivations. After the shadow layers in LLMs, a reference fusion step is\nderived to compose a final mixed reasoning sequence with key tokens from\nparallel chunks, which compensates the cross-reference vision interactions that\nare neglected in MoRef-attention. By splitting and fusing the long vision token\nsequences, Free-MoRef achieves improved performance under much lower computing\ncosts in reasoning multiplexed context length, demonstrating strong efficiency\nand effectiveness. Experiments on VideoMME, MLVU, LongVideoBench show that\nFree-MoRef achieves full perception of 2$\\times$ to 8$\\times$ longer input\nframes without compression on a single A100 GPU while keeping instant\nresponses, thereby bringing significant performance gains, even surpassing\ndedicatedly trained long-video-MLLMs. Codes are available at\nhttps://github.com/wkfdb/Free-MoRef",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02134v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02134v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.461,
      "distributed_training_score": 0.421,
      "datasets_score": 0.281,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a training-free approach for Video-MLLMs to handle longer contexts using Mixture of Experts-inspired attention and token fusion, without any involvement of diffusion models, iterative refinement processes, or multi-step logical reasoning. There is no component related to adapting diffusion for Chain-of-Thought or holistic correction.",
      "distributed_training_justification": "The paper involves parallel processing of multiple reference chunks in attention mechanisms during inference, which touches on parallel computing concepts. However, it does not address distributed training, multi-node setups, or partitioning data/computation for model training across processors; it is limited to efficient inference on a single GPU.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02137",
      "title": "Fitness aligned structural modeling enables scalable virtual screening\n  with AuroBind",
      "authors": [
        "Zhongyue Zhang",
        "Jiahua Rao",
        "Jie Zhong",
        "Weiqiang Bai",
        "Dongxue Wang",
        "Shaobo Ning",
        "Lifeng Qiao",
        "Sheng Xu",
        "Runze Ma",
        "Will Hua",
        "Jack Xiaoyu Chen",
        "Odin Zhang",
        "Wei Lu",
        "Hanyi Feng",
        "He Yang",
        "Xinchao Shi",
        "Rui Li",
        "Wanli Ouyang",
        "Xinzhu Ma",
        "Jiahao Wang",
        "Jixian Zhang",
        "Jia Duan",
        "Siqi Sun",
        "Jian Zhang",
        "Shuangjia Zheng"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Most human proteins remain undrugged, over 96% of human proteins remain\nunexploited by approved therapeutics. While structure-based virtual screening\npromises to expand the druggable proteome, existing methods lack atomic-level\nprecision and fail to predict binding fitness, limiting translational impact.\nWe present AuroBind, a scalable virtual screening framework that fine-tunes a\ncustom atomic-level structural model on million-scale chemogenomic data.\nAuroBind integrates direct preference optimization, self-distillation from\nhigh-confidence complexes, and a teacher-student acceleration strategy to\njointly predict ligand-bound structures and binding fitness. The proposed\nmodels outperform state-of-the-art models on structural and functional\nbenchmarks while enabling 100,000-fold faster screening across ultra-large\ncompound libraries. In a prospective screen across ten disease-relevant\ntargets, AuroBind achieved experimental hit rates of 7-69%, with top compounds\nreaching sub-nanomolar to picomolar potency. For the orphan GPCRs GPR151 and\nGPR160, AuroBind identified both agonists and antagonists with success rates of\n16-30%, and functional assays confirmed GPR160 modulation in liver and prostate\ncancer models. AuroBind offers a generalizable framework for structure-function\nlearning and high-throughput molecular screening, bridging the gap between\nstructure prediction and therapeutic discovery.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02137v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02137v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.298,
      "diffusion_reasoning_score": 0.315,
      "distributed_training_score": 0.365,
      "datasets_score": 0.271,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02140",
      "title": "AID4AD: Aerial Image Data for Automated Driving Perception",
      "authors": [
        "Daniel Lengerer",
        "Mathias Pechinger",
        "Klaus Bogenberger",
        "Carsten Markgraf"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This work investigates the integration of spatially aligned aerial imagery\ninto perception tasks for automated vehicles (AVs). As a central contribution,\nwe present AID4AD, a publicly available dataset that augments the nuScenes\ndataset with high-resolution aerial imagery precisely aligned to its local\ncoordinate system. The alignment is performed using SLAM-based point cloud maps\nprovided by nuScenes, establishing a direct link between aerial data and\nnuScenes local coordinate system. To ensure spatial fidelity, we propose an\nalignment workflow that corrects for localization and projection distortions. A\nmanual quality control process further refines the dataset by identifying a set\nof high-quality alignments, which we publish as ground truth to support future\nresearch on automated registration. We demonstrate the practical value of\nAID4AD in two representative tasks: in online map construction, aerial imagery\nserves as a complementary input that improves the mapping process; in motion\nprediction, it functions as a structured environmental representation that\nreplaces high-definition maps. Experiments show that aerial imagery leads to a\n15-23% improvement in map construction accuracy and a 2% gain in trajectory\nprediction performance. These results highlight the potential of aerial imagery\nas a scalable and adaptable source of environmental context in automated\nvehicle systems, particularly in scenarios where high-definition maps are\nunavailable, outdated, or costly to maintain. AID4AD, along with evaluation\ncode and pretrained models, is publicly released to foster further research in\nthis direction: https://github.com/DriverlessMobility/AID4AD.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02140v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02140v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.351,
      "datasets_score": 0.471,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves the creation and introduction of a new dataset, AID4AD, which augments the nuScenes dataset with aligned aerial imagery for automated driving applications. It details dataset curation methodologies, including an alignment workflow using SLAM-based maps and manual quality control, and evaluates the dataset's utility in tasks like map construction and motion prediction. This directly aligns with research on dataset creation, curation, benchmarking, and analysis in machine learning and AI, making it a core example of the topic.",
      "llm_score_status": "completed",
      "summary": "This paper introduces AID4AD, a new publicly available dataset that aligns high-resolution aerial imagery with the nuScenes dataset to enhance automated vehicle perception tasks. By developing a precise alignment workflow using SLAM-based point clouds and manual quality control, the authors evaluate its utility in online map construction and motion prediction, demonstrating improvements of 15-23% in map accuracy and 2% in trajectory prediction performance, positioning aerial imagery as a scalable alternative to high-definition maps.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a new dataset and alignment method that combines existing aerial imagery and SLAM techniques in a novel way for automated driving perception, though it does not introduce a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of computer vision for automated driving, given the public release of the dataset and demonstrated performance gains, but its influence may be limited to specific applications where HD maps are unavailable.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a high-quality contribution with a new dataset and practical evaluations that advance research in AV perception, making it valuable for specialists in the field to be aware of and potentially utilize.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/62e0a5ace9bf52a0f525dd2d2f1af3ef2578285a",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 5,
      "average_h_index": 3.25,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Daniel Lengerer",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2219615425"
        },
        {
          "name": "Mathias Pechinger",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2046871475"
        },
        {
          "name": "Klaus Bogenberger",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2285077751"
        },
        {
          "name": "C. Markgraf",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/32596797"
        }
      ]
    },
    {
      "id": "2508.02143",
      "title": "TrackletGait: A Robust Framework for Gait Recognition in the Wild",
      "authors": [
        "Shaoxiong Zhang",
        "Jinkai Zheng",
        "Shangdong Zhu",
        "Chenggang Yan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Gait recognition aims to identify individuals based on their body shape and\nwalking patterns. Though much progress has been achieved driven by deep\nlearning, gait recognition in real-world surveillance scenarios remains quite\nchallenging to current methods. Conventional approaches, which rely on periodic\ngait cycles and controlled environments, struggle with the non-periodic and\noccluded silhouette sequences encountered in the wild. In this paper, we\npropose a novel framework, TrackletGait, designed to address these challenges\nin the wild. We propose Random Tracklet Sampling, a generalization of existing\nsampling methods, which strikes a balance between robustness and representation\nin capturing diverse walking patterns. Next, we introduce Haar Wavelet-based\nDownsampling to preserve information during spatial downsampling. Finally, we\npresent a Hardness Exclusion Triplet Loss, designed to exclude low-quality\nsilhouettes by discarding hard triplet samples. TrackletGait achieves\nstate-of-the-art results, with 77.8 and 80.4 rank-1 accuracy on the Gait3D and\nGREW datasets, respectively, while using only 10.3M backbone parameters.\nExtensive experiments are also conducted to further investigate the factors\naffecting gait recognition in the wild.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02143v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02143v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.308,
      "distributed_training_score": 0.356,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02146",
      "title": "ScrewSplat: An End-to-End Method for Articulated Object Recognition",
      "authors": [
        "Seungyeon Kim",
        "Junsu Ha",
        "Young Hun Kim",
        "Yonghyeon Lee",
        "Frank C. Park"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Articulated object recognition -- the task of identifying both the geometry\nand kinematic joints of objects with movable parts -- is essential for enabling\nrobots to interact with everyday objects such as doors and laptops. However,\nexisting approaches often rely on strong assumptions, such as a known number of\narticulated parts; require additional inputs, such as depth images; or involve\ncomplex intermediate steps that can introduce potential errors -- limiting\ntheir practicality in real-world settings. In this paper, we introduce\nScrewSplat, a simple end-to-end method that operates solely on RGB\nobservations. Our approach begins by randomly initializing screw axes, which\nare then iteratively optimized to recover the object's underlying kinematic\nstructure. By integrating with Gaussian Splatting, we simultaneously\nreconstruct the 3D geometry and segment the object into rigid, movable parts.\nWe demonstrate that our method achieves state-of-the-art recognition accuracy\nacross a diverse set of articulated objects, and further enables zero-shot,\ntext-guided manipulation using the recovered kinematic model.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02146v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02146v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.285,
      "weak_supervision_score": 0.297,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.281,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02148",
      "title": "Large-Scale Model Enabled Semantic Communication Based on Robust\n  Knowledge Distillation",
      "authors": [
        "Kuiyuan DIng",
        "Caili Guo",
        "Yang Yang",
        "Zhongtian Du",
        "Walid Saad"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "eess.IV (Image and Video Processing)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "Large-scale models (LSMs) can be an effective framework for semantic\nrepresentation and understanding, thereby providing a suitable tool for\ndesigning semantic communication (SC) systems. However, their direct deployment\nis often hindered by high computational complexity and resource requirements.\nIn this paper, a novel robust knowledge distillation based semantic\ncommunication (RKD-SC) framework is proposed to enable efficient and\n\\textcolor{black}{channel-noise-robust} LSM-powered SC. The framework addresses\ntwo key challenges: determining optimal compact model architectures and\neffectively transferring knowledge while maintaining robustness against channel\nnoise. First, a knowledge distillation-based lightweight differentiable\narchitecture search (KDL-DARTS) algorithm is proposed. This algorithm\nintegrates knowledge distillation loss and a complexity penalty into the neural\narchitecture search process to identify high-performance, lightweight semantic\nencoder architectures. Second, a novel two-stage robust knowledge distillation\n(RKD) algorithm is developed to transfer semantic capabilities from an LSM\n(teacher) to a compact encoder (student) and subsequently enhance system\nrobustness. To further improve resilience to channel impairments, a\nchannel-aware transformer (CAT) block is introduced as the channel codec,\ntrained under diverse channel conditions with variable-length outputs.\nExtensive simulations on image classification tasks demonstrate that the RKD-SC\nframework significantly reduces model parameters while preserving a high degree\nof the teacher model's performance and exhibiting superior robustness compared\nto existing methods.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02148v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02148v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.46,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on knowledge distillation and semantic communication systems, including neural architecture search and robustness against channel noise. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "The paper addresses model compression and knowledge transfer for semantic communication but does not discuss distributed training, parallel computing, or partitioning data/computation across multiple nodes or processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02149",
      "title": "AURORA: Augmented Understanding via Structured Reasoning and\n  Reinforcement Learning for Reference Audio-Visual Segmentation",
      "authors": [
        "Ziyang Luo",
        "Nian Liu",
        "Fahad Shahbaz Khan",
        "Junwei Han"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Reference Audio-Visual Segmentation (Ref-AVS) tasks challenge models to\nprecisely locate sounding objects by integrating visual, auditory, and textual\ncues. Existing methods often lack genuine semantic understanding, tending to\nmemorize fixed reasoning patterns. Furthermore, jointly training for reasoning\nand segmentation can compromise pixel-level precision. To address these issues,\nwe introduce AURORA, a novel framework designed to enhance genuine reasoning\nand language comprehension in reference audio-visual segmentation. We employ a\nstructured Chain-of-Thought (CoT) prompting mechanism to guide the model\nthrough a step-by-step reasoning process and introduce a novel segmentation\nfeature distillation loss to effectively integrate these reasoning abilities\nwithout sacrificing segmentation performance. To further cultivate the model's\ngenuine reasoning capabilities, we devise a further two-stage training\nstrategy: first, a ``corrective reflective-style training\" stage utilizes\nself-correction to enhance the quality of reasoning paths, followed by\nreinforcement learning via Group Reward Policy Optimization (GRPO) to bolster\nrobustness in challenging scenarios. Experiments demonstrate that AURORA\nachieves state-of-the-art performance on Ref-AVS benchmarks and generalizes\neffectively to unreferenced segmentation.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02149v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02149v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.498,
      "distributed_training_score": 0.329,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces AURORA, which uses Chain-of-Thought (CoT) prompting, supervised fine-tuning, and reinforcement learning for audio-visual segmentation, but it does not involve diffusion models or adapt the iterative refinement process of diffusion for reasoning tasks. There is no component that treats the reasoning path as a holistically corrected entity through diffusion-based steps, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02150",
      "title": "Beyond the Trade-off: Self-Supervised Reinforcement Learning for\n  Reasoning Models' Instruction Following",
      "authors": [
        "Qingyu Ren",
        "Qianyu He",
        "Bowei Zhang",
        "Jie Zeng",
        "Jiaqing Liang",
        "Yanghua Xiao",
        "Weikang Zhou",
        "Zeye Sun",
        "Fei Yu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reasoning models excel in complex problem solving but exhibit a concerning\ntrade off between reasoning capabilities and instruction following abilities.\nExisting approaches for improving instruction following rely on stronger\nexternal models, creating methodological bottlenecks and practical limitations\nincluding increased costs and accessibility constraints. We propose a\nself-supervised RL framework that leverages reasoning models' own internal\nsignals to improve instruction following capabilities without external\nsupervision. Extensive experiments demonstrate that our framework significantly\nimproves instruction following capabilities while maintaining reasoning\nperformance, offering a scalable and cost-effective approach to enhance\ninstruction following in reasoning models. The data and code are publicly\navailable at https://github.com/Rainier-rq/verl-if.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02150v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02150v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.475,
      "weak_supervision_score": 0.415,
      "diffusion_reasoning_score": 0.511,
      "distributed_training_score": 0.333,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes a self-supervised RL framework that uses the model's internal signals for training, without any human feedback or human-ranked data. RLHF specifically requires human involvement to create a reward model, which is absent here, making the paper unrelated to this topic.",
      "weak_supervision_justification": "The paper employs programmatic methods to generate reward signals from the model's internal signals, such as decomposing instructions and handling constraints without external labels, which aligns with weak supervision's use of noisy or derived sources for training data. However, it is not a primary focus, as the core contribution is on RL rather than weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper focuses on a self-supervised RL framework for improving instruction following, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. There is no component involving diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the trade-off in reasoning models between advanced reasoning capabilities and instruction following by proposing a self-supervised reinforcement learning (RL) framework that utilizes the model's internal signals to enhance instruction following without external supervision. The methodology involves decomposing multi-constraint instructions into simpler ones, modeling rewards for soft constraints via constraint-wise binary classification, and ensuring efficient training, with experiments demonstrating significant improvements in instruction following while preserving reasoning performance, thus offering a scalable and cost-effective alternative to existing approaches.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting reinforcement learning to use internal signals for enhancing instruction following in reasoning models, cleverly combining existing techniques to address a known trade-off without external dependencies. While not introducing an entirely new paradigm, this application represents a fresh and practical extension of RL methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in AI subfields focused on instruction tuning and model reliability, as it provides a cost-effective method that could be built upon for real-world applications. However, its impact may be confined to specific areas rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a valuable contribution by solving a practical limitation in reasoning models, making it essential for researchers interested in AI instruction following and reinforcement learning. Its innovative framework and experimental results offer insights that could inform future work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/38ac3246e20d996845892f40d36338a1bfa1ed2f",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 8,
      "average_h_index": 3.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Qingyu Ren",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2339288601"
        },
        {
          "name": "Qi He",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2152880833"
        },
        {
          "name": "Bowei Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2329898168"
        },
        {
          "name": "Jie Zeng",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2243910100"
        },
        {
          "name": "Jiaqing Liang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2364083147"
        },
        {
          "name": "Yanghua Xiao",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2267005966"
        },
        {
          "name": "Weikang Zhou",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2309277002"
        },
        {
          "name": "Zeye Sun",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2339422214"
        },
        {
          "name": "Fei Yu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2310860800"
        }
      ]
    },
    {
      "id": "2508.02151",
      "title": "AttriCtrl: Fine-Grained Control of Aesthetic Attribute Intensity in\n  Diffusion Models",
      "authors": [
        "Die Chen",
        "Zhongjie Duan",
        "Zhiwen Li",
        "Cen Chen",
        "Daoyuan Chen",
        "Yaliang Li",
        "Yinda Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent breakthroughs in text-to-image diffusion models have significantly\nenhanced both the visual fidelity and semantic controllability of generated\nimages. However, fine-grained control over aesthetic attributes remains\nchallenging, especially when users require continuous and intensity-specific\nadjustments. Existing approaches often rely on vague textual prompts, which are\ninherently ambiguous in expressing both the aesthetic semantics and the desired\nintensity, or depend on costly human preference data for alignment, limiting\ntheir scalability and practicality. To address these limitations, we propose\nAttriCtrl, a plug-and-play framework for precise and continuous control of\naesthetic attributes. Specifically, we quantify abstract aesthetics by\nleveraging semantic similarity from pre-trained vision-language models, and\nemploy a lightweight value encoder that maps scalar intensities in $[0,1]$ to\nlearnable embeddings within diffusion-based generation. This design enables\nintuitive and customizable aesthetic manipulation, with minimal training\noverhead and seamless integration into existing generation pipelines. Extensive\nexperiments demonstrate that AttriCtrl achieves accurate control over\nindividual attributes as well as flexible multi-attribute composition.\nMoreover, it is fully compatible with popular open-source controllable\ngeneration frameworks, showcasing strong integration capability and practical\nutility across diverse generation scenarios.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02151v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02151v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.531,
      "distributed_training_score": 0.316,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing control over aesthetic attributes in text-to-image diffusion models, specifically for image generation tasks. It does not involve adapting the diffusion process for solving complex logical tasks, multi-step reasoning, or treating a 'Chain-of-Thought' as an entity. Therefore, there is no connection to diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02152",
      "title": "Efficient Chambolle-Pock based algorithms for Convoltional sparse\n  representation",
      "authors": [
        "Yi Liu",
        "Junjing Li",
        "Yang Chen",
        "Haowei Tang",
        "Pengcheng Zhang",
        "Tianling Lyu",
        "Zhiguo Gui"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Recently convolutional sparse representation (CSR), as a sparse\nrepresentation technique, has attracted increasing attention in the field of\nimage processing, due to its good characteristic of translate-invariance. The\ncontent of CSR usually consists of convolutional sparse coding (CSC) and\nconvolutional dictionary learning (CDL), and many studies focus on how to solve\nthe corresponding optimization problems. At present, the most efficient\noptimization scheme for CSC is based on the alternating direction method of\nmultipliers (ADMM). However, the ADMM-based approach involves a penalty\nparameter that needs to be carefully selected, and improper parameter selection\nmay result in either no convergence or very slow convergence. In this paper, a\nnovel fast and efficient method using Chambolle-Pock(CP) framework is proposed,\nwhich does not require extra manual selection parameters in solving processing,\nand has faster convergence speed. Furthermore, we propose an anisotropic total\nvariation penalty of the coefficient maps for CSC and apply the CP algorithm to\nsolve it. In addition, we also apply the CP framework to solve the\ncorresponding CDL problem. Experiments show that for noise-free image the\nproposed CSC algorithms can achieve rival results of the latest ADMM-based\napproach, while outperforms in removing noise from Gaussian noise pollution\nimage.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02152v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02152v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.249,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.298,
      "distributed_training_score": 0.308,
      "datasets_score": 0.255,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02155",
      "title": "DreamPainter: Image Background Inpainting for E-commerce Scenarios",
      "authors": [
        "Sijie Zhao",
        "Jing Cheng",
        "Yaoyao Wu",
        "Hao Xu",
        "Shaohui Jiao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Although diffusion-based image genenation has been widely explored and\napplied, background generation tasks in e-commerce scenarios still face\nsignificant challenges. The first challenge is to ensure that the generated\nproducts are consistent with the given product inputs while maintaining a\nreasonable spatial arrangement, harmonious shadows, and reflections between\nforeground products and backgrounds. Existing inpainting methods fail to\naddress this due to the lack of domain-specific data. The second challenge\ninvolves the limitation of relying solely on text prompts for image control, as\neffective integrating visual information to achieve precise control in\ninpainting tasks remains underexplored. To address these challenges, we\nintroduce DreamEcom-400K, a high-quality e-commerce dataset containing accurate\nproduct instance masks, background reference images, text prompts, and\naesthetically pleasing product images. Based on this dataset, we propose\nDreamPainter, a novel framework that not only utilizes text prompts for control\nbut also flexibly incorporates reference image information as an additional\ncontrol signal. Extensive experiments demonstrate that our approach\nsignificantly outperforms state-of-the-art methods, maintaining high product\nconsistency while effectively integrating both text prompt and reference image\ninformation.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02155v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02155v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.449,
      "distributed_training_score": 0.321,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on diffusion-based image inpainting for e-commerce scenarios, utilizing diffusion models for visual generation tasks such as background creation and consistency with products. However, it does not involve adapting diffusion processes for complex logical tasks, multi-step reasoning, or treating a Chain-of-Thought as an entity. The core contributions are in image generation and control, not logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02157",
      "title": "Unified Category-Level Object Detection and Pose Estimation from RGB\n  Images using 3D Prototypes",
      "authors": [
        "Tom Fischer",
        "Xiaojie Zhang",
        "Eddy Ilg"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recognizing objects in images is a fundamental problem in computer vision.\nAlthough detecting objects in 2D images is common, many applications require\ndetermining their pose in 3D space. Traditional category-level methods rely on\nRGB-D inputs, which may not always be available, or employ two-stage approaches\nthat use separate models and representations for detection and pose estimation.\nFor the first time, we introduce a unified model that integrates detection and\npose estimation into a single framework for RGB images by leveraging neural\nmesh models with learned features and multi-model RANSAC. Our approach achieves\nstate-of-the-art results for RGB category-level pose estimation on REAL275,\nimproving on the current state-of-the-art by 22.9% averaged across all\nscale-agnostic metrics. Finally, we demonstrate that our unified method\nexhibits greater robustness compared to single-stage baselines. Our code and\nmodels are available at\nhttps://github.com/Fischer-Tom/unified-detection-and-pose-estimation.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02157v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02157v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.342,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02165",
      "title": "Subject or Style: Adaptive and Training-Free Mixture of LoRAs",
      "authors": [
        "Jia-Chen Zhang",
        "Yu-Jie Xiong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Fine-tuning models via Low-Rank Adaptation (LoRA) demonstrates remarkable\nperformance in subject-driven or style-driven generation tasks. Studies have\nexplored combinations of different LoRAs to jointly generate learned styles and\ncontent. However, current methods struggle to balance the original subject and\nstyle, and often require additional training. Recently, K-LoRA proposed a\ntraining-free LoRA fusion method. But it involves multiple hyperparameters,\nmaking it difficult to adapt to all styles and subjects. In this paper, we\npropose EST-LoRA, a training-free adaptive LoRA fusion method. It\ncomprehensively considers three critical factors: \\underline{E}nergy of matrix,\n\\underline{S}tyle discrepancy scores and \\underline{T}ime steps. Analogous to\nthe Mixture of Experts (MoE) architecture, the model adaptively selects between\nsubject LoRA and style LoRA within each attention layer. This integrated\nselection mechanism ensures balanced contributions from both components during\nthe generation process. Experimental results show that EST-LoRA outperforms\nstate-of-the-art methods in both qualitative and quantitative evaluations and\nachieves faster generation speed compared to other efficient fusion approaches.\nOur code is publicly available at:\nhttps://anonymous.4open.science/r/EST-LoRA-F318.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02165v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02165v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.384,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a method for adaptive LoRA fusion in diffusion models for style transfer in image generation, focusing on factors like matrix energy, style discrepancy, and time steps. It does not involve adapting the diffusion process for complex logical tasks, such as treating a Chain-of-Thought as a single entity for multi-step reasoning. The work is centered on visual generation, not logical or reasoning applications.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02168",
      "title": "After the Party: Navigating the Mapping From Color to Ambient Lighting",
      "authors": [
        "Florin-Alexandru Vasluianu",
        "Tim Seizinger",
        "Zongwei Wu",
        "Radu Timofte"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Illumination in practical scenarios is inherently complex, involving colored\nlight sources, occlusions, and diverse material interactions that produce\nintricate reflectance and shading effects. However, existing methods often\noversimplify this challenge by assuming a single light source or uniform,\nwhite-balanced lighting, leaving many of these complexities unaddressed. In\nthis paper, we introduce CL3AN, the first large-scale, high-resolution dataset\nof its kind designed to facilitate the restoration of images captured under\nmultiple Colored Light sources to their Ambient-Normalized counterparts.\nThrough benchmarking, we find that leading approaches often produce artifacts,\nsuch as illumination inconsistencies, texture leakage, and color distortion,\nprimarily due to their limited ability to precisely disentangle illumination\nfrom reflectance. Motivated by this insight, we achieve such a desired\ndecomposition through a novel learning framework that leverages explicit\nchromaticity-luminance components guidance, drawing inspiration from the\nprinciples of the Retinex model. Extensive evaluations on existing benchmarks\nand our dataset demonstrate the effectiveness of our approach, showcasing\nenhanced robustness under non-homogeneous color lighting and material-specific\nreflectance variations, all while maintaining a highly competitive\ncomputational cost. The benchmark, codes, and models are available at\nwww.github.com/fvasluianu97/RLN2.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02168v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02168v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.316,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02172",
      "title": "GaussianCross: Cross-modal Self-supervised 3D Representation Learning\n  via Gaussian Splatting",
      "authors": [
        "Lei Yao",
        "Yi Wang",
        "Yi Zhang",
        "Moyun Liu",
        "Lap-Pui Chau"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "The significance of informative and robust point representations has been\nwidely acknowledged for 3D scene understanding. Despite existing\nself-supervised pre-training counterparts demonstrating promising performance,\nthe model collapse and structural information deficiency remain prevalent due\nto insufficient point discrimination difficulty, yielding unreliable\nexpressions and suboptimal performance. In this paper, we present\nGaussianCross, a novel cross-modal self-supervised 3D representation learning\narchitecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques\nto address current challenges. GaussianCross seamlessly converts\nscale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian\nrepresentation without missing details, enabling stable and generalizable\npre-training. Subsequently, a tri-attribute adaptive distillation splatting\nmodule is incorporated to construct a 3D feature field, facilitating synergetic\nfeature capturing of appearance, geometry, and semantic cues to maintain\ncross-modal consistency. To validate GaussianCross, we perform extensive\nevaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In\nparticular, GaussianCross shows a prominent parameter and data efficiency,\nachieving superior performance through linear probing (<0.1% parameters) and\nlimited data training (1% of scenes) compared to state-of-the-art methods.\nFurthermore, GaussianCross demonstrates strong generalization capabilities,\nimproving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on\nScanNet200 semantic and instance segmentation tasks, respectively, supporting\nthe effectiveness of our approach. The code, weights, and visualizations are\npublicly available at\n\\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02172v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02172v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.36,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02177",
      "title": "Deep classification algorithm for De-identification of DICOM medical\n  images",
      "authors": [
        "Bufano Michele",
        "Kotter Elmar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Background : De-identification of DICOM (Digital Imaging and Communi-cations\nin Medicine) files is an essential component of medical image research.\nPersonal Identifiable Information (PII) and/or Personal Health Identifying\nInformation (PHI) need to be hidden or removed due to legal reasons. According\nto the Health Insurance Portability and Accountability Act (HIPAA) and privacy\nrules, also full-face photographic images and any compa-rable images are direct\nidentifiers and are considered protected health information that also need to\nbe de-identified. Objective : The study aimed to implement a method that permit\nto de-identify the PII and PHI information present in the header and burned on\nthe pixel data of DICOM. Methods : To execute the de-identification, we\nimplemented an algorithm based on the safe harbor method, defined by HIPAA. Our\nalgorithm uses input customizable parameter to classify and then possibly\nde-identify individual DICOM tags. Results : The most sensible information,\nlike names, history, personal data and institution were successfully\nrecognized. Conclusions : We developed a python algorithm that is able to\nclassify infor-mation present in a DICOM file. The flexibility provided by the\nuse of customi-zable input parameters, which allow the user to customize the\nentire process de-pending on the case (e.g., the language), makes the entire\nprogram very promis-ing for both everyday use and research purposes. Our code\nis available at https://github.com/rtdicomexplorer/deep_deidentification.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02177v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02177v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.318,
      "distributed_training_score": 0.301,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02178",
      "title": "Reconsidering Overthinking: Penalizing Internal and External Redundancy\n  in CoT Reasoning",
      "authors": [
        "Jialiang Hong",
        "Taihang Zhen",
        "Kai Chen",
        "Jiaheng Liu",
        "Wenpeng Zhu",
        "Jing Huo",
        "Yang Gao",
        "Depeng Wang",
        "Haitao Wan",
        "Xi Yang",
        "Boyan Wang",
        "Fanyu Meng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Reasoning Models (LRMs) often produce excessively verbose reasoning\ntraces, a phenomenon known as overthinking, which hampers both efficiency and\ninterpretability. Prior works primarily address this issue by reducing response\nlength, without fully examining the underlying semantic structure of the\nreasoning process. In this paper, we revisit overthinking by decomposing it\ninto two distinct forms: internal redundancy, which consists of\nlow-contribution reasoning steps within the first correct solution (FCS), and\nexternal redundancy, which refers to unnecessary continuation after the FCS. To\nmitigate both forms, we propose a dual-penalty reinforcement learning\nframework. For internal redundancy, we adopt a sliding-window semantic analysis\nto penalize low-gain reasoning steps that contribute little toward reaching the\ncorrect answer. For external redundancy, we penalize its proportion beyond the\nFCS to encourage earlier termination. Our method significantly compresses\nreasoning traces with minimal accuracy loss, and generalizes effectively to\nout-of-domain tasks such as question answering and code generation. Crucially,\nwe find that external redundancy can be safely removed without degrading\nperformance, whereas internal redundancy must be reduced more cautiously to\navoid impairing correctness. These findings suggest that our method not only\nimproves reasoning efficiency but also enables implicit, semantic-aware control\nover Chain-of-Thought length, paving the way for more concise and interpretable\nLRMs.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02178v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02178v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.538,
      "distributed_training_score": 0.377,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses a reinforcement learning framework to penalize redundancies in Chain-of-Thought reasoning, but it does not involve human feedback, a separate reward model trained on human-ranked data, or alignment with human preferences. The rewards are based on automated semantic analysis, making it distinct from RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses redundancy in Chain-of-Thought reasoning through reinforcement learning and semantic analysis, with no reference to diffusion models, iterative refinement processes, or treating the reasoning path as a holistic entity for multi-step correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02179",
      "title": "Weakly Supervised Multimodal Temporal Forgery Localization via Multitask\n  Learning",
      "authors": [
        "Wenbo Xu",
        "Wei Lu",
        "Xiangyang Luo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The spread of Deepfake videos has caused a trust crisis and impaired social\nstability. Although numerous approaches have been proposed to address the\nchallenges of Deepfake detection and localization, there is still a lack of\nsystematic research on the weakly supervised multimodal fine-grained temporal\nforgery localization (WS-MTFL). In this paper, we propose a novel weakly\nsupervised multimodal temporal forgery localization via multitask learning\n(WMMT), which addresses the WS-MTFL under the multitask learning paradigm. WMMT\nachieves multimodal fine-grained Deepfake detection and temporal partial\nforgery localization using merely video-level annotations. Specifically, visual\nand audio modality detection are formulated as two binary classification tasks.\nThe multitask learning paradigm is introduced to integrate these tasks into a\nmultimodal task. Furthermore, WMMT utilizes a Mixture-of-Experts structure to\nadaptively select appropriate features and localization head, achieving\nexcellent flexibility and localization precision in WS-MTFL. A feature\nenhancement module with temporal property preserving attention mechanism is\nproposed to identify the intra- and inter-modality feature deviation and\nconstruct comprehensive video features. To further explore the temporal\ninformation for weakly supervised learning, an extensible deviation perceiving\nloss has been proposed, which aims to enlarge the deviation of adjacent\nsegments of the forged samples and reduce the deviation of genuine samples.\nExtensive experiments demonstrate the effectiveness of multitask learning for\nWS-MTFL, and the WMMT achieves comparable results to fully supervised\napproaches in several evaluation metrics.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02179v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02179v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.435,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.356,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a weakly supervised method for multimodal temporal forgery localization (WS-MTFL), which uses video-level annotations instead of precise, hand-labeled frame-level data. This directly aligns with the definition of weak supervision, as it programmatically leverages high-level annotations to train the model for fine-grained tasks, demonstrating core principles like handling noisy or imprecise labels through multitask learning and deviation perceiving loss.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces WMMT, a weakly supervised method for multimodal temporal forgery localization in Deepfake videos, which uses multitask learning to integrate visual and audio modality detection with only video-level annotations. The approach incorporates a Mixture-of-Experts structure for adaptive feature selection, a temporal property preserving attention mechanism for enhancing intra- and inter-modality features, and an extensible deviation perceiving loss to amplify temporal deviations in forged samples, achieving performance comparable to fully supervised methods in experiments.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by extending weakly supervised temporal forgery localization to multimodal scenarios and introducing innovative components like the Mixture-of-Experts and deviation perceiving loss, which cleverly combine existing ideas to address new challenges in Deepfake detection.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of computer vision and Deepfake forensics due to its practical approach for reducing annotation costs, though its influence may remain confined to specialized applications in AI security.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution to multimodal Deepfake detection with innovative techniques that advance weakly supervised learning, making it valuable for researchers in computer vision and AI ethics.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c0cc6ba882e84fc95666843858ca30197fdbea8d",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 4,
      "average_h_index": 3.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Wenbo Xu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359401421"
        },
        {
          "name": "Wei Lu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2262498478"
        },
        {
          "name": "Xiangyang Luo",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2290337001"
        }
      ]
    },
    {
      "id": "2508.02180",
      "title": "Test-Time Model Adaptation for Quantized Neural Networks",
      "authors": [
        "Zeshuai Deng",
        "Guohao Chen",
        "Shuaicheng Niu",
        "Hui Luo",
        "Shuhai Zhang",
        "Yifan Yang",
        "Renjie Chen",
        "Wei Luo",
        "Mingkui Tan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Quantizing deep models prior to deployment is a widely adopted technique to\nspeed up inference for various real-time applications, such as autonomous\ndriving. However, quantized models often suffer from severe performance\ndegradation in dynamic environments with potential domain shifts and this\ndegradation is significantly more pronounced compared with their full-precision\ncounterparts, as shown by our theoretical and empirical illustrations. To\naddress the domain shift problem, test-time adaptation (TTA) has emerged as an\neffective solution by enabling models to learn adaptively from test data.\nUnfortunately, existing TTA methods are often impractical for quantized models\nas they typically rely on gradient backpropagation--an operation that is\nunsupported on quantized models due to vanishing gradients, as well as memory\nand latency constraints. In this paper, we focus on TTA for quantized models to\nimprove their robustness and generalization ability efficiently. We propose a\ncontinual zeroth-order adaptation (ZOA) framework that enables efficient model\nadaptation using only two forward passes, eliminating the computational burden\nof existing methods. Moreover, we propose a domain knowledge management scheme\nto store and reuse different domain knowledge with negligible memory\nconsumption, reducing the interference of different domain knowledge and\nfostering the knowledge accumulation during long-term adaptation. Experimental\nresults on three classical architectures, including quantized transformer-based\nand CNN-based models, demonstrate the superiority of our methods for quantized\nmodel adaptation. On the quantized W6A6 ViT-B model, our ZOA is able to achieve\na 5.0\\% improvement over the state-of-the-art FOA on ImageNet-C dataset. The\nsource code is available at https://github.com/DengZeshuai/ZOA.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02180v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02180v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.45,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on test-time adaptation for quantized neural networks using zeroth-order optimization, which involves forward passes for adaptation to domain shifts. It does not involve diffusion models, iterative refinement for logical tasks, or treating a Chain-of-Thought as an entity for multi-step correction. There is no component related to diffusion-based processes or complex logical reasoning.",
      "distributed_training_justification": "The paper addresses adaptation of quantized models at test time, emphasizing efficiency on edge devices, but does not discuss distributed training, parallel computing, or multi-node machine learning. It lacks any mention of partitioning data, architecture, or computation across processors for accelerating training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02186",
      "title": "Failure Cases Are Better Learned But Boundary Says Sorry: Facilitating\n  Smooth Perception Change for Accuracy-Robustness Trade-Off in Adversarial\n  Training",
      "authors": [
        "Yanyun Wang",
        "Li Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Adversarial Training (AT) is one of the most effective methods to train\nrobust Deep Neural Networks (DNNs). However, AT creates an inherent trade-off\nbetween clean accuracy and adversarial robustness, which is commonly attributed\nto the more complicated decision boundary caused by the insufficient learning\nof hard adversarial samples. In this work, we reveal a counterintuitive fact\nfor the first time: From the perspective of perception consistency, hard\nadversarial samples that can still attack the robust model after AT are already\nlearned better than those successfully defended. Thus, different from previous\nviews, we argue that it is rather the over-sufficient learning of hard\nadversarial samples that degrades the decision boundary and contributes to the\ntrade-off problem. Specifically, the excessive pursuit of perception\nconsistency would force the model to view the perturbations as noise and ignore\nthe information within them, which should have been utilized to induce a\nsmoother perception transition towards the decision boundary to support its\nestablishment to an appropriate location. In response, we define a new AT\nobjective named Robust Perception, encouraging the model perception to change\nsmoothly with input perturbations, based on which we propose a novel Robust\nPerception Adversarial Training (RPAT) method, effectively mitigating the\ncurrent accuracy-robustness trade-off. Experiments on CIFAR-10, CIFAR-100, and\nTiny-ImageNet with ResNet-18, PreActResNet-18, and WideResNet-34-10 demonstrate\nthe effectiveness of our method beyond four common baselines and 12\nstate-of-the-art (SOTA) works. The code is available at\nhttps://github.com/FlaAI/RPAT.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02186v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02186v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.365,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02187",
      "title": "A Moment Matching-Based Method for Sparse and Noisy Point Cloud\n  Registration",
      "authors": [
        "Xingyi Li",
        "Han Zhang",
        "Ziliang Wang",
        "Yukai Yang",
        "Weidong Chen"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Point cloud registration is a key step in robotic perception tasks, such as\nSimultaneous Localization and Mapping (SLAM). It is especially challenging in\nconditions with sparse points and heavy noise. Traditional registration\nmethods, such as Iterative Closest Point (ICP) and Normal Distributions\nTransform (NDT), often have difficulties in achieving a robust and accurate\nalignment under these conditions. In this paper, we propose a registration\nframework based on moment matching. In particular, the point clouds are\nregarded as i.i.d. samples drawn from the same distribution observed in the\nsource and target frames. We then match the generalized Gaussian Radial Basis\nmoments calculated from the point clouds to estimate the rigid transformation\nbetween two frames. Moreover, such method does not require explicit\npoint-to-point correspondences among the point clouds. We further show the\nconsistency of the proposed method. Experiments on synthetic and real-world\ndatasets show that our approach achieves higher accuracy and robustness than\nexisting methods. In addition, we integrate our framework into a 4D Radar SLAM\nsystem. The proposed method significantly improves the localization performance\nand achieves results comparable to LiDAR-based systems. These findings\ndemonstrate the potential of moment matching technique for robust point cloud\nregistration in sparse and noisy scenarios.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02187v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02187v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.268,
      "weak_supervision_score": 0.305,
      "diffusion_reasoning_score": 0.312,
      "distributed_training_score": 0.292,
      "datasets_score": 0.247,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02189",
      "title": "Learning Dynamics of Meta-Learning in Small Model Pretraining",
      "authors": [
        "David Demitri Africa",
        "Yuval Weiss",
        "Paula Buttery",
        "Richard Diehl Martinez"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models are powerful but costly. We ask whether meta-learning\ncan make the pretraining of small language models not only better but also more\ninterpretable. We integrate first-order MAML with subset-masked LM pretraining,\nproducing four LLama-style decoder-only models (11M-570M params), and evaluate\nit on a fundamental NLP task with many settings and real-world applications.\nCompared with vanilla training, our model (i) reaches the same loss up to 1.6x\nsooner, (ii) improves F1 on multilingual Universal NER under equal compute, and\n(iii) makes the training dynamics easy to read: first the network's\nrepresentations fan out (\"diversify\") and later they collapse into a smaller,\nshared subspace (\"compress\"). This two-stage shift shows up as a rise-and-fall\nin both effective-rank curves and attention-head entropy. The same curves\npinpoint which layers specialise earliest and which later reconverge, giving a\ncompact, interpretable signature of meta-adaptation. Code, checkpoints and\nWandB logs are released.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02189v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02189v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.421,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.407,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is on meta-learning for pretraining small language models, focusing on techniques like MAML and subset-masked training to improve efficiency and interpretability. It does not involve programmatically generating labels from noisy or imprecise sources, which is central to weak supervision.",
      "diffusion_reasoning_justification": "The paper centers on meta-learning and training dynamics for language models, with no reference to diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought.",
      "distributed_training_justification": "The paper's focus is on enhancing pretraining via meta-learning techniques, such as MAML, without discussing parallel computing, data partitioning, or multi-node systems for accelerating training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02190",
      "title": "FedVLA: Federated Vision-Language-Action Learning with Dual Gating\n  Mixture-of-Experts for Robotic Manipulation",
      "authors": [
        "Cui Miao",
        "Tao Chang",
        "Meihan Wu",
        "Hongbin Xu",
        "Chun Li",
        "Ming Li",
        "Xiaodong Wang"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by enabling robots to interpret language instructions for task\nexecution. However, training these models often relies on large-scale\nuser-specific data, raising concerns about privacy and security, which in turn\nlimits their broader adoption. To address this, we propose FedVLA, the first\nfederated VLA learning framework, enabling distributed model training that\npreserves data privacy without compromising performance. Our framework\nintegrates task-aware representation learning, adaptive expert selection, and\nexpert-driven federated aggregation, enabling efficient and privacy-preserving\ntraining of VLA models. Specifically, we introduce an Instruction Oriented\nScene-Parsing mechanism, which decomposes and enhances object-level features\nbased on task instructions, improving contextual understanding. To effectively\nlearn diverse task patterns, we design a Dual Gating Mixture-of-Experts (DGMoE)\nmechanism, where not only input tokens but also self-aware experts adaptively\ndecide their activation. Finally, we propose an Expert-Driven Aggregation\nstrategy at the federated server, where model aggregation is guided by\nactivated experts, ensuring effective cross-client knowledge transfer.Extensive\nsimulations and real-world robotic experiments demonstrate the effectiveness of\nour proposals. Notably, DGMoE significantly improves computational efficiency\ncompared to its vanilla counterpart, while FedVLA achieves task success rates\ncomparable to centralized training, effectively preserving data privacy.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02190v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02190v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.422,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on federated learning for Vision-Language-Action (VLA) models in robotic manipulation, emphasizing privacy-preserving distributed training. It does not involve human feedback, reward models based on human rankings, or reinforcement learning techniques for model alignment with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper introduces FedVLA, a federated learning framework that enables distributed model training across multiple clients without sharing raw data, incorporating elements like Expert-Driven Aggregation. This directly aligns with distributed training concepts, as it partitions data and computation across nodes for efficient, privacy-preserving model optimization.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces FedVLA, a pioneering federated learning framework for Vision-Language-Action (VLA) models in robotic manipulation, designed to preserve data privacy by enabling distributed training across clients without sharing raw data. It incorporates key components such as Instruction Oriented Scene-Parsing for enhanced task-aware feature extraction, Dual Gating Mixture-of-Experts for adaptive and efficient knowledge routing, and Expert-Driven Aggregation for effective cross-client model integration, with experiments showing performance comparable to centralized training and improved computational efficiency.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces the first federated VLA learning framework with novel elements like Dual Gating Mixture-of-Experts and Expert-Driven Aggregation, significantly advancing privacy-preserving techniques in multi-modal robotic learning.",
      "impact_score": "High",
      "impact_justification": "This work addresses critical privacy concerns in robotic AI, potentially influencing widespread adoption of federated learning in real-world applications and future research on distributed multi-modal systems.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "As a high-quality, innovative contribution that tackles privacy in VLA models for robotics, it provides essential insights and methods for researchers working in federated learning and AI applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/101df2782989af5ee3f0b0e5299961e930dfbc1a",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 5,
      "average_h_index": 1.7142857142857142,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Cui Miao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2323043999"
        },
        {
          "name": "Tao Chang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2134405766"
        },
        {
          "name": "Meihan Wu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2155259749"
        },
        {
          "name": "Hongbin Xu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2350451897"
        },
        {
          "name": "Chun Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2333403544"
        },
        {
          "name": "Ming Li",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2150652684"
        },
        {
          "name": "Xiaodong Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2261449784"
        }
      ]
    },
    {
      "id": "2508.02191",
      "title": "Neuromorphic Computing with Multi-Frequency Oscillations: A Bio-Inspired\n  Approach to Artificial Intelligence",
      "authors": [
        "Boheng Liu",
        "Ziyu Li",
        "Xia Wu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite remarkable capabilities, artificial neural networks exhibit limited\nflexible, generalizable intelligence. This limitation stems from their\nfundamental divergence from biological cognition that overlooks both neural\nregions' functional specialization and the temporal dynamics critical for\ncoordinating these specialized systems. We propose a tripartite brain-inspired\narchitecture comprising functionally specialized perceptual, auxiliary, and\nexecutive systems. Moreover, the integration of temporal dynamics through the\nsimulation of multi-frequency neural oscillation and synaptic dynamic\nadaptation mechanisms enhances the architecture, thereby enabling more flexible\nand efficient artificial cognition. Initial evaluations demonstrate superior\nperformance compared to state-of-the-art temporal processing approaches, with\n2.18\\% accuracy improvements while reducing required computation iterations by\n48.44\\%, and achieving higher correlation with human confidence patterns.\nThough currently demonstrated on visual processing tasks, this architecture\nestablishes a theoretical foundation for brain-like intelligence across\ncognitive domains, potentially bridging the gap between artificial and\nbiological intelligence.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02191v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02191v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.376,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a tripartite brain-inspired architecture for artificial intelligence, focusing on functional specialization and temporal dynamics like neural oscillations and synaptic adaptation to enhance cognition in tasks such as visual processing. It draws from biological cognition and compares to models like LSTM, Transformers, and CTM, but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. There is no mention of adapting diffusion for holistic chain-of-thought correction, making the paper unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02192",
      "title": "CMIC: Content-Adaptive Mamba for Learned Image Compression",
      "authors": [
        "Yunuo Chen",
        "Zezheng Lyu",
        "Bing He",
        "Hongwei Hu",
        "Qi Wang",
        "Yuan Tian",
        "Li Song",
        "Wenjun Zhang",
        "Guo Lu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent Learned image compression (LIC) leverages Mamba-style state-space\nmodels (SSMs) for global receptive fields with linear complexity. However,\nvanilla Mamba is content-agnostic, relying on fixed and predefined selective\nscans, which restricts its ability to dynamically and fully exploit content\ndependencies. We introduce Content-Adaptive Mamba (CAM), a dynamic SSM that\naddresses two critical limitations. First, it employs content-aware token\nreorganization, clustering and reordering tokens based on content similarity to\nprioritize proximity in feature space over Euclidean space. Second, it\nintegrates global priors into SSM via a prompt dictionary, effectively\nmitigating the strict causality and long-range decay in the token interactions\nof Mamba. These innovations enable CAM to better capture global dependencies\nwhile preserving computational efficiency. Leveraging CAM, our Content-Adaptive\nMamba-based LIC model (CMIC) achieves state-of-the-art rate-distortion\nperformance, surpassing VTM-21.0 by -15.91\\%, -21.34\\%, and -17.58\\% BD-rate on\nKodak, Tecnick, and CLIC benchmarks, respectively.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02192v3",
      "pdf_url": "http://arxiv.org/pdf/2508.02192v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.325,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on improving learned image compression using a content-adaptive state-space model (Mamba), focusing on token reorganization and global dependency modeling for better rate-distortion performance. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning processes. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02197",
      "title": "A Message Passing Realization of Expected Free Energy Minimization",
      "authors": [
        "Wouter W. L. Nuijten",
        "Mykola Lukashchuk",
        "Thijs van de Laar",
        "Bert de Vries"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present a message passing approach to Expected Free Energy (EFE)\nminimization on factor graphs, based on the theory introduced in\narXiv:2504.14898. By reformulating EFE minimization as Variational Free Energy\nminimization with epistemic priors, we transform a combinatorial search problem\ninto a tractable inference problem solvable through standard variational\ntechniques. Applying our message passing method to factorized state-space\nmodels enables efficient policy inference. We evaluate our method on\nenvironments with epistemic uncertainty: a stochastic gridworld and a partially\nobservable Minigrid task. Agents using our approach consistently outperform\nconventional KL-control agents on these tasks, showing more robust planning and\nefficient exploration under uncertainty. In the stochastic gridworld\nenvironment, EFE-minimizing agents avoid risky paths, while in the partially\nobservable minigrid setting, they conduct more systematic information-seeking.\nThis approach bridges active inference theory with practical implementations,\nproviding empirical evidence for the efficiency of epistemic priors in\nartificial agents.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02197v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02197v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.305,
      "datasets_score": 0.231,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02208",
      "title": "Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for\n  Proof-Centric Problems",
      "authors": [
        "Yebo Peng",
        "Zixiang Liu",
        "Yaoming Li",
        "Zhizhuo Yang",
        "Xinye Xu",
        "Bowen Ye",
        "Weijun Yuan",
        "Zihan Wang",
        "Tong Yang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Evaluating the mathematical capability of Large Language Models (LLMs) is a\ncritical yet challenging frontier. Existing benchmarks fall short, particularly\nfor proof-centric problems, as manual creation is unscalable and costly,\nleaving the true mathematical abilities of LLMs largely unassessed. To overcome\nthese barriers, we propose Proof2Hybrid, the first fully automated framework\nthat synthesizes high-quality, proof-centric benchmarks from natural language\nmathematical corpora. The key novelty of our solution is Proof2X, a roadmap of\nconverting mathematical proofs into various kinds of questions that are easy to\nverify. Instructed by this roadmap, we propose a new type of hybrid-formatted\nquestions, named ``$m$-out-of-$n$ multiple judge questions'', specifically\ndesigned to enable robust, automatic evaluation while being resilient to\nguessing and superficial pattern matching inherent in traditional formats. As a\ndemonstration of our framework, we introduce AlgGeoTest, a benchmark for\nalgebraic geometry--a frontier domain of modern mathematics--comprising 456\nchallenging items. Our extensive evaluations on state-of-the-art LLMs using\nAlgGeoTest reveal profound deficits in their comprehension of algebraic\ngeometry, providing a more precise measure of their true mathematical\ncapabilities. Our framework and benchmark pave the way for a new wave of\nin-depth research into the mathematical intelligence of AI systems.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02208v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02208v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.471,
      "distributed_training_score": 0.36,
      "datasets_score": 0.389,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for automatically synthesizing mathematical benchmarks, specifically for proof-centric problems, using methods like Proof2X and hybrid question formats. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion techniques for multi-step logical reasoning. The focus is solely on benchmark creation and evaluation of LLMs' mathematical capabilities, with no connection to the described diffusion-based reasoning topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02209",
      "title": "Balancing Information Accuracy and Response Timeliness in Networked LLMs",
      "authors": [
        "Yigit Turkmen",
        "Baturalp Buyukates",
        "Melih Bastopcu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.IT (Information Theory)",
        "cs.NI (Networking and Internet Architecture)",
        "math.IT (Information Theory)"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have transformed many\nfields including scientific discovery, content generation, biomedical text\nmining, and educational technology. However, the substantial requirements for\ntraining data, computational resources, and energy consumption pose significant\nchallenges for their practical deployment. A promising alternative is to\nleverage smaller, specialized language models and aggregate their outputs to\nimprove overall response quality. In this work, we investigate a networked LLM\nsystem composed of multiple users, a central task processor, and clusters of\ntopic-specialized LLMs. Each user submits categorical binary (true/false)\nqueries, which are routed by the task processor to a selected cluster of $m$\nLLMs. After gathering individual responses, the processor returns a final\naggregated answer to the user. We characterize both the information accuracy\nand response timeliness in this setting, and formulate a joint optimization\nproblem to balance these two competing objectives. Our extensive simulations\ndemonstrate that the aggregated responses consistently achieve higher accuracy\nthan those of individual LLMs. Notably, this improvement is more significant\nwhen the participating LLMs exhibit similar standalone performance.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02209v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02209v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.42,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.443,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on aggregating responses from networked LLMs to balance accuracy and timeliness, with no mention of training models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses query routing and response aggregation in LLMs but does not involve diffusion models, iterative refinement for logical reasoning, or multi-step Chain-of-Thought processes.",
      "distributed_training_justification": "The paper addresses inference in a networked LLM system for query handling, not algorithms or systems for distributed training, parallel computing, or accelerating model training across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02215",
      "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
      "authors": [
        "Yike Zhang",
        "Zhiyuan He",
        "Huiqiang Jiang",
        "Chengruidong Zhang",
        "Yuqing Yang",
        "Jianyong Wang",
        "Lili Qiu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02215v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02215v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.422,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution focuses on optimizing inference efficiency in large language models through KV cache pruning, specifically targeting memory reduction and speedup during decoding. It does not address distributed training, parallel computing for training, or strategies for partitioning data/computation across multiple nodes, which are central to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02220",
      "title": "Welcome New Doctor: Continual Learning with Expert Consultation and\n  Autoregressive Inference for Whole Slide Image Analysis",
      "authors": [
        "Doanh Cao Bui",
        "Jin Tae Kwak"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Whole Slide Image (WSI) analysis, with its ability to reveal detailed tissue\nstructures in magnified views, plays a crucial role in cancer diagnosis and\nprognosis. Due to their giga-sized nature, WSIs require substantial storage and\ncomputational resources for processing and training predictive models. With the\nrapid increase in WSIs used in clinics and hospitals, there is a growing need\nfor a continual learning system that can efficiently process and adapt existing\nmodels to new tasks without retraining or fine-tuning on previous tasks. Such a\nsystem must balance resource efficiency with high performance. In this study,\nwe introduce COSFormer, a Transformer-based continual learning framework\ntailored for multi-task WSI analysis. COSFormer is designed to learn\nsequentially from new tasks wile avoiding the need to revisit full historical\ndatasets. We evaluate COSFormer on a sequence of seven WSI datasets covering\nseven organs and six WSI-related tasks under both class-incremental and\ntask-incremental settings. The results demonstrate COSFormer's superior\ngeneralizability and effectiveness compared to existing continual learning\nframeworks, establishing it as a robust solution for continual WSI analysis in\nclinical applications.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02220v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02220v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.361,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02222",
      "title": "FinCPRG: A Bidirectional Generation Pipeline for Hierarchical Queries\n  and Rich Relevance in Financial Chinese Passage Retrieval",
      "authors": [
        "Xuan Xu",
        "Beilin Chu",
        "Qinhong Lin",
        "Yixiao Zhong",
        "Fufang Wen",
        "Jiaqi Liu",
        "Binjie Fei",
        "Yu Li",
        "Zhongliang Yang",
        "Linna Zhou"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)"
      ],
      "abstract": "In recent years, large language models (LLMs) have demonstrated significant\npotential in constructing passage retrieval datasets. However, existing methods\nstill face limitations in expressing cross-doc query needs and controlling\nannotation quality. To address these issues, this paper proposes a\nbidirectional generation pipeline, which aims to generate 3-level hierarchical\nqueries for both intra-doc and cross-doc scenarios and mine additional\nrelevance labels on top of direct mapping annotation. The pipeline introduces\ntwo query generation methods: bottom-up from single-doc text and top-down from\nmulti-doc titles. The bottom-up method uses LLMs to disassemble and generate\nstructured queries at both sentence-level and passage-level simultaneously from\nintra-doc passages. The top-down approach incorporates three key financial\nelements--industry, topic, and time--to divide report titles into clusters and\nprompts LLMs to generate topic-level queries from each cluster. For relevance\nannotation, our pipeline not only relies on direct mapping annotation from the\ngeneration relationship but also implements an indirect positives mining method\nto enrich the relevant query-passage pairs. Using this pipeline, we constructed\na Financial Passage Retrieval Generated dataset (FinCPRG) from almost 1.3k\nChinese financial research reports, which includes hierarchical queries and\nrich relevance labels. Through evaluations of mined relevance labels,\nbenchmarking and training experiments, we assessed the quality of FinCPRG and\nvalidated its effectiveness as a passage retrieval dataset for both training\nand benchmarking.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02222v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02222v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.384,
      "datasets_score": 0.408,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on using LLMs for generating queries and relevance labels in a dataset construction pipeline, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences. It does not involve RLHF elements.",
      "weak_supervision_justification": "The paper employs LLMs to programmatically generate queries and relevance labels, such as through indirect positives mining, which aligns with weak supervision by using noisy or automated labeling instead of hand-labeled data. However, it does not deeply explore weak supervision techniques as a primary focus, making it moderately relevant.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical reasoning, or multi-step chain-of-thought processes. It centers on LLM-based query generation and dataset construction for passage retrieval, with no components related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation, analysis, and evaluation of a new dataset (FinCPRG) for financial passage retrieval, including methodologies for dataset curation, hierarchical query generation, relevance labeling, and benchmarking experiments, directly aligning with research on datasets for ML and AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces FinCPRG, a bidirectional generation pipeline leveraging large language models to address limitations in existing passage retrieval datasets by generating hierarchical queries for both intra-document and cross-document scenarios in the financial domain. The methodology combines bottom-up query generation from single-document texts and top-down generation from multi-document titles, along with indirect positives mining for rich relevance labels, resulting in a dataset constructed from approximately 1,300 Chinese financial reports; evaluations demonstrate its effectiveness for training and benchmarking retrieval models, showing high-quality annotations and improved performance in financial retrieval tasks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining bottom-up and top-down query generation methods with indirect positives mining, offering a clever adaptation of existing LLM techniques for hierarchical queries in a specific domain like financial Chinese passage retrieval. While it advances the state-of-the-art in synthetic dataset construction, it primarily builds on established ideas rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in information retrieval and AI subfields by providing a specialized dataset for financial and Chinese-language applications, potentially serving as a benchmark for future studies in low-resource domains. However, its applicability is somewhat limited to niche areas, reducing its broader commercial or widespread academic influence.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to passage retrieval dataset generation, particularly for specialized domains, making it important for researchers in IR and AI to be aware of for potential applications in synthetic data and financial analytics. While not essential for all, its practical innovations and experimental validations warrant attention from relevant experts.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c2c4a13723c66e6f0ed9d0a75c67b9cba9579d01",
      "total_authors": 10,
      "authors_found": 9,
      "highest_h_index": 3,
      "average_h_index": 1.1111111111111112,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Xuan Xu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2299783227"
        },
        {
          "name": "Beilin Chu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2184626780"
        },
        {
          "name": "Qinhong Lin",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yixiao Zhong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376373405"
        },
        {
          "name": "Fufang Wen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2366076456"
        },
        {
          "name": "Jiaqi Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2366145767"
        },
        {
          "name": "Binjie Fei",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2366076487"
        },
        {
          "name": "Yu Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2367006800"
        },
        {
          "name": "Zhongliang Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2268641224"
        },
        {
          "name": "Linna Zhou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2300029622"
        }
      ]
    },
    {
      "id": "2508.02238",
      "title": "An Event-based Fast Intensity Reconstruction Scheme for UAV Real-time\n  Perception",
      "authors": [
        "Xin Dong",
        "Yiwei Zhang",
        "Yangjie Cui",
        "Jinwu Xiang",
        "Daochun Li",
        "Zhan Tu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Event cameras offer significant advantages, including a wide dynamic range,\nhigh temporal resolution, and immunity to motion blur, making them highly\npromising for addressing challenging visual conditions. Extracting and\nutilizing effective information from asynchronous event streams is essential\nfor the onboard implementation of event cameras. In this paper, we propose a\nstreamlined event-based intensity reconstruction scheme, event-based single\nintegration (ESI), to address such implementation challenges. This method\nguarantees the portability of conventional frame-based vision methods to\nevent-based scenarios and maintains the intrinsic advantages of event cameras.\nThe ESI approach reconstructs intensity images by performing a single\nintegration of the event streams combined with an enhanced decay algorithm.\nSuch a method enables real-time intensity reconstruction at a high frame rate,\ntypically 100 FPS. Furthermore, the relatively low computation load of ESI fits\nonboard implementation suitably, such as in UAV-based visual tracking\nscenarios. Extensive experiments have been conducted to evaluate the\nperformance comparison of ESI and state-of-the-art algorithms. Compared to\nstate-of-the-art algorithms, ESI demonstrates remarkable runtime efficiency\nimprovements, superior reconstruction quality, and a high frame rate. As a\nresult, ESI enhances UAV onboard perception significantly under visual\nadversary surroundings. In-flight tests, ESI demonstrates effective performance\nfor UAV onboard visual tracking under extremely low illumination\nconditions(2-10lux), whereas other comparative algorithms fail due to\ninsufficient frame rate, poor image quality, or limited real-time performance.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02238v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02238v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.284,
      "weak_supervision_score": 0.287,
      "diffusion_reasoning_score": 0.333,
      "distributed_training_score": 0.313,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02240",
      "title": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor",
      "authors": [
        "Xiaoliu Guan",
        "Lielin Jiang",
        "Hanqi Chen",
        "Xu Zhang",
        "Jiaxing Yan",
        "Guanzhong Wang",
        "Yi Liu",
        "Zetao Zhang",
        "Yu Wu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02240v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02240v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.511,
      "distributed_training_score": 0.434,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on accelerating inference in diffusion models for visual generation tasks, such as image and video synthesis, by using Taylor expansion and confidence gating. It does not involve adapting diffusion models for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks through iterative refinement.",
      "distributed_training_justification": "The paper's main contribution is accelerating inference in diffusion models through feature prediction and caching, without any discussion of distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation to speed up model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02243",
      "title": "I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal\n  Entity Linking",
      "authors": [
        "Ziyan Liu",
        "Junwen Li",
        "Kaiwen Li",
        "Tong Ruan",
        "Chao Wang",
        "Xinyan He",
        "Zongyu Wang",
        "Xuezhi Cao",
        "Jingping Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Multimodal entity linking plays a crucial role in a wide range of\napplications. Recent advances in large language model-based methods have become\nthe dominant paradigm for this task, effectively leveraging both textual and\nvisual modalities to enhance performance. Despite their success, these methods\nstill face two challenges, including unnecessary incorporation of image data in\ncertain scenarios and the reliance only on a one-time extraction of visual\nfeatures, which can undermine their effectiveness and accuracy. To address\nthese challenges, we propose a novel LLM-based framework for the multimodal\nentity linking task, called Intra- and Inter-modal Collaborative Reflections.\nThis framework prioritizes leveraging text information to address the task.\nWhen text alone is insufficient to link the correct entity through intra- and\ninter-modality evaluations, it employs a multi-round iterative strategy that\nintegrates key visual clues from various aspects of the image to support\nreasoning and enhance matching accuracy. Extensive experiments on three widely\nused public datasets demonstrate that our framework consistently outperforms\ncurrent state-of-the-art methods in the task, achieving improvements of 3.2%,\n5.1%, and 1.6%, respectively. Our code is available at\nhttps://github.com/ziyan-xiaoyu/I2CR/.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02243v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02243v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.322,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes an iterative framework for multimodal entity linking using LLMs, involving multi-round processes to integrate visual clues. However, it does not adapt the iterative refinement process of diffusion models for logical tasks. There is no mention of diffusion models, treating reasoning paths as entities for holistic correction, or multi-step logical reasoning via diffusion. Thus, the paper's contributions are unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02247",
      "title": "ByteGen: A Tokenizer-Free Generative Model for Orderbook Events in Byte\n  Space",
      "authors": [
        "Yang Li",
        "Zhi Chen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Generative modeling of high-frequency limit order book (LOB) dynamics is a\ncritical yet unsolved challenge in quantitative finance, essential for robust\nmarket simulation and strategy backtesting. Existing approaches are often\nconstrained by simplifying stochastic assumptions or, in the case of modern\ndeep learning models like Transformers, rely on tokenization schemes that\naffect the high-precision, numerical nature of financial data through\ndiscretization and binning. To address these limitations, we introduce ByteGen,\na novel generative model that operates directly on the raw byte streams of LOB\nevents. Our approach treats the problem as an autoregressive next-byte\nprediction task, for which we design a compact and efficient 32-byte packed\nbinary format to represent market messages without information loss. The core\nnovelty of our work is the complete elimination of feature engineering and\ntokenization, enabling the model to learn market dynamics from its most\nfundamental representation. We achieve this by adapting the H-Net architecture,\na hybrid Mamba-Transformer model that uses a dynamic chunking mechanism to\ndiscover the inherent structure of market messages without predefined rules.\nOur primary contributions are: 1) the first end-to-end, byte-level framework\nfor LOB modeling; 2) an efficient packed data representation; and 3) a\ncomprehensive evaluation on high-frequency data. Trained on over 34 million\nevents from CME Bitcoin futures, ByteGen successfully reproduces key stylized\nfacts of financial markets, generating realistic price distributions,\nheavy-tailed returns, and bursty event timing. Our findings demonstrate that\nlearning directly from byte space is a promising and highly flexible paradigm\nfor modeling complex financial systems, achieving competitive performance on\nstandard market quality metrics without the biases of tokenization.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02247v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02247v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.362,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces ByteGen, a generative model for limit order book events using a hybrid Mamba-Transformer architecture focused on byte-level processing and autoregressive prediction. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02254",
      "title": "Semi-Supervised Semantic Segmentation via Derivative Label Propagation",
      "authors": [
        "Yuanbin Fu",
        "Xiaojie Guo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Semi-supervised semantic segmentation, which leverages a limited set of\nlabeled images, helps to relieve the heavy annotation burden. While\npseudo-labeling strategies yield promising results, there is still room for\nenhancing the reliability of pseudo-labels. Hence, we develop a semi-supervised\nframework, namely DerProp, equipped with a novel derivative label propagation\nto rectify imperfect pseudo-labels. Our label propagation method imposes\ndiscrete derivative operations on pixel-wise feature vectors as additional\nregularization, thereby generating strictly regularized similarity metrics.\nDoing so effectively alleviates the ill-posed problem that identical\nsimilarities correspond to different features, through constraining the\nsolution space. Extensive experiments are conducted to verify the rationality\nof our design, and demonstrate our superiority over other methods. Codes are\navailable at https://github.com/ForawardStar/DerProp/.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02254v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02254v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.298,
      "weak_supervision_score": 0.441,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.348,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on semi-supervised semantic segmentation, where pseudo-labels are generated programmatically from model predictions on unlabeled data to reduce reliance on hand-labeled data. This aligns with weak supervision's core idea of using noisy or imprecise labels from high-level sources, as pseudo-labels can be considered programmatically derived and imperfect. However, the method still incorporates a small amount of manually annotated data, distinguishing it from pure weak supervision approaches that minimize or eliminate labeled data entirely, thus making it moderately relevant rather than highly so.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces DerProp, a semi-supervised semantic segmentation framework that addresses the limitations of pseudo-labeling by incorporating derivative label propagation (DLP) to enhance pseudo-label reliability. By applying discrete derivative operations on pixel-wise feature vectors, the method regularizes similarity metrics to mitigate the ill-posed problem where identical similarities could lead to different features, ultimately improving segmentation accuracy as validated through extensive experiments on benchmark datasets.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing label propagation techniques with discrete derivative operations to address the ill-posed problem in pseudo-label generation, offering a clever new way to refine similarities in semi-supervised semantic segmentation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of semi-supervised learning for computer vision tasks, such as semantic segmentation in applications like autonomous driving, due to its effective enhancement of pseudo-label reliability.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution with a novel method that improves state-of-the-art results in semantic segmentation, making it essential for researchers focused on semi-supervised techniques in computer vision.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b2400feebe8abc2d69d22643a7e3965b2a6b136f",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 4,
      "average_h_index": 2.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yuanbin Fu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/108145101"
        },
        {
          "name": "Xiaojie Guo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375291883"
        }
      ]
    },
    {
      "id": "2508.02255",
      "title": "StutterCut: Uncertainty-Guided Normalised Cut for Dysfluency\n  Segmentation",
      "authors": [
        "Suhita Ghosh",
        "Melanie Jouaiti",
        "Jan-Ole Perschewski",
        "Sebastian Stober"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Detecting and segmenting dysfluencies is crucial for effective speech therapy\nand real-time feedback. However, most methods only classify dysfluencies at the\nutterance level. We introduce StutterCut, a semi-supervised framework that\nformulates dysfluency segmentation as a graph partitioning problem, where\nspeech embeddings from overlapping windows are represented as graph nodes. We\nrefine the connections between nodes using a pseudo-oracle classifier trained\non weak (utterance-level) labels, with its influence controlled by an\nuncertainty measure from Monte Carlo dropout. Additionally, we extend the\nweakly labelled FluencyBank dataset by incorporating frame-level dysfluency\nboundaries for four dysfluency types. This provides a more realistic benchmark\ncompared to synthetic datasets. Experiments on real and synthetic datasets show\nthat StutterCut outperforms existing methods, achieving higher F1 scores and\nmore precise stuttering onset detection.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02255v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02255v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.422,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.349,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, StutterCut, utilizes a semi-supervised framework that trains a pseudo-oracle classifier on weak utterance-level labels to guide dysfluency segmentation. This directly embodies weak supervision by programmatically generating insights from high-level, noisy labels, avoiding the need for precise, hand-labeled data, and aligns with the topic's definition of using imprecise sources for model training and refinement.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "StutterCut is a semi-supervised framework designed to segment dysfluencies in speech by formulating the problem as a graph partitioning task, where speech embeddings from overlapping windows are treated as nodes and refined using a pseudo-oracle classifier guided by uncertainty from Monte Carlo dropout. The method extends the weakly labeled FluencyBank dataset to include frame-level labels for better benchmarking, and experiments on real and synthetic datasets demonstrate superior performance over existing approaches, with higher F1 scores and more accurate detection of stuttering onsets.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining graph partitioning with uncertainty-guided refinement to address dysfluency segmentation, offering a clever adaptation of existing techniques for a specific, understudied problem. While it builds on known methods like Normalized Cut, the integration of a pseudo-oracle and uncertainty measures provides a new way to handle weak labels.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in speech processing and AI applications for speech therapy by providing a more precise segmentation method and a new dataset, potentially leading to citations and developments in subfields like audio analysis for disabilities. However, its specialized focus on dysfluency segmentation limits broader applicability beyond niche areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to dysfluency detection with innovative methodology and practical dataset extension, making it important for researchers in AI and speech processing to be aware of for advancing related work. While not essential for all, it provides strong insights and outperforms existing methods, warranting attention in its domain.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/971e09122cbd37fbe8dd4f0202021610688f65cb",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 5,
      "average_h_index": 3.25,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Suhita Ghosh",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/1736713742"
        },
        {
          "name": "Mélanie Jouaiti",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2266557561"
        },
        {
          "name": "Jan-Ole Perschewski",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/107962695"
        },
        {
          "name": "Sebastian Stober",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2240914568"
        }
      ]
    },
    {
      "id": "2508.02258",
      "title": "Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented\n  Generation for Pathology VLMs via Reinforcement Learning",
      "authors": [
        "Wenchuan Zhang",
        "Jingru Guo",
        "Hengzhe Zhang",
        "Penghao Zhang",
        "Jie Chen",
        "Shuwan Zhang",
        "Zhang Zhang",
        "Yuhao Yi",
        "Hong Bu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Although Vision Language Models (VLMs) have shown strong generalization in\nmedical imaging, pathology presents unique challenges due to ultra-high\nresolution, complex tissue structures, and nuanced clinical semantics. These\nfactors make pathology VLMs prone to hallucinations, i.e., generating outputs\ninconsistent with visual evidence, which undermines clinical trust. Existing\nRAG approaches in this domain largely depend on text-based knowledge bases,\nlimiting their ability to leverage diagnostic visual cues. To address this, we\npropose Patho-AgenticRAG, a multimodal RAG framework with a database built on\npage-level embeddings from authoritative pathology textbooks. Unlike\ntraditional text-only retrieval systems, it supports joint text-image search,\nenabling direct retrieval of textbook pages that contain both the queried text\nand relevant visual cues, thus avoiding the loss of critical image-based\ninformation. Patho-AgenticRAG also supports reasoning, task decomposition, and\nmulti-turn search interactions, improving accuracy in complex diagnostic\nscenarios. Experiments show that Patho-AgenticRAG significantly outperforms\nexisting multimodal models in complex pathology tasks like multiple-choice\ndiagnosis and visual question answering. Our project is available at the\nPatho-AgenticRAG repository:\nhttps://github.com/Wenchuan-Zhang/Patho-AgenticRAG.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02258v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02258v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.395,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.496,
      "distributed_training_score": 0.323,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a multimodal RAG framework for pathology VLMs, utilizing reinforcement learning for agentic planning and retrieval, without any reference to diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity. It focuses on retrieval, reasoning via agents, and RL optimization, which do not align with diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02260",
      "title": "Decomposing the Entropy-Performance Exchange: The Missing Keys to\n  Unlocking Effective Reinforcement Learning",
      "authors": [
        "Jia Deng",
        "Jie Chen",
        "Zhipeng Chen",
        "Wayne Xin Zhao",
        "Ji-Rong Wen"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recently, reinforcement learning with verifiable rewards (RLVR) has been\nwidely used for enhancing the reasoning abilities of large language models\n(LLMs). A core challenge in RLVR involves managing the exchange between entropy\nand performance of policies. Despite the importance of this exchange, a\nfine-grained understanding of when and how this exchange operates most\neffectively remains limited. To bridge this gap, we conduct a systematic\nempirical analysis of the entropy-performance exchange mechanism of RLVR across\ndifferent levels of granularity. Specifically, we first divide the training\nprocess into two distinct stages based on entropy dynamics, i.e., rising stage\nand plateau stage, and then systematically investigate how this mechanism\nvaries across stage-level, instance-level, and token-level granularitiess. Our\nanalysis reveals that, in the rising stage, entropy reduction in negative\nsamples facilitates the learning of effective reasoning patterns, which in turn\ndrives rapid performance gains. Moreover, in the plateau stage, learning\nefficiency strongly correlates with high-entropy tokens present in\nlow-perplexity samples and those located at the end of sequences. Motivated by\nthese findings, we propose two methods that dynamically adjust the reward\nsignal using perplexity and positional information to focus RL updates on\ntokens that exhibit high learning potential, achieving improvements compared to\nthe baseline methods on various LLMs.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02260v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02260v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.494,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.487,
      "distributed_training_score": 0.387,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Reinforcement Learning with Verifiable Rewards (RLVR), which uses automated verifier-assigned rewards to enhance LLMs' reasoning, without any involvement of human feedback, human-ranked data, or a separate reward model trained on human preferences. Thus, it does not align with RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines entropy-performance dynamics in RLVR for LLMs, including reward adjustments and granularity analysis, but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02261",
      "title": "SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene\n  Completion",
      "authors": [
        "Rui Qian",
        "Haozhi Cao",
        "Tianchen Deng",
        "Shenghai Yuan",
        "Lihua Xie"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Monocular 3D Semantic Scene Completion (SSC) is a challenging yet promising\ntask that aims to infer dense geometric and semantic descriptions of a scene\nfrom a single image. While recent object-centric paradigms significantly\nimprove efficiency by leveraging flexible 3D Gaussian primitives, they still\nrely heavily on a large number of randomly initialized primitives, which\ninevitably leads to 1) inefficient primitive initialization and 2) outlier\nprimitives that introduce erroneous artifacts. In this paper, we propose\nSplatSSC, a novel framework that resolves these limitations with a depth-guided\ninitialization strategy and a principled Gaussian aggregator. Instead of random\ninitialization, SplatSSC utilizes a dedicated depth branch composed of a\nGroup-wise Multi-scale Fusion (GMF) module, which integrates multi-scale image\nand depth features to generate a sparse yet representative set of initial\nGaussian primitives. To mitigate noise from outlier primitives, we develop the\nDecoupled Gaussian Aggregator (DGA), which enhances robustness by decomposing\ngeometric and semantic predictions during the Gaussian-to-voxel splatting\nprocess. Complemented with a specialized Probability Scale Loss, our method\nachieves state-of-the-art performance on the Occ-ScanNet dataset, outperforming\nprior approaches by over 6.3% in IoU and 4.1% in mIoU, while reducing both\nlatency and memory consumption by more than 9.3%. The code will be released\nupon acceptance.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02261v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02261v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.25,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.321,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02265",
      "title": "Semi-Supervised Dual-Threshold Contrastive Learning for Ultrasound Image\n  Classification and Segmentation",
      "authors": [
        "Peng Zhang",
        "Zhihui Lai",
        "Heng Kong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Confidence-based pseudo-label selection usually generates overly confident\nyet incorrect predictions, due to the early misleadingness of model and\noverfitting inaccurate pseudo-labels in the learning process, which heavily\ndegrades the performance of semi-supervised contrastive learning. Moreover,\nsegmentation and classification tasks are treated independently and the\naffinity fails to be fully explored. To address these issues, we propose a\nnovel semi-supervised dual-threshold contrastive learning strategy for\nultrasound image classification and segmentation, named Hermes. This strategy\ncombines the strengths of contrastive learning with semi-supervised learning,\nwhere the pseudo-labels assist contrastive learning by providing additional\nguidance. Specifically, an inter-task attention and saliency module is also\ndeveloped to facilitate information sharing between the segmentation and\nclassification tasks. Furthermore, an inter-task consistency learning strategy\nis designed to align tumor features across both tasks, avoiding negative\ntransfer for reducing features discrepancy. To solve the lack of publicly\navailable ultrasound datasets, we have collected the SZ-TUS dataset, a thyroid\nultrasound image dataset. Extensive experiments on two public ultrasound\ndatasets and one private dataset demonstrate that Hermes consistently\noutperforms several state-of-the-art methods across various semi-supervised\nsettings.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02265v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02265v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.293,
      "distributed_training_score": 0.3,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves semi-supervised learning with pseudo-labels, which are programmatically generated from model predictions and can be noisy or imprecise, aligning directly with weak supervision. It addresses the challenges of low-quality pseudo-labels through a dual-threshold strategy, emphasizing the use of high-level, unreliable labels to train models without relying solely on hand-labeled data. This core mechanism fits the definition of weak supervision as it leverages programmatically created labels to enhance learning on unlabeled ultrasound images.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Hermes, a semi-supervised dual-threshold contrastive learning framework designed to improve ultrasound image classification and segmentation by addressing issues with pseudo-label quality and task integration. It employs a dual-threshold strategy combining confidence and uncertainty for better pseudo-label selection, an inter-task attention and saliency module for information sharing between tasks, and an inter-task consistency learning approach to align features, demonstrating superior performance on public and private datasets, including a newly collected thyroid ultrasound dataset (SZ-TUS).",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing ideas like contrastive learning and uncertainty estimation to enhance pseudo-label quality and integrate segmentation and classification tasks, offering a notable improvement for semi-supervised learning in ultrasound images rather than introducing a entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of medical image analysis, particularly for ultrasound applications, due to its practical enhancements and the release of a new dataset, though its influence may remain confined to specialized areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides high-quality contributions to semi-supervised learning for medical imaging, making it valuable for researchers in computer vision and healthcare, as it addresses relevant challenges and introduces useful innovations.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e82b86bfd102c46265e807fa1f717567f9533a84",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 10,
      "average_h_index": 4.333333333333333,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Peng Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2348356877"
        },
        {
          "name": "Z. Lai",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2244343730"
        },
        {
          "name": "Heng Kong",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2054238302"
        }
      ]
    },
    {
      "id": "2508.02269",
      "title": "AirTrafficGen: Configurable Air Traffic Scenario Generation with Large\n  Language Models",
      "authors": [
        "Dewi Sid William Gould",
        "George De Ath",
        "Ben Carvell",
        "Nick Pepper"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The manual design of scenarios for Air Traffic Control (ATC) training is a\ndemanding and time-consuming bottleneck that limits the diversity of\nsimulations available to controllers. To address this, we introduce a novel,\nend-to-end approach, $\\texttt{AirTrafficGen}$, that leverages large language\nmodels (LLMs) to automate and control the generation of complex ATC scenarios.\nOur method uses a purpose-built, graph-based representation to encode sector\ntopology (including airspace geometry, routes, and fixes) into a format LLMs\ncan process. Through rigorous benchmarking, we show that state-of-the-art\nmodels like Gemini 2.5 Pro, OpenAI o3, GPT-oss-120b and GPT-5 can generate\nhigh-traffic scenarios while maintaining operational realism. Our engineered\nprompting enables fine-grained control over interaction presence, type, and\nlocation. Initial findings suggest these models are also capable of iterative\nrefinement, correcting flawed scenarios based on simple textual feedback. This\napproach provides a scalable alternative to manual scenario design, addressing\nthe need for a greater volume and variety of ATC training and validation\nsimulations. More broadly, this work showcases the potential of LLMs for\ncomplex planning in safety-critical domains.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02269v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02269v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.389,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using pre-trained LLMs for generating and refining air traffic scenarios via engineered prompting and textual feedback, but it does not involve training a reward model on human-ranked data or fine-tuning the main model through reinforcement learning. The iterative refinement mentioned is based on simple feedback in prompts, not RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes LLMs for scenario generation and iterative refinement through feedback, but it does not adapt the iterative refinement process of diffusion models or treat a Chain-of-Thought as a single entity for multi-step logical reasoning. There is no evidence of diffusion-based mechanisms in the described approach.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02271",
      "title": "Dynaword: From One-shot to Continuously Developed Datasets",
      "authors": [
        "Kenneth Enevoldsen",
        "Kristian Nørgaard Jensen",
        "Jan Kostkan",
        "Balázs Szabó",
        "Márton Kardos",
        "Kirten Vad",
        "Johan Heinsen",
        "Andrea Blasi Núñez",
        "Gianluca Barmina",
        "Jacob Nielsen",
        "Rasmus Larsen",
        "Peter Vahlstrup",
        "Per Møldrup Dalum",
        "Desmond Elliott",
        "Lukas Galke",
        "Peter Schneider-Kamp",
        "Kristoffer Nielbo"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large-scale datasets are foundational for research and development in natural\nlanguage processing. However, current approaches face three key challenges: (1)\nreliance on ambiguously licensed sources restricting use, sharing, and\nderivative works; (2) static dataset releases that prevent community\ncontributions and diminish longevity; and (3) quality assurance processes\nrestricted to publishing teams rather than leveraging community expertise.\n  To address these limitations, we introduce two contributions: the Dynaword\napproach and Danish Dynaword. The Dynaword approach is a framework for creating\nlarge-scale, open datasets that can be continuously updated through community\ncollaboration. Danish Dynaword is a concrete implementation that validates this\napproach and demonstrates its potential. Danish Dynaword contains over four\ntimes as many tokens as comparable releases, is exclusively openly licensed,\nand has received multiple contributions across industry and research. The\nrepository includes light-weight tests to ensure data formatting, quality, and\ndocumentation, establishing a sustainable framework for ongoing community\ncontributions and dataset evolution.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02271v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02271v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.382,
      "datasets_score": 0.482,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a framework for creating and maintaining open, continuously updated datasets, focusing on licensing, reproducibility, and community collaboration. It does not involve machine learning techniques like programmatically generating labels from noisy sources for model training, which is central to weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper directly addresses research on datasets by introducing the Dynaword framework for dataset creation and curation, emphasizing methodologies for open licensing, reproducibility, documentation, and community contributions. It also presents Danish Dynaword as a new dataset implementation, aligning with topics like new dataset introduction and dataset analysis.",
      "llm_score_status": "completed",
      "summary": "The paper addresses key challenges in natural language processing datasets, such as ambiguous licensing, static releases, and limited community involvement, by introducing the Dynaword approach—a framework for creating large-scale, openly licensed datasets that support continuous updates and community contributions. It validates this methodology through the implementation of Danish Dynaword, which significantly increases available tokens, ensures traceable licensing and reproducibility, and demonstrates practical benefits like enhanced quality assurance and extensibility.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing open-source principles with dataset development to enable continuous updates and community collaboration, addressing known issues in NLP datasets without introducing an entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the NLP and AI subfields, particularly for low-to-mid-resource languages, as it promotes sustainable dataset practices that could enhance future research and development.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a high-quality framework that tackles important practical issues in dataset creation, making it a valuable resource for researchers in computational linguistics and AI.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7e335e2c5971877c12e2c8bfb10d3cab8dbb5d24",
      "total_authors": 17,
      "authors_found": 17,
      "highest_h_index": 5,
      "average_h_index": 1.2352941176470589,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Kenneth C. Enevoldsen",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2307474874"
        },
        {
          "name": "Kristian Norgaard Jensen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374487884"
        },
        {
          "name": "Jan Kostkan",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2127378156"
        },
        {
          "name": "B. Szab'o",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2348258795"
        },
        {
          "name": "Márton Kardos",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2306780791"
        },
        {
          "name": "Kirten Vad",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374489506"
        },
        {
          "name": "Johan Heinsen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374947054"
        },
        {
          "name": "Andrea Blasi N'unez",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374485106"
        },
        {
          "name": "Gianluca Barmina",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374489512"
        },
        {
          "name": "Jacob Nielsen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2311381323"
        },
        {
          "name": "Rasmus Larsen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374487111"
        },
        {
          "name": "P. Vahlstrup",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2976957"
        },
        {
          "name": "Per Moldrup Dalum",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374486670"
        },
        {
          "name": "Desmond Elliott",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374487893"
        },
        {
          "name": "Lukas Galke",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2330189040"
        },
        {
          "name": "Peter Schneider-Kamp",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2266016715"
        },
        {
          "name": "Kristoffer L. Nielbo",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2306781384"
        }
      ]
    },
    {
      "id": "2508.02276",
      "title": "CellForge: Agentic Design of Virtual Cell Models",
      "authors": [
        "Xiangru Tang",
        "Zhuoyun Yu",
        "Jiapeng Chen",
        "Yan Cui",
        "Daniel Shao",
        "Weixu Wang",
        "Fang Wu",
        "Yuchen Zhuang",
        "Wenqi Shi",
        "Zhi Huang",
        "Arman Cohan",
        "Xihong Lin",
        "Fabian Theis",
        "Smita Krishnaswamy",
        "Mark Gerstein"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Virtual cell modeling represents an emerging frontier at the intersection of\nartificial intelligence and biology, aiming to predict quantities such as\nresponses to diverse perturbations quantitatively. However, autonomously\nbuilding computational models for virtual cells is challenging due to the\ncomplexity of biological systems, the heterogeneity of data modalities, and the\nneed for domain-specific expertise across multiple disciplines. Here, we\nintroduce CellForge, an agentic system that leverages a multi-agent framework\nthat transforms presented biological datasets and research objectives directly\ninto optimized computational models for virtual cells. More specifically, given\nonly raw single-cell multi-omics data and task descriptions as input, CellForge\noutputs both an optimized model architecture and executable code for training\nvirtual cell models and inference. The framework integrates three core modules:\nTask Analysis for presented dataset characterization and relevant literature\nretrieval, Method Design, where specialized agents collaboratively develop\noptimized modeling strategies, and Experiment Execution for automated\ngeneration of code. The agents in the Design module are separated into experts\nwith differing perspectives and a central moderator, and have to\ncollaboratively exchange solutions until they achieve a reasonable consensus.\nWe demonstrate CellForge's capabilities in single-cell perturbation prediction,\nusing six diverse datasets that encompass gene knockouts, drug treatments, and\ncytokine stimulations across multiple modalities. CellForge consistently\noutperforms task-specific state-of-the-art methods. Overall, CellForge\ndemonstrates how iterative interaction between LLM agents with differing\nperspectives provides better solutions than directly addressing a modeling\nchallenge. Our code is publicly available at\nhttps://github.com/gersteinlab/CellForge.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02276v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02276v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.393,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of CellForge, a multi-agent AI framework for autonomously designing and optimizing virtual cell models through iterative agent collaboration, literature analysis, and code generation. While it involves iterative refinement in agent discussions (e.g., proposing, critiquing, and converging on model designs), this process is based on multi-agent interactions and LLM-based consensus-building, not on adapting diffusion models for logical tasks. The paper does not reference diffusion-based mechanisms, such as treating a Chain-of-Thought as a holistically refined entity via diffusion processes, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02278",
      "title": "SGAD: Semantic and Geometric-aware Descriptor for Local Feature Matching",
      "authors": [
        "Xiangzeng Liu",
        "Chi Wang",
        "Guanglu Shi",
        "Xiaodong Zhang",
        "Qiguang Miao",
        "Miao Fan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Local feature matching remains a fundamental challenge in computer vision.\nRecent Area to Point Matching (A2PM) methods have improved matching accuracy.\nHowever, existing research based on this framework relies on inefficient\npixel-level comparisons and complex graph matching that limit scalability. In\nthis work, we introduce the Semantic and Geometric-aware Descriptor Network\n(SGAD), which fundamentally rethinks area-based matching by generating highly\ndiscriminative area descriptors that enable direct matching without complex\ngraph optimization. This approach significantly improves both accuracy and\nefficiency of area matching. We further improve the performance of area\nmatching through a novel supervision strategy that decomposes the area matching\ntask into classification and ranking subtasks. Finally, we introduce the\nHierarchical Containment Redundancy Filter (HCRF) to eliminate overlapping\nareas by analyzing containment graphs. SGAD demonstrates remarkable performance\ngains, reducing runtime by 60x (0.82s vs. 60.23s) compared to MESA. Extensive\nevaluations show consistent improvements across multiple point matchers:\nSGAD+LoFTR reduces runtime compared to DKM, while achieving higher accuracy\n(0.82s vs. 1.51s, 65.98 vs. 61.11) in outdoor pose estimation, and SGAD+ROMA\ndelivers +7.39% AUC@5{\\deg} in indoor pose estimation, establishing a new\nstate-of-the-art.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02278v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02278v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.351,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02279",
      "title": "Dialogue Systems Engineering: A Survey and Future Directions",
      "authors": [
        "Mikio Nakano",
        "Hironori Takeuchi",
        "Sadahiro Yoshikawa",
        "Yoichi Matsuyama",
        "Kazunori Komatani"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "This paper proposes to refer to the field of software engineering related to\nthe life cycle of dialogue systems as Dialogue Systems Engineering, and surveys\nthis field while also discussing its future directions. With the advancement of\nlarge language models, the core technologies underlying dialogue systems have\nsignificantly progressed. As a result, dialogue system technology is now\nexpected to be applied to solving various societal issues and in business\ncontexts. To achieve this, it is important to build, operate, and continuously\nimprove dialogue systems correctly and efficiently. Accordingly, in addition to\napplying existing software engineering knowledge, it is becoming increasingly\nimportant to evolve software engineering tailored specifically to dialogue\nsystems. In this paper, we enumerate the knowledge areas of dialogue systems\nengineering based on those of software engineering, as defined in the Software\nEngineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based\non this survey, we identify unexplored topics in each area and discuss the\nfuture direction of dialogue systems engineering.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02279v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02279v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.336,
      "datasets_score": 0.406,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper primarily surveys software engineering aspects for dialogue systems, such as their life cycle and future directions, based on SWEBOK. While it briefly mentions machine learning methods and data in the introduction as part of broader dialogue system research, it does not focus on creating, analyzing, benchmarking, or evaluating datasets. Thus, datasets are only indirectly related through the general context of dialogue systems.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02281",
      "title": "Do Edges Matter? Investigating Edge-Enhanced Pre-Training for Medical\n  Image Segmentation",
      "authors": [
        "Paul Zaha",
        "Lars Böcking",
        "Simeon Allmendinger",
        "Leopold Müller",
        "Niklas Kühl"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Medical image segmentation is crucial for disease diagnosis and treatment\nplanning, yet developing robust segmentation models often requires substantial\ncomputational resources and large datasets. Existing research shows that\npre-trained and finetuned foundation models can boost segmentation performance.\nHowever, questions remain about how particular image preprocessing steps may\ninfluence segmentation performance across different medical imaging modalities.\nIn particular, edges-abrupt transitions in pixel intensity-are widely\nacknowledged as vital cues for object boundaries but have not been\nsystematically examined in the pre-training of foundation models. We address\nthis gap by investigating to which extend pre-training with data processed\nusing computationally efficient edge kernels, such as kirsch, can improve\ncross-modality segmentation capabilities of a foundation model. Two versions of\na foundation model are first trained on either raw or edge-enhanced data across\nmultiple medical imaging modalities, then finetuned on selected raw subsets\ntailored to specific medical modalities. After systematic investigation using\nthe medical domains Dermoscopy, Fundus, Mammography, Microscopy, OCT, US, and\nXRay, we discover both increased and reduced segmentation performance across\nmodalities using edge-focused pre-training, indicating the need for a selective\napplication of this approach. To guide such selective applications, we propose\na meta-learning strategy. It uses standard deviation and image entropy of the\nraw image to choose between a model pre-trained on edge-enhanced or on raw data\nfor optimal performance. Our experiments show that integrating this\nmeta-learning layer yields an overall segmentation performance improvement\nacross diverse medical imaging tasks by 16.42% compared to models pre-trained\non edge-enhanced data only and 19.30% compared to models pre-trained on raw\ndata only.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02281v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02281v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.362,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02288",
      "title": "Unleashing the Temporal Potential of Stereo Event Cameras for\n  Continuous-Time 3D Object Detection",
      "authors": [
        "Jae-Young Kang",
        "Hoonhee Cho",
        "Kuk-Jin Yoon"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D object detection is essential for autonomous systems, enabling precise\nlocalization and dimension estimation. While LiDAR and RGB cameras are widely\nused, their fixed frame rates create perception gaps in high-speed scenarios.\nEvent cameras, with their asynchronous nature and high temporal resolution,\noffer a solution by capturing motion continuously. The recent approach, which\nintegrates event cameras with conventional sensors for continuous-time\ndetection, struggles in fast-motion scenarios due to its dependency on\nsynchronized sensors. We propose a novel stereo 3D object detection framework\nthat relies solely on event cameras, eliminating the need for conventional 3D\nsensors. To compensate for the lack of semantic and geometric information in\nevent data, we introduce a dual filter mechanism that extracts both.\nAdditionally, we enhance regression by aligning bounding boxes with\nobject-centric information. Experiments show that our method outperforms prior\napproaches in dynamic environments, demonstrating the potential of event\ncameras for robust, continuous-time 3D perception. The code is available at\nhttps://github.com/mickeykang16/Ev-Stereo3D.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02288v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02288v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.241,
      "weak_supervision_score": 0.278,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.31,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02291",
      "title": "Flexible Automatic Identification and Removal (FAIR)-Pruner: An\n  Efficient Neural Network Pruning Method",
      "authors": [
        "Chenqing Lin",
        "Mostafa Hussien",
        "Chengyao Yu",
        "Mohamed Cheriet",
        "Osama Abdelrahman",
        "Ruixing Ming"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Neural network pruning is a critical compression technique that facilitates\nthe deployment of large-scale neural networks on resource-constrained edge\ndevices, typically by identifying and eliminating redundant or insignificant\nparameters to reduce computational and memory overhead. This paper proposes the\nFlexible Automatic Identification and Removal (FAIR)-Pruner, a novel method for\nneural network structured pruning. Specifically, FAIR-Pruner first evaluates\nthe importance of each unit (e.g., neuron or channel) through the Utilization\nScore quantified by the Wasserstein distance. To reflect the performance\ndegradation after unit removal, it then introduces the Reconstruction Error,\nwhich is computed via the Taylor expansion of the loss function. Finally,\nFAIR-Pruner identifies superfluous units with negligible impact on model\nperformance by controlling the proposed Tolerance of Difference, which measures\ndifferences between unimportant units and those that cause performance\ndegradation. A major advantage of FAIR-Pruner lies in its capacity to\nautomatically determine the layer-wise pruning rates, which yields a more\nefficient subnetwork structure compared to applying a uniform pruning rate.\nAnother advantage of the FAIR-Pruner is its great one-shot performance without\npost-pruning fine-tuning. Furthermore, with utilization scores and\nreconstruction errors, users can flexibly obtain pruned models under different\npruning ratios. Comprehensive experimental validation on diverse benchmark\ndatasets (e.g., ImageNet) and various neural network architectures (e.g., VGG)\ndemonstrates that FAIR-Pruner achieves significant model compression while\nmaintaining high accuracy.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02291v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02291v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.41,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a method for neural network pruning to compress models by identifying and removing redundant parameters, focusing on techniques like Wasserstein distance and Taylor expansion for efficient deployment on edge devices. It does not address distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes, as its scope is limited to model optimization post-training rather than accelerating the training process.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02292",
      "title": "FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI\n  Research and Deployment",
      "authors": [
        "Wentao Zhang",
        "Yilei Zhao",
        "Chuqiao Zong",
        "Xinrun Wang",
        "Bo An"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Financial AI holds great promise for transforming modern finance, with the\npotential to support a wide range of tasks such as market forecasting,\nportfolio management, quantitative trading, and automated analysis. However,\nexisting platforms remain limited in task coverage, lack robust multimodal data\nintegration, and offer insufficient support for the training and deployment of\nlarge language models (LLMs). In response to these limitations, we present\nFinWorld, an all-in-one open-source platform that provides end-to-end support\nfor the entire financial AI workflow, from data acquisition to experimentation\nand deployment. FinWorld distinguishes itself through native integration of\nheterogeneous financial data, unified support for diverse AI paradigms, and\nadvanced agent automation, enabling seamless development and deployment.\nLeveraging data from 2 representative markets, 4 stock pools, and over 800\nmillion financial data points, we conduct comprehensive experiments on 4 key\nfinancial AI tasks. These experiments systematically evaluate deep learning and\nreinforcement learning algorithms, with particular emphasis on RL-based\nfinetuning for LLMs and LLM Agents. The empirical results demonstrate that\nFinWorld significantly enhances reproducibility, supports transparent\nbenchmarking, and streamlines deployment, thereby providing a strong foundation\nfor future research and real-world applications. Code is available at\nGithub~\\footnote{https://github.com/DVampire/FinWorld}.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02292v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02292v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.428,
      "datasets_score": 0.384,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on RL-based fine-tuning for LLMs and agents in financial tasks, such as trading and forecasting, but does not involve human feedback, a reward model trained on human-ranked data, or alignment with human preferences, which are essential for RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's platform includes explicit support for distributed multi-GPU training and testing, enabling parallel computing and high-performance processing across multiple environments, directly aligning with distributed training techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "FinWorld is an open-source platform designed to address the limitations of existing financial AI tools by providing an end-to-end solution for tasks like market forecasting, portfolio management, and quantitative trading, through native integration of heterogeneous financial data and support for diverse AI paradigms including ML, DL, RL, and LLMs. The methodology involves a unified framework with features such as multi-task support, multimodal data integration, high extensibility, and advanced automation, demonstrated through comprehensive experiments on four key financial AI tasks using over 800 million data points from various markets, which highlight improved reproducibility, transparent benchmarking, and streamlined deployment.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing ideas into a unified, extensible platform that addresses key gaps in task coverage and data integration, rather than introducing entirely new problems or techniques.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the financial AI subfield due to its open-source nature and comprehensive benchmarking tools, though its influence may be limited to specialized applications rather than broader commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable, high-quality contribution for researchers and practitioners in financial AI by providing a practical platform for development and deployment, making it worth reading for those engaged in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/81c16aaa6485fc684e0760d474584aabe0f14200",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 13,
      "average_h_index": 6.4,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Wentao Zhang",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2267419431"
        },
        {
          "name": "Yilei Zhao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2267697818"
        },
        {
          "name": "Chuqiao Zong",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2288278883"
        },
        {
          "name": "Xinrun Wang",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/10737491"
        },
        {
          "name": "Bo An",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2277251460"
        }
      ]
    },
    {
      "id": "2508.02293",
      "title": "Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning",
      "authors": [
        "Muhammad Aqeel",
        "Shakiba Sharifi",
        "Marco Cristani",
        "Francesco Setti"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "So-called unsupervised anomaly detection is better described as\nsemi-supervised, as it assumes all training data are nominal. This assumption\nsimplifies training but requires manual data curation, introducing bias and\nlimiting adaptability. We propose Confident Meta-learning (CoMet), a novel\ntraining strategy that enables deep anomaly detection models to learn from\nuncurated datasets where nominal and anomalous samples coexist, eliminating the\nneed for explicit filtering. Our approach integrates Soft Confident Learning,\nwhich assigns lower weights to low-confidence samples, and Meta-Learning, which\nstabilizes training by regularizing updates based on training validation loss\ncovariance. This prevents overfitting and enhances robustness to noisy data.\nCoMet is model-agnostic and can be applied to any anomaly detection method\ntrainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2\nwith two state-of-the-art models demonstrate the effectiveness of our approach,\nconsistently improving over the baseline methods, remaining insensitive to\nanomalies in the training set, and setting a new state-of-the-art across all\ndatasets.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02293v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02293v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.457,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.377,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, Confident Meta-learning (CoMet), involves training anomaly detection models on uncurated datasets with potentially noisy or anomalous samples by assigning weights based on sample confidence, which helps mitigate the impact of imperfect data. This aligns with weak supervision's focus on handling noisy or imprecise labels, as both approaches aim to learn from imperfect training data without relying on perfectly hand-labeled examples. However, the paper does not involve programmatically generating labels from high-level sources, which is a core aspect of weak supervision, making the relevance moderate rather than high.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper addresses the limitations of traditional unsupervised anomaly detection, which is essentially semi-supervised due to the assumption of anomaly-free training data, by proposing Confident Meta-learning (CoMet), a novel training strategy that combines Soft Confident Learning to assign lower weights to low-confidence samples and Meta-Learning to stabilize training and prevent overfitting. This approach enables deep anomaly detection models to learn from uncurated datasets containing both nominal and anomalous samples, making it model-agnostic and applicable to methods trainable via gradient descent; experiments on datasets like MVTec-AD, VIADUCT, and KSDD2 with state-of-the-art models demonstrate significant improvements, achieving new benchmarks in performance and robustness.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by combining Soft Confident Learning and Meta-Learning to handle uncurated datasets in anomaly detection, significantly advancing the state-of-the-art by eliminating the need for manual data curation. This represents a fresh approach to a persistent problem in unsupervised learning, rather than just refining existing methods.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research in computer vision and machine learning by enabling more robust anomaly detection in real-world applications, such as industrial manufacturing, and could lead to broader adoption due to its model-agnostic nature and state-of-the-art results. Its ability to handle noisy data makes it particularly relevant for practical scenarios, potentially driving commercial innovations.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality and innovative contribution that advances anomaly detection techniques, making it valuable for researchers in computer vision and machine learning to understand and build upon. While not transformative across all fields, its practical implications and strong experimental results justify it as an important read in its subfield.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d726e991e68ed892bd4c42c8c8150ebc71ae1b9e",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 6,
      "average_h_index": 4.25,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Muhammad Aqeel",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2289873810"
        },
        {
          "name": "Shakiba Sharifi",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2316560444"
        },
        {
          "name": "Marco Cristani",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2238815087"
        },
        {
          "name": "Francesco Setti",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2329986546"
        }
      ]
    },
    {
      "id": "2508.02298",
      "title": "CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative\n  Credit Assignment",
      "authors": [
        "Guofu Xie",
        "Yunsheng Shi",
        "Hongtao Tian",
        "Ting Yao",
        "Xiao Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback, helping to mitigate reward hacking. However, current RLVR methods\ntypically treat whole responses as single actions, assigning the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies and inefficient learning.\nMethods like PPO provide credit assignment through value estimation, but often\nyield inaccurate and unverifiable signals due to limited sampling. On the other\nhand, methods using Process Reward Models can provide step-by-step judgments\nfor each reasoning step, but they require high-quality process supervision\nlabels and are time-consuming when applied in online reinforcement learning\n(RL). To overcome these limitations, we introduce a simple but efficient method\nCredit Assignment Policy Optimization (CAPO). Given a reasoning response\nrollout from the policy model, CAPO directly leverages an off-the-shelf,\ngeneral-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to\ngenerate all step-wise critique by one pass, thereby providing verifiable\ntoken-level rewards to refine the tokens that were originally assigned\nidentical rule-based rewards. This enables more fine-grained credit assignment\nin an effective way. Furthermore, to enhance the accuracy and robustness of\nCAPO, we employ voting mechanisms that scale with the number of generated\ncritiques. Extensive experiments using different backbones like Llama and Qwen\nmodels and in different sizes show that CAPO consistently outperforms\nsupervised learning-based and RL-based fine-tuning methods across six\nchallenging mathematical benchmarks and three out-of-domain benchmarks.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02298v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02298v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.452,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.336,
      "datasets_score": 0.253,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on RLVR with rule-based and LLM-generated rewards for credit assignment, which is in the broader RL domain for LLMs. However, it does not involve training a reward model on human-ranked data or use human preferences, making it only indirectly related to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is about enhancing LLM reasoning through credit assignment in RLVR, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02307",
      "title": "Whole-body Representation Learning For Competing Preclinical Disease\n  Risk Assessment",
      "authors": [
        "Dmitrii Seletkov",
        "Sophie Starck",
        "Ayhan Can Erdur",
        "Yundi Zhang",
        "Daniel Rueckert",
        "Rickmer Braren"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Reliable preclinical disease risk assessment is essential to move public\nhealthcare from reactive treatment to proactive identification and prevention.\nHowever, image-based risk prediction algorithms often consider one condition at\na time and depend on hand-crafted features obtained through segmentation tools.\nWe propose a whole-body self-supervised representation learning method for the\npreclinical disease risk assessment under a competing risk modeling. This\napproach outperforms whole-body radiomics in multiple diseases, including\ncardiovascular disease (CVD), type 2 diabetes (T2D), chronic obstructive\npulmonary disease (COPD), and chronic kidney disease (CKD). Simulating a\npreclinical screening scenario and subsequently combining with cardiac MRI, it\nsharpens further the prediction for CVD subgroups: ischemic heart disease\n(IHD), hypertensive diseases (HD), and stroke. The results indicate the\ntranslational potential of whole-body representations as a standalone screening\nmodality and as part of a multi-modal framework within clinical workflows for\nearly personalized risk stratification. The code is available at\nhttps://github.com/yayapa/WBRLforCR/",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02307v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02307v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.347,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02312",
      "title": "A Survey on Data Security in Large Language Models",
      "authors": [
        "Kang Chen",
        "Xiuze Zhou",
        "Yuanguo Lin",
        "Jinhe Su",
        "Yuanhui Yu",
        "Li Shen",
        "Fan Lin"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs), now a foundation in advancing natural language\nprocessing, power applications such as text generation, machine translation,\nand conversational systems. Despite their transformative potential, these\nmodels inherently rely on massive amounts of training data, often collected\nfrom diverse and uncurated sources, which exposes them to serious data security\nrisks. Harmful or malicious data can compromise model behavior, leading to\nissues such as toxic output, hallucinations, and vulnerabilities to threats\nsuch as prompt injection or data poisoning. As LLMs continue to be integrated\ninto critical real-world systems, understanding and addressing these\ndata-centric security risks is imperative to safeguard user trust and system\nreliability. This survey offers a comprehensive overview of the main data\nsecurity risks facing LLMs and reviews current defense strategies, including\nadversarial training, RLHF, and data augmentation. Additionally, we categorize\nand analyze relevant datasets used for assessing robustness and security across\ndifferent domains, providing guidance for future research. Finally, we\nhighlight key research directions that focus on secure model updates,\nexplainability-driven defenses, and effective governance frameworks, aiming to\npromote the safe and responsible development of LLM technology. This work aims\nto inform researchers, practitioners, and policymakers, driving progress toward\ndata security in LLMs.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02312v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02312v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.447,
      "weak_supervision_score": 0.441,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.414,
      "datasets_score": 0.409,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper explicitly discusses RLHF as a key defense strategy for addressing data security risks in LLMs, including its role in improving model robustness and aligning outputs with human preferences, making it a central part of the survey's contributions.",
      "weak_supervision_justification": "The paper does not mention weak supervision or any methods involving programmatically generated labels; its focus is solely on data security risks, defenses, and datasets in LLMs, without reference to alternative supervision techniques.",
      "diffusion_reasoning_justification": "The paper makes no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning; it centers on data security in LLMs, including risks and defenses, without any components related to diffusion-based approaches.",
      "distributed_training_justification": "The paper does not address distributed training, parallel computing, or multi-node machine learning; its content is limited to data security risks, defenses, and datasets for LLMs, with no discussion of training acceleration methods.",
      "datasets_justification": "The paper includes a dedicated section on categorizing, analyzing, and evaluating datasets for assessing robustness and security in LLMs, providing guidance on their use, which aligns directly with research on dataset creation, benchmarking, and analysis.",
      "llm_score_status": "completed",
      "summary": "This survey paper on data security in Large Language Models (LLMs) aims to provide a comprehensive overview of risks such as data poisoning, prompt injection, and privacy breaches arising from massive, uncurated training data, while reviewing defense strategies like adversarial training, Reinforcement Learning from Human Feedback (RLHF), and data augmentation to mitigate these threats. It categorizes relevant datasets for robustness assessment, analyzes their applications across domains, and highlights future research directions including secure model updates, explainability-driven defenses, and governance frameworks to promote safe LLM development, ultimately informing researchers, practitioners, and policymakers.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by offering a comprehensive taxonomy and synthesis of data security risks specific to LLMs, filling gaps in existing surveys through clever categorization and analysis of threats and defenses.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the AI security subfield due to its structured overview of risks, defenses, and future directions, potentially influencing research and practices in LLM development.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This high-quality survey provides valuable insights into a critical and evolving area of AI security, making it essential for researchers and practitioners working on LLMs to stay informed.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/32bc9a01f4b0cbf8976bc682c282181dc6e0c1af",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 1,
      "average_h_index": 0.7142857142857143,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Kang Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359168216"
        },
        {
          "name": "Xiuze Zhou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359175868"
        },
        {
          "name": "Yuan-Pin Lin",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2285406891"
        },
        {
          "name": "Jinhe Su",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375967817"
        },
        {
          "name": "Yuanhui Yu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359627832"
        },
        {
          "name": "Li Shen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2361892512"
        },
        {
          "name": "Fan Lin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375001848"
        }
      ]
    },
    {
      "id": "2508.02314",
      "title": "Large AI Models for Wireless Physical Layer",
      "authors": [
        "Jiajia Guo",
        "Yiming Cui",
        "Shi Jin",
        "Jun Zhang"
      ],
      "categories": [
        "cs.IT (Information Theory)",
        "cs.AI (Artificial Intelligence)",
        "math.IT (Information Theory)"
      ],
      "abstract": "Large artificial intelligence models (LAMs) are transforming wireless\nphysical layer technologies through their robust generalization, multitask\nprocessing, and multimodal capabilities. This article reviews recent\nadvancements in LAM applications for physical layer communications, addressing\nlimitations of conventional AI-based approaches. LAM applications are\nclassified into two strategies: leveraging pre-trained LAMs and developing\nnative LAMs designed specifically for physical layer tasks. The motivations and\nkey frameworks of these approaches are comprehensively examined through\nmultiple use cases. Both strategies significantly improve performance and\nadaptability across diverse wireless scenarios. Future research directions,\nincluding efficient architectures, interpretability, standardized datasets, and\ncollaboration between large and small models, are proposed to advance LAM-based\nphysical layer solutions for next-generation communication systems.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02314v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02314v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.422,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper reviews applications of large AI models (LAMs) for wireless physical layer technologies, focusing on their use in communication tasks and addressing challenges like generalization. While it mentions that LAMs benefit from increased computational capacity and scaling laws, which often imply distributed training in practice, it does not directly discuss distributed training methods, parallel computing, or strategies for partitioning data/computation across nodes. Thus, the topic is only indirectly related through the general need for scalable training of large models, but it is not a core focus of the paper.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02317",
      "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo",
      "authors": [
        "Qianli Ma",
        "Yaowei Zheng",
        "Zhelun Shi",
        "Zhongkai Zhao",
        "Bin Jia",
        "Ziyue Huang",
        "Zhiqi Lin",
        "Youjie Li",
        "Jiacheng Yang",
        "Yanghua Peng",
        "Zhi Zhang",
        "Xin Liu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. We present VeOmni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. VeOmni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. VeOmni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. Using VeOmni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02317v3",
      "pdf_url": "http://arxiv.org/pdf/2508.02317v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.534,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a framework for distributed training of omni-modal LLMs, emphasizing scalability and parallelism, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning. There is no mention of adapting diffusion for reasoning tasks.",
      "distributed_training_justification": "The paper's main contribution is VeOmni, a framework that introduces model-centric distributed recipes for efficient training, including 3D parallelism, FSDP, SP, and EP, to scale omni-modal LLMs across multiple GPUs, directly aligning with distributed training, parallel computing, and multi-node machine learning techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "VeOmni is a framework designed to enhance the training of omni-modal large language models by introducing model-centric distributed recipes that decouple communication from computation, enabling efficient 3D parallelism and seamless integration of new modalities with minimal code changes. The methodology focuses on flexible configuration and composable parallel strategies, such as fully sharded data parallel and expert parallelism, achieving high throughput of over 2,800 tokens per second per GPU for a 30B parameter model and scaling to 160K context lengths across 128 GPUs, demonstrating superior efficiency and scalability in handling heterogeneous omni-modal architectures.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework that decouples communication from computation for omni-modal training, significantly advancing the state-of-the-art by addressing scalability issues in heterogeneous model architectures. This innovation enables efficient 3D parallelism and flexible modality integration, which is not adequately covered in existing systems.",
      "impact_score": "High",
      "impact_justification": "VeOmni's framework has the potential to influence a wide range of future research in multi-modal AI and commercial applications by making large-scale omni-modal training more efficient and accessible. Its demonstrated scalability on large models could lead to broader adoption in distributed computing for AI, accelerating advancements in areas like visual question answering and multimodal generation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution to distributed training for omni-modal LLMs, making it essential for researchers in AI and distributed computing to understand its innovations and applications. While not universally groundbreaking, it provides valuable insights for those working on scalable AI systems.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ed621f6e9c83216521114b483560b91fc1eb2f69",
      "total_authors": 12,
      "authors_found": 11,
      "highest_h_index": 3,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Qianli Ma",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yaowei Zheng",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2341817744"
        },
        {
          "name": "Zhelun Shi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375224335"
        },
        {
          "name": "Zhongkai Zhao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2355244769"
        },
        {
          "name": "Bin Jia",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374485798"
        },
        {
          "name": "Ziyue Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374490693"
        },
        {
          "name": "Zhiqi Lin",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2350817918"
        },
        {
          "name": "Youjie Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374476340"
        },
        {
          "name": "Jiacheng Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374916579"
        },
        {
          "name": "Yanghua Peng",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2356941574"
        },
        {
          "name": "Zhi Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2287236091"
        },
        {
          "name": "Xin Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375042996"
        }
      ]
    },
    {
      "id": "2508.02319",
      "title": "Is Uncertainty Quantification a Viable Alternative to Learned Deferral?",
      "authors": [
        "Anna M. Wundram",
        "Christian F. Baumgartner"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Artificial Intelligence (AI) holds the potential to dramatically improve\npatient care. However, it is not infallible, necessitating\nhuman-AI-collaboration to ensure safe implementation. One aspect of AI safety\nis the models' ability to defer decisions to a human expert when they are\nlikely to misclassify autonomously. Recent research has focused on methods that\nlearn to defer by optimising a surrogate loss function that finds the optimal\ntrade-off between predicting a class label or deferring. However, during\nclinical translation, models often face challenges such as data shift.\nUncertainty quantification methods aim to estimate a model's confidence in its\npredictions. However, they may also be used as a deferral strategy which does\nnot rely on learning from specific training distribution. We hypothesise that\nmodels developed to quantify uncertainty are more robust to out-of-distribution\n(OOD) input than learned deferral models that have been trained in a supervised\nfashion. To investigate this hypothesis, we constructed an extensive evaluation\nstudy on a large ophthalmology dataset, examining both learned deferral models\nand established uncertainty quantification methods, assessing their performance\nin- and out-of-distribution. Specifically, we evaluate their ability to\naccurately classify glaucoma from fundus images while deferring cases with a\nhigh likelihood of error. We find that uncertainty quantification methods may\nbe a promising choice for AI deferral.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02319v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02319v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.431,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.364,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on supervised learning methods for AI deferral and uncertainty quantification in medical imaging, such as optimizing surrogate loss functions and evaluating robustness to out-of-distribution data. It does not involve reinforcement learning, human feedback, or aligning models with preferences through a reward model.",
      "weak_supervision_justification": "The paper describes training learned deferral models using standard supervised approaches with surrogate loss functions on a labeled ophthalmology dataset, without any mention of programmatically generating noisy labels or relying on high-level, imprecise sources for training data.",
      "diffusion_reasoning_justification": "The paper examines classification, deferral mechanisms, and uncertainty quantification for medical AI, with no reference to diffusion models, iterative refinement processes, or multi-step logical reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02320",
      "title": "Zero-shot Compositional Action Recognition with Neural Logic Constraints",
      "authors": [
        "Gefan Ye",
        "Lin Li",
        "Kexin Li",
        "Jun Xiao",
        "Long Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Zero-shot compositional action recognition (ZS-CAR) aims to identify unseen\nverb-object compositions in the videos by exploiting the learned knowledge of\nverb and object primitives during training. Despite compositional learning's\nprogress in ZS-CAR, two critical challenges persist: 1) Missing compositional\nstructure constraint, leading to spurious correlations between primitives; 2)\nNeglecting semantic hierarchy constraint, leading to semantic ambiguity and\nimpairing the training process. In this paper, we argue that human-like\nsymbolic reasoning offers a principled solution to these challenges by\nexplicitly modeling compositional and hierarchical structured abstraction. To\nthis end, we propose a logic-driven ZS-CAR framework LogicCAR that integrates\ndual symbolic constraints: Explicit Compositional Logic and Hierarchical\nPrimitive Logic. Specifically, the former models the restrictions within the\ncompositions, enhancing the compositional reasoning ability of our model. The\nlatter investigates the semantical dependencies among different primitives,\nempowering the models with fine-to-coarse reasoning capacity. By formalizing\nthese constraints in first-order logic and embedding them into neural network\narchitectures, LogicCAR systematically bridges the gap between symbolic\nabstraction and existing models. Extensive experiments on the Sth-com dataset\ndemonstrate that our LogicCAR outperforms existing baseline methods, proving\nthe effectiveness of our logic-driven constraints.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02320v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02320v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.306,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.295,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework called LogicCAR that integrates first-order logic and fuzzy logic constraints into neural networks for zero-shot compositional action recognition. It focuses on symbolic reasoning for compositional and hierarchical structures, without any mention of diffusion models, iterative refinement processes, or multi-step logical reasoning paths. Therefore, it does not align with diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02323",
      "title": "Dream-to-Recon: Monocular 3D Reconstruction with Diffusion-Depth\n  Distillation from Single Images",
      "authors": [
        "Philipp Wulff",
        "Felix Wimbauer",
        "Dominik Muhle",
        "Daniel Cremers"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Volumetric scene reconstruction from a single image is crucial for a broad\nrange of applications like autonomous driving and robotics. Recent volumetric\nreconstruction methods achieve impressive results, but generally require\nexpensive 3D ground truth or multi-view supervision. We propose to leverage\npre-trained 2D diffusion models and depth prediction models to generate\nsynthetic scene geometry from a single image. This can then be used to distill\na feed-forward scene reconstruction model. Our experiments on the challenging\nKITTI-360 and Waymo datasets demonstrate that our method matches or outperforms\nstate-of-the-art baselines that use multi-view supervision, and offers unique\nadvantages, for example regarding dynamic scenes.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02323v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02323v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.293,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.519,
      "distributed_training_score": 0.342,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses pre-trained 2D diffusion models in a render-refine-repeat framework for generating and refining synthetic views to aid 3D reconstruction, which involves iterative refinement similar to diffusion processes. However, this is applied to visual and geometric tasks, not to multi-step logical reasoning or treating a Chain-of-Thought as a holistic entity for solving complex logical tasks. Thus, while there is a shared iterative mechanism, it does not align with the core definition of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02324",
      "title": "Qwen-Image Technical Report",
      "authors": [
        "Chenfei Wu",
        "Jiahao Li",
        "Jingren Zhou",
        "Junyang Lin",
        "Kaiyuan Gao",
        "Kun Yan",
        "Sheng-ming Yin",
        "Shuai Bai",
        "Xiao Xu",
        "Yilei Chen",
        "Yuxiang Chen",
        "Zecheng Tang",
        "Zekai Zhang",
        "Zhengyi Wang",
        "An Yang",
        "Bowen Yu",
        "Chen Cheng",
        "Dayiheng Liu",
        "Deqing Li",
        "Hang Zhang",
        "Hao Meng",
        "Hu Wei",
        "Jingyuan Ni",
        "Kai Chen",
        "Kuan Cao",
        "Liang Peng",
        "Lin Qu",
        "Minggang Wu",
        "Peng Wang",
        "Shuting Yu",
        "Tingkun Wen",
        "Wensen Feng",
        "Xiaoxiao Xu",
        "Yi Wang",
        "Yichang Zhang",
        "Yongqiang Zhu",
        "Yujia Wu",
        "Yuxuan Cai",
        "Zenan Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present Qwen-Image, an image generation foundation model in the Qwen\nseries that achieves significant advances in complex text rendering and precise\nimage editing. To address the challenges of complex text rendering, we design a\ncomprehensive data pipeline that includes large-scale data collection,\nfiltering, annotation, synthesis, and balancing. Moreover, we adopt a\nprogressive training strategy that starts with non-text-to-text rendering,\nevolves from simple to complex textual inputs, and gradually scales up to\nparagraph-level descriptions. This curriculum learning approach substantially\nenhances the model's native text rendering capabilities. As a result,\nQwen-Image not only performs exceptionally well in alphabetic languages such as\nEnglish, but also achieves remarkable progress on more challenging logographic\nlanguages like Chinese. To enhance image editing consistency, we introduce an\nimproved multi-task training paradigm that incorporates not only traditional\ntext-to-image (T2I) and text-image-to-image (TI2I) tasks but also\nimage-to-image (I2I) reconstruction, effectively aligning the latent\nrepresentations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed\nthe original image into Qwen2.5-VL and the VAE encoder to obtain semantic and\nreconstructive representations, respectively. This dual-encoding mechanism\nenables the editing module to strike a balance between preserving semantic\nconsistency and maintaining visual fidelity. Qwen-Image achieves\nstate-of-the-art performance, demonstrating its strong capabilities in both\nimage generation and editing across multiple benchmarks.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02324v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02324v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.338,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper primarily discusses Qwen-Image, a model for text-to-image generation and image editing using diffusion-based architectures. While it mentions diffusion models for iterative refinement in visual tasks, such as generating images from prompts, it does not adapt this process for complex logical tasks or treat a 'Chain-of-Thought' as an entity for holistic correction. The focus is on visual outputs like text rendering and editing, with no clear component for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02329",
      "title": "CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via\n  Instruction Editing Data and Long Captions",
      "authors": [
        "Ziteng Wang",
        "Siqi Yang",
        "Limeng Qiao",
        "Lin Ma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Despite the success of Vision-Language Models (VLMs) like CLIP in aligning\nvision and language, their proficiency in detailed, fine-grained visual\ncomprehension remains a key challenge. We present CLIP-IN, a novel framework\nthat bolsters CLIP's fine-grained perception through two core innovations.\nFirstly, we leverage instruction-editing datasets, originally designed for\nimage manipulation, as a unique source of hard negative image-text pairs.\nCoupled with a symmetric hard negative contrastive loss, this enables the model\nto effectively distinguish subtle visual-semantic differences. Secondly,\nCLIP-IN incorporates long descriptive captions, utilizing rotary positional\nencodings to capture rich semantic context often missed by standard CLIP. Our\nexperiments demonstrate that CLIP-IN achieves substantial gains on the MMVP\nbenchmark and various fine-grained visual recognition tasks, without\ncompromising robust zero-shot performance on broader classification and\nretrieval tasks. Critically, integrating CLIP-IN's visual representations into\nMultimodal Large Language Models significantly reduces visual hallucinations\nand enhances reasoning abilities. This work underscores the considerable\npotential of synergizing targeted, instruction-based contrastive learning with\ncomprehensive descriptive information to elevate the fine-grained understanding\nof VLMs.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02329v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02329v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.336,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is enhancing CLIP for fine-grained visual understanding through instruction-editing data and long captions, using contrastive learning and rotary positional encodings. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as defined by the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02339",
      "title": "Correspondence-Free Fast and Robust Spherical Point Pattern Registration",
      "authors": [
        "Anik Sarker",
        "Alan T. Asbeck"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Existing methods for rotation estimation between two spherical\n($\\mathbb{S}^2$) patterns typically rely on spherical cross-correlation\nmaximization between two spherical function. However, these approaches exhibit\ncomputational complexities greater than cubic $O(n^3)$ with respect to rotation\nspace discretization and lack extensive evaluation under significant outlier\ncontamination. To this end, we propose a rotation estimation algorithm between\ntwo spherical patterns with linear time complexity $O(n)$. Unlike existing\nspherical-function-based methods, we explicitly represent spherical patterns as\ndiscrete 3D point sets on the unit sphere, reformulating rotation estimation as\na spherical point-set alignment (i.e., Wahba problem for 3D unit vectors).\nGiven the geometric nature of our formulation, our spherical pattern alignment\nalgorithm naturally aligns with the Wahba problem framework for 3D unit\nvectors. Specifically, we introduce three novel algorithms: (1) SPMC (Spherical\nPattern Matching by Correlation), (2) FRS (Fast Rotation Search), and (3) a\nhybrid approach (SPMC+FRS) that combines the advantages of the previous two\nmethods. Our experiments demonstrate that in the $\\mathbb{S}^2$ domain and in\ncorrespondence-free settings, our algorithms are over 10x faster and over 10x\nmore accurate than current state-of-the-art methods for the Wahba problem with\noutliers. We validate our approach through extensive simulations on a new\ndataset of spherical patterns, the ``Robust Vector Alignment Dataset.\n\"Furthermore, we adapt our methods to two real-world tasks: (i) Point Cloud\nRegistration (PCR) and (ii) rotation estimation for spherical images.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02339v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02339v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.238,
      "weak_supervision_score": 0.259,
      "diffusion_reasoning_score": 0.234,
      "distributed_training_score": 0.264,
      "datasets_score": 0.232,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02340",
      "title": "Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search",
      "authors": [
        "Fan Hu",
        "Zijie Xin",
        "Xirong Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.IR (Information Retrieval)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Ad-hoc Video Search (AVS) involves using a textual query to search for\nmultiple relevant videos in a large collection of unlabeled short videos. The\nmain challenge of AVS is the visual diversity of relevant videos. A simple\nquery such as \"Find shots of a man and a woman dancing together indoors\" can\nspan a multitude of environments, from brightly lit halls and shadowy bars to\ndance scenes in black-and-white animations. It is therefore essential to\nretrieve relevant videos as comprehensively as possible. Current solutions for\nthe AVS task primarily fuse multiple features into one or more common spaces,\nyet overlook the need for diverse spaces. To fully exploit the expressive\ncapability of individual features, we propose LPD, short for Learning Partially\nDecorrelated common spaces. LPD incorporates two key innovations:\nfeature-specific common space construction and the de-correlation loss.\nSpecifically, LPD learns a separate common space for each video and text\nfeature, and employs de-correlation loss to diversify the ordering of negative\nsamples across different spaces. To enhance the consistency of multi-space\nconvergence, we designed an entropy-based fair multi-space triplet ranking\nloss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify\nthe effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces\nhighlight its ability to enhance result diversity.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02340v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02340v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.393,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on learning partially-decorrelated common spaces for ad-hoc video search, emphasizing feature fusion, de-correlation loss, and triplet ranking loss to improve video retrieval diversity. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02343",
      "title": "MicroMix: Efficient Mixed-Precision Quantization with Microscaling\n  Formats for Large Language Models",
      "authors": [
        "Wenyuan Liu",
        "Haoqian Meng",
        "Yilun Luo",
        "Peng Zhang",
        "Xindian Ma"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Quantization significantly accelerates inference in large language models\n(LLMs) by replacing original high-precision matrices with low-precision\ncounterparts. Recent advances in weight-activation quantization have primarily\nfocused on mapping both weights and activations to the INT4 format. Although\nthe new FP4 Tensor Cores in NVIDIA's Blackwell architecture offer up to 4x\nspeedup over FP16, existing INT4-based kernels fail to fully exploit this\ncapability due to mismatched data formats. To bridge this gap, we propose\nMicroMix, a co-designed mixed-precision quantization algorithm and matrix\nmultiplication kernel based on Microscaling (MX) data formats. Tailored for the\nBlackwell architecture, the MicroMix kernel supports arbitrary combinations of\nMXFP4, MXFP6, and MXFP8 channels, and produces BFloat16 outputs. To achieve a\nfavorable trade-off between accuracy and efficiency for each linear layer, we\nintroduce quantization thresholds that identify activation elements where\nlower-precision formats (MXFP4 or MXFP6) incur excessive quantization error.\nOur algorithm selectively allocates higher-precision channels to preserve\naccuracy while maintaining compute efficiency. MicroMix achieves competitive or\nsuperior performance across diverse downstream tasks, including zero-shot and\nfew-shot learning, language modeling, code generation, and mathematical\nreasoning. On both consumer-grade (RTX 5070Ti laptop) and server-grade (RTX\n5090) GPUs, our kernel delivers at least 20% faster execution than\nTensorRT-FP8. Furthermore, when applied to various Llama and Qwen models,\nMicroMix consistently improves prefill latency and memory efficiency across a\nrange of batch sizes compared to TensorRT baselines. Our code is available at\nhttps://github.com/lwy2020/MicroMix.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02343v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02343v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.457,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a mixed-precision quantization technique for accelerating inference in large language models, focusing on optimizing matrix operations on single GPUs like NVIDIA's Blackwell architecture. It does not involve distributed training, parallel computing across multiple nodes, or strategies for partitioning data, model architecture, or computation during training. The work is centered on inference efficiency, not training acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02344",
      "title": "Traffic-R1: Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal\n  Control Systems",
      "authors": [
        "Xingchen Zou",
        "Yuhao Yang",
        "Zheng Chen",
        "Xixuan Hao",
        "Yiqi Chen",
        "Chao Huang",
        "Yuxuan Liang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Traffic signal control (TSC) is vital for mitigating congestion and\nsustaining urban mobility. In this paper, we introduce Traffic-R1, a foundation\nmodel with human-like reasoning for TSC systems. Our model is developed through\nself-exploration and iteration of reinforced large language models (LLMs) with\nexpert guidance in a simulated traffic environment. Compared to traditional\nreinforcement learning (RL) and recent LLM-based methods, Traffic-R1 offers\nthree significant advantages. First, Traffic-R1 delivers zero-shot\ngeneralisation, transferring unchanged to new road networks and\nout-of-distribution incidents by utilizing its internal traffic control\npolicies and human-like reasoning. Second, its 3B-parameter architecture is\nlightweight enough for real-time inference on mobile-class chips, enabling\nlarge-scale edge deployment. Third, Traffic-R1 provides an explainable TSC\nprocess and facilitates multi-intersection communication through its\nself-iteration and a new synchronous communication network. Extensive\nbenchmarks demonstrate that Traffic-R1 sets a new state of the art,\noutperforming strong baselines and training-intensive RL controllers. In\npractice, the model now manages signals for more than 55,000 drivers daily,\nshortening average queues by over 5% and halving operator workload. Our\ncheckpoint is available at https://huggingface.co/Season998/Traffic-R1.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02344v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02344v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.459,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.455,
      "distributed_training_score": 0.403,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes a two-stage RL finetuning process where human expert decisions are used to calculate action rewards in the offline stage, aligning the model with human preferences. This directly incorporates elements of RLHF, as it involves human-ranked data (expert decisions) to guide the reward model and fine-tune the main LLM, meeting the core definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper mentions training the model to generate Chain-of-Thought responses for reasoning, but it does not involve diffusion models or an iterative refinement process for logical tasks. There is no adaptation of diffusion techniques for holistic correction of reasoning paths, making this topic unrelated.",
      "distributed_training_justification": "The paper focuses on RL finetuning for a TSC model but does not discuss distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes. There is no mention of multi-node machine learning techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Traffic-R1, a lightweight reinforced large language model (LLM) designed for traffic signal control (TSC) to enhance urban mobility with human-like reasoning. It employs a two-stage agentic RL fine-tuning process on a 3B-parameter base model, involving offline training with expert data and online exploration in simulated environments, resulting in zero-shot generalization, efficient edge deployment, and explainable decision-making; key findings show it outperforms baselines in benchmarks and real-world applications, reducing average queues by over 5% and operator workload by more than 50% for over 55,000 daily drivers.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by integrating reinforced LLMs with human-like reasoning for TSC, advancing the state-of-the-art through zero-shot generalization and explainable processes that go beyond traditional RL methods. This represents a significant innovation in applying foundation models to real-world traffic management.",
      "impact_score": "High",
      "impact_justification": "The work's real-world deployment, managing signals for over 55,000 drivers and achieving measurable improvements in efficiency, suggests it could broadly influence future AI applications in traffic control and urban planning. Its potential for scalability and adoption in commercial systems makes it likely to drive advancements in related subfields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, innovative contribution with practical impact in AI for traffic systems, making it valuable for researchers and practitioners in the field. However, its specialized focus on TSC may not be essential for those outside urban AI or transportation domains.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/95d8c3e6b48ce53bfc645bacac254fdf9429dbf4",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 11,
      "average_h_index": 3.142857142857143,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Xingchen Zou",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2289104042"
        },
        {
          "name": "Yuhao Yang",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2229513820"
        },
        {
          "name": "Zheng Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374819693"
        },
        {
          "name": "Xixuan Hao",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2289679955"
        },
        {
          "name": "Yiqi Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374917553"
        },
        {
          "name": "Chao Huang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2303328860"
        },
        {
          "name": "Yuxuan Liang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2337074203"
        }
      ]
    },
    {
      "id": "2508.02348",
      "title": "mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at\n  T-Junctions Utilizing Road Layout Extraction via Camera",
      "authors": [
        "Byeonggyu Park",
        "Hee-Yeun Kim",
        "Byonghyok Choi",
        "Hansang Cho",
        "Byungkwan Kim",
        "Soomok Lee",
        "Mingu Jeon",
        "Seong-Woo Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Pedestrians Localization in Non-Line-of-Sight (NLoS) regions within urban\nenvironments poses a significant challenge for autonomous driving systems.\nWhile mmWave radar has demonstrated potential for detecting objects in such\nscenarios, the 2D radar point cloud (PCD) data is susceptible to distortions\ncaused by multipath reflections, making accurate spatial inference difficult.\nAdditionally, although camera images provide high-resolution visual\ninformation, they lack depth perception and cannot directly observe objects in\nNLoS regions. In this paper, we propose a novel framework that interprets radar\nPCD through road layout inferred from camera for localization of NLoS\npedestrians. The proposed method leverages visual information from the camera\nto interpret 2D radar PCD, enabling spatial scene reconstruction. The\neffectiveness of the proposed approach is validated through experiments\nconducted using a radar-camera system mounted on a real vehicle. The\nlocalization performance is evaluated using a dataset collected in outdoor NLoS\ndriving environments, demonstrating the practical applicability of the method.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02348v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02348v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.287,
      "diffusion_reasoning_score": 0.319,
      "distributed_training_score": 0.302,
      "datasets_score": 0.262,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02359",
      "title": "Toward a reliable PWM-based light-emitting diode visual stimulus for\n  improved SSVEP response with minimal visual fatigue",
      "authors": [
        "Surej Mouli",
        "Ramaswamy Palaniappan"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Steady state visual evoked response (SSVEP) is widely used in visual-based\ndiagnosis and applications such as brain computer interfacing due to its high\ninformation transfer rate and the capability to activate commands through\nsimple gaze control. However, one major impediment in using flashing visual\nstimulus to obtain SSVEP is eye fatigue that prevents continued long term use\npreventing practical deployment. This combined with the difficulty in\nestablishing precise pulse-width modulation (PWM) that results in poorer\naccuracy warrants the development of appropriate approach to solve these\nissues. Various studies have suggested the usage of high frequencies of visual\nstimulus to reduce the visual fatigue for the user but this results in poor\nresponse performance. Here, the authors study the use of extremely high\nduty-cycles in the stimulus in the hope of solving these constraints.\nElectroencephalogram data was recorded with PWM duty-cycles of 50 to 95%\ngenerated by a precise custom-made light-emitting diode hardware and tested ten\nsubjects responded that increasing duty-cycles had less visual strain for all\nthe frequency values and the SSVEP exhibited a subject-independent peak\nresponse for duty-cycle of 85%. This could pave the way for increased usage of\nSSVEP for practical applications.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02359v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02359v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.288,
      "weak_supervision_score": 0.243,
      "diffusion_reasoning_score": 0.255,
      "distributed_training_score": 0.247,
      "datasets_score": 0.222,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02362",
      "title": "Text2Lip: Progressive Lip-Synced Talking Face Generation from Text via\n  Viseme-Guided Rendering",
      "authors": [
        "Xu Wang",
        "Shengeng Tang",
        "Fei Wang",
        "Lechao Cheng",
        "Dan Guo",
        "Feng Xue",
        "Richang Hong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generating semantically coherent and visually accurate talking faces requires\nbridging the gap between linguistic meaning and facial articulation. Although\naudio-driven methods remain prevalent, their reliance on high-quality paired\naudio visual data and the inherent ambiguity in mapping acoustics to lip motion\npose significant challenges in terms of scalability and robustness. To address\nthese issues, we propose Text2Lip, a viseme-centric framework that constructs\nan interpretable phonetic-visual bridge by embedding textual input into\nstructured viseme sequences. These mid-level units serve as a linguistically\ngrounded prior for lip motion prediction. Furthermore, we design a progressive\nviseme-audio replacement strategy based on curriculum learning, enabling the\nmodel to gradually transition from real audio to pseudo-audio reconstructed\nfrom enhanced viseme features via cross-modal attention. This allows for robust\ngeneration in both audio-present and audio-free scenarios. Finally, a\nlandmark-guided renderer synthesizes photorealistic facial videos with accurate\nlip synchronization. Extensive evaluations show that Text2Lip outperforms\nexisting approaches in semantic fidelity, visual realism, and modality\nrobustness, establishing a new paradigm for controllable and flexible talking\nface generation. Our project homepage is https://plyon1.github.io/Text2Lip/.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02362v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02362v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.288,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02363",
      "title": "Transport-Guided Rectified Flow Inversion: Improved Image Editing Using\n  Optimal Transport Theory",
      "authors": [
        "Marian Lupascu",
        "Mihai-Sorin Stupariu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Effective image inversion in rectified flow models - mapping real images to\neditable latent representations - is crucial for practical image editing\napplications; however, achieving optimal balance between reconstruction\nfidelity and editing flexibility remains a fundamental challenge. In this work,\nwe introduce the Optimal Transport Inversion Pipeline (OTIP), a zero-shot\nframework that leverages optimal transport theory to guide the inversion\nprocess in rectified flow models. Our underlying hypothesis is that\nincorporating transport-based guidance during the reverse diffusion process can\neffectively balance reconstruction accuracy and editing controllability through\nprincipled trajectory optimization. The method computes optimal transport paths\nbetween image and noise distributions while maintaining computational\nefficiency. Our approach achieves high-fidelity reconstruction with LPIPS\nscores of 0.001 and SSIM of 0.992 on face editing benchmarks, demonstrating\nsuperior preservation of fine-grained details compared to existing methods. We\nevaluate the framework across multiple editing tasks, observing 7.8% to 12.9%\nimprovements in reconstruction loss over RF-Inversion on the LSUN-Bedroom and\nLSUN-Church datasets, respectively. For semantic face editing, our method\nachieves an 11.2% improvement in identity preservation and a 1.6% enhancement\nin perceptual quality, while maintaining computational efficiency comparable to\nbaseline approaches. Qualitatively, our method produces visually compelling\nedits with superior semantic consistency and fine-grained detail preservation\nacross diverse editing scenarios. Code is available at:\nhttps://github.com/marianlupascu/OT-Inversion",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02363v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02363v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.27,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.287,
      "datasets_score": 0.223,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an optimal transport-guided inversion pipeline for rectified flow models in image editing, focusing on improving reconstruction and editing of visual data. It does not involve adapting diffusion processes for multi-step logical reasoning, solving complex logical tasks, or treating a 'Chain-of-Thought' as an entity, as required by the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02372",
      "title": "TRUDI and TITUS: A Multi-Perspective Dataset and A Three-Stage\n  Recognition System for Transportation Unit Identification",
      "authors": [
        "Emre Gülsoylu",
        "André Kelm",
        "Lennart Bengtson",
        "Matthias Hirsch",
        "Christian Wilms",
        "Tim Rolff",
        "Janick Edinger",
        "Simone Frintrop"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Identifying transportation units (TUs) is essential for improving the\nefficiency of port logistics. However, progress in this field has been hindered\nby the lack of publicly available benchmark datasets that capture the diversity\nand dynamics of real-world port environments. To address this gap, we present\nthe TRUDI dataset-a comprehensive collection comprising 35,034 annotated\ninstances across five categories: container, tank container, trailer, ID text,\nand logo. The images were captured at operational ports using both ground-based\nand aerial cameras, under a wide variety of lighting and weather conditions.\nFor the identification of TUs-which involves reading the 11-digit alphanumeric\nID typically painted on each unit-we introduce TITUS, a dedicated pipeline that\noperates in three stages: (1) segmenting the TU instances, (2) detecting the\nlocation of the ID text, and (3) recognising and validating the extracted ID.\nUnlike alternative systems, which often require similar scenes, specific camera\nangles or gate setups, our evaluation demonstrates that TITUS reliably\nidentifies TUs from a range of camera perspectives and in varying lighting and\nweather conditions. By making the TRUDI dataset publicly available, we provide\na robust benchmark that enables the development and comparison of new\napproaches. This contribution supports digital transformation efforts in\nmultipurpose ports and helps to increase the efficiency of entire logistics\nchains.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02372v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02372v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.235,
      "weak_supervision_score": 0.306,
      "diffusion_reasoning_score": 0.292,
      "distributed_training_score": 0.323,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02374",
      "title": "Uni-Layout: Integrating Human Feedback in Unified Layout Generation and\n  Evaluation",
      "authors": [
        "Shuo Lu",
        "Yanyin Chen",
        "Wei Feng",
        "Jiahao Fan",
        "Fengheng Li",
        "Zheng Zhang",
        "Jingjing Lv",
        "Junjie Shen",
        "Ching Law",
        "Jian Liang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.IR (Information Retrieval)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Layout generation plays a crucial role in enhancing both user experience and\ndesign efficiency. However, current approaches suffer from task-specific\ngeneration capabilities and perceptually misaligned evaluation metrics, leading\nto limited applicability and ineffective measurement. In this paper, we propose\n\\textit{Uni-Layout}, a novel framework that achieves unified generation,\nhuman-mimicking evaluation and alignment between the two. For universal\ngeneration, we incorporate various layout tasks into a single taxonomy and\ndevelop a unified generator that handles background or element contents\nconstrained tasks via natural language prompts. To introduce human feedback for\nthe effective evaluation of layouts, we build \\textit{Layout-HF100k}, the first\nlarge-scale human feedback dataset with 100,000 expertly annotated layouts.\nBased on \\textit{Layout-HF100k}, we introduce a human-mimicking evaluator that\nintegrates visual and geometric information, employing a Chain-of-Thought\nmechanism to conduct qualitative assessments alongside a confidence estimation\nmodule to yield quantitative measurements. For better alignment between the\ngenerator and the evaluator, we integrate them into a cohesive system by\nadopting Dynamic-Margin Preference Optimization (DMPO), which dynamically\nadjusts margins based on preference strength to better align with human\njudgments. Extensive experiments show that \\textit{Uni-Layout} significantly\noutperforms both task-specific and general-purpose methods. Our code is\npublicly available at https://github.com/JD-GenX/Uni-Layout.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02374v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02374v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.436,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.345,
      "datasets_score": 0.387,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces Uni-Layout, which uses human feedback from the Layout-HF100k dataset to train an evaluator and align a layout generator via Dynamic-Margin Preference Optimization (DMPO). This process involves training a separate model on human-ranked data and optimizing the main model based on preferences, similar to RLHF components. However, it does not explicitly employ reinforcement learning algorithms (e.g., PPO) for fine-tuning, focusing instead on preference optimization methods, making it related but not a direct match to the RLHF definition.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Uni-Layout, a unified framework for layout generation that categorizes diverse tasks into a taxonomy based on background and element constraints, enabling a single generator to handle various scenarios via natural language prompts. It addresses evaluation shortcomings by creating Layout-HF100k, a dataset of 100,000 human-annotated layouts, and developing a human-mimicking evaluator with visual and geometric branches, Chain-of-Thought reasoning, and Dynamic-Margin Preference Optimization for better alignment with human preferences, demonstrating superior performance over existing methods in experiments.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel unified framework, a first-of-its-kind human feedback dataset, and a dynamic alignment method, significantly advancing the state-of-the-art in layout generation and evaluation by integrating human perception in a comprehensive manner.",
      "impact_score": "High",
      "impact_justification": "The work's innovative integration of human feedback and unified approach could influence a wide range of applications in design automation, AI-assisted interfaces, and related fields, likely leading to broader adoption and further research advancements.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a strong, innovative contribution with practical tools and insights that are valuable for researchers in computer vision and machine learning, though it may not be essential for those outside the specific subfield.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1a4eecd5c2eb2eee32108f698ee3ce004179dbdc",
      "total_authors": 10,
      "authors_found": 10,
      "highest_h_index": 5,
      "average_h_index": 2.8,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Shuo Lu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2321663025"
        },
        {
          "name": "Yanyin Chen",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2344864661"
        },
        {
          "name": "Wei Feng",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2274209620"
        },
        {
          "name": "Jiahao Fan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2205246940"
        },
        {
          "name": "Fengheng Li",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2220589458"
        },
        {
          "name": "Zheng Zhang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2274192761"
        },
        {
          "name": "Jingjing Lv",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2220084007"
        },
        {
          "name": "Jun-Jun Shen",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2157691684"
        },
        {
          "name": "Ching Law",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2374473949"
        },
        {
          "name": "Jian Liang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2374785895"
        }
      ]
    },
    {
      "id": "2508.02384",
      "title": "SMART-Ship: A Comprehensive Synchronized Multi-modal Aligned Remote\n  Sensing Targets Dataset and Benchmark for Berthed Ships Analysis",
      "authors": [
        "Chen-Chen Fan",
        "Peiyao Guo",
        "Linping Zhang",
        "Kehan Qi",
        "Haolin Huang",
        "Yong-Qiang Mao",
        "Yuxi Suo",
        "Zhizhuo Jiang",
        "Yu Liu",
        "You He"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Given the limitations of satellite orbits and imaging conditions, multi-modal\nremote sensing (RS) data is crucial in enabling long-term earth observation.\nHowever, maritime surveillance remains challenging due to the complexity of\nmulti-scale targets and the dynamic environments. To bridge this critical gap,\nwe propose a Synchronized Multi-modal Aligned Remote sensing Targets dataset\nfor berthed ships analysis (SMART-Ship), containing spatiotemporal registered\nimages with fine-grained annotation for maritime targets from five modalities:\nvisible-light, synthetic aperture radar (SAR), panchromatic, multi-spectral,\nand near-infrared. Specifically, our dataset consists of 1092 multi-modal image\nsets, covering 38,838 ships. Each image set is acquired within one week and\nregistered to ensure spatiotemporal consistency. Ship instances in each set are\nannotated with polygonal location information, fine-grained categories,\ninstance-level identifiers, and change region masks, organized hierarchically\nto support diverse multi-modal RS tasks. Furthermore, we define standardized\nbenchmarks on five fundamental tasks and comprehensively compare representative\nmethods across the dataset. Thorough experiment evaluations validate that the\nproposed SMART-Ship dataset could support various multi-modal RS interpretation\ntasks and reveal the promising directions for further exploration.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02384v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02384v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.278,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.289,
      "distributed_training_score": 0.329,
      "datasets_score": 0.475,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new dataset (SMART-Ship) for multi-modal remote sensing, including details on its creation, curation (e.g., data acquisition, annotation processes), and benchmarking for various AI tasks. This directly aligns with the topic, as it involves new dataset introduction, curation methodologies, benchmark evaluation, and analysis for machine learning applications in remote sensing.",
      "llm_score_status": "completed",
      "summary": "The paper introduces the SMART-Ship dataset, a comprehensive collection of 1092 synchronized multi-modal remote sensing image sets from five modalities—visible-light, SAR, panchromatic, multi-spectral, and near-infrared—specifically designed for analyzing berthed ships. It details the dataset's acquisition through spatiotemporal registration, provides fine-grained annotations including polygonal locations, categories, instance identifiers, and change masks, and establishes benchmarks for five key tasks (detection, re-identification, cross-modal generation, pan-sharpening, and change detection), demonstrating its utility in enhancing multi-modal remote sensing interpretation and highlighting opportunities for future research.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new dataset with synchronized multi-modal data and fine-grained annotations for maritime targets, which significantly advances the state-of-the-art in remote sensing by addressing gaps in existing datasets for ship analysis.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research and applications in multi-modal remote sensing, particularly in maritime surveillance, by providing a robust benchmark that can be built upon for improved accuracy and robustness in various RS tasks.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality, valuable contribution through its innovative dataset and benchmarks, making it essential for researchers in computer vision and remote sensing to be aware of for advancing maritime analysis.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/bfb5c5c202be5943a0728796850677ad24cf4d96",
      "total_authors": 10,
      "authors_found": 10,
      "highest_h_index": 6,
      "average_h_index": 1.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Chenchen Fan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2346059694"
        },
        {
          "name": "Peiyao Guo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375248387"
        },
        {
          "name": "Linping Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2049447923"
        },
        {
          "name": "Kehan Qi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2347604667"
        },
        {
          "name": "Haolin Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375337567"
        },
        {
          "name": "Yong-Qiang Mao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2350717787"
        },
        {
          "name": "Yuxi Suo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374485747"
        },
        {
          "name": "Zhizhuo Jiang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/1388837742"
        },
        {
          "name": "Yu Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2336141039"
        },
        {
          "name": "You He",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374859807"
        }
      ]
    },
    {
      "id": "2508.02386",
      "title": "Enhancing Object Discovery for Unsupervised Instance Segmentation and\n  Object Detection",
      "authors": [
        "Xingyu Feng",
        "Hebei Gao",
        "Hong Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We propose Cut-Once-and-LEaRn (COLER), a simple approach for unsupervised\ninstance segmentation and object detection. COLER first uses our developed\nCutOnce to generate coarse pseudo labels, then enables the detector to learn\nfrom these masks. CutOnce applies Normalized Cut only once and does not rely on\nany clustering methods, but it can generate multiple object masks in an image.\nWe have designed several novel yet simple modules that not only allow CutOnce\nto fully leverage the object discovery capabilities of self-supervised models,\nbut also free it from reliance on mask post-processing. During training, COLER\nachieves strong performance without requiring specially designed loss functions\nfor pseudo labels, and its performance is further improved through\nself-training. COLER is a zero-shot unsupervised model that outperforms\nprevious state-of-the-art methods on multiple benchmarks.We believe our method\ncan help advance the field of unsupervised object localization.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02386v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02386v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.289,
      "weak_supervision_score": 0.415,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.345,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves generating coarse pseudo labels using CutOnce and training a detector (COLER) on these programmatically derived labels, which are likely noisy or imprecise, rather than relying on manual annotations. This directly aligns with weak supervision, as it uses high-level, automatically generated labels from self-supervised models to enable unsupervised instance segmentation and object detection, thereby reducing dependence on perfectly hand-labeled data.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces COLER, a method for unsupervised instance segmentation and object detection, which utilizes CutOnce to generate coarse pseudo labels by applying Normalized Cut only once on features extracted from self-supervised models, thereby avoiding clustering or post-processing for efficiency. The methodology involves training COLER on these pseudo labels without specialized loss functions and enhancing it through self-training, resulting in superior performance on benchmarks like COCO val2017, where it detects more objects faster than previous state-of-the-art approaches.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying Normalized Cut once without relying on clustering or recursion, offering a clever adaptation of existing techniques for more efficient unsupervised object discovery.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in unsupervised computer vision by providing a faster and more effective method for object detection, potentially leading to broader adoption in related subfields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution with practical advancements in efficiency and performance for unsupervised tasks, making it essential for researchers focused on computer vision and object detection.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3c8a91ba0cbe83d5db57ad030f0aa7bce40288c4",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 5,
      "average_h_index": 2.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Xingyu Feng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375091833"
        },
        {
          "name": "He-Bei Gao",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/46225375"
        },
        {
          "name": "Hong Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2282099716"
        }
      ]
    },
    {
      "id": "2508.02387",
      "title": "$ε$-Softmax: Approximating One-Hot Vectors for Mitigating Label\n  Noise",
      "authors": [
        "Jialiang Wang",
        "Xiong Zhou",
        "Deming Zhai",
        "Junjun Jiang",
        "Xiangyang Ji",
        "Xianming Liu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Noisy labels pose a common challenge for training accurate deep neural\nnetworks. To mitigate label noise, prior studies have proposed various robust\nloss functions to achieve noise tolerance in the presence of label noise,\nparticularly symmetric losses. However, they usually suffer from the\nunderfitting issue due to the overly strict symmetric condition. In this work,\nwe propose a simple yet effective approach for relaxing the symmetric\ncondition, namely $\\epsilon$-softmax, which simply modifies the outputs of the\nsoftmax layer to approximate one-hot vectors with a controllable error\n$\\epsilon$. Essentially, $\\epsilon$-softmax not only acts as an alternative for\nthe softmax layer, but also implicitly plays the crucial role in modifying the\nloss function. We prove theoretically that $\\epsilon$-softmax can achieve\nnoise-tolerant learning with controllable excess risk bound for almost any loss\nfunction. Recognizing that $\\epsilon$-softmax-enhanced losses may slightly\nreduce fitting ability on clean datasets, we further incorporate them with one\nsymmetric loss, thereby achieving a better trade-off between robustness and\neffective learning. Extensive experiments demonstrate the superiority of our\nmethod in mitigating synthetic and real-world label noise. The code is\navailable at https://github.com/cswjl/eps-softmax.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02387v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02387v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.461,
      "diffusion_reasoning_score": 0.313,
      "distributed_training_score": 0.33,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, ε-Softmax, directly addresses mitigating label noise in deep neural networks, which is a core aspect of weak supervision. It focuses on training models with noisy or imprecise labels, as seen in real-world datasets, aligning with the definition of weak supervision where labels are programmatically generated and not perfectly hand-labeled. The theoretical and experimental work on noise-tolerant learning further supports its relevance.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces ε-Softmax, a novel modification to the softmax layer that approximates one-hot vectors with a controllable error ε to mitigate label noise in deep neural networks. By enhancing the outputs to better satisfy the symmetric condition for loss functions, the method achieves noise-tolerant learning with theoretical guarantees on excess risk, while addressing underfitting issues through integration with existing losses, as validated by extensive experiments on synthetic and real-world datasets.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing ε-Softmax as a clever modification to approximate one-hot vectors, combining existing ideas in a new way to enhance noise tolerance without the limitations of prior methods. However, it builds on established concepts like symmetric losses rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of robust machine learning and computer vision, given its practical plug-and-play nature and demonstrated superiority in handling label noise. While it addresses a common issue, its influence may be confined to specific applications rather than broadly transformative.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a strong, innovative contribution with theoretical and experimental support for mitigating label noise, making it valuable for researchers in machine learning and computer vision. It is not essential for all readers but highly relevant for those focused on robust training techniques.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/757f6d955853b73b9491ffc29e772ecf5f029aae",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 17,
      "average_h_index": 8.5,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Jialiang Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2315120338"
        },
        {
          "name": "Xiong Zhou",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2109520799"
        },
        {
          "name": "Deming Zhai",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/2431195"
        },
        {
          "name": "Junjun Jiang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2202553944"
        },
        {
          "name": "Xiangyang Ji",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2274762179"
        },
        {
          "name": "Xianming Liu",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/2110655701"
        }
      ]
    },
    {
      "id": "2508.02391",
      "title": "Inference-time Scaling for Diffusion-based Audio Super-resolution",
      "authors": [
        "Yizhu Jin",
        "Zhen Ye",
        "Zeyue Tian",
        "Haohe Liu",
        "Qiuqiang Kong",
        "Yike Guo",
        "Wei Xue"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Diffusion models have demonstrated remarkable success in generative tasks,\nincluding audio super-resolution (SR). In many applications like movie\npost-production and album mastering, substantial computational budgets are\navailable for achieving superior audio quality. However, while existing\ndiffusion approaches typically increase sampling steps to improve quality, the\nperformance remains fundamentally limited by the stochastic nature of the\nsampling process, leading to high-variance and quality-limited outputs. Here,\nrather than simply increasing the number of sampling steps, we propose a\ndifferent paradigm through inference-time scaling for SR, which explores\nmultiple solution trajectories during the sampling process. Different\ntask-specific verifiers are developed, and two search algorithms, including the\nrandom search and zero-order search for SR, are introduced. By actively guiding\nthe exploration of the high-dimensional solution space through\nverifier-algorithm combinations, we enable more robust and higher-quality\noutputs. Through extensive validation across diverse audio domains (speech,\nmusic, sound effects) and frequency ranges, we demonstrate consistent\nperformance gains, achieving improvements of up to 9.70% in aesthetics, 5.88%\nin speaker similarity, 15.20% in word error rate, and 46.98% in spectral\ndistance for speech SR from 4kHz to 24kHz, showcasing the effectiveness of our\napproach. Audio samples are available at:\nhttps://racerk.github.io/tt-scale-audiosr/.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02391v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02391v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.516,
      "distributed_training_score": 0.369,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of inference-time scaling techniques for diffusion-based audio super-resolution, focusing on improving audio quality through search algorithms and verifiers during the generative process. It does not involve adapting diffusion models for complex logical tasks, such as treating a Chain-of-Thought as an entity for multi-step reasoning. The work is centered on perceptual audio enhancement, lacking any component for logical reasoning or iterative correction of reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02401",
      "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation",
      "authors": [
        "Xiaolin Lin",
        "Jingcun Wang",
        "Olga Kondrateva",
        "Yiyu Shi",
        "Bing Li",
        "Grace Li Zhang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02401v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02401v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.431,
      "distributed_training_score": 0.374,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on KV cache compression in LLMs using attention head analysis to optimize memory efficiency, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It deals exclusively with attention mechanisms and token eviction strategies, which do not align with the topic's emphasis on adapting diffusion for holistic reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02405",
      "title": "Improving Generalization of Language-Conditioned Robot Manipulation",
      "authors": [
        "Chenglin Cui",
        "Chaoran Zhu",
        "Changjae Oh",
        "Andrea Cavallaro"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The control of robots for manipulation tasks generally relies on visual\ninput. Recent advances in vision-language models (VLMs) enable the use of\nnatural language instructions to condition visual input and control robots in a\nwider range of environments. However, existing methods require a large amount\nof data to fine-tune VLMs for operating in unseen environments. In this paper,\nwe present a framework that learns object-arrangement tasks from just a few\ndemonstrations. We propose a two-stage framework that divides\nobject-arrangement tasks into a target localization stage, for picking the\nobject, and a region determination stage for placing the object. We present an\ninstance-level semantic fusion module that aligns the instance-level image\ncrops with the text embedding, enabling the model to identify the target\nobjects defined by the natural language instructions. We validate our method on\nboth simulation and real-world robotic environments. Our method, fine-tuned\nwith a few demonstrations, improves generalization capability and demonstrates\nzero-shot ability in real-robot manipulation scenarios.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02405v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02405v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.402,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.347,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on few-shot fine-tuning of VLMs using demonstrations for robot manipulation, without involving reinforcement learning, a reward model, or human-ranked data to align preferences. It is primarily supervised learning-based, not RLHF.",
      "weak_supervision_justification": "The paper uses a small number of direct demonstrations for fine-tuning, which are precise examples rather than programmatically generated, noisy, or imprecise labels from high-level sources. This aligns more with standard few-shot learning than weak supervision.",
      "diffusion_reasoning_justification": "The paper proposes a framework for vision-language alignment in robot manipulation without any mention of diffusion models, iterative refinement for reasoning, or multi-step logical processes. It focuses on VLMs and object localization, not diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02408",
      "title": "GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT\n  Reconstruction",
      "authors": [
        "Yikuang Yuluo",
        "Yue Ma",
        "Kuan Shen",
        "Tongtong Jin",
        "Wang Liao",
        "Yangpu Ma",
        "Fuquan Wang"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT\nreconstruction. However, existing methods rely on the average gradient\nmagnitude of points within the view, often leading to severe needle-like\nartifacts under sparse-view conditions. To address this challenge, we propose\nGR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses\nneedle-like artifacts and improves reconstruction accuracy under sparse-view\nconditions. Our framework introduces two key innovations: (1) a Denoised Point\nCloud Initialization Strategy that reduces initialization errors and\naccelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that\nrefines gradient computation using graph-based density differences, improving\nsplitting accuracy and density representation. Experiments on X-3D and\nreal-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR\nimprovements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These\nresults highlight the applicability of GR-Gaussian for accurate CT\nreconstruction under challenging sparse-view conditions.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02408v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02408v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.242,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.337,
      "datasets_score": 0.277,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02409",
      "title": "Hydra: Accurate Multi-Modal Leaf Wetness Sensing with mm-Wave and Camera\n  Fusion",
      "authors": [
        "Yimeng Liu",
        "Maolin Gan",
        "Huaili Zeng",
        "Li Liu",
        "Younsuk Dong",
        "Zhichao Cao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Leaf Wetness Duration (LWD), the time that water remains on leaf surfaces, is\ncrucial in the development of plant diseases. Existing LWD detection lacks\nstandardized measurement techniques, and variations across different plant\ncharacteristics limit its effectiveness. Prior research proposes diverse\napproaches, but they fail to measure real natural leaves directly and lack\nresilience in various environmental conditions. This reduces the precision and\nrobustness, revealing a notable practical application and effectiveness gap in\nreal-world agricultural settings. This paper presents Hydra, an innovative\napproach that integrates millimeter-wave (mm-Wave) radar with camera technology\nto detect leaf wetness by determining if there is water on the leaf. We can\nmeasure the time to determine the LWD based on this detection. Firstly, we\ndesign a Convolutional Neural Network (CNN) to selectively fuse multiple\nmm-Wave depth images with an RGB image to generate multiple feature images.\nThen, we develop a transformer-based encoder to capture the inherent connection\namong the multiple feature images to generate a feature map, which is further\nfed to a classifier for detection. Moreover, we augment the dataset during\ntraining to generalize our model. Implemented using a frequency-modulated\ncontinuous-wave (FMCW) radar within the 76 to 81 GHz band, Hydra's performance\nis meticulously evaluated on plants, demonstrating the potential to classify\nleaf wetness with up to 96% accuracy across varying scenarios. Deploying Hydra\nin the farm, including rainy, dawn, or poorly light nights, it still achieves\nan accuracy rate of around 90%.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02409v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02409v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.31,
      "distributed_training_score": 0.322,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02411",
      "title": "HGTS-Former: Hierarchical HyperGraph Transformer for Multivariate Time\n  Series Analysis",
      "authors": [
        "Xiao Wang",
        "Hao Si",
        "Fan Zhang",
        "Xiaoya Zhou",
        "Dengdi Sun",
        "Wanli Lyu",
        "Qingquan Yang",
        "Jin Tang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Multivariate time series analysis has long been one of the key research\ntopics in the field of artificial intelligence. However, analyzing complex time\nseries data remains a challenging and unresolved problem due to its high\ndimensionality, dynamic nature, and complex interactions among variables.\nInspired by the strong structural modeling capability of hypergraphs, this\npaper proposes a novel hypergraph-based time series transformer backbone\nnetwork, termed HGTS-Former, to address the multivariate coupling in time\nseries data. Specifically, given the multivariate time series signal, we first\nnormalize and embed each patch into tokens. Then, we adopt the multi-head\nself-attention to enhance the temporal representation of each patch. The\nhierarchical hypergraphs are constructed to aggregate the temporal patterns\nwithin each channel and fine-grained relations between different variables.\nAfter that, we convert the hyperedge into node features through the EdgeToNode\nmodule and adopt the feed-forward network to further enhance the output\nfeatures. Extensive experiments conducted on two multivariate time series tasks\nand eight datasets fully validated the effectiveness of our proposed\nHGTS-Former. The source code will be released on\nhttps://github.com/Event-AHU/Time_Series_Analysis.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02411v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02411v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.29,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.36,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02419",
      "title": "Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination\n  via Attention Lens",
      "authors": [
        "Haohan Zheng",
        "Zhenguo Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Large vision-language models (LVLMs) have demonstrated remarkable multimodal\ncomprehension and reasoning capabilities, but they still suffer from severe\nobject hallucination. Previous studies primarily attribute the flaw to\nlinguistic prior caused by the scale mismatch between visual encoders and large\nlanguage models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon\nLLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,\ngenerating descriptions inconsistent with visual cues. However, through an\nin-depth investigation of the hallucinated mechanisms, we empirically reveal a\npreviously overlooked phenomenon: LVLMs may ignore not only visual information\nbut also textual modality during hallucination, a behavior termed as modality\nbias, which indicates that LVLMs struggle to simultaneously attend to both\nvisual and textual modalities, leading to fragmented understanding of\nuser-provided instructions. Based on this observation, we propose a simple yet\neffective training-free method to mitigate object hallucination. Concretely, we\nintervene and adjust the attention weights of textual and visual tokens,\nbalancing cross-modal compatibility for better alignment with user intentions.\nFurthermore, we adopt a contrastive decoding strategy to reduce the LVLM's\noverreliance on its parametric knowledge, synergistically enhancing our\nattention manipulation. Extensive experiments confirm the widespread presence\nof modality bias in LVLMs. Notably, our method effectively mitigates\nhallucination across multiple open-source LVLMs and benchmarks, highlighting\nits generalizability and efficacy.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02419v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02419v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.346,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on analyzing and mitigating modality bias in LVLMs through attention manipulation and contrastive decoding, without any involvement of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a method for hallucination mitigation in LVLMs using attention interventions, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02421",
      "title": "Emergence of Fair Leaders via Mediators in Multi-Agent Reinforcement\n  Learning",
      "authors": [
        "Akshay Dodwadmath",
        "Setareh Maghsudi"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Stackelberg games and their resulting equilibria have received increasing\nattention in the multi-agent reinforcement learning literature. Each stage of a\ntraditional Stackelberg game involves a leader(s) acting first, followed by the\nfollowers. In situations where the roles of leader(s) and followers can be\ninterchanged, the designated role can have considerable advantages, for\nexample, in first-mover advantage settings. Then the question arises: Who\nshould be the leader and when? A bias in the leader selection process can lead\nto unfair outcomes. This problem is aggravated if the agents are\nself-interested and care only about their goals and rewards. We formally define\nthis leader selection problem and show its relation to fairness in agents'\nreturns. Furthermore, we propose a multi-agent reinforcement learning framework\nthat maximizes fairness by integrating mediators. Mediators have previously\nbeen used in the simultaneous action setting with varying levels of control,\nsuch as directly performing agents' actions or just recommending them. Our\nframework integrates mediators in the Stackelberg setting with minimal control\n(leader selection). We show that the presence of mediators leads to\nself-interested agents taking fair actions, resulting in higher overall\nfairness in agents' returns.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02421v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02421v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.325,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on multi-agent reinforcement learning in Stackelberg games, emphasizing fairness through mediators for leader selection, without any involvement of human feedback, human preferences, or reward models trained on human-ranked data. RLHF specifically requires human input to align AI models, which is absent here, making the paper's contributions unrelated.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02425",
      "title": "Multi-Class Human/Object Detection on Robot Manipulators using\n  Proprioceptive Sensing",
      "authors": [
        "Justin Hehli",
        "Marco Heiniger",
        "Maryam Rezayati",
        "Hans Wernher van de Venn"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In physical human-robot collaboration (pHRC) settings, humans and robots\ncollaborate directly in shared environments. Robots must analyze interactions\nwith objects to ensure safety and facilitate meaningful workflows. One critical\naspect is human/object detection, where the contacted object is identified.\nPast research introduced binary machine learning classifiers to distinguish\nbetween soft and hard objects. This study improves upon those results by\nevaluating three-class human/object detection models, offering more detailed\ncontact analysis. A dataset was collected using the Franka Emika Panda robot\nmanipulator, exploring preprocessing strategies for time-series analysis.\nModels including LSTM, GRU, and Transformers were trained on these datasets.\nThe best-performing model achieved 91.11\\% accuracy during real-time testing,\ndemonstrating the feasibility of multi-class detection models. Additionally, a\ncomparison of preprocessing strategies suggests a sliding window approach is\noptimal for this task.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02425v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02425v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.427,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.357,
      "datasets_score": 0.392,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves developing and evaluating multi-class classification models (e.g., LSTM, GRU, Transformers) for human/object detection using proprioceptive sensor data from robot manipulators. It does not involve reinforcement learning, human feedback, reward models, or any process of aligning AI models with human preferences, as defined for RLHF. Therefore, the paper has no connection to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02427",
      "title": "CABENCH: Benchmarking Composable AI for Solving Complex Tasks through\n  Composing Ready-to-Use Models",
      "authors": [
        "Tung-Thuy Pham",
        "Duy-Quan Luong",
        "Minh-Quan Duong",
        "Trung-Hieu Nguyen",
        "Thu-Trang Nguyen",
        "Son Nguyen",
        "Hieu Dinh Vo"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Composable AI offers a scalable and effective paradigm for tackling complex\nAI tasks by decomposing them into sub-tasks and solving each sub-task using\nready-to-use well-trained models. However, systematically evaluating methods\nunder this setting remains largely unexplored. In this paper, we introduce\nCABENCH, the first public benchmark comprising 70 realistic composable AI\ntasks, along with a curated pool of 700 models across multiple modalities and\ndomains. We also propose an evaluation framework to enable end-to-end\nassessment of composable AI solutions. To establish initial baselines, we\nprovide human-designed reference solutions and compare their performance with\ntwo LLM-based approaches. Our results illustrate the promise of composable AI\nin addressing complex real-world problems while highlighting the need for\nmethods that can fully unlock its potential by automatically generating\neffective execution pipelines.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02427v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02427v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.392,
      "datasets_score": 0.453,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the introduction of CABENCH, a benchmark for Composable AI that involves task decomposition and model composition using ready-to-use models. It does not discuss diffusion models, iterative refinement for reasoning, or any multi-step logical processes based on diffusion techniques. Therefore, there is no connection to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces CABENCH, a new benchmark with 70 realistic tasks and a curated pool of 700 models, along with an evaluation framework for assessing Composable AI methods. This directly aligns with research on creating, benchmarking, and evaluating datasets for AI applications, as it provides a structured resource for systematic investigation and future comparisons.",
      "llm_score_status": "completed",
      "summary": "This paper introduces CABENCH, the first public benchmark for Composable AI, featuring 70 realistic tasks and a curated pool of 700 ready-to-use models across various modalities, to systematically evaluate methods that decompose complex tasks and compose existing models into executable pipelines. It proposes an end-to-end evaluation framework, provides human-designed reference solutions, and compares them with LLM-based approaches, revealing that human-orchestrated pipelines significantly outperform LLMs, thus underscoring the potential and challenges of Composable AI for real-world applications.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark and problem formulation for Composable AI, significantly advancing the field by providing the first systematic evaluation tool for composing ready-to-use models to solve complex tasks.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research in AI and software engineering by offering a standardized benchmark and framework, potentially enabling broader adoption and innovation in composable AI systems for real-world applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to AI research, particularly for those interested in task decomposition and model composition, making it essential for specialists in the field to be aware of its insights and resources.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/fc2153709fec34573a37f8a0a27ecd45cd564715",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 7,
      "average_h_index": 3.0,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Tung-Thuy Pham",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374478156"
        },
        {
          "name": "Duy-Quan Luong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374485461"
        },
        {
          "name": "Minh-Quan Duong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374484836"
        },
        {
          "name": "Trung-Hieu Nguyen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2333970877"
        },
        {
          "name": "Thu-Trang Nguyen",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2116086203"
        },
        {
          "name": "Son Nguyen",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2128647754"
        },
        {
          "name": "H. Vo",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/1833292"
        }
      ]
    },
    {
      "id": "2508.02429",
      "title": "Multimodal Large Language Models for End-to-End Affective Computing:\n  Benchmarking and Boosting with Generative Knowledge Prompting",
      "authors": [
        "Miaosen Luo",
        "Jiesen Long",
        "Zequn Li",
        "Yunying Yang",
        "Yuncheng Jiang",
        "Sijie Mai"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Multimodal Affective Computing (MAC) aims to recognize and interpret human\nemotions by integrating information from diverse modalities such as text,\nvideo, and audio. Recent advancements in Multimodal Large Language Models\n(MLLMs) have significantly reshaped the landscape of MAC by offering a unified\nframework for processing and aligning cross-modal information. However,\npractical challenges remain, including performance variability across complex\nMAC tasks and insufficient understanding of how architectural designs and data\ncharacteristics impact affective analysis. To address these gaps, we conduct a\nsystematic benchmark evaluation of state-of-the-art open-source MLLMs capable\nof concurrently processing audio, visual, and textual modalities across\nmultiple established MAC datasets. Our evaluation not only compares the\nperformance of these MLLMs but also provides actionable insights into model\noptimization by analyzing the influence of model architectures and dataset\nproperties. Furthermore, we propose a novel hybrid strategy that combines\ngenerative knowledge prompting with supervised fine-tuning to enhance MLLMs'\naffective computing capabilities. Experimental results demonstrate that this\nintegrated approach significantly improves performance across various MAC\ntasks, offering a promising avenue for future research and development in this\nfield. Our code is released on https://github.com/LuoMSen/MLLM-MAC.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02429v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02429v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.438,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.367,
      "datasets_score": 0.417,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on benchmarking Multimodal Large Language Models (MLLMs) for affective computing and proposes a hybrid strategy involving supervised fine-tuning and prompting, but it does not involve training with human-ranked data, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses MLLMs for affective computing tasks, including prompt engineering and fine-tuning, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contributions include systematic benchmarking and evaluation of MLLMs on established Multimodal Affective Computing datasets, as well as analysis of dataset properties like modality dominance and domain, which directly align with dataset benchmarking and evaluation in AI research.",
      "llm_score_status": "completed",
      "summary": "This paper benchmarks state-of-the-art open-source Multimodal Large Language Models (MLLMs) for Multimodal Affective Computing (MAC), which involves recognizing emotions from text, video, and audio, by evaluating their performance on established datasets and analyzing factors like model architecture and dataset properties. It introduces a novel hybrid strategy combining generative knowledge prompting with supervised fine-tuning, demonstrating significant performance improvements in MAC tasks and providing actionable insights for model optimization.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new systematic benchmarking of MLLMs for concurrent processing of text, audio, and visual modalities in MAC, along with a novel hybrid strategy that advances affective computing techniques.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research and development in MLLMs for affective computing by providing comprehensive benchmarks and an effective optimization strategy, potentially extending to commercial applications in emotion recognition technologies.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers high-quality contributions with practical insights and a novel method that advances the field of MAC, making it valuable for researchers in AI and machine learning focused on multimodal systems.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f6964a29d91fb6db2807f8670488963d346128a3",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 1,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Miaosen Luo",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356492496"
        },
        {
          "name": "Jiesen Long",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375002358"
        },
        {
          "name": "Zequn Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374478893"
        },
        {
          "name": "Yunying Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375156928"
        },
        {
          "name": "Yuncheng Jiang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2355777387"
        },
        {
          "name": "Sijie Mai",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2355650280"
        }
      ]
    },
    {
      "id": "2508.02431",
      "title": "Identifying actionable driver mutations in lung cancer using an\n  efficient Asymmetric Transformer Decoder",
      "authors": [
        "Biagio Brattoli",
        "Jack Shi",
        "Jongchan Park",
        "Taebum Lee",
        "Donggeun Yoo",
        "Sergio Pereira"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Identifying actionable driver mutations in non-small cell lung cancer (NSCLC)\ncan impact treatment decisions and significantly improve patient outcomes.\nDespite guideline recommendations, broader adoption of genetic testing remains\nchallenging due to limited availability and lengthy turnaround times. Machine\nLearning (ML) methods for Computational Pathology (CPath) offer a potential\nsolution; however, research often focuses on only one or two common mutations,\nlimiting the clinical value of these tools and the pool of patients who can\nbenefit from them. This study evaluates various Multiple Instance Learning\n(MIL) techniques to detect six key actionable NSCLC driver mutations: ALK,\nBRAF, EGFR, ERBB2, KRAS, and MET ex14. Additionally, we introduce an Asymmetric\nTransformer Decoder model that employs queries and key-values of varying\ndimensions to maintain a low query dimensionality. This approach efficiently\nextracts information from patch embeddings and minimizes overfitting risks,\nproving highly adaptable to the MIL setting. Moreover, we present a method to\ndirectly utilize tissue type in the model, addressing a typical MIL limitation\nwhere either all regions or only some specific regions are analyzed, neglecting\nbiological relevance. Our method outperforms top MIL models by an average of\n3%, and over 4% when predicting rare mutations such as ERBB2 and BRAF, moving\nML-based tests closer to being practical alternatives to standard genetic\ntesting.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02431v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02431v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.388,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves developing an Asymmetric Transformer Decoder for Multiple Instance Learning in computational pathology to detect genetic mutations in lung cancer. It focuses on image analysis and mutation prediction from histological data, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning tasks. Therefore, it does not align with diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02439",
      "title": "Glioblastoma Overall Survival Prediction With Vision Transformers",
      "authors": [
        "Yin Lin",
        "Riccardo Barbieri",
        "Domenico Aquino",
        "Giuseppe Lauria",
        "Marina Grisoli",
        "Elena De Momi",
        "Alberto Redaelli",
        "Simona Ferrante"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Glioblastoma is one of the most aggressive and common brain tumors, with a\nmedian survival of 10-15 months. Predicting Overall Survival (OS) is critical\nfor personalizing treatment strategies and aligning clinical decisions with\npatient outcomes. In this study, we propose a novel Artificial Intelligence\n(AI) approach for OS prediction using Magnetic Resonance Imaging (MRI) images,\nexploiting Vision Transformers (ViTs) to extract hidden features directly from\nMRI images, eliminating the need of tumor segmentation. Unlike traditional\napproaches, our method simplifies the workflow and reduces computational\nresource requirements.\n  The proposed model was evaluated on the BRATS dataset, reaching an accuracy\nof 62.5% on the test set, comparable to the top-performing methods.\nAdditionally, it demonstrated balanced performance across precision, recall,\nand F1 score, overcoming the best model in these metrics. The dataset size\nlimits the generalization of the ViT which typically requires larger datasets\ncompared to convolutional neural networks. This limitation in generalization is\nobserved across all the cited studies. This work highlights the applicability\nof ViTs for downsampled medical imaging tasks and establishes a foundation for\nOS prediction models that are computationally efficient and do not rely on\nsegmentation.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02439v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02439v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.283,
      "weak_supervision_score": 0.295,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.327,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the use of Vision Transformers for predicting glioblastoma overall survival from MRI images, focusing on image feature extraction and classification without any involvement of diffusion models, iterative refinement processes, or multi-step logical reasoning. There is no mention or adaptation of diffusion-based techniques for solving logical tasks, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02442",
      "title": "Assessing the Reliability and Validity of Large Language Models for\n  Automated Assessment of Student Essays in Higher Education",
      "authors": [
        "Andrea Gaggioli",
        "Giuseppe Casaburi",
        "Leonardo Ercolani",
        "Francesco Collova'",
        "Pietro Torre",
        "Fabrizio Davide"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study investigates the reliability and validity of five advanced Large\nLanguage Models (LLMs), Claude 3.5, DeepSeek v2, Gemini 2.5, GPT-4, and Mistral\n24B, for automated essay scoring in a real world higher education context. A\ntotal of 67 Italian-language student essays, written as part of a university\npsychology course, were evaluated using a four-criterion rubric (Pertinence,\nCoherence, Originality, Feasibility). Each model scored all essays across three\nprompt replications to assess intra-model stability. Human-LLM agreement was\nconsistently low and non-significant (Quadratic Weighted Kappa), and\nwithin-model reliability across replications was similarly weak (median\nKendall's W < 0.30). Systematic scoring divergences emerged, including a\ntendency to inflate Coherence and inconsistent handling of context-dependent\ndimensions. Inter-model agreement analysis revealed moderate convergence for\nCoherence and Originality, but negligible concordance for Pertinence and\nFeasibility. Although limited in scope, these findings suggest that current\nLLMs may struggle to replicate human judgment in tasks requiring disciplinary\ninsight and contextual sensitivity. Human oversight remains critical when\nevaluating open-ended academic work, particularly in interpretive domains.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02442v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02442v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.321,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates the performance of existing LLMs for essay scoring but does not discuss, implement, or involve reinforcement learning from human feedback. It focuses on reliability and validity assessments without any mention of training models using human-ranked data or reward models.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper assesses LLMs for automated essay scoring and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined. It only evaluates standard LLMs on scoring tasks without any adaptation of diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02443",
      "title": "Uncertainty Estimation for Novel Views in Gaussian Splatting from\n  Primitive-Based Representations of Error and Visibility",
      "authors": [
        "Thomas Gottwald",
        "Edgar Heinert",
        "Matthias Rottmann"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this work, we present a novel method for uncertainty estimation (UE) in\nGaussian Splatting. UE is crucial for using Gaussian Splatting in critical\napplications such as robotics and medicine. Previous methods typically estimate\nthe variance of Gaussian primitives and use the rendering process to obtain\npixel-wise uncertainties. Our method establishes primitive representations of\nerror and visibility of trainings views, which carries meaningful uncertainty\ninformation. This representation is obtained by projection of training error\nand visibility onto the primitives. Uncertainties of novel views are obtained\nby rendering the primitive representations of uncertainty for those novel\nviews, yielding uncertainty feature maps. To aggregate these uncertainty\nfeature maps of novel views, we perform a pixel-wise regression on holdout\ndata. In our experiments, we analyze the different components of our method,\ninvestigating various combinations of uncertainty feature maps and regression\nmodels. Furthermore, we considered the effect of separating splatting into\nforeground and background. Our UEs show high correlations to true errors,\noutperforming state-of-the-art methods, especially on foreground objects. The\ntrained regression models show generalization capabilities to new scenes,\nallowing uncertainty estimation without the need for holdout data.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02443v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02443v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.227,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.333,
      "distributed_training_score": 0.256,
      "datasets_score": 0.229,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02451",
      "title": "Dynamic Forgetting and Spatio-Temporal Periodic Interest Modeling for\n  Local-Life Service Recommendation",
      "authors": [
        "Zhaoyu Hu",
        "Hao Guo",
        "Yuan Tian",
        "Erpeng Xue",
        "Jianyang Wang",
        "Xianyang Qi",
        "Hongxiang Lin",
        "Lei Wang",
        "Sheng Chen"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In the context of the booming digital economy, recommendation systems, as a\nkey link connecting users and numerous services, face challenges in modeling\nuser behavior sequences on local-life service platforms, including the sparsity\nof long sequences and strong spatio-temporal dependence. Such challenges can be\naddressed by drawing an analogy to the forgetting process in human memory. This\nis because users' responses to recommended content follow the recency effect\nand the cyclicality of memory. By exploring this, this paper introduces the\nforgetting curve and proposes Spatio-Temporal periodic Interest Modeling (STIM)\nwith long sequences for local-life service recommendation. STIM integrates\nthree key components: a dynamic masking module based on the forgetting curve,\nwhich is used to extract both recent spatiotemporal features and periodic\nspatiotemporal features; a query-based mixture of experts (MoE) approach that\ncan adaptively activate expert networks under different dynamic masks, enabling\nthe collaborative modeling of time, location, and items; and a hierarchical\nmulti-interest network unit, which captures multi-interest representations by\nmodeling the hierarchical interactions between the shallow and deep semantics\nof users' recent behaviors. By introducing the STIM method, we conducted online\nA/B tests and achieved a 1.54\\% improvement in gross transaction volume (GTV).\nIn addition, extended offline experiments also showed improvements. STIM has\nbeen deployed in a large-scale local-life service recommendation system,\nserving hundreds of millions of daily active users in core application\nscenarios.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02451v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02451v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.329,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02455",
      "title": "TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions\n  in IDEs",
      "authors": [
        "Daniele Cipollone",
        "Egor Bogomolov",
        "Arie van Deursen",
        "Maliheh Izadi"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Token-level code completion is one of the most critical features in modern\nIntegrated Development Environments (IDEs). It assists developers by suggesting\nrelevant identifiers and APIs during coding. While completions are typically\nderived from static analysis, their usefulness depends heavily on how they are\nranked, as correct predictions buried deep in the list are rarely seen by\nusers. Most current systems rely on hand-crafted heuristics or lightweight\nmachine learning models trained on user logs, which can be further improved to\ncapture context information and generalize across projects and coding styles.\nIn this work, we propose a new scoring approach to ranking static completions\nusing language models in a lightweight and model-agnostic way. Our method\norganizes all valid completions into a prefix tree and performs a single greedy\ndecoding pass to collect token-level scores across the tree. This enables a\nprecise token-aware ranking without needing beam search, prompt engineering, or\nmodel adaptations. The approach is fast, architecture-agnostic, and compatible\nwith already deployed models for code completion. These findings highlight a\npractical and effective pathway for integrating language models into already\nexisting tools within IDEs, and ultimately providing smarter and more\nresponsive developer assistance.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02455v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02455v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.356,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a method called TreeRanker for ranking code suggestions in IDEs using language models via greedy decoding on a prefix tree. It focuses on improving ranking efficiency and accuracy without involving training, fine-tuning, or human feedback mechanisms. RLHF specifically requires training a reward model on human-ranked data and using reinforcement learning to align models with preferences, which is not addressed or utilized in this work.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02460",
      "title": "InfoSyncNet: Information Synchronization Temporal Convolutional Network\n  for Visual Speech Recognition",
      "authors": [
        "Junxiao Xue",
        "Xiaozhen Liu",
        "Xuecheng Wu",
        "Fei Yu",
        "Jun Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Estimating spoken content from silent videos is crucial for applications in\nAssistive Technology (AT) and Augmented Reality (AR). However, accurately\nmapping lip movement sequences in videos to words poses significant challenges\ndue to variability across sequences and the uneven distribution of information\nwithin each sequence. To tackle this, we introduce InfoSyncNet, a non-uniform\nsequence modeling network enhanced by tailored data augmentation techniques.\nCentral to InfoSyncNet is a non-uniform quantization module positioned between\nthe encoder and decoder, enabling dynamic adjustment to the network's focus and\neffectively handling the natural inconsistencies in visual speech data.\nAdditionally, multiple training strategies are incorporated to enhance the\nmodel's capability to handle variations in lighting and the speaker's\norientation. Comprehensive experiments on the LRW and LRW1000 datasets confirm\nthe superiority of InfoSyncNet, achieving new state-of-the-art accuracies of\n92.0% and 60.7% Top-1 ACC. The code is available for download (see comments).",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02460v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02460v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.357,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02464",
      "title": "SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with\n  Vision Foundation Models",
      "authors": [
        "Yonghuang Wu",
        "Wenwen Zeng",
        "Xuan Xie",
        "Chengqian Zhao",
        "Guoqing Wu",
        "Jinhua Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Foundation models like Segment Anything Model (SAM) excel in promptable\nsegmentation but suffer from an intent gap: they segment only explicitly\nprompted objects, failing to generalize to semantically related instances\nimplicitly desired by users. This limitation is critical in domains with dense\nhomogeneous objects (e.g., biomedical nuclei segmentation), where sparse visual\nprompts typically yield incomplete results, rendering dense annotations\nimpractical due to prohibitive cost. To bridge this gap, we introduce SAMPO\n(Segment Anything Model with Preference Optimization), a novel framework that\nteaches visual foundation models to infer high-level categorical intent from\nsparse visual interactions. Unlike conventional pixel-level fine-tuning, SAMPO\noptimizes models to implicitly capture target-class characteristics through\npreference optimization. This approach, which operates without dependency on\nlanguage models, enables robust multi-object segmentation even under sparse\nprompting and demonstrates superior data efficiency during fine-tuning.\nValidated on three medical segmentation tasks, SAMPO achieves state-of-the-art\nperformance: on challenging tasks like PanNuke-T2, our method, when fine-tuned\nwith only 10% of the training data, significantly outperforms all existing\nmethods trained on the full 100% dataset, achieving an improvement of over 9\npercentage points compared to the best baseline. Our work establishes a new\nparadigm for intent-aware alignment in visual foundation models, removing\ndependencies on auxiliary prompt generators or language-model-assisted\npreference learning.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02464v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02464v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.458,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.332,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper employs preference optimization to align the model with user intent, similar to RLHF concepts, but it does not explicitly use human feedback or a separate reward model trained on human-ranked data. Instead, it focuses on visual prompts for visual foundation models, making it only loosely connected to RLHF.",
      "weak_supervision_justification": "The paper uses sparse visual prompts and limited data (e.g., 10% of training data) to achieve effective segmentation, which aligns with weak supervision by relying on imprecise or high-level inputs rather than dense, hand-labeled data. However, it does not emphasize programmatically generated labels, limiting full relevance.",
      "diffusion_reasoning_justification": "The paper focuses on visual segmentation and preference optimization for foundation models, with no mention of diffusion models, iterative refinement for logical reasoning, or multi-step reasoning processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces SAMPO, a framework designed to address the \"intent gap\" in visual foundation models like the Segment Anything Model (SAM), where models segment only explicitly prompted objects rather than inferring broader user intents. By utilizing preference optimization on sparse visual prompts, SAMPO enables the model to learn and segment multiple semantically related objects without relying on language models, demonstrating superior data efficiency and achieving state-of-the-art performance on medical segmentation tasks, such as outperforming baselines on PanNuke-T2 with just 10% of the training data.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by adapting preference optimization for visual foundation models to infer user intent from sparse prompts, significantly advancing the state-of-the-art in intent-aware segmentation beyond traditional language-dependent methods.",
      "impact_score": "High",
      "impact_justification": "The work could influence future research and applications in computer vision, particularly in domains like biomedical imaging, by enabling more efficient segmentation with minimal prompts and reducing annotation costs.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable and innovative contribution to visual foundation models, making it essential for researchers in computer vision to understand its methods and implications for intent-aware segmentation.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/70a4b5430d7cb928b0955bcacd9aa6de7af7f499",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 2,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yonghuang Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2314534567"
        },
        {
          "name": "Wenwen Zeng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2327930480"
        },
        {
          "name": "Xuan Xie",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2262081249"
        },
        {
          "name": "Chengqian Zhao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2327651502"
        },
        {
          "name": "Guoqing Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374958044"
        },
        {
          "name": "Jinhua Yu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2327918686"
        }
      ]
    },
    {
      "id": "2508.02470",
      "title": "AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language\n  and Multi-Agent Collaboration",
      "authors": [
        "Hyunjn An",
        "Yongwon Kim",
        "Wonduk Seo",
        "Joonil Park",
        "Daye Kang",
        "Changhoon Oh",
        "Dokyun Kim",
        "Seunghyun Lee"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.MA (Multiagent Systems)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "While many tools are available for designing AI, non-experts still face\nchallenges in clearly expressing their intent and managing system complexity.\nWe introduce AIAP, a no-code platform that integrates natural language input\nwith visual workflows. AIAP leverages a coordinated multi-agent system to\ndecompose ambiguous user instructions into modular, actionable steps, hidden\nfrom users behind a unified interface. A user study involving 32 participants\nshowed that AIAP's AI-generated suggestions, modular workflows, and automatic\nidentification of data, actions, and context significantly improved\nparticipants' ability to develop services intuitively. These findings highlight\nthat natural language-based visual programming significantly reduces barriers\nand enhances user experience in AI service design.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02470v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02470v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.3,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development of AIAP, a no-code platform that uses natural language input, visual workflows, and multi-agent collaboration to assist non-experts in building AI services. It focuses on usability enhancements through user studies and does not involve training AI models with human feedback, creating reward models, or applying reinforcement learning for alignment with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02477",
      "title": "Multi-class Image Anomaly Detection for Practical Applications:\n  Requirements and Robust Solutions",
      "authors": [
        "Jaehyuk Heo",
        "Pilsung Kang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in image anomaly detection have extended unsupervised\nlearning-based models from single-class settings to multi-class frameworks,\naiming to improve efficiency in training time and model storage. When a single\nmodel is trained to handle multiple classes, it often underperforms compared to\nclass-specific models in terms of per-class detection accuracy. Accordingly,\nprevious studies have primarily focused on narrowing this performance gap.\nHowever, the way class information is used, or not used, remains a relatively\nunderstudied factor that could influence how detection thresholds are defined\nin multi-class image anomaly detection. These thresholds, whether\nclass-specific or class-agnostic, significantly affect detection outcomes. In\nthis study, we identify and formalize the requirements that a multi-class image\nanomaly detection model must satisfy under different conditions, depending on\nwhether class labels are available during training and evaluation. We then\nre-examine existing methods under these criteria. To meet these challenges, we\npropose Hierarchical Coreset (HierCore), a novel framework designed to satisfy\nall defined requirements. HierCore operates effectively even without class\nlabels, leveraging a hierarchical memory bank to estimate class-wise decision\ncriteria for anomaly detection. We empirically validate the applicability and\nrobustness of existing methods and HierCore under four distinct scenarios,\ndetermined by the presence or absence of class labels in the training and\nevaluation phases. The experimental results demonstrate that HierCore\nconsistently meets all requirements and maintains strong, stable performance\nacross all settings, highlighting its practical potential for real-world\nmulti-class anomaly detection tasks.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02477v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02477v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.323,
      "distributed_training_score": 0.375,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02479",
      "title": "Fine-grained Multiple Supervisory Network for Multi-modal Manipulation\n  Detecting and Grounding",
      "authors": [
        "Xinquan Yu",
        "Wei Lu",
        "Xiangyang Luo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The task of Detecting and Grounding Multi-Modal Media Manipulation (DGM$^4$)\nis a branch of misinformation detection. Unlike traditional binary\nclassification, it includes complex subtasks such as forgery content\nlocalization and forgery method classification. Consider that existing methods\nare often limited in performance due to neglecting the erroneous interference\ncaused by unreliable unimodal data and failing to establish comprehensive\nforgery supervision for mining fine-grained tampering traces. In this paper, we\npresent a Fine-grained Multiple Supervisory (FMS) network, which incorporates\nmodality reliability supervision, unimodal internal supervision and cross-modal\nsupervision to provide comprehensive guidance for DGM$^4$ detection. For\nmodality reliability supervision, we propose the Multimodal Decision Supervised\nCorrection (MDSC) module. It leverages unimodal weak supervision to correct the\nmulti-modal decision-making process. For unimodal internal supervision, we\npropose the Unimodal Forgery Mining Reinforcement (UFMR) module. It amplifies\nthe disparity between real and fake information within unimodal modality from\nboth feature-level and sample-level perspectives. For cross-modal supervision,\nwe propose the Multimodal Forgery Alignment Reasoning (MFAR) module. It\nutilizes soft-attention interactions to achieve cross-modal feature perception\nfrom both consistency and inconsistency perspectives, where we also design the\ninteraction constraints to ensure the interaction quality. Extensive\nexperiments demonstrate the superior performance of our FMS compared to\nstate-of-the-art methods.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02479v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02479v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.437,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.341,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution includes the Multimodal Decision Supervised Correction (MDSC) module, which explicitly leverages unimodal weak supervision to assess and correct multi-modal decision-making. This aligns closely with the definition of weak supervision, as it uses noisy or imprecise unimodal signals to guide model training without relying on perfectly labeled data, making it a core element of the proposed Fine-grained Multiple Supervisory (FMS) network.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces the Fine-grained Multiple Supervisory (FMS) network to enhance the detection and grounding of multi-modal media manipulation (DGM^4), addressing limitations in existing methods by mitigating interference from unreliable unimodal data and providing comprehensive forgery supervision through three novel modules: Multimodal Decision Supervised Correction (MDSC) for modality reliability, Unimodal Forgery Mining Reinforcement (UFMR) for intra-modal enhancement, and Multimodal Forgery Alignment Reasoning (MFAR) for cross-modal interactions. The methodology involves weak supervision, contrastive learning, feature-level and sample-level amplification, and soft-attention mechanisms, with extensive experiments demonstrating superior performance compared to state-of-the-art approaches in forgery content localization and method classification.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing new modules like MDSC, UFMR, and MFAR that cleverly combine existing techniques to handle unreliable modalities and provide fine-grained supervision, though it builds on known problems in multimodal misinformation detection. This represents a clever adaptation rather than a truly groundbreaking new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of multimodal misinformation detection due to its practical enhancements in handling unreliable data and fine-grained forgery analysis. However, its influence may be limited to specific applications in computer vision rather than broadly across AI or commercial sectors.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution with innovative modules that advance fine-grained manipulation detection, making it important for researchers in computer vision and misinformation fields. While not essential for all, it provides useful insights and methodologies that could inform ongoing work in the area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/eb8752890eba09249dd1f15efc7c5ab45749e27f",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 4,
      "average_h_index": 3.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Xinquan Yu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2337453566"
        },
        {
          "name": "Wei Lu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2262498478"
        },
        {
          "name": "Xiangyang Luo",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2290337001"
        }
      ]
    },
    {
      "id": "2508.02480",
      "title": "MindShot: Multi-Shot Video Reconstruction from fMRI with LLM Decoding",
      "authors": [
        "Wenwen Zeng",
        "Yonghuang Wu",
        "Yifan Chen",
        "Xuan Xie",
        "Chengqian Zhao",
        "Feiyu Yin",
        "Guoqing Wu",
        "Jinhua Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Reconstructing dynamic videos from fMRI is important for understanding visual\ncognition and enabling vivid brain-computer interfaces. However, current\nmethods are critically limited to single-shot clips, failing to address the\nmulti-shot nature of real-world experiences. Multi-shot reconstruction faces\nfundamental challenges: fMRI signal mixing across shots, the temporal\nresolution mismatch between fMRI and video obscuring rapid scene changes, and\nthe lack of dedicated multi-shot fMRI-video datasets. To overcome these\nlimitations, we propose a novel divide-and-decode framework for multi-shot fMRI\nvideo reconstruction. Our core innovations are: (1) A shot boundary predictor\nmodule explicitly decomposing mixed fMRI signals into shot-specific segments.\n(2) Generative keyframe captioning using LLMs, which decodes robust textual\ndescriptions from each segment, overcoming temporal blur by leveraging\nhigh-level semantics. (3) Novel large-scale data synthesis (20k samples) from\nexisting datasets. Experimental results demonstrate our framework outperforms\nstate-of-the-art methods in multi-shot reconstruction fidelity. Ablation\nstudies confirm the critical role of fMRI decomposition and semantic\ncaptioning, with decomposition significantly improving decoded caption CLIP\nsimilarity by 71.8%. This work establishes a new paradigm for multi-shot fMRI\nreconstruction, enabling accurate recovery of complex visual narratives through\nexplicit decomposition and semantic prompting.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02480v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02480v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.343,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on multi-shot video reconstruction from fMRI using a divide-and-decode framework, shot boundary prediction, and LLM-based captioning for video generation. It does not involve diffusion models, iterative refinement processes for logical tasks, or treating a Chain-of-Thought as a single entity for holistic correction. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02490",
      "title": "PHM-Bench: A Domain-Specific Benchmarking Framework for Systematic\n  Evaluation of Large Models in Prognostics and Health Management",
      "authors": [
        "Puyu Yang",
        "Laifa Tao",
        "Zijian Huang",
        "Haifei Liu",
        "Wenyan Cao",
        "Hao Ji",
        "Jianan Qiu",
        "Qixuan Huang",
        "Xuanyuan Su",
        "Yuhang Xie",
        "Jun Zhang",
        "Shangyu Li",
        "Chen Lu",
        "Zhixuan Lian"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "With the rapid advancement of generative artificial intelligence, large\nlanguage models (LLMs) are increasingly adopted in industrial domains, offering\nnew opportunities for Prognostics and Health Management (PHM). These models\nhelp address challenges such as high development costs, long deployment cycles,\nand limited generalizability. However, despite the growing synergy between PHM\nand LLMs, existing evaluation methodologies often fall short in structural\ncompleteness, dimensional comprehensiveness, and evaluation granularity. This\nhampers the in-depth integration of LLMs into the PHM domain. To address these\nlimitations, this study proposes PHM-Bench, a novel three-dimensional\nevaluation framework for PHM-oriented large models. Grounded in the triadic\nstructure of fundamental capability, core task, and entire lifecycle, PHM-Bench\nis tailored to the unique demands of PHM system engineering. It defines\nmulti-level evaluation metrics spanning knowledge comprehension, algorithmic\ngeneration, and task optimization. These metrics align with typical PHM tasks,\nincluding condition monitoring, fault diagnosis, RUL prediction, and\nmaintenance decision-making. Utilizing both curated case sets and publicly\navailable industrial datasets, our study enables multi-dimensional evaluation\nof general-purpose and domain-specific models across diverse PHM tasks.\nPHM-Bench establishes a methodological foundation for large-scale assessment of\nLLMs in PHM and offers a critical benchmark to guide the transition from\ngeneral-purpose to PHM-specialized models.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02490v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02490v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.433,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.372,
      "datasets_score": 0.42,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper introduces a benchmarking framework for evaluating large language models in Prognostics and Health Management (PHM), focusing on metrics for knowledge comprehension and task optimization. It does not involve reinforcement learning, human feedback, reward models, or any alignment with human preferences, so it is unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is PHM-Bench, a framework that includes creating and curating case sets, utilizing publicly available industrial datasets, and conducting benchmark evaluations for LLMs in PHM tasks. This directly aligns with research on dataset curation methodologies, benchmarking, and evaluation for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces PHM-Bench, a novel benchmarking framework designed to systematically evaluate large language models (LLMs) in the field of Prognostics and Health Management (PHM), addressing shortcomings in existing evaluation methods such as structural incompleteness and lack of dimensional comprehensiveness. The framework is structured around three dimensions—fundamental capability, core tasks, and entire lifecycle—and incorporates multi-level metrics for knowledge comprehension, algorithmic generation, and task optimization, applied to PHM tasks like condition monitoring and fault diagnosis using curated case sets and public datasets, ultimately providing a foundation for assessing and advancing LLMs in PHM applications.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmarking framework specifically tailored for evaluating LLMs in PHM, which addresses significant gaps in existing methodologies and advances the state-of-the-art by providing a structured, multi-dimensional approach.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future research and commercial applications in PHM by establishing a standardized evaluation method for LLMs, facilitating their integration into industrial domains and likely leading to broader advancements in AI-driven predictive maintenance.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a valuable and innovative framework that is highly relevant for researchers and practitioners in AI and PHM, making it essential for those working on LLM applications in industrial settings to understand and build upon.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d4be9f90866e1c86277c6cc5e52e57d2a07de087",
      "total_authors": 14,
      "authors_found": 14,
      "highest_h_index": 18,
      "average_h_index": 2.0714285714285716,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Puyu Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2350500873"
        },
        {
          "name": "Laifa Tao",
          "h_index": 18,
          "profile_url": "https://www.semanticscholar.org/author/30926472"
        },
        {
          "name": "Zijian Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2350657837"
        },
        {
          "name": "Haifei Liu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2310439835"
        },
        {
          "name": "Wenyan Cao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2310404041"
        },
        {
          "name": "Hao Ji",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375307544"
        },
        {
          "name": "Jianan Qiu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2352888818"
        },
        {
          "name": "Qixuan Huang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2305421790"
        },
        {
          "name": "Xuanyuan Su",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/1726123146"
        },
        {
          "name": "Yuhang Xie",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375076748"
        },
        {
          "name": "Jun Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374797004"
        },
        {
          "name": "Shangyu Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2274884183"
        },
        {
          "name": "Chen Lu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2329712751"
        },
        {
          "name": "Zhixuan Lian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2339775294"
        }
      ]
    },
    {
      "id": "2508.02493",
      "title": "Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian\n  Splatting",
      "authors": [
        "Jianchao Wang",
        "Peng Zhou",
        "Cen Li",
        "Rong Quan",
        "Jie Qin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) is a powerful and computationally efficient\nrepresentation for 3D reconstruction. Despite its strengths, 3DGS often\nproduces floating artifacts, which are erroneous structures detached from the\nactual geometry and significantly degrade visual fidelity. The underlying\nmechanisms causing these artifacts, particularly in low-quality initialization\nscenarios, have not been fully explored. In this paper, we investigate the\norigins of floating artifacts from a frequency-domain perspective and identify\nunder-optimized Gaussians as the primary source. Based on our analysis, we\npropose \\textit{Eliminating-Floating-Artifacts} Gaussian Splatting (EFA-GS),\nwhich selectively expands under-optimized Gaussians to prioritize accurate\nlow-frequency learning. Additionally, we introduce complementary depth-based\nand scale-based strategies to dynamically refine Gaussian expansion,\neffectively mitigating detail erosion. Extensive experiments on both synthetic\nand real-world datasets demonstrate that EFA-GS substantially reduces floating\nartifacts while preserving high-frequency details, achieving an improvement of\n1.68 dB in PSNR over baseline method on our RWLQ dataset. Furthermore, we\nvalidate the effectiveness of our approach in downstream 3D editing tasks. We\nprovide our implementation in https://jcwang-gh.github.io/EFA-GS.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02493v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02493v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.257,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.301,
      "datasets_score": 0.269,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02495",
      "title": "Clinical Expert Uncertainty Guided Generalized Label Smoothing for\n  Medical Noisy Label Learning",
      "authors": [
        "Kunyu Zhang",
        "Lin Gu",
        "Liangchen Liu",
        "Yingke Chen",
        "Binyang Wang",
        "Jin Yan",
        "Yingying Zhu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Many previous studies have proposed extracting image labels from clinical\nnotes to create large-scale medical image datasets at a low cost. However,\nthese approaches inherently suffer from label noise due to uncertainty from the\nclinical experts. When radiologists and physicians analyze medical images to\nmake diagnoses, they often include uncertainty-aware notes such as ``maybe'' or\n``not excluded''. Unfortunately, current text-mining methods overlook these\nnuances, resulting in the creation of noisy labels. Existing methods for\nhandling noisy labels in medical image analysis, which typically address the\nproblem through post-processing techniques, have largely ignored the important\nissue of expert-driven uncertainty contributing to label noise. To better\nincorporate the expert-written uncertainty in clinical notes into medical image\nanalysis and address the label noise issue, we first examine the impact of\nclinical expert uncertainty on label noise. We then propose a clinical expert\nuncertainty-aware benchmark, along with a label smoothing method, which\nsignificantly improves performance compared to current state-of-the-art\napproaches.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02495v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02495v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.494,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.307,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using clinical notes with inherent uncertainty to generate and refine labels for medical image training, which aligns closely with weak supervision. It programmatically extracts labels from high-level, noisy sources (e.g., radiology reports containing phrases like \"probable\" or \"maybe\") rather than relying on precise hand-labeled data. The proposed LU-ViT framework enhances this by converting uncertainty into probabilistic supervision via label smoothing, directly embodying weak supervision principles by leveraging imprecise sources to improve model robustness.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper addresses the challenge of noisy labels in medical image datasets, particularly chest X-rays, caused by clinical expert uncertainty in radiology reports. It introduces the Learning from Uncertainty Vision Transformer (LU-ViT) framework, which extracts uncertainty scores from reports and integrates them into a dynamic label smoothing method that adapts smoothing parameters based on confidence levels, leading to improved performance in noisy label learning as demonstrated through experiments on benchmark datasets.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel framework, LU-ViT, that leverages clinical expert uncertainty as probabilistic supervision for label smoothing, significantly advancing the state-of-the-art in handling systematic label noise in medical imaging beyond traditional random noise assumptions.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of medical image analysis and noisy label learning, as it provides a specialized method that could enhance AI applications in healthcare, though its influence may be limited to specific domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable and innovative contribution to handling label noise in medical AI, making it essential for researchers in computer vision and machine learning focused on healthcare applications to be aware of its methods and findings.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/462b27629918ffbed07a19c564c92c3215b69135",
      "total_authors": 7,
      "authors_found": 6,
      "highest_h_index": 2,
      "average_h_index": 0.6666666666666666,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Kunyu Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375087506"
        },
        {
          "name": "Lin Gu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375867657"
        },
        {
          "name": "Liangchen Liu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2267869532"
        },
        {
          "name": "Yingke Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376141610"
        },
        {
          "name": "Bingyang Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375069298"
        },
        {
          "name": "Jin Yan",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Yingying Zhu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2294921049"
        }
      ]
    },
    {
      "id": "2508.02503",
      "title": "OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical\n  Modeling",
      "authors": [
        "Maxime Bouscary",
        "Saurabh Amin"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "LLM-based solvers have emerged as a promising means of automating problem\nmodeling and solving. However, they remain unreliable and often depend on\niterative repair loops that result in significant latency. We introduce\nOptiHive, an LLM-based framework that produces high-quality solvers for\noptimization problems from natural-language descriptions without iterative\nself-correction. OptiHive uses a single batched LLM query to generate diverse\ncomponents (solvers, problem instances, and validation tests) and filters out\nerroneous components to ensure fully interpretable outputs. Taking into account\nthe imperfection of the generated components, we employ a statistical model to\ninfer their true performance, enabling principled uncertainty quantification\nand solver selection. On tasks ranging from traditional optimization problems\nto challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive\nsignificantly outperforms baselines, increasing the optimality rate from 5\\% to\n92\\% on the most complex problems.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02503v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02503v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.436,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.374,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on LLM-based optimization and statistical modeling for solver generation and selection, with no mention of human feedback, reward models, or reinforcement learning techniques for model alignment.",
      "weak_supervision_justification": "The framework generates noisy components (solvers, instances, and tests) using LLMs and applies statistical inference to handle imperfections, which shares similarities with weak supervision's use of noisy data sources, but it does not involve training models with programmatically generated labels as its primary focus.",
      "diffusion_reasoning_justification": "The paper does not utilize diffusion models or iterative refinement processes for logical reasoning; it employs a single batched LLM query and statistical modeling for solver selection, without any multi-step Chain-of-Thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02506",
      "title": "Decomposed Reasoning with Reinforcement Learning for Relevance\n  Assessment in UGC Platforms",
      "authors": [
        "Xiaowei Yuan",
        "Lei Jin",
        "Haoxin Zhang",
        "Yan Gao",
        "Yi Wu",
        "Yao Hu",
        "Ziyang Huang",
        "Jun Zhao",
        "Kang Liu"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Retrieval-augmented generation (RAG) plays a critical role in user-generated\ncontent (UGC) platforms, but its effectiveness depends heavily on accurate\nrelevance assessment of query-document pairs. Despite recent advances in\napplying large language models (LLMs) to relevance modeling, UGC platforms\npresent unique challenges: 1) ambiguous user intent due to sparse user feedback\nin RAG scenarios, and 2) substantial noise introduced by informal and\nunstructured language. To address these issues, we propose the Reinforced\nReasoning Model for Relevance Assessment (R3A), which introduces a decomposed\nreasoning framework over queries and candidate documents before scoring. R3A\nfirst leverages auxiliary high-ranked documents within the platform to infer\nlatent query intent. It then performs verbatim fragment extraction to justify\nrelevance decisions, thereby reducing errors caused by noisy UGC. Based on a\nreinforcement learning framework, R3A is optimized to mitigate distortions\narising from ambiguous queries and unstructured content. Experimental results\nshow that R3A significantly outperforms existing baseline methods in terms of\nrelevance accuracy, across both offline benchmarks and online experiments.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02506v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02506v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.471,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.497,
      "distributed_training_score": 0.337,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses a reinforcement learning framework to optimize the R3A model for relevance assessment, but it does not involve training a reward model on human-ranked data or explicitly align with human preferences. Instead, it focuses on mitigating ambiguities in UGC platforms, making it only loosely connected to RLHF.",
      "weak_supervision_justification": "The paper addresses noisy and unstructured UGC by programmatically using auxiliary high-ranked documents to infer query intent and extract relevant fragments, which aligns with weak supervision's use of imprecise or noisy sources for label generation. However, it does not explicitly focus on training models with programmatically generated labels, limiting full relevance.",
      "diffusion_reasoning_justification": "The paper proposes a decomposed reasoning framework with reinforcement learning but does not involve diffusion models, iterative refinement processes, or treating a Chain-of-Thought as a holistic entity for multi-step logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces the Reinforced Reasoning Model for Relevance Assessment (R3A) to address challenges in retrieval-augmented generation (RAG) systems on user-generated content (UGC) platforms, such as ambiguous user intent and noisy, unstructured language. R3A employs a decomposed reasoning framework that uses auxiliary high-ranked documents to infer query intent and extracts verbatim fragments from candidate documents to reduce errors, optimized via reinforcement learning for improved accuracy. Experimental results demonstrate that R3A outperforms baseline methods in both offline benchmarks and online A/B testing, with a distilled version showing strong practical effectiveness.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining decomposed reasoning and reinforcement learning to tackle specific UGC challenges, offering a clever adaptation of existing techniques rather than a completely new paradigm. While it advances relevance assessment, it builds on established methods like LLMs and RL without introducing a fundamentally novel problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research and applications in information retrieval and AI for UGC platforms, given its demonstrated performance improvements and real-world validation through online experiments. However, its impact may be confined to specific subfields rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a high-quality contribution with practical innovations for relevance assessment in UGC, making it valuable for researchers and practitioners in AI and information retrieval. While not essential for all, it offers insights that could inform ongoing work in RAG systems.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b8016dc8a53d683b7821eca0285ac8b692a310f9",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 4,
      "average_h_index": 1.6666666666666667,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Xiaowei Yuan",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2283821740"
        },
        {
          "name": "Lei Jin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374978724"
        },
        {
          "name": "Haoxin Zhang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2290148641"
        },
        {
          "name": "Yan Gao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375092140"
        },
        {
          "name": "Yi Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374928345"
        },
        {
          "name": "Yao Hu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374923093"
        },
        {
          "name": "Ziyang Huang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2273912817"
        },
        {
          "name": "Jun Zhao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2286766397"
        },
        {
          "name": "Kang Liu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2283768710"
        }
      ]
    },
    {
      "id": "2508.02507",
      "title": "Rethinking Transparent Object Grasping: Depth Completion with Monocular\n  Depth Estimation and Instance Mask",
      "authors": [
        "Yaofeng Cheng",
        "Xinkai Gao",
        "Sen Zhang",
        "Chao Zeng",
        "Fusheng Zha",
        "Lining Sun",
        "Chenguang Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Due to the optical properties, transparent objects often lead depth cameras\nto generate incomplete or invalid depth data, which in turn reduces the\naccuracy and reliability of robotic grasping. Existing approaches typically\ninput the RGB-D image directly into the network to output the complete depth,\nexpecting the model to implicitly infer the reliability of depth values.\nHowever, while effective in training datasets, such methods often fail to\ngeneralize to real-world scenarios, where complex light interactions lead to\nhighly variable distributions of valid and invalid depth data. To address this,\nwe propose ReMake, a novel depth completion framework guided by an instance\nmask and monocular depth estimation. By explicitly distinguishing transparent\nregions from non-transparent ones, the mask enables the model to concentrate on\nlearning accurate depth estimation in these areas from RGB-D input during\ntraining. This targeted supervision reduces reliance on implicit reasoning and\nimproves generalization to real-world scenarios. Additionally, monocular depth\nestimation provides depth context between the transparent object and its\nsurroundings, enhancing depth prediction accuracy. Extensive experiments show\nthat our method outperforms existing approaches on both benchmark datasets and\nreal-world scenarios, demonstrating superior accuracy and generalization\ncapability. Code and videos are available at\nhttps://chengyaofeng.github.io/ReMake.github.io/.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02507v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02507v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.322,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02511",
      "title": "Test-time Prompt Intervention",
      "authors": [
        "Chenxu Yang",
        "Qingyi Si",
        "Mz Dai",
        "Dingyu Yao",
        "Mingyu Zheng",
        "Minghui Chen",
        "Zheng Lin",
        "Weiping Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Test-time compute has led to remarkable success in the large language model\n(LLM) community, particularly for complex tasks, where longer chains of thought\n(CoTs) are generated to enhance reasoning capabilities. However, growing\nevidence reveals that such reasoning models often produce CoTs plagued by\nexcessive redundancy, including unnecessary verification steps and repetitive\nreasoning shifts. The root cause lies in post-training of them that overly rely\non outcome reward paradigms, as the data of process reward paradigms, which\nregulate intermediate reasoning steps, is difficult to construct at scale. To\naddress this, we propose PI, a novel framework for Test-time Prompt\nIntervention. PI provides an interface to dynamically guide and regulate\nreasoning paths during inference through timely (When module) and proper (How\nmodule) interventions and post-intervention sampling (Which module). This\nallows human problem-solving expertise and cognitive science principles to be\nseamlessly integrated into LLMs' reasoning processes, enhancing controllability\nand interpretability. Extensive experiments across multiple models and datasets\ndemonstrate that PI significantly shortens CoTs while reducing hallucination,\nyielding more concise and reliable reasoning.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02511v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02511v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.493,
      "distributed_training_score": 0.36,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper critiques outcome-based rewards used in training methods like GRPO, which are similar to RLHF, but its main contribution is a test-time prompt intervention framework that does not involve training a reward model with human feedback or fine-tuning via reinforcement learning. Instead, it focuses on dynamic interventions during inference, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a framework for test-time prompt interventions in LLMs to regulate reasoning paths, with no mention or use of diffusion models, iterative refinement processes, or treating chains of thought as entities for holistic correction. It lacks any components related to diffusion-based approaches for logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02512",
      "title": "QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots",
      "authors": [
        "Sheng Wu",
        "Fei Teng",
        "Hao Shi",
        "Qi Jiang",
        "Kai Luo",
        "Kaiwei Wang",
        "Kailun Yang"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Panoramic cameras, capturing comprehensive 360-degree environmental data, are\nsuitable for quadruped robots in surrounding perception and interaction with\ncomplex environments. However, the scarcity of high-quality panoramic training\ndata-caused by inherent kinematic constraints and complex sensor calibration\nchallenges-fundamentally limits the development of robust perception systems\ntailored to these embodied platforms. To address this issue, we propose\nQuaDreamer-the first panoramic data generation engine specifically designed for\nquadruped robots. QuaDreamer focuses on mimicking the motion paradigm of\nquadruped robots to generate highly controllable, realistic panoramic videos,\nproviding a data source for downstream tasks. Specifically, to effectively\ncapture the unique vertical vibration characteristics exhibited during\nquadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts\ncontrollable vertical signals through frequency-domain feature filtering and\nprovides high-quality prompts. To facilitate high-quality panoramic video\ngeneration under jitter signal control, we propose a Scene-Object Controller\n(SOC) that effectively manages object motion and boosts background jitter\ncontrol through the attention mechanism. To address panoramic distortions in\nwide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream\narchitecture that synergizes frequency-texture refinement for local detail\nenhancement with spatial-structure correction for global geometric consistency.\nWe further demonstrate that the generated video sequences can serve as training\ndata for the quadruped robot's panoramic visual perception model, enhancing the\nperformance of multi-object tracking in 360-degree scenes. The source code and\nmodel weights will be publicly available at\nhttps://github.com/losehu/QuaDreamer.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02512v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02512v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.29,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.332,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02513",
      "title": "Modular Arithmetic: Language Models Solve Math Digit by Digit",
      "authors": [
        "Tanja Baeumel",
        "Daniil Gurgurov",
        "Yusser al Ghussin",
        "Josef van Genabith",
        "Simon Ostermann"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While recent work has begun to uncover the internal strategies that Large\nLanguage Models (LLMs) employ for simple arithmetic tasks, a unified\nunderstanding of their underlying mechanisms is still lacking. We extend recent\nfindings showing that LLMs represent numbers in a digit-wise manner and present\nevidence for the existence of digit-position-specific circuits that LLMs use to\nperform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that\noperate independently on different digit positions (units, tens, hundreds).\nNotably, such circuits exist independently of model size and of tokenization\nstrategy, i.e. both for models that encode longer numbers digit-by-digit and as\none token. Using Feature Importance and Causal Interventions, we identify and\nvalidate the digit-position-specific circuits, revealing a compositional and\ninterpretable structure underlying the solving of arithmetic problems in LLMs.\nOur interventions selectively alter the model's prediction at targeted digit\npositions, demonstrating the causal role of digit-position circuits in solving\narithmetic tasks.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02513v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02513v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.277,
      "diffusion_reasoning_score": 0.489,
      "distributed_training_score": 0.35,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on how Large Language Models (LLMs) handle arithmetic tasks through digit-position-specific circuits, using methods like feature importance and causal interventions. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for multi-step logical reasoning. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02516",
      "title": "Engagement Prediction of Short Videos with Large Multimodal Models",
      "authors": [
        "Wei Sun",
        "Linhan Cao",
        "Yuqin Cao",
        "Weixia Zhang",
        "Wen Wen",
        "Kaiwei Zhang",
        "Zijian Chen",
        "Fangfang Lu",
        "Xiongkuo Min",
        "Guangtao Zhai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The rapid proliferation of user-generated content (UGC) on short-form video\nplatforms has made video engagement prediction increasingly important for\noptimizing recommendation systems and guiding content creation. However, this\ntask remains challenging due to the complex interplay of factors such as\nsemantic content, visual quality, audio characteristics, and user background.\nPrior studies have leveraged various types of features from different\nmodalities, such as visual quality, semantic content, background sound, etc.,\nbut often struggle to effectively model their cross-feature and cross-modality\ninteractions. In this work, we empirically investigate the potential of large\nmultimodal models (LMMs) for video engagement prediction. We adopt two\nrepresentative LMMs: VideoLLaMA2, which integrates audio, visual, and language\nmodalities, and Qwen2.5-VL, which models only visual and language modalities.\nSpecifically, VideoLLaMA2 jointly processes key video frames, text-based\nmetadata, and background sound, while Qwen2.5-VL utilizes only key video frames\nand text-based metadata. Trained on the SnapUGC dataset, both models\ndemonstrate competitive performance against state-of-the-art baselines,\nshowcasing the effectiveness of LMMs in engagement prediction. Notably,\nVideoLLaMA2 consistently outperforms Qwen2.5-VL, highlighting the importance of\naudio features in engagement prediction. By ensembling two types of models, our\nmethod achieves first place in the ICCV VQualA 2025 EVQA-SnapUGC Challenge on\nshort-form video engagement prediction. The code is available at\nhttps://github.com/sunwei925/LMM-EVQA.git.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02516v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02516v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.308,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the application of large multimodal models (LMMs) like VideoLLaMA2 and Qwen2.5-VL for predicting video engagement, focusing on processing visual, audio, and textual data to estimate metrics such as Engagement Continuation Rate (ECR). It involves feature extraction, regression techniques, and model ensembling, but does not mention or utilize diffusion models, iterative refinement processes, or any adaptation of diffusion for logical reasoning tasks. As a result, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02521",
      "title": "Towards Reliable Audio Deepfake Attribution and Model Recognition: A\n  Multi-Level Autoencoder-Based Framework",
      "authors": [
        "Andrea Di Pierno",
        "Luca Guarnera",
        "Dario Allegra",
        "Sebastiano Battiato"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "The proliferation of audio deepfakes poses a growing threat to trust in\ndigital communications. While detection methods have advanced, attributing\naudio deepfakes to their source models remains an underexplored yet crucial\nchallenge. In this paper we introduce LAVA (Layered Architecture for Voice\nAttribution), a hierarchical framework for audio deepfake detection and model\nrecognition that leverages attention-enhanced latent representations extracted\nby a convolutional autoencoder trained solely on fake audio. Two specialized\nclassifiers operate on these features: Audio Deepfake Attribution (ADA), which\nidentifies the generation technology, and Audio Deepfake Model Recognition\n(ADMR), which recognize the specific generative model instance. To improve\nrobustness under open-set conditions, we incorporate confidence-based rejection\nthresholds. Experiments on ASVspoof2021, FakeOrReal, and CodecFake show strong\nperformance: the ADA classifier achieves F1-scores over 95% across all\ndatasets, and the ADMR module reaches 96.31% macro F1 across six classes.\nAdditional tests on unseen attacks from ASVpoof2019 LA and error propagation\nanalysis confirm LAVA's robustness and reliability. The framework advances the\nfield by introducing a supervised approach to deepfake attribution and model\nrecognition under open-set conditions, validated on public benchmarks and\naccompanied by publicly released models and code. Models and code are available\nat https://www.github.com/adipiz99/lava-framework.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02521v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02521v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.227,
      "weak_supervision_score": 0.217,
      "diffusion_reasoning_score": 0.226,
      "distributed_training_score": 0.207,
      "datasets_score": 0.221,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02525",
      "title": "Accurate and Interpretable Postmenstrual Age Prediction via Multimodal\n  Large Language Model",
      "authors": [
        "Qifan Chen",
        "Jin Cui",
        "Cindy Duan",
        "Yushuo Han",
        "Yifei Shi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Accurate estimation of postmenstrual age (PMA) at scan is crucial for\nassessing neonatal development and health. While deep learning models have\nachieved high accuracy in predicting PMA from brain MRI, they often function as\nblack boxes, offering limited transparency and interpretability in clinical\ndecision support. In this work, we address the dual challenge of accuracy and\ninterpretability by adapting a multimodal large language model (MLLM) to\nperform both precise PMA prediction and clinically relevant explanation\ngeneration. We introduce a parameter-efficient fine-tuning (PEFT) strategy\nusing instruction tuning and Low-Rank Adaptation (LoRA) applied to the\nQwen2.5-VL-7B model. The model is trained on four 2D cortical surface\nprojection maps derived from neonatal MRI scans. By employing distinct prompts\nfor training and inference, our approach enables the MLLM to handle a\nregression task during training and generate clinically relevant explanations\nduring inference. The fine-tuned model achieves a low prediction error with a\n95 percent confidence interval of 0.78 to 1.52 weeks, while producing\ninterpretable outputs grounded in developmental features, marking a significant\nstep toward transparent and trustworthy AI systems in perinatal neuroscience.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02525v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02525v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.347,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on parameter-efficient fine-tuning using instruction tuning and LoRA for a multimodal large language model, without any mention of training a reward model on human-ranked data or using reinforcement learning to align the model with human preferences. While a user study evaluates explanations, it does not involve RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs instruction tuning and prompt-based methods for PMA prediction and explanation generation, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning. There is no component for holistically correcting a Chain-of-Thought.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02528",
      "title": "From Pixels to Pathology: Restoration Diffusion for\n  Diagnostic-Consistent Virtual IHC",
      "authors": [
        "Jingsong Liu",
        "Xiaofeng Deng",
        "Han Li",
        "Azar Kazemi",
        "Christian Grashei",
        "Gesa Wilkens",
        "Xin You",
        "Tanja Groll",
        "Nassir Navab",
        "Carolin Mogler",
        "Peter J. Schüffler"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Hematoxylin and eosin (H&E) staining is the clinical standard for assessing\ntissue morphology, but it lacks molecular-level diagnostic information. In\ncontrast, immunohistochemistry (IHC) provides crucial insights into biomarker\nexpression, such as HER2 status for breast cancer grading, but remains costly\nand time-consuming, limiting its use in time-sensitive clinical workflows. To\naddress this gap, virtual staining from H&E to IHC has emerged as a promising\nalternative, yet faces two core challenges: (1) Lack of fair evaluation of\nsynthetic images against misaligned IHC ground truths, and (2) preserving\nstructural integrity and biological variability during translation. To this\nend, we present an end-to-end framework encompassing both generation and\nevaluation in this work. We introduce Star-Diff, a structure-aware staining\nrestoration diffusion model that reformulates virtual staining as an image\nrestoration task. By combining residual and noise-based generation pathways,\nStar-Diff maintains tissue structure while modeling realistic biomarker\nvariability. To evaluate the diagnostic consistency of the generated IHC\npatches, we propose the Semantic Fidelity Score (SFS), a\nclinical-grading-task-driven metric that quantifies class-wise semantic\ndegradation based on biomarker classification accuracy. Unlike pixel-level\nmetrics such as SSIM and PSNR, SFS remains robust under spatial misalignment\nand classifier uncertainty. Experiments on the BCI dataset demonstrate that\nStar-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity\nand diagnostic relevance. With rapid inference and strong clinical alignment,it\npresents a practical solution for applications such as intraoperative virtual\nIHC synthesis.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02528v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02528v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.488,
      "distributed_training_score": 0.322,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for image restoration and generation in histopathological image translation (from H&E to IHC), emphasizing structural preservation and biomarker variability. It does not involve adapting diffusion for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks, as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02530",
      "title": "Understanding the Risks of Asphalt Art on the Reliability of\n  Surveillance Perception Systems",
      "authors": [
        "Jin Ma",
        "Abyad Enan",
        "Long Cheng",
        "Mashrur Chowdhury"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Artistic crosswalks featuring asphalt art, introduced by different\norganizations in recent years, aim to enhance the visibility and safety of\npedestrians. However, their visual complexity may interfere with surveillance\nsystems that rely on vision-based object detection models. In this study, we\ninvestigate the impact of asphalt art on pedestrian detection performance of a\npretrained vision-based object detection model. We construct realistic\ncrosswalk scenarios by compositing various street art patterns into a fixed\nsurveillance scene and evaluate the model's performance in detecting\npedestrians on asphalt-arted crosswalks under both benign and adversarial\nconditions. A benign case refers to pedestrian crosswalks painted with existing\nnormal asphalt art, whereas an adversarial case involves digitally crafted or\naltered asphalt art perpetrated by an attacker. Our results show that while\nsimple, color-based designs have minimal effect, complex artistic patterns,\nparticularly those with high visual salience, can significantly degrade\npedestrian detection performance. Furthermore, we demonstrate that\nadversarially crafted asphalt art can be exploited to deliberately obscure real\npedestrians or generate non-existent pedestrian detections. These findings\nhighlight a potential vulnerability in urban vision-based pedestrian\nsurveillance systems and underscore the importance of accounting for\nenvironmental visual variations when designing robust pedestrian perception\nmodels.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02530v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02530v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.307,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02533",
      "title": "Precision-Aware Video Compression for Reducing Bandwidth Requirements in\n  Video Communication for Vehicle Detection-Based Applications",
      "authors": [
        "Abyad Enan",
        "Jon C Calhoun",
        "Mashrur Chowdhury"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Computer vision has become a popular tool in intelligent transportation\nsystems (ITS), enabling various applications through roadside traffic cameras\nthat capture video and transmit it in real time to computing devices within the\nsame network. The efficiency of this video transmission largely depends on the\navailable bandwidth of the communication system. However, limited bandwidth can\nlead to communication bottlenecks, hindering the real-time performance of ITS\napplications. To mitigate this issue, lossy video compression techniques can be\nused to reduce bandwidth requirements, at the cost of degrading video quality.\nThis degradation can negatively impact the accuracy of applications that rely\non real-time vehicle detection. Additionally, vehicle detection accuracy is\ninfluenced by environmental factors such as weather and lighting conditions,\nsuggesting that compression levels should be dynamically adjusted in response\nto these variations. In this work, we utilize a framework called\nPrecision-Aware Video Compression (PAVC), where a roadside video camera\ncaptures footage of vehicles on roadways, compresses videos, and then transmits\nthem to a processing unit, running a vehicle detection algorithm for\nsafety-critical applications, such as real-time collision risk assessment. The\nsystem dynamically adjusts the video compression level based on current weather\nand lighting conditions to maintain vehicle detection accuracy while minimizing\nbandwidth usage. Our results demonstrate that PAVC improves vehicle detection\naccuracy by up to 13% and reduces communication bandwidth requirements by up to\n8.23x in areas with moderate bandwidth availability. Moreover, in locations\nwith severely limited bandwidth, PAVC reduces bandwidth requirements by up to\n72x while preserving vehicle detection performance.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02533v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02533v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.261,
      "weak_supervision_score": 0.275,
      "diffusion_reasoning_score": 0.272,
      "distributed_training_score": 0.329,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02541",
      "title": "Automatic Identification of Machine Learning-Specific Code Smells",
      "authors": [
        "Peter Hamfelt",
        "Ricardo Britto",
        "Lincoln Rocha",
        "Camilo Almendra"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Machine learning (ML) has rapidly grown in popularity, becoming vital to many\nindustries. Currently, the research on code smells in ML applications lacks\ntools and studies that address the identification and validity of ML-specific\ncode smells. This work investigates suitable methods and tools to design and\ndevelop a static code analysis tool (MLpylint) based on code smell criteria.\nThis research employed the Design Science Methodology. In the problem\nidentification phase, a literature review was conducted to identify ML-specific\ncode smells. In solution design, a secondary literature review and\nconsultations with experts were performed to select methods and tools for\nimplementing the tool. We evaluated the tool on data from 160 open-source ML\napplications sourced from GitHub. We also conducted a static validation through\nan expert survey involving 15 ML professionals. The results indicate the\neffectiveness and usefulness of the MLpylint. We aim to extend our current\napproach by investigating ways to introduce MLpylint seamlessly into\ndevelopment workflows, fostering a more productive and innovative developer\nenvironment.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02541v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02541v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.426,
      "diffusion_reasoning_score": 0.292,
      "distributed_training_score": 0.276,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is the development and evaluation of a static code analysis tool (MLpylint) for identifying code smells in machine learning applications. This focuses on software engineering aspects like code quality and best practices, with no mention of weak supervision techniques, such as programmatically generating training labels from noisy sources. Therefore, there is no direct or indirect connection to the topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02546",
      "title": "What are you sinking? A geometric approach on attention sink",
      "authors": [
        "Valeria Ruscio",
        "Umberto Nanni",
        "Fabrizio Silvestri"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Attention sink (AS) is a consistent pattern in transformer attention maps\nwhere certain tokens (often special tokens or positional anchors)\ndisproportionately attract attention from other tokens. We show that in\ntransformers, AS is not an architectural artifact, but it is the manifestation\nof a fundamental geometric principle: the establishment of reference frames\nthat anchor representational spaces. We analyze several architectures and\nidentify three distinct reference frame types, centralized, distributed, and\nbidirectional, that correlate with the attention sink phenomenon. We show that\nthey emerge during the earliest stages of training as optimal solutions to the\nproblem of establishing stable coordinate systems in high-dimensional spaces.\nWe show the influence of architecture components, particularly position\nencoding implementations, on the specific type of reference frame. This\nperspective transforms our understanding of transformer attention mechanisms\nand provides insights for both architecture design and the relationship with\nAS.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02546v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02546v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.459,
      "distributed_training_score": 0.341,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on the geometric principles of attention sinks in transformer models, analyzing reference frames and their emergence in attention mechanisms. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02548",
      "title": "The KG-ER Conceptual Schema Language",
      "authors": [
        "Enrico Franconi",
        "Benoît Groz",
        "Jan Hidders",
        "Nina Pardal",
        "Sławek Staworko",
        "Jan Van den Bussche",
        "Piotr Wieczorek"
      ],
      "categories": [
        "cs.DB (Databases)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We propose KG-ER, a conceptual schema language for knowledge graphs that\ndescribes the structure of knowledge graphs independently of their\nrepresentation (relational databases, property graphs, RDF) while helping to\ncapture the semantics of the information stored in a knowledge graph.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02548v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02548v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.268,
      "weak_supervision_score": 0.281,
      "diffusion_reasoning_score": 0.321,
      "distributed_training_score": 0.208,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02549",
      "title": "MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming",
      "authors": [
        "Shuo Wang",
        "Yongcai Wang",
        "Wanting Li",
        "Yucheng Wang",
        "Maiyue Chen",
        "Kaihui Wang",
        "Zhizhong Su",
        "Xudong Cai",
        "Yeying Jin",
        "Deying Li",
        "Zhaoxin Fan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth\ninputs to provide rich spatial cues for action planning, but these sensors can\nbe costly or less accessible in real-world deployments. Recent approaches based\non Vision-Language Action (VLA) models achieve strong results with monocular\ninput, yet they still lag behind methods using panoramic RGB-D information. We\npresent MonoDream, a lightweight VLA framework that enables monocular agents to\nlearn a Unified Navigation Representation (UNR). This shared feature\nrepresentation jointly aligns navigation-relevant visual semantics (e.g.,\nglobal layout, depth, and future cues) and language-grounded action intent,\nenabling more reliable action prediction. MonoDream further introduces Latent\nPanoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to\npredict latent features of panoramic RGB and depth observations at both current\nand future steps based on only monocular input. Experiments on multiple VLN\nbenchmarks show that MonoDream consistently improves monocular navigation\nperformance and significantly narrows the gap with panoramic-based agents.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02549v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02549v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.445,
      "distributed_training_score": 0.329,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for monocular Vision-Language Navigation (VLN) that uses a Unified Navigation Representation and Latent Panoramic Dreaming tasks to enhance navigation from limited inputs. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02550",
      "title": "Stakeholder Perspectives on Humanistic Implementation of Computer\n  Perception in Healthcare: A Qualitative Study",
      "authors": [
        "Kristin M. Kostick-Quenet",
        "Meghan E. Hurley",
        "Syed Ayaz",
        "John Herrington",
        "Casey Zampella",
        "Julia Parish-Morris",
        "Birkan Tunç",
        "Gabriel Lázaro-Muñoz",
        "J. S. Blumenthal-Barby",
        "Eric A. Storch"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Computer perception (CP) technologies (digital phenotyping, affective\ncomputing and related passive sensing approaches) offer unprecedented\nopportunities to personalize healthcare, but provoke concerns about privacy,\nbias and the erosion of empathic, relationship-centered practice. A\ncomprehensive understanding of perceived risks, benefits, and implementation\nchallenges from those who design, deploy and experience these tools in\nreal-world settings remains elusive. This study provides the first\nevidence-based account of key stakeholder perspectives on the relational,\ntechnical, and governance challenges raised by the integration of CP\ntechnologies into patient care. We conducted in-depth, semi-structured\ninterviews with 102 stakeholders: adolescent patients and their caregivers,\nfrontline clinicians, technology developers, and ethics, legal, policy or\nphilosophy scholars. Transcripts underwent thematic analysis by a\nmultidisciplinary team; reliability was enhanced through double coding and\nconsensus adjudication. Stakeholders articulated seven interlocking concern\ndomains: (1) trustworthiness and data integrity; (2) patient-specific\nrelevance; (3) utility and workflow integration; (4) regulation and governance;\n(5) privacy and data protection; (6) direct and indirect patient harms; and (7)\nphilosophical critiques of reductionism. To operationalize humanistic\nsafeguards, we propose \"personalized roadmaps\": co-designed plans that\npredetermine which metrics will be monitored, how and when feedback is shared,\nthresholds for clinical action, and procedures for reconciling discrepancies\nbetween algorithmic inferences and lived experience. By translating these\ninsights into personalized roadmaps, we offer a practical framework for\ndevelopers, clinicians and policymakers seeking to harness continuous\nbehavioral data while preserving the humanistic core of care.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02550v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02550v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.298,
      "distributed_training_score": 0.285,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a qualitative study on stakeholder perspectives regarding the ethical implementation of computer perception technologies in healthcare, including concerns like privacy and governance. It does not involve training AI models, using human feedback for reward modeling, or applying reinforcement learning techniques, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02557",
      "title": "RL-U$^2$Net: A Dual-Branch UNet with Reinforcement Learning-Assisted\n  Multimodal Feature Fusion for Accurate 3D Whole-Heart Segmentation",
      "authors": [
        "Jierui Qu",
        "Jianchun Zhao"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate whole-heart segmentation is a critical component in the precise\ndiagnosis and interventional planning of cardiovascular diseases. Integrating\ncomplementary information from modalities such as computed tomography (CT) and\nmagnetic resonance imaging (MRI) can significantly enhance segmentation\naccuracy and robustness. However, existing multi-modal segmentation methods\nface several limitations: severe spatial inconsistency between modalities\nhinders effective feature fusion; fusion strategies are often static and lack\nadaptability; and the processes of feature alignment and segmentation are\ndecoupled and inefficient. To address these challenges, we propose a\ndual-branch U-Net architecture enhanced by reinforcement learning for feature\nalignment, termed RL-U$^2$Net, designed for precise and efficient multi-modal\n3D whole-heart segmentation. The model employs a dual-branch U-shaped network\nto process CT and MRI patches in parallel, and introduces a novel RL-XAlign\nmodule between the encoders. The module employs a cross-modal attention\nmechanism to capture semantic correspondences between modalities and a\nreinforcement-learning agent learns an optimal rotation strategy that\nconsistently aligns anatomical pose and texture features. The aligned features\nare then reconstructed through their respective decoders. Finally, an\nensemble-learning-based decision module integrates the predictions from\nindividual patches to produce the final segmentation result. Experimental\nresults on the publicly available MM-WHS 2017 dataset demonstrate that the\nproposed RL-U$^2$Net outperforms existing state-of-the-art methods, achieving\nDice coefficients of 93.1% on CT and 87.0% on MRI, thereby validating the\neffectiveness and superiority of the proposed approach.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02557v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02557v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.356,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves using reinforcement learning to optimize feature alignment between CT and MRI images for 3D whole-heart segmentation, specifically through an RL-XAlign module that learns rotation strategies. However, this does not align with RLHF, which requires training a reward model on human-ranked data and using it to fine-tune a model based on human preferences. The paper lacks any mention of human feedback, making it unrelated to the RLHF topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02560",
      "title": "Explainable AI Methods for Neuroimaging: Systematic Failures of Common\n  Tools, the Need for Domain-Specific Validation, and a Proposal for Safe\n  Application",
      "authors": [
        "Nys Tjade Siegel",
        "James H. Cole",
        "Mohamad Habes",
        "Stefan Haufe",
        "Kerstin Ritter",
        "Marc-André Schulz"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Trustworthy interpretation of deep learning models is critical for\nneuroimaging applications, yet commonly used Explainable AI (XAI) methods lack\nrigorous validation, risking misinterpretation. We performed the first\nlarge-scale, systematic comparison of XAI methods on ~45,000 structural brain\nMRIs using a novel XAI validation framework. This framework establishes\nverifiable ground truth by constructing prediction tasks with known signal\nsources - from localized anatomical features to subject-specific clinical\nlesions - without artificially altering input images. Our analysis reveals\nsystematic failures in two of the most widely used methods: GradCAM\nconsistently failed to localize predictive features, while Layer-wise Relevance\nPropagation generated extensive, artifactual explanations that suggest\nincompatibility with neuroimaging data characteristics. Our results indicate\nthat these failures stem from a domain mismatch, where methods with design\nprinciples tailored to natural images require substantial adaptation for\nneuroimaging data. In contrast, the simpler, gradient-based method SmoothGrad,\nwhich makes fewer assumptions about data structure, proved consistently\naccurate, suggesting its conceptual simplicity makes it more robust to this\ndomain shift. These findings highlight the need for domain-specific adaptation\nand validation of XAI methods, suggest that interpretations from prior\nneuroimaging studies using standard XAI methodology warrant re-evaluation, and\nprovide urgent guidance for practical application of XAI in neuroimaging.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02560v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02560v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.31,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluating Explainable AI (XAI) methods for neuroimaging, specifically comparing techniques like GradCAM, Layer-wise Relevance Propagation, and SmoothGrad on brain MRIs. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. The core contribution is domain-specific validation of XAI, with no mention or application of diffusion-based approaches, making it entirely unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02566",
      "title": "Dynamic Feature Selection based on Rule-based Learning for Explainable\n  Classification with Uncertainty Quantification",
      "authors": [
        "Javier Fumanal-Idocin",
        "Raquel Fernandez-Peralta",
        "Javier Andreu-Perez"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Dynamic feature selection (DFS) offers a compelling alternative to\ntraditional, static feature selection by adapting the selected features to each\nindividual sample. Unlike classical methods that apply a uniform feature set,\nDFS customizes feature selection per sample, providing insight into the\ndecision-making process for each case. DFS is especially significant in\nsettings where decision transparency is key, i.e., clinical decisions; however,\nexisting methods use opaque models, which hinder their applicability in\nreal-life scenarios. This paper introduces a novel approach leveraging a\nrule-based system as a base classifier for the DFS process, which enhances\ndecision interpretability compared to neural estimators. We also show how this\nmethod provides a quantitative measure of uncertainty for each feature query\nand can make the feature selection process computationally lighter by\nconstraining the feature search space. We also discuss when greedy selection of\nconditional mutual information is equivalent to selecting features that\nminimize the difference with respect to the global model predictions. Finally,\nwe demonstrate the competitive performance of our rule-based DFS approach\nagainst established and state-of-the-art greedy and RL methods, which are\nmostly considered opaque, compared to our explainable rule-based system.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02566v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02566v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.318,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses existing RL-based approaches for Dynamic Feature Selection (DFS) in the literature but does not propose or utilize Reinforcement Learning from Human Feedback (RLHF). Its main contribution is a rule-based system for DFS, with no mention of human-ranked data, reward models, or aligning AI with human preferences.",
      "weak_supervision_justification": "The paper focuses on rule-based learning for DFS and does not involve training models with programmatically generated labels from noisy or imprecise sources. It relies on a global model and rule-based classifiers without any reference to weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper's main contribution is on rule-based DFS with uncertainty quantification, and it does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02574",
      "title": "EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based\n  Sentiment Analysis in Healthcare",
      "authors": [
        "Eman Alamoudi",
        "Ellis Solaiman"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.SI (Social and Information Networks)"
      ],
      "abstract": "Arabic-language patient feedback remains under-analysed because dialect\ndiversity and scarce aspect-level sentiment labels hinder automated assessment.\nTo address this gap, we introduce EHSAN, a data-centric hybrid pipeline that\nmerges ChatGPT pseudo-labelling with targeted human review to build the first\nexplainable Arabic aspect-based sentiment dataset for healthcare. Each sentence\nis annotated with an aspect and sentiment label (positive, negative, or\nneutral), forming a pioneering Arabic dataset aligned with healthcare themes,\nwith ChatGPT-generated rationales provided for each label to enhance\ntransparency. To evaluate the impact of annotation quality on model\nperformance, we created three versions of the training data: a fully supervised\nset with all labels reviewed by humans, a semi-supervised set with 50% human\nreview, and an unsupervised set with only machine-generated labels. We\nfine-tuned two transformer models on these datasets for both aspect and\nsentiment classification. Experimental results show that our Arabic-specific\nmodel achieved high accuracy even with minimal human supervision, reflecting\nonly a minor performance drop when using ChatGPT-only labels. Reducing the\nnumber of aspect classes notably improved classification metrics across the\nboard. These findings demonstrate an effective, scalable approach to Arabic\naspect-based sentiment analysis (SA) in healthcare, combining large language\nmodel annotation with human expertise to produce a robust and explainable\ndataset. Future directions include generalisation across hospitals, prompt\nrefinement, and interpretable data-driven modelling.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02574v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02574v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.402,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.299,
      "datasets_score": 0.406,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using ChatGPT to generate pseudo-labels for training data, which aligns directly with weak supervision by programmatically creating noisy or imprecise labels from a large language model. It experiments with fully supervised, semi-supervised, and unsupervised variants, demonstrating how these weakly supervised labels impact model performance, thus embodying the core principles of weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces a new Arabic aspect-based sentiment dataset for healthcare, detailing its creation through a hybrid pipeline of ChatGPT pseudo-labelling and human review. It analyzes the dataset's quality by evaluating different versions on model performance, benchmarking classification metrics, and discussing curation methodologies, which fits squarely within research on dataset creation, analysis, and evaluation for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces EHSAN, a hybrid framework that leverages ChatGPT for pseudo-labelling combined with human review to create the first explainable Arabic dataset for aspect-based sentiment analysis in healthcare, addressing the challenges of dialect diversity and scarce labels in Arabic patient feedback. The methodology involves annotating sentences with aspects and sentiments, then fine-tuning transformer models on three versions of training data (fully supervised, semi-supervised, and unsupervised), with key findings showing high accuracy even with minimal human supervision and improved metrics when reducing aspect classes, demonstrating a scalable and effective approach for Arabic sentiment analysis.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining ChatGPT pseudo-labelling with human review for creating a new Arabic healthcare dataset, offering a clever adaptation of existing techniques to a underrepresented language and domain, though it does not introduce a entirely new architecture or problem.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like Arabic NLP and healthcare AI due to its novel dataset and hybrid method for low-resource languages, but its influence may remain confined to specific areas rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides valuable insights and a practical framework for sentiment analysis in underrepresented languages, making it essential for researchers in AI, NLP, and healthcare to be aware of its contributions and potential applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/80af482b0ff8d4c4fc31030c10d6c53c33b244b2",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 16,
      "average_h_index": 8.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Eman Alamoudi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2374487091"
        },
        {
          "name": "E. Solaiman",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/2344081"
        }
      ]
    },
    {
      "id": "2508.02583",
      "title": "CAMA: Enhancing Mathematical Reasoning in Large Language Models with\n  Causal Knowledge",
      "authors": [
        "Lei Zan",
        "Keli Zhang",
        "Ruichu Cai",
        "Lujia Pan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong performance across a\nwide range of tasks, yet they still struggle with complex mathematical\nreasoning, a challenge fundamentally rooted in deep structural dependencies. To\naddress this challenge, we propose \\textbf{CA}usal \\textbf{MA}thematician\n(\\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit,\nreusable mathematical structure. In the learning stage, CAMA first constructs\nthe \\textbf{M}athematical \\textbf{C}ausal \\textbf{G}raph (\\textbf{MCG}), a\nhigh-level representation of solution strategies, by combining LLM priors with\ncausal discovery algorithms applied to a corpus of question-solution pairs. The\nresulting MCG encodes essential knowledge points and their causal dependencies.\nTo better align the graph with downstream reasoning tasks, CAMA further refines\nthe MCG through iterative feedback derived from a selected subset of the\nquestion-solution pairs. In the reasoning stage, given a new question, CAMA\ndynamically extracts a task-relevant subgraph from the MCG, conditioned on both\nthe question content and the LLM's intermediate reasoning trace. This subgraph,\nwhich encodes the most pertinent knowledge points and their causal\ndependencies, is then injected back into the LLM to guide its reasoning\nprocess. Empirical results on real-world datasets show that CAMA significantly\nimproves LLM performance on challenging mathematical problems. Furthermore, our\nexperiments demonstrate that structured guidance consistently outperforms\nunstructured alternatives, and that incorporating asymmetric causal\nrelationships yields greater improvements than using symmetric associations\nalone.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02583v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02583v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.555,
      "distributed_training_score": 0.342,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces CAMA, a framework using causal graphs to enhance mathematical reasoning in LLMs through knowledge extraction and subgraph guidance. It does not involve diffusion models, iterative refinement processes for logical tasks, or treating Chain-of-Thought as a holistically corrected entity, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02584",
      "title": "MArgE: Meshing Argumentative Evidence from Multiple Large Language\n  Models for Justifiable Claim Verification",
      "authors": [
        "Ming Pok Ng",
        "Junqi Jiang",
        "Gabriel Freedman",
        "Antonio Rago",
        "Francesca Toni"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Leveraging outputs from multiple large language models (LLMs) is emerging as\na method for harnessing their power across a wide range of tasks while\nmitigating their capacity for making errors, e.g., hallucinations. However,\ncurrent approaches to combining insights from multiple LLMs often involve\nunstructured interactions (e.g., free debate), resulting in model generations\nthat are not faithfully justifiable. In this work, we introduce MArgE, a novel\nframework to provide formal structure to the evidence from each LLM, in the\nform of a tree of extracted arguments, for the task of claim verification. We\nuse a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks\nand semantics from the field of computational argumentation, to construct\nstructured argument trees for given claims. This process creates an inspectable\npathway from the initial arguments to the final claim verification decisions,\nproviding a faithful justification thereof. We show experimentally that MArgE\ncan significantly outperform single LLMs, including three open-source models\n(4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior\nmethods for unstructured multi-LLM debates. We thus demonstrate the advantages\nof incorporating formal, argumentative reasoning mechanisms when combining\nmultiple LLM outputs.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02584v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02584v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.464,
      "distributed_training_score": 0.338,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution, MArgE, focuses on structuring arguments from multiple LLMs into a tree using computational argumentation for claim verification. It involves generating arguments, evaluating their quality, and applying argumentative semantics, but does not mention or adapt diffusion models, iterative refinement processes, or holistic correction of reasoning paths as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02587",
      "title": "Parameter-Efficient Routed Fine-Tuning: Mixture-of-Experts Demands\n  Mixture of Adaptation Modules",
      "authors": [
        "Yilun Liu",
        "Yunpu Ma",
        "Yuetian Lu",
        "Shuo Chen",
        "Zifeng Ding",
        "Volker Tresp"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Mixture-of-Experts (MoE) benefits from a dynamic routing mechanism among\ntheir specialized experts, which existing Parameter- Efficient Fine-Tuning\n(PEFT) strategies fail to leverage. This motivates us to investigate whether\nadaptation modules themselves should incorporate routing mechanisms to align\nwith MoE's multi-expert architecture. We analyze dynamics of core components\nwhen applying PEFT to MoE language models and examine how different routing\nstrategies affect adaptation effectiveness. Extensive experiments adapting\nOLMoE-1B-7B and Mixtral-8x7B on various commonsense and math reasoning tasks\nvalidate the performance and efficiency of our routed approach. We identify the\noptimal configurations for different scenarios and provide empirical analyses\nwith practical insights to facilitate better PEFT and MoE applications.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02587v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02587v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.433,
      "distributed_training_score": 0.419,
      "datasets_score": 0.268,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Parameter-Efficient Fine-Tuning (PEFT) for Mixture-of-Experts (MoE) language models, emphasizing routing mechanisms and adaptation modules for tasks like commonsense and math reasoning. It does not mention diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. Therefore, there is no connection to diffusion-based approaches.",
      "distributed_training_justification": "The paper's main contribution is on designing PEFT strategies for MoE models, including routing and adaptation, with experiments on specific models and tasks. It does not address distributed training techniques, parallel computing algorithms, or multi-node machine learning systems, such as partitioning data or computation across processors. While MoE models may imply efficiency gains, the paper does not explore these aspects.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02593",
      "title": "Explainable AI for Automated User-specific Feedback in Surgical Skill\n  Acquisition",
      "authors": [
        "Catalina Gomez",
        "Lalithkumar Seenivasan",
        "Xinrui Zou",
        "Jeewoo Yoon",
        "Sirui Chu",
        "Ariel Leong",
        "Patrick Kramer",
        "Yu-Chun Ku",
        "Jose L. Porras",
        "Alejandro Martin-Gomez",
        "Masaru Ishii",
        "Mathias Unberath"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Traditional surgical skill acquisition relies heavily on expert feedback, yet\ndirect access is limited by faculty availability and variability in subjective\nassessments. While trainees can practice independently, the lack of\npersonalized, objective, and quantitative feedback reduces the effectiveness of\nself-directed learning. Recent advances in computer vision and machine learning\nhave enabled automated surgical skill assessment, demonstrating the feasibility\nof automatic competency evaluation. However, it is unclear whether such\nArtificial Intelligence (AI)-driven feedback can contribute to skill\nacquisition. Here, we examine the effectiveness of explainable AI\n(XAI)-generated feedback in surgical training through a human-AI study. We\ncreate a simulation-based training framework that utilizes XAI to analyze\nvideos and extract surgical skill proxies related to primitive actions. Our\nintervention provides automated, user-specific feedback by comparing trainee\nperformance to expert benchmarks and highlighting deviations from optimal\nexecution through understandable proxies for actionable guidance. In a\nprospective user study with medical students, we compare the impact of\nXAI-guided feedback against traditional video-based coaching on task outcomes,\ncognitive load, and trainees' perceptions of AI-assisted learning. Results\nshowed improved cognitive load and confidence post-intervention. While no\ndifferences emerged between the two feedback types in reducing performance gaps\nor practice adjustments, trends in the XAI group revealed desirable effects\nwhere participants more closely mimicked expert practice. This work encourages\nthe study of explainable AI in surgical education and the development of\ndata-driven, adaptive feedback mechanisms that could transform learning\nexperiences and competency assessment.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02593v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02593v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.458,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.324,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development and evaluation of an Explainable AI (XAI) framework for providing automated feedback in surgical skill acquisition, using computer vision to analyze videos and compare trainee performance to expert benchmarks. It does not involve training an AI model with human feedback, creating a reward model from human-ranked data, or using reinforcement learning to fine-tune models based on human preferences. Instead, the focus is on delivering interpretable feedback to humans, which is unrelated to the core concepts of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02601",
      "title": "StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis\n  in Low-Data Regimes",
      "authors": [
        "Siyi Liu",
        "Yujia Zheng",
        "Yongqi Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The application of machine learning on tabular data in specialized domains is\nseverely limited by data scarcity. While generative models offer a solution,\ntraditional methods falter in low-data regimes, and recent Large Language\nModels (LLMs) often ignore the explicit dependency structure of tabular data,\nleading to low-fidelity synthetics. To address these limitations, we introduce\nStructSynth, a novel framework that integrates the generative power of LLMs\nwith robust structural control. StructSynth employs a two-stage architecture.\nFirst, it performs explicit structure discovery to learn a Directed Acyclic\nGraph (DAG) from the available data. Second, this learned structure serves as a\nhigh-fidelity blueprint to steer the LLM's generation process, forcing it to\nadhere to the learned feature dependencies and thereby ensuring the generated\ndata respects the underlying structure by design. Our extensive experiments\ndemonstrate that StructSynth produces synthetic data with significantly higher\nstructural integrity and downstream utility than state-of-the-art methods. It\nproves especially effective in challenging low-data scenarios, successfully\nnavigating the trade-off between privacy preservation and statistical fidelity.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02601v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02601v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.482,
      "diffusion_reasoning_score": 0.467,
      "distributed_training_score": 0.391,
      "datasets_score": 0.403,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on a framework for structure-aware tabular data synthesis using LLMs and DAGs, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper addresses low-data regimes and uses LLMs for data generation, which could indirectly relate to handling noisy or programmatically derived data, but it does not involve training models with weak labels or high-level supervision sources as defined.",
      "diffusion_reasoning_justification": "The paper briefly mentions diffusion models in the context of generative approaches for data augmentation, but it does not adapt diffusion for multi-step logical reasoning or chain-of-thought refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper involves generating and evaluating synthetic tabular datasets for utility in low-data scenarios, which touches on dataset creation and benchmarking, though its primary focus is on the synthesis framework rather than dataset curation or analysis methodologies.",
      "llm_score_status": "completed",
      "summary": "The paper introduces StructSynth, a novel two-stage framework designed to generate high-fidelity synthetic tabular data in low-data environments by leveraging Large Language Models (LLMs). It first discovers the underlying feature dependencies as a Directed Acyclic Graph (DAG) from limited data, then uses this structure to guide the LLM's generation process, ensuring the synthetic data maintains structural integrity and outperforms existing methods in terms of utility and fidelity.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework that combines explicit structure discovery with LLM generation, significantly advancing state-of-the-art methods for tabular data synthesis in low-data regimes by addressing unhandled dependencies.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of generative models for tabular data, as it offers practical improvements for low-data scenarios in domains like healthcare and finance.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to AI and machine learning research on data synthesis, making it essential for those working in low-data environments to be aware of its innovations.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ab5004cf39d589ff9285d3ffacf4a770e6b68d44",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 1,
      "average_h_index": 0.3333333333333333,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Siyi Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2362233241"
        },
        {
          "name": "Yujia Zheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374616614"
        },
        {
          "name": "Yongqi Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373596879"
        }
      ]
    },
    {
      "id": "2508.02605",
      "title": "ReMoMask: Retrieval-Augmented Masked Motion Generation",
      "authors": [
        "Zhengdao Li",
        "Siheng Wang",
        "Zeyu Zhang",
        "Hao Tang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-Motion (T2M) generation aims to synthesize realistic and semantically\naligned human motion sequences from natural language descriptions. However,\ncurrent approaches face dual challenges: Generative models (e.g., diffusion\nmodels) suffer from limited diversity, error accumulation, and physical\nimplausibility, while Retrieval-Augmented Generation (RAG) methods exhibit\ndiffusion inertia, partial-mode collapse, and asynchronous artifacts. To\naddress these limitations, we propose ReMoMask, a unified framework integrating\nthree key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples\nnegative sample scale from batch size via momentum queues, substantially\nimproving cross-modal retrieval precision; 2) A Semantic Spatio-temporal\nAttention mechanism enforces biomechanical constraints during part-level fusion\nto eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates\nminor unconditional generation to enhance generalization. Built upon MoMask's\nRVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal\nsteps. Extensive experiments on standard benchmarks demonstrate the\nstate-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97%\nimprovement in FID scores on HumanML3D and KIT-ML, respectively, compared to\nthe previous SOTA method RAG-T2M. Code:\nhttps://github.com/AIGeeksGroup/ReMoMask. Website:\nhttps://aigeeksgroup.github.io/ReMoMask.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02605v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02605v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.478,
      "distributed_training_score": 0.301,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on text-to-motion generation using retrieval-augmented masked models, addressing challenges in motion synthesis like diversity and physical plausibility. While it mentions diffusion models as a limitation in existing approaches, it does not adapt diffusion for multi-step logical reasoning or treat a 'Chain-of-Thought' as an entity for iterative refinement. ReMoMask is built on RVQ-VAE and emphasizes retrieval and attention mechanisms, not diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02609",
      "title": "Entity Representation Learning Through Onsite-Offsite Graph for\n  Pinterest Ads",
      "authors": [
        "Jiayin Jin",
        "Zhimeng Pan",
        "Yang Tang",
        "Jiarui Feng",
        "Kungang Li",
        "Chongyuan Xiang",
        "Jiacheng Li",
        "Runze Su",
        "Siping Ji",
        "Han Sun",
        "Ling Leng",
        "Prathibha Deshikachar"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Graph Neural Networks (GNN) have been extensively applied to industry\nrecommendation systems, as seen in models like GraphSage\\cite{GraphSage},\nTwHIM\\cite{TwHIM}, LiGNN\\cite{LiGNN} etc. In these works, graphs were\nconstructed based on users' activities on the platforms, and various graph\nmodels were developed to effectively learn node embeddings. In addition to\nusers' onsite activities, their offsite conversions are crucial for Ads models\nto capture their shopping interest. To better leverage offsite conversion data\nand explore the connection between onsite and offsite activities, we\nconstructed a large-scale heterogeneous graph based on users' onsite ad\ninteractions and opt-in offsite conversion activities. Furthermore, we\nintroduced TransRA (TransR\\cite{TransR} with Anchors), a novel Knowledge Graph\nEmbedding (KGE) model, to more efficiently integrate graph embeddings into Ads\nranking models. However, our Ads ranking models initially struggled to directly\nincorporate Knowledge Graph Embeddings (KGE), and only modest gains were\nobserved during offline experiments. To address this challenge, we employed the\nLarge ID Embedding Table technique and innovated an attention based KGE\nfinetuning approach within the Ads ranking models. As a result, we observed a\nsignificant AUC lift in Click-Through Rate (CTR) and Conversion Rate (CVR)\nprediction models. Moreover, this framework has been deployed in Pinterest's\nAds Engagement Model and contributed to $2.69\\%$ CTR lift and $1.34\\%$ CPC\nreduction. We believe the techniques presented in this paper can be leveraged\nby other large-scale industrial models.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02609v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02609v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.389,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02611",
      "title": "Meta-RAG on Large Codebases Using Code Summarization",
      "authors": [
        "Vali Tawosi",
        "Salwa Alamir",
        "Xiaomo Liu",
        "Manuela Veloso"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Model (LLM) systems have been at the forefront of applied\nArtificial Intelligence (AI) research in a multitude of domains. One such\ndomain is software development, where researchers have pushed the automation of\na number of code tasks through LLM agents. Software development is a complex\necosystem, that stretches far beyond code implementation and well into the\nrealm of code maintenance. In this paper, we propose a multi-agent system to\nlocalize bugs in large pre-existing codebases using information retrieval and\nLLMs. Our system introduces a novel Retrieval Augmented Generation (RAG)\napproach, Meta-RAG, where we utilize summaries to condense codebases by an\naverage of 79.8\\%, into a compact, structured, natural language representation.\nWe then use an LLM agent to determine which parts of the codebase are critical\nfor bug resolution, i.e. bug localization. We demonstrate the usefulness of\nMeta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores\n84.67 % and 53.0 % for file-level and function-level correct localization\nrates, respectively, achieving state-of-the-art performance.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02611v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02611v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.334,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02621",
      "title": "HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous\n  Healthcare Research",
      "authors": [
        "Yinghao Zhu",
        "Yifan Qi",
        "Zixiang Wang",
        "Lei Gu",
        "Dehao Sui",
        "Haoran Hu",
        "Xichen Zhang",
        "Ziyi He",
        "Liantao Ma",
        "Lequan Yu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "The efficacy of AI agents in healthcare research is hindered by their\nreliance on static, predefined strategies. This creates a critical limitation:\nagents can become better tool-users but cannot learn to become better strategic\nplanners, a crucial skill for complex domains like healthcare. We introduce\nHealthFlow, a self-evolving AI agent that overcomes this limitation through a\nnovel meta-level evolution mechanism. HealthFlow autonomously refines its own\nhigh-level problem-solving policies by distilling procedural successes and\nfailures into a durable, strategic knowledge base. To anchor our research and\nfacilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark\nfeaturing complex, realistic health data analysis tasks derived from\npeer-reviewed clinical research. Our comprehensive experiments demonstrate that\nHealthFlow's self-evolving approach significantly outperforms state-of-the-art\nagent frameworks. This work marks a necessary shift from building better\ntool-users to designing smarter, self-evolving task-managers, paving the way\nfor more autonomous and effective AI for scientific discovery.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02621v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02621v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.453,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.357,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on HealthFlow, an AI agent that evolves through autonomous analysis of its own execution traces and experiences, without involving human feedback or a reward model trained on human preferences. There is no mention of RLHF techniques, such as fine-tuning with human-ranked data, making it unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes HealthFlow's meta-level evolution mechanism for strategic planning, which involves synthesizing knowledge from experiences, but it does not incorporate diffusion models or iterative refinement processes for chain-of-thought reasoning. There is no evidence of adapting diffusion techniques for logical tasks, so it does not align with this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02622",
      "title": "Noosemia: toward a Cognitive and Phenomenological Account of\n  Intentionality Attribution in Human-Generative AI Interaction",
      "authors": [
        "Enrico De Santis",
        "Antonello Rizzi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "This paper introduces and formalizes Noosem\\`ia, a novel\ncognitive-phenomenological pattern emerging from human interaction with\ngenerative AI systems, particularly those enabling dialogic or multimodal\nexchanges. We propose a multidisciplinary framework to explain how, under\ncertain conditions, users attribute intentionality, agency, and even\ninteriority to these systems - a process grounded not in physical resemblance,\nbut in linguistic performance, epistemic opacity, and emergent technological\ncomplexity. By linking an LLM declination of meaning holism to our technical\nnotion of the LLM Contextual Cognitive Field, we clarify how LLMs construct\nmeaning relationally and how coherence and a simulacrum of agency arise at the\nhuman-AI interface. The analysis situates noosemia alongside pareidolia,\nanimism, the intentional stance and the uncanny valley, distinguishing its\nunique characteristics. We also introduce a-noosemia to describe the\nphenomenological withdrawal of such projections. The paper concludes with\nreflections on the broader philosophical, epistemological and social\nimplications of noosemic dynamics and directions for future research.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02622v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02622v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.271,
      "datasets_score": 0.259,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the introduction of \"Noosemia,\" a concept exploring cognitive and phenomenological aspects of intentionality attribution in human interactions with generative AI, particularly LLMs. It discusses topics like the Eliza Effect, epistemic opacity, and human-AI dynamics, but does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning as defined. There is no connection to adapting diffusion for complex tasks or Chain-of-Thought reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02625",
      "title": "AutoML-Med: A Framework for Automated Machine Learning in Medical\n  Tabular Data",
      "authors": [
        "Riccardo Francia",
        "Maurizio Leone",
        "Giorgio Leonardi",
        "Stefania Montani",
        "Marzio Pennisi",
        "Manuel Striani",
        "Sandra D'Alfonso"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Medical datasets are typically affected by issues such as missing values,\nclass imbalance, a heterogeneous feature types, and a high number of features\nversus a relatively small number of samples, preventing machine learning models\nfrom obtaining proper results in classification and regression tasks. This\npaper introduces AutoML-Med, an Automated Machine Learning tool specifically\ndesigned to address these challenges, minimizing user intervention and\nidentifying the optimal combination of preprocessing techniques and predictive\nmodels. AutoML-Med's architecture incorporates Latin Hypercube Sampling (LHS)\nfor exploring preprocessing methods, trains models using selected metrics, and\nutilizes Partial Rank Correlation Coefficient (PRCC) for fine-tuned\noptimization of the most influential preprocessing steps. Experimental results\ndemonstrate AutoML-Med's effectiveness in two different clinical settings,\nachieving higher balanced accuracy and sensitivity, which are crucial for\nidentifying at-risk patients, compared to other state-of-the-art tools.\nAutoML-Med's ability to improve prediction results, especially in medical\ndatasets with sparse data and class imbalance, highlights its potential to\nstreamline Machine Learning applications in healthcare.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02625v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02625v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.321,
      "distributed_training_score": 0.346,
      "datasets_score": 0.402,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper primarily focuses on introducing and evaluating the AutoML-Med framework for handling challenges in medical tabular data, such as missing values and class imbalance. While it mentions using specific datasets (e.g., a Multiple Sclerosis dataset and a Type 2 Diabetes dataset) for experiments and compares results against benchmarks, it does not involve creating new datasets, curating methodologies, or deeply analyzing datasets themselves. The dataset discussions are contextual to demonstrate the framework's effectiveness, making the paper only tangentially related to dataset-focused research.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02629",
      "title": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and\n  Decision in Embodied Agents",
      "authors": [
        "Yibin Liu",
        "Zhixuan Liang",
        "Zanxin Chen",
        "Tianxing Chen",
        "Mengkang Hu",
        "Wanxi Dong",
        "Congsheng Xu",
        "Zhaoming Han",
        "Yusen Qin",
        "Yao Mu"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Recent advances in multimodal large language models (MLLMs) have enabled\nricher perceptual grounding for code policy generation in embodied agents.\nHowever, most existing systems lack effective mechanisms to adaptively monitor\npolicy execution and repair codes during task completion. In this work, we\nintroduce HyCodePolicy, a hybrid language-based control framework that\nsystematically integrates code synthesis, geometric grounding, perceptual\nmonitoring, and iterative repair into a closed-loop programming cycle for\nembodied agents. Technically, given a natural language instruction, our system\nfirst decomposes it into subgoals and generates an initial executable program\ngrounded in object-centric geometric primitives. The program is then executed\nin simulation, while a vision-language model (VLM) observes selected\ncheckpoints to detect and localize execution failures and infer failure\nreasons. By fusing structured execution traces capturing program-level events\nwith VLM-based perceptual feedback, HyCodePolicy infers failure causes and\nrepairs programs. This hybrid dual feedback mechanism enables self-correcting\nprogram synthesis with minimal human supervision. Our results demonstrate that\nHyCodePolicy significantly improves the robustness and sample efficiency of\nrobot manipulation policies, offering a scalable strategy for integrating\nmultimodal reasoning into autonomous decision-making pipelines.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02629v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02629v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.496,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.34,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces HyCodePolicy, a framework for self-correcting robot manipulation through code synthesis, monitoring, and repair using VLMs, with minimal human supervision. It does not involve training a reward model on human-ranked data or using reinforcement learning to fine-tune a model, which are core to RLHF. Instead, it focuses on autonomous feedback loops without human feedback integration.",
      "weak_supervision_justification": "The paper describes a system for generating and repairing code policies in embodied agents, but it does not involve training machine learning models using programmatically generated or noisy labels. Weak supervision typically pertains to label generation for model training, which is not addressed here; the work centers on runtime monitoring and repair, not supervisory techniques.",
      "diffusion_reasoning_justification": "The paper features iterative program repair and multi-step reasoning via VLMs and symbolic feedback, but it does not adapt the iterative refinement process of diffusion models for logical tasks. There is no mention of treating a Chain-of-Thought as a holistically correctable entity using diffusion, making this topic inapplicable.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02630",
      "title": "What Is Your AI Agent Buying? Evaluation, Implications and Emerging\n  Questions for Agentic E-Commerce",
      "authors": [
        "Amine Allouah",
        "Omar Besbes",
        "Josué D Figueroa",
        "Yash Kanoria",
        "Akshit Kumar"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)",
        "cs.HC (Human-Computer Interaction)",
        "cs.MA (Multiagent Systems)",
        "econ.GN (General Economics)"
      ],
      "abstract": "Online marketplaces will be transformed by autonomous AI agents acting on\nbehalf of consumers. Rather than humans browsing and clicking,\nvision-language-model (VLM) agents can parse webpages, evaluate products, and\ntransact. This raises a fundamental question: what do AI agents buy, and why?\nWe develop ACES, a sandbox environment that pairs a platform-agnostic VLM agent\nwith a fully programmable mock marketplace to study this question. We first\nconduct basic rationality checks in the context of simple tasks, and then, by\nrandomizing product positions, prices, ratings, reviews, sponsored tags, and\nplatform endorsements, we obtain causal estimates of how frontier VLMs actually\nshop. Models show strong but heterogeneous position effects: all favor the top\nrow, yet different models prefer different columns, undermining the assumption\nof a universal \"top\" rank. They penalize sponsored tags and reward\nendorsements. Sensitivities to price, ratings, and reviews are directionally\nhuman-like but vary sharply in magnitude across models. Motivated by scenarios\nwhere sellers use AI agents to optimize product listings, we show that a\nseller-side agent that makes minor tweaks to product descriptions, targeting AI\nbuyer preferences, can deliver substantial market-share gains if AI-mediated\nshopping dominates. We also find that modal product choices can differ across\nmodels and, in some cases, demand may concentrate on a few select products,\nraising competition questions. Together, our results illuminate how AI agents\nmay behave in e-commerce settings and surface concrete seller strategy,\nplatform design, and regulatory questions in an AI-mediated ecosystem.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02630v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02630v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.309,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development and evaluation of a sandbox environment (ACES) to study AI agent behavior in e-commerce, focusing on aspects like rationality, biases, and market dynamics using pre-existing vision-language models. It does not involve training or fine-tuning models with human feedback, nor does it discuss reinforcement learning techniques for aligning AI with human preferences. Therefore, the paper lacks any direct connection to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02634",
      "title": "Actionable Counterfactual Explanations Using Bayesian Networks and Path\n  Planning with Applications to Environmental Quality Improvement",
      "authors": [
        "Enrique Valero-Leal",
        "Pedro Larrañaga",
        "Concha Bielza"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Counterfactual explanations study what should have changed in order to get an\nalternative result, enabling end-users to understand machine learning\nmechanisms with counterexamples. Actionability is defined as the ability to\ntransform the original case to be explained into a counterfactual one. We\ndevelop a method for actionable counterfactual explanations that, unlike\npredecessors, does not directly leverage training data. Rather, data is only\nused to learn a density estimator, creating a search landscape in which to\napply path planning algorithms to solve the problem and masking the endogenous\ndata, which can be sensitive or private. We put special focus on estimating the\ndata density using Bayesian networks, demonstrating how their enhanced\ninterpretability is useful in high-stakes scenarios in which fairness is\nraising concern. Using a synthetic benchmark comprised of 15 datasets, our\nproposal finds more actionable and simpler counterfactuals than the current\nstate-of-the-art algorithms. We also test our algorithm with a real-world\nEnvironmental Protection Agency dataset, facilitating a more efficient and\nequitable study of policies to improve the quality of life in United States of\nAmerica counties. Our proposal captures the interaction of variables, ensuring\nequity in decisions, as policies to improve certain domains of study (air,\nwater quality, etc.) can be detrimental in others. In particular, the\nsociodemographic domain is often involved, where we find important variables\nrelated to the ongoing housing crisis that can potentially have a severe\nnegative impact on communities.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02634v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02634v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.272,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02640",
      "title": "An Efficient Continuous-Time MILP for Integrated Aircraft Hangar\n  Scheduling and Layout",
      "authors": [
        "Shayan Farhang Pazhooh",
        "Hossein Shams Shemirani"
      ],
      "categories": [
        "math.OC (Optimization and Control)",
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)"
      ],
      "abstract": "Efficient management of aircraft maintenance hangars is a critical\noperational challenge, involving complex, interdependent decisions regarding\naircraft scheduling and spatial allocation. This paper introduces a novel\ncontinuous-time mixed-integer linear programming (MILP) model to solve this\nintegrated spatio-temporal problem. By treating time as a continuous variable,\nour formulation overcomes the scalability limitations of traditional\ndiscrete-time approaches. The performance of the exact model is benchmarked\nagainst a constructive heuristic, and its practical applicability is\ndemonstrated through a custom-built visualization dashboard. Computational\nresults are compelling: the model solves instances with up to 25 aircraft to\nproven optimality, often in mere seconds, and for large-scale cases of up to 40\naircraft, delivers high-quality solutions within known optimality gaps. In all\ntested scenarios, the resulting solutions consistently and significantly\noutperform the heuristic, which highlights the framework's substantial economic\nbenefits and provides valuable managerial insights into the trade-off between\nsolution time and optimality.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02640v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02640v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.29,
      "weak_supervision_score": 0.242,
      "diffusion_reasoning_score": 0.292,
      "distributed_training_score": 0.294,
      "datasets_score": 0.225,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02644",
      "title": "D2PPO: Diffusion Policy Policy Optimization with Dispersive Loss",
      "authors": [
        "Guowei Zou",
        "Weibing Li",
        "Hejun Wu",
        "Yukun Qian",
        "Yuhang Wang",
        "Haitao Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Diffusion policies excel at robotic manipulation by naturally modeling\nmultimodal action distributions in high-dimensional spaces. Nevertheless,\ndiffusion policies suffer from diffusion representation collapse: semantically\nsimilar observations are mapped to indistinguishable features, ultimately\nimpairing their ability to handle subtle but critical variations required for\ncomplex robotic manipulation. To address this problem, we propose D2PPO\n(Diffusion Policy Policy Optimization with Dispersive Loss). D2PPO introduces\ndispersive loss regularization that combats representation collapse by treating\nall hidden representations within each batch as negative pairs. D2PPO compels\nthe network to learn discriminative representations of similar observations,\nthereby enabling the policy to identify subtle yet crucial differences\nnecessary for precise manipulation. In evaluation, we find that early-layer\nregularization benefits simple tasks, while late-layer regularization sharply\nenhances performance on complex manipulation tasks. On RoboMimic benchmarks,\nD2PPO achieves an average improvement of 22.7% in pre-training and 26.1% after\nfine-tuning, setting new SOTA results. In comparison with SOTA, results of\nreal-world experiments on a Franka Emika Panda robot show the excitingly high\nsuccess rate of our method. The superiority of our method is especially evident\nin complex tasks. Project page: https://guowei-zou.github.io/d2ppo/",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02644v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02644v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.538,
      "distributed_training_score": 0.431,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on diffusion policies for robotic manipulation with dispersive loss and PPO fine-tuning, but it does not involve human feedback, a reward model trained on human-ranked data, or any alignment with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper applies diffusion models to generate actions for robotic tasks through iterative denoising, but it does not adapt this process for multi-step logical reasoning, chain-of-thought refinement, or solving complex logical tasks as defined.",
      "distributed_training_justification": "The paper does not address distributed training, parallel computing, or multi-node setups; it focuses solely on algorithmic improvements to diffusion policies without any mention of partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02645",
      "title": "Evaluating Variance in Visual Question Answering Benchmarks",
      "authors": [
        "Nikitha SR"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal large language models (MLLMs) have emerged as powerful tools for\nvisual question answering (VQA), enabling reasoning and contextual\nunderstanding across visual and textual modalities. Despite their advancements,\nthe evaluation of MLLMs on VQA benchmarks often relies on point estimates,\noverlooking the significant variance in performance caused by factors such as\nstochastic model outputs, training seed sensitivity, and hyperparameter\nconfigurations. This paper critically examines these issues by analyzing\nvariance across 14 widely used VQA benchmarks, covering diverse tasks such as\nvisual reasoning, text understanding, and commonsense reasoning. We\nsystematically study the impact of training seed, framework non-determinism,\nmodel scale, and extended instruction finetuning on performance variability.\nAdditionally, we explore Cloze-style evaluation as an alternate assessment\nstrategy, studying its effectiveness in reducing stochasticity and improving\nreliability across benchmarks. Our findings highlight the limitations of\ncurrent evaluation practices and advocate for variance-aware methodologies to\nfoster more robust and reliable development of MLLMs.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02645v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02645v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.359,
      "datasets_score": 0.424,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluating variance in VQA benchmarks for MLLMs, including factors like stochastic outputs and hyperparameters, but it does not mention or involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper analyzes variance in performance across 14 existing VQA benchmarks, which involves evaluating and benchmarking datasets for tasks like visual reasoning and commonsense understanding. While it critiques evaluation practices and does not introduce new datasets or curation methods, its focus on benchmark analysis aligns with dataset evaluation aspects of the topic.",
      "llm_score_status": "completed",
      "summary": "This paper examines the variance in performance evaluations of Multimodal Large Language Models (MLLMs) on Visual Question Answering (VQA) benchmarks, highlighting how factors such as stochastic outputs, training seeds, and hyperparameters lead to unreliable results. By analyzing 14 diverse VQA benchmarks and exploring alternatives like Cloze-style evaluation, the authors systematically assess the impact of these variables on performance and advocate for variance-aware methodologies to improve the robustness and reliability of MLLM development.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by emphasizing variance in VQA evaluations, which cleverly combines existing benchmark techniques with a new focus on overlooked factors like stochasticity and hyperparameters, rather than introducing a entirely novel problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "This work is likely to be cited and influence evaluation practices within the subfield of MLLMs and VQA by promoting more reliable methodologies, though its broader applicability to other areas of AI may be limited.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "As a high-quality paper that highlights critical issues in AI benchmarking, it offers valuable insights for researchers in computer vision and natural language processing, making it essential for those working on MLLM evaluations to engage with its findings.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/29a6f023a77dd55303a7f84484424a0e54dc58db",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Nikitha",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374481734"
        }
      ]
    },
    {
      "id": "2508.02660",
      "title": "PMGS: Reconstruction of Projectile Motion across Large Spatiotemporal\n  Spans via 3D Gaussian Splatting",
      "authors": [
        "Yijun Xu",
        "Jingrui Zhang",
        "Yuhan Chen",
        "Dingwen Wang",
        "Lei Yu",
        "Chu He"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Modeling complex rigid motion across large spatiotemporal spans remains an\nunresolved challenge in dynamic reconstruction. Existing paradigms are mainly\nconfined to short-term, small-scale deformation and offer limited consideration\nfor physical consistency. This study proposes PMGS, focusing on reconstructing\nProjectile Motion via 3D Gaussian Splatting. The workflow comprises two stages:\n1) Target Modeling: achieving object-centralized reconstruction through dynamic\nscene decomposition and an improved point density control; 2) Motion Recovery:\nrestoring full motion sequences by learning per-frame SE(3) poses. We introduce\nan acceleration consistency constraint to bridge Newtonian mechanics and pose\nestimation, and design a dynamic simulated annealing strategy that adaptively\nschedules learning rates based on motion states. Futhermore, we devise a Kalman\nfusion scheme to optimize error accumulation from multi-source observations to\nmitigate disturbances. Experiments show PMGS's superior performance in\nreconstructing high-speed nonlinear rigid motion compared to mainstream dynamic\nmethods.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02660v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02660v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.288,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.347,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02669",
      "title": "MedVLThinker: Simple Baselines for Multimodal Medical Reasoning",
      "authors": [
        "Xiaoke Huang",
        "Juncheng Wu",
        "Hui Liu",
        "Xianfeng Tang",
        "Yuyin Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Large Reasoning Models (LRMs) have introduced a new paradigm in AI by\nenabling models to ``think before responding\" via chain-of-thought reasoning.\nHowever, the absence of open and reproducible recipes for building\nreasoning-centric medical LMMs hinders community-wide research, analysis, and\ncomparison. In this paper, we present MedVLThinker, a suite of simple yet\nstrong baselines. Our fully open recipe consists of: (1) systematic data\ncuration for both text-only and image-text medical data, filtered according to\nvarying levels of reasoning difficulty, and (2) two training paradigms:\nSupervised Fine-Tuning (SFT) on distilled reasoning traces and Reinforcement\nLearning with Verifiable Rewards (RLVR) based on final answer correctness.\nAcross extensive experiments on the Qwen2.5-VL model family (3B, 7B) and six\nmedical QA benchmarks, we find that RLVR consistently and significantly\noutperforms SFT. Additionally, under the RLVR framework, a key,\ncounter-intuitive finding is that training on our curated text-only reasoning\ndata provides a more substantial performance boost than training on multimodal\nimage-text data. Our best open 7B model, trained using the RLVR recipe on\ntext-only data, establishes a new state-of-the-art on existing public VQA\nbenchmarks, surpassing all previous open-source medical LMMs. Furthermore,\nscaling our model to 32B achieves performance on par with the proprietary\nGPT-4o. We release all curated data, models, and code to provide the community\nwith a strong, open foundation for future research in multimodal medical\nreasoning.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02669v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02669v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.488,
      "distributed_training_score": 0.387,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves developing baselines for multimodal medical reasoning using chain-of-thought techniques, Supervised Fine-Tuning (SFT), and Reinforcement Learning with Verifiable Rewards (RLVR). It does not mention, adapt, or utilize diffusion models or their iterative refinement processes for reasoning tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02671",
      "title": "Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on\n  Vision-Language Models",
      "authors": [
        "Haoyang Li",
        "Liang Wang",
        "Chao Wang",
        "Siyu Zhou",
        "Jing Jiang",
        "Yan Peng",
        "Guodong Long"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "For CLIP-based prompt tuning, introducing more data as additional knowledge\nfor enhancing fine-tuning process is proved to be an effective approach.\nExisting data amplification strategies for prompt tuning typically rely on\nexternal knowledge (e.g., large language models or pre-structured knowledge\nbases), resulting in higher costs for data collection and processing, while\ngenerally ignoring further utilization of features in image modality. To\naddress this, we propose Augmentation-driven Prompt Tuning (AugPT), a\nself-contained distillation-based prompt tuning approach using only internal\naugmentation on raw dataset to better exploit known features. Specifically,\nAugPT employs self-supervised augmentation on unlabeled images in the training\nset, and introduces a novel gating mechanism based on consensus test, reusing\nthe pre-trained prompt tuning backbone model to spontaneously filter noisy\nsamples, further enhancing the quality of augmented views. Extensive\nexperiments validate that AugPT simultaneously enhances model performance and\ngeneralization capability without using appended external knowledge. The code\nof AugPT is available at: https://github.com/JREion/AugPT .",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02671v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02671v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.439,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.372,
      "datasets_score": 0.38,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on prompt tuning and internal data augmentation for vision-language models, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper uses self-supervised augmentation and filtering of noisy samples from unlabeled images, which indirectly relates to handling imprecise data sources, but it does not involve programmatically generating labels or rely on weak supervision methods for training.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical reasoning, or multi-step chain-of-thought processes; it centers on prompt tuning and data augmentation for vision-language tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02765",
      "title": "The Architecture of Trust: A Framework for AI-Augmented Real Estate\n  Valuation in the Era of Structured Data",
      "authors": [
        "Petteri Teikari",
        "Mike Jarrell",
        "Maryam Azh",
        "Harri Pesola"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The Uniform Appraisal Dataset (UAD) 3.6's mandatory 2026 implementation\ntransforms residential property valuation from narrative reporting to\nstructured, machine-readable formats. This paper provides the first\ncomprehensive analysis of this regulatory shift alongside concurrent AI\nadvances in computer vision, natural language processing, and autonomous\nsystems. We develop a three-layer framework for AI-augmented valuation\naddressing technical implementation and institutional trust requirements. Our\nanalysis reveals how regulatory standardization converging with AI capabilities\nenables fundamental market restructuring with profound implications for\nprofessional practice, efficiency, and systemic risk. We make four key\ncontributions: (1) documenting institutional failures including inter-appraiser\nvariability and systematic biases undermining valuation reliability; (2)\ndeveloping an architectural framework spanning physical data acquisition,\nsemantic understanding, and cognitive reasoning that integrates emerging\ntechnologies while maintaining professional oversight; (3) addressing trust\nrequirements for high-stakes financial applications including regulatory\ncompliance, algorithmic fairness, and uncertainty quantification; (4) proposing\nevaluation methodologies beyond generic AI benchmarks toward domain-specific\nprotocols. Our findings indicate successful transformation requires not merely\ntechnological sophistication but careful human-AI collaboration, creating\nsystems that augment rather than replace professional expertise while\naddressing historical biases and information asymmetries in real estate\nmarkets.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02765v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02765v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.429,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.356,
      "datasets_score": 0.422,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on a framework for AI-augmented real estate valuation, emphasizing human-AI collaboration and professional oversight, but it does not discuss reinforcement learning techniques, including training models with human feedback or reward models. There is no mention of aligning AI with human preferences through RLHF methods.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper discusses the Uniform Appraisal Dataset (UAD) 3.6 as a structured data format driving regulatory changes and enabling AI applications, including data acquisition and analysis. However, it does not primarily focus on creating, analyzing, benchmarking, or evaluating datasets for ML; instead, it uses datasets as context for a broader AI framework.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02766",
      "title": "The Silicon Reasonable Person: Can AI Predict How Ordinary People Judge\n  Reasonableness?",
      "authors": [
        "Yonathan A. Arbel"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In everyday life, people make countless reasonableness judgments that\ndetermine appropriate behavior in various contexts. Predicting these judgments\nchallenges the legal system, as judges' intuitions may not align with broader\nsocietal views. This Article investigates whether large language models (LLMs)\ncan learn to identify patterns driving human reasonableness judgments.\n  Using randomized controlled trials comparing humans and models across\nmultiple legal contexts with over 10,000 simulated judgments, we demonstrate\nthat certain models capture not just surface-level responses but potentially\ntheir underlying decisional architecture. Strikingly, these systems prioritize\nsocial cues over economic efficiency in negligence determinations, mirroring\nhuman behavior despite contradicting textbook treatments.\n  These findings suggest practical applications: judges could calibrate\nintuitions against broader patterns, lawmakers could test policy\ninterpretations, and resource-constrained litigants could preview argument\nreception. As AI agents increasingly make autonomous real-world decisions,\nunderstanding whether they've internalized recognizable ethical frameworks\nbecomes essential for anticipating their behavior.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02766v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02766v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.231,
      "datasets_score": 0.281,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates LLMs for predicting human reasonableness judgments using randomized controlled trials, but it does not mention or involve reinforcement learning from human feedback. There is no description of training a reward model with human-ranked data or fine-tuning via reinforcement learning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses LLMs learning patterns from data to predict judgments, but it does not reference diffusion models, iterative refinement processes, or multi-step logical reasoning as described. There is no indication of treating Chain-of-Thought as a holistic entity for correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02773",
      "title": "Web3 x AI Agents: Landscape, Integrations, and Foundational Challenges",
      "authors": [
        "Yiming Shen",
        "Jiashuo Zhang",
        "Zhenzhe Shao",
        "Wenxuan Luo",
        "Yanlin Wang",
        "Ting Chen",
        "Zibin Zheng",
        "Jiachi Chen"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "econ.GN (General Economics)"
      ],
      "abstract": "The convergence of Web3 technologies and AI agents represents a rapidly\nevolving frontier poised to reshape decentralized ecosystems. This paper\npresents the first and most comprehensive analysis of the intersection between\nWeb3 and AI agents, examining five critical dimensions: landscape, economics,\ngovernance, security, and trust mechanisms. Through an analysis of 133 existing\nprojects, we first develop a taxonomy and systematically map the current market\nlandscape (RQ1), identifying distinct patterns in project distribution and\ncapitalization. Building upon these findings, we further investigate four key\nintegrations: (1) the role of AI agents in participating in and optimizing\ndecentralized finance (RQ2); (2) their contribution to enhancing Web3\ngovernance mechanisms (RQ3); (3) their capacity to strengthen Web3 security via\nintelligent vulnerability detection and automated smart contract auditing\n(RQ4); and (4) the establishment of robust reliability frameworks for AI agent\noperations leveraging Web3's inherent trust infrastructure (RQ5). By\nsynthesizing these dimensions, we identify key integration patterns, highlight\nfoundational challenges related to scalability, security, and ethics, and\noutline critical considerations for future research toward building robust,\nintelligent, and trustworthy decentralized systems with effective AI agent\ninteractions.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02773v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02773v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.417,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.354,
      "datasets_score": 0.405,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on the landscape, integrations, and challenges of Web3 technologies with AI agents, including taxonomy development and applications in decentralized finance, governance, security, and trust. It does not discuss reinforcement learning, human feedback mechanisms, or training AI models with human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper includes the creation and provision of an open-source dataset of 133 Web3 AI agent projects for future research, which aligns with dataset introduction and analysis in AI applications. However, this is not the primary focus, as the main contributions center on taxonomy, integrations, and challenges rather than in-depth dataset curation or benchmarking.",
      "llm_score_status": "completed",
      "summary": "This paper provides a comprehensive analysis of the intersection between Web3 technologies and AI agents, examining five key dimensions: landscape, economics, governance, security, and trust mechanisms, through the study of 133 existing projects. It develops a taxonomy to map the market landscape, investigates integrations such as AI agents' roles in decentralized finance, governance enhancement, security improvements, and reliability frameworks, while highlighting challenges like scalability, security, and ethics, and offers an open-source dataset for future research.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new and comprehensive taxonomy for Web3-AI agent projects, representing a significant advancement in understanding this emerging field by analyzing 133 projects and identifying novel integration patterns. This work addresses a previously underexplored intersection, making it more than just an incremental refinement.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research and commercial applications in Web3 and AI by providing a foundational taxonomy, highlighting key integrations, and offering an open-source dataset that can be built upon. Its focus on critical challenges like scalability and ethics positions it to drive advancements in decentralized systems.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a high-quality and valuable contribution to the emerging field of Web3-AI integrations, offering essential insights and resources for researchers and practitioners. While not groundbreaking for all audiences, it is significant for those in AI, blockchain, and related areas to stay informed on current developments and challenges.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/72f40123bb734314189d3c6c13321e36d02a929a",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 15,
      "average_h_index": 5.75,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Yiming Shen",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2315950112"
        },
        {
          "name": "Jiashuo Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2370951422"
        },
        {
          "name": "Zhenzhe Shao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2349814890"
        },
        {
          "name": "Wenxuan Luo",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2335092599"
        },
        {
          "name": "Yanlin Wang",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2239164852"
        },
        {
          "name": "Ting Chen",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2239103211"
        },
        {
          "name": "Zibin Zheng",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2267902535"
        },
        {
          "name": "Jiachi Chen",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/2254800142"
        }
      ]
    },
    {
      "id": "2508.02789",
      "title": "Cognitive Loop via In-Situ Optimization: Self-Adaptive Reasoning for\n  Science",
      "authors": [
        "Newman Cheng",
        "Gordon Broadbent",
        "William Chappell"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The capacity for artificial intelligence (AI) to formulate, evolve, and test\naltered thought patterns under dynamic conditions indicates advanced cognition\nthat is crucial for scientific discovery. The existing AI development landscape\nfalls into two categories: 1) frameworks over non-reasoning models that\nnatively incorporate opinions on how humans think, and 2) reasoning models that\nabstract precise control of the reasoning intuition away from end users. While\npowerful, for scientists to maximize utility of AI in scientific discovery,\nthey not only require accuracy and transparency in reasoning, but also\nsteerability. Hence, we introduce an alternative approach that enables deep and\nprecise control over the reasoning process called: a cognitive loop via in-situ\noptimization (CLIO). CLIO enables large language models (LLMs) to\nself-formulate ways of approaching a problem, adapt behavior when\nself-confidence is low, and ultimately provide scientists with a final belief\nor answer. Through CLIO's open design, scientists can observe uncertainty\nlevels, understand how final belief states are formulated using graph\nstructures, and interject corrections. Without any further post-training,\nOpenAI's GPT-4.1 with CLIO yields an accuracy of 22.37\\% in text-based biology\nand medicine questions on Humanity's Last Exam (HLE). This yields a 13.82\\% net\nor 161.64\\% relative increase when compared to the base GPT-4.1 model and\nsurpasses OpenAI's o3 performance in high and low reasoning effort modes. We\nfurther discovered that oscillations within internal uncertainty measures are\nkey in determining the accuracy of CLIO's results, revealing how its open\ndesign and internal mechanisms can provide insight and control into scientific\ndecision-making processes.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02789v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02789v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.49,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.534,
      "distributed_training_score": 0.367,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses CLIO as an alternative or complement to reinforcement learning post-training, mentioning user steering and corrections in real-time, which involves human feedback. However, it does not describe training a reward model on human-ranked data or using reinforcement learning algorithms for fine-tuning, making it only indirectly related to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on in-situ optimization and cognitive loops for reasoning in LLMs, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for holistic correction. Thus, it lacks any components characteristic of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02799",
      "title": "Extracting Range-Doppler Information of Moving Targets from Wi-Fi\n  Channel State Information",
      "authors": [
        "Jessica Sanson",
        "Rahul C. Shah",
        "Maximilian Pinaroc",
        "Valerio Frascolla"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper presents, for the first time, a method to extract both range and\nDoppler information from commercial Wi-Fi Channel State Information (CSI) using\na monostatic (single transceiver) setup. Utilizing the CSI phase in Wi-Fi\nsensing from a Network Interface Card (NIC) not designed for full-duplex\noperation is challenging due to (1) Hardware asynchronization, which introduces\nsignificant phase errors, and (2) Proximity of transmit (Tx) and receive (Rx)\nantennas, which creates strong coupling that overwhelms the motion signal of\ninterest. We propose a new signal processing approach that addresses both\nchallenges via three key innovations: Time offset cancellation, Phase alignment\ncorrection, and Tx/Rx coupling mitigation. Our method achieves cm-level\naccuracy in range and Doppler estimation for moving targets, validated using a\ncommercial Intel Wi-Fi AX211 NIC. Our results show successful detection and\ntracking of moving objects in realistic environments, establishing the\nfeasibility of high-precision sensing using standard Wi-Fi packet\ncommunications and off-the-shelf hardware without requiring any modification or\nspecialized full-duplex capabilities.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02799v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02799v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.241,
      "weak_supervision_score": 0.274,
      "diffusion_reasoning_score": 0.272,
      "distributed_training_score": 0.294,
      "datasets_score": 0.271,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02801",
      "title": "Adaptive Knowledge Distillation for Device-Directed Speech Detection",
      "authors": [
        "Hyung Gun Chi",
        "Florian Pesce",
        "Wonil Chang",
        "Oggi Rudovic",
        "Arturo Argueta",
        "Stefan Braun",
        "Vineet Garg",
        "Ahmed Hussen Abdelaziz"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Device-directed speech detection (DDSD) is a binary classification task that\nseparates the user's queries to a voice assistant (VA) from background speech\nor side conversations. This is important for achieving naturalistic user\nexperience. To this end, we propose knowledge distillation (KD) to enhance DDSD\naccuracy while ensuring efficient deployment. Specifically, we introduce a\nnovel adaptive KD method that transfers knowledge from general representations\nof an ASR large pre-trained acoustic encoder (teacher). We apply task-specific\nadapters, on top of the (frozen) teacher encoder, trained jointly with the\nstudent model on DDSD. We demonstrate that the proposed adaptive KD outperforms\nthe student model without distillation in the keyword and keyword-free\n(follow-up) invocations, with an improvement of +26% and +19% in terms of Equal\nError Rate, respectively. We also show that this approach generalizes across\nthe transformer and conformer-based model architectures.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02801v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02801v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.406,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a method for adaptive knowledge distillation in speech detection, focusing on transferring knowledge from a pre-trained model to a smaller one. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought.",
      "distributed_training_justification": "The paper discusses knowledge distillation for efficient model deployment in speech detection, but it does not cover distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes or processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02806",
      "title": "PyCAT4: A Hierarchical Vision Transformer-based Framework for 3D Human\n  Pose Estimation",
      "authors": [
        "Zongyou Yang",
        "Jonathan Loo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recently, a significant improvement in the accuracy of 3D human pose\nestimation has been achieved by combining convolutional neural networks (CNNs)\nwith pyramid grid alignment feedback loops. Additionally, innovative\nbreakthroughs have been made in the field of computer vision through the\nadoption of Transformer-based temporal analysis architectures. Given these\nadvancements, this study aims to deeply optimize and improve the existing Pymaf\nnetwork architecture. The main innovations of this paper include: (1)\nIntroducing a Transformer feature extraction network layer based on\nself-attention mechanisms to enhance the capture of low-level features; (2)\nEnhancing the understanding and capture of temporal signals in video sequences\nthrough feature temporal fusion techniques; (3) Implementing spatial pyramid\nstructures to achieve multi-scale feature fusion, effectively balancing feature\nrepresentations differences across different scales. The new PyCAT4 model\nobtained in this study is validated through experiments on the COCO and 3DPW\ndatasets. The results demonstrate that the proposed improvement strategies\nsignificantly enhance the network's detection capability in human pose\nestimation, further advancing the development of human pose estimation\ntechnology.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02806v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02806v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.298,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.343,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02807",
      "title": "DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a\n  Stage-Wise Diffusion Transformer Framework",
      "authors": [
        "Tongchun Zuo",
        "Zaiyu Huang",
        "Shuliang Ning",
        "Ente Lin",
        "Chao Liang",
        "Zerong Zheng",
        "Jianwen Jiang",
        "Yuan Zhang",
        "Mingyuan Gao",
        "Xin Dong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Video virtual try-on (VVT) technology has garnered considerable academic\ninterest owing to its promising applications in e-commerce advertising and\nentertainment. However, most existing end-to-end methods rely heavily on scarce\npaired garment-centric datasets and fail to effectively leverage priors of\nadvanced visual models and test-time inputs, making it challenging to\naccurately preserve fine-grained garment details and maintain temporal\nconsistency in unconstrained scenarios. To address these challenges, we propose\nDreamVVT, a carefully designed two-stage framework built upon Diffusion\nTransformers (DiTs), which is inherently capable of leveraging diverse unpaired\nhuman-centric data to enhance adaptability in real-world scenarios. To further\nleverage prior knowledge from pretrained models and test-time inputs, in the\nfirst stage, we sample representative frames from the input video and utilize a\nmulti-frame try-on model integrated with a vision-language model (VLM), to\nsynthesize high-fidelity and semantically consistent keyframe try-on images.\nThese images serve as complementary appearance guidance for subsequent video\ngeneration. \\textbf{In the second stage}, skeleton maps together with\nfine-grained motion and appearance descriptions are extracted from the input\ncontent, and these along with the keyframe try-on images are then fed into a\npretrained video generation model enhanced with LoRA adapters. This ensures\nlong-term temporal coherence for unseen regions and enables highly plausible\ndynamic motions. Extensive quantitative and qualitative experiments demonstrate\nthat DreamVVT surpasses existing methods in preserving detailed garment content\nand temporal stability in real-world scenarios. Our project page\nhttps://virtu-lab.github.io/",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02807v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02807v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.463,
      "distributed_training_score": 0.365,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using Diffusion Transformers (DiTs) for video virtual try-on, specifically for generating realistic images and videos with temporal consistency. While it employs diffusion processes for iterative refinement in visual generation tasks, such as synthesizing keyframes and full videos, there is no evidence of adapting diffusion models for multi-step logical reasoning or treating a 'Chain-of-Thought' as a holistic entity for logical tasks. The application is purely generative and visual, not involving complex logical problem-solving.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02808",
      "title": "Clinically Grounded Agent-based Report Evaluation: An Interpretable\n  Metric for Radiology Report Generation",
      "authors": [
        "Radhika Dua",
        "Young Joon",
        "Kwon",
        "Siddhant Dogra",
        "Daniel Freedman",
        "Diana Ruan",
        "Motaz Nashawaty",
        "Danielle Rigau",
        "Daniel Alexander Alber",
        "Kang Zhang",
        "Kyunghyun Cho",
        "Eric Karl Oermann"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Radiological imaging is central to diagnosis, treatment planning, and\nclinical decision-making. Vision-language foundation models have spurred\ninterest in automated radiology report generation (RRG), but safe deployment\nrequires reliable clinical evaluation of generated reports. Existing metrics\noften rely on surface-level similarity or behave as black boxes, lacking\ninterpretability. We introduce ICARE (Interpretable and Clinically-grounded\nAgent-based Report Evaluation), an interpretable evaluation framework\nleveraging large language model agents and dynamic multiple-choice question\nanswering (MCQA). Two agents, each with either the ground-truth or generated\nreport, generate clinically meaningful questions and quiz each other. Agreement\non answers captures preservation and consistency of findings, serving as\ninterpretable proxies for clinical precision and recall. By linking scores to\nquestion-answer pairs, ICARE enables transparent, and interpretable assessment.\nClinician studies show ICARE aligns significantly more with expert judgment\nthan prior metrics. Perturbation analyses confirm sensitivity to clinical\ncontent and reproducibility, while model comparisons reveal interpretable error\npatterns.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02808v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02808v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.315,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces ICARE, an agent-based framework for evaluating radiology reports using large language models to generate and answer multiple-choice questions, focusing on clinical precision and recall. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. The main contribution is in interpretable evaluation metrics for medical reports, with no connection to adapting diffusion for complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02826",
      "title": "TransAM: Transformer-Based Agent Modeling for Multi-Agent Systems via\n  Local Trajectory Encoding",
      "authors": [
        "Conor Wallace",
        "Umer Siddique",
        "Yongcan Cao"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Agent modeling is a critical component in developing effective policies\nwithin multi-agent systems, as it enables agents to form beliefs about the\nbehaviors, intentions, and competencies of others. Many existing approaches\nassume access to other agents' episodic trajectories, a condition often\nunrealistic in real-world applications. Consequently, a practical agent\nmodeling approach must learn a robust representation of the policies of the\nother agents based only on the local trajectory of the controlled agent. In\nthis paper, we propose \\texttt{TransAM}, a novel transformer-based agent\nmodeling approach to encode local trajectories into an embedding space that\neffectively captures the policies of other agents. We evaluate the performance\nof the proposed method in cooperative, competitive, and mixed multi-agent\nenvironments. Extensive experimental results demonstrate that our approach\ngenerates strong policy representations, improves agent modeling, and leads to\nhigher episodic returns.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02826v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02826v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.356,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces TransAM, a transformer-based method for agent modeling in multi-agent systems, focusing on encoding local trajectories to infer other agents' policies. It does not involve diffusion models, iterative refinement processes, or any mechanism for multi-step logical reasoning as described in the topic. Instead, it relies on transformers for sequence modeling in reinforcement learning contexts.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02827",
      "title": "Automated Validation of LLM-based Evaluators for Software Engineering\n  Artifacts",
      "authors": [
        "Ora Nova Fandina",
        "Eitan Farchi",
        "Shmulik Froimovich",
        "Rami Katan",
        "Alice Podolsky",
        "Orna Raz",
        "Avi Ziv"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Automation in software engineering increasingly relies on large language\nmodels (LLMs) to generate, review, and assess code artifacts. However,\nestablishing LLMs as reliable evaluators remains an open challenge: human\nevaluations are costly, subjective and non scalable, while existing automated\nmethods fail to discern fine grained variations in artifact quality.\n  We introduce REFINE (Ranking Evaluators for FIne grained Nuanced Evaluation),\nan automated framework for benchmarking LLM based evaluators across software\nengineering tasks. REFINE comprises of two modules: Hierarchy Dataset Builder\napplies novel generation techniques to automatically synthesize artifacts with\nprogressively reduced quality, and Evaluator Tester quantifies each candidate\nevaluator configuration by measuring how closely its rankings align with\nexpected ordering.\n  A key feature of REFINE is controllability: users can tune the granularity of\ndegradation to progressively refine evaluator configurations, from coarse\nfiltering to stress testing on subtle quality gaps.\n  While the methodology is general, we focus on coding tasks reflecting the\npractical demands in our production setting. REFINE was integrated into IBM's\ninternal development workflows and applied to code generation, translation, and\nsummarization for COBOL, an enterprise critical programming language, using\nindustrial data. It was used to identify LLM as a Judge configurations that\nlifted alignment scores from below $0.7$ to above $0.9$ in some coding tasks.\nThese nuance sensitive evaluators are now actively used by model training teams\nto support model release decisions.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02827v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02827v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.457,
      "weak_supervision_score": 0.425,
      "diffusion_reasoning_score": 0.447,
      "distributed_training_score": 0.369,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on automating the evaluation of LLM-based evaluators for software engineering tasks, improving alignment scores for rankings. While it mentions alignment (a concept in RLHF), it does not involve training a reward model with human-ranked data or using reinforcement learning; instead, it automates validation without human feedback, making it only indirectly related.",
      "weak_supervision_justification": "The paper's REFINE framework programmatically generates datasets with varying quality levels and expected orderings, similar to weak supervision's use of noisy or high-level sources for labels. This automates evaluation without hand-labeled data, aligning with weak supervision principles, though it is applied specifically to benchmarking evaluators rather than model training.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for logical reasoning tasks. It focuses on generating and testing LLM evaluators for code artifacts, with techniques like DeQrease for quality degradation, but lacks any components related to multi-step diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces REFINE, an automated framework designed to validate and benchmark Large Language Model (LLM)-based evaluators for software engineering artifacts by generating hierarchies of code with progressively reduced quality and assessing how well evaluators rank them according to expected orderings. The methodology includes novel techniques like DeQrease for controlled degradation and Domain-Aware Error Injection for targeted perturbations, applied to tasks such as code translation, summarization, and generation for COBOL, resulting in improved evaluator configurations that achieved high alignment scores and were integrated into IBM's production workflows.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework, REFINE, along with novel techniques like DeQrease and Domain-Aware Error Injection, which significantly advance the state-of-the-art in automated validation of LLM evaluators by enabling fine-grained, scalable assessment for software engineering tasks.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and commercial applications in software engineering and AI, as evidenced by its integration into IBM's workflows and demonstrated improvements in evaluator reliability for critical tasks like COBOL code handling.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality, practical contribution with novel methods and real-world applications that are valuable for researchers and practitioners in AI and software engineering, making it essential for those working on LLM evaluation.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/01b881d5a3471d4c0270b07c77604b9ebdf5fc95",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 18,
      "average_h_index": 5.285714285714286,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Ora Nova Fandina",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2119306400"
        },
        {
          "name": "E. Farchi",
          "h_index": 18,
          "profile_url": "https://www.semanticscholar.org/author/2338905"
        },
        {
          "name": "Shmulik Froimovich",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2328008212"
        },
        {
          "name": "Rami Katan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2302803535"
        },
        {
          "name": "Alice Podolsky",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2374961653"
        },
        {
          "name": "Orna Raz",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/117442051"
        },
        {
          "name": "Avi Ziv",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2374150652"
        }
      ]
    },
    {
      "id": "2508.02829",
      "title": "Elucidating the Role of Feature Normalization in IJEPA",
      "authors": [
        "Adam Colton"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In the standard image joint embedding predictive architecture (IJEPA),\nfeatures at the output of the teacher encoder are layer normalized (LN) before\nserving as a distillation target for the student encoder and predictor. We\npropose that this feature normalization disrupts the natural energy hierarchy\nof visual tokens, where high-energy tokens (those with larger L2 norms) encode\nsemantically important image regions. LN forces all features to have identical\nL2 norms, effectively equalizing their energies and preventing the model from\nprioritizing semantically rich regions. We find that IJEPA models trained with\nfeature LN exhibit loss maps with significant checkerboard-like artifacts. We\npropose that feature LN be replaced with a DynTanh activation as the latter\nbetter preserves token energies and allows high-energy tokens to greater\ncontribute to the prediction loss. We show that IJEPA trained with feature\nDynTanh exhibits a longer-tailed loss distribution and fixes the checkerboard\nartifacts in the loss map. Our empirical results show that our simple\nmodification improves ImageNet linear probe accuracy from 38% to 42.7% for\nViT-Small and reduces RMSE by 0.08 on NYU Depth V2 monocular depth estimation.\nThese results suggest that preserving natural token energies is crucial for\neffective self-supervised visual representation learning.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02829v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02829v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.334,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02831",
      "title": "GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing",
      "authors": [
        "Mikołaj Zieliński",
        "Krzysztof Byrski",
        "Tomasz Szczepanik",
        "Przemysław Spurek"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently\ntransformed 3D scene representation and rendering. NeRF achieves high-fidelity\nnovel view synthesis by learning volumetric representations through neural\nnetworks, but its implicit encoding makes editing and physical interaction\nchallenging. In contrast, GS represents scenes as explicit collections of\nGaussian primitives, enabling real-time rendering, faster training, and more\nintuitive manipulation. This explicit structure has made GS particularly\nwell-suited for interactive editing and integration with physics-based\nsimulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural\nRadiance Fields Interactive Editing), a hybrid model that combines the\nphotorealistic rendering quality of NeRF with the editable and structured\nrepresentation of GS. Instead of using spherical harmonics for appearance\nmodeling, we assign each Gaussian a trainable feature embedding. These\nembeddings are used to condition a NeRF network based on the k nearest\nGaussians to each query point. To make this conditioning efficient, we\nintroduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest\nGaussian search based on a modified ray-tracing pipeline. We also integrate a\nmulti-resolution hash grid to initialize and update Gaussian features.\nTogether, these components enable real-time, locality-aware editing: as\nGaussian primitives are repositioned or modified, their interpolated influence\nis immediately reflected in the rendered output. By combining the strengths of\nimplicit and explicit representations, GENIE supports intuitive scene\nmanipulation, dynamic interaction, and compatibility with physical simulation,\nbridging the gap between geometry-based editing and neural rendering. The code\ncan be found under (https://github.com/MikolajZielinski/genie)",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02831v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02831v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.27,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.315,
      "datasets_score": 0.255,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02841",
      "title": "A Multi-Agent System for Complex Reasoning in Radiology Visual Question\n  Answering",
      "authors": [
        "Ziruo Yi",
        "Jinyu Liu",
        "Ting Xiao",
        "Mark V. Albert"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Radiology visual question answering (RVQA) provides precise answers to\nquestions about chest X-ray images, alleviating radiologists' workload. While\nrecent methods based on multimodal large language models (MLLMs) and\nretrieval-augmented generation (RAG) have shown promising progress in RVQA,\nthey still face challenges in factual accuracy, hallucinations, and cross-modal\nmisalignment. We introduce a multi-agent system (MAS) designed to support\ncomplex reasoning in RVQA, with specialized agents for context understanding,\nmultimodal reasoning, and answer validation. We evaluate our system on a\nchallenging RVQA set curated via model disagreement filtering, comprising\nconsistently hard cases across multiple MLLMs. Extensive experiments\ndemonstrate the superiority and effectiveness of our system over strong MLLM\nbaselines, with a case study illustrating its reliability and interpretability.\nThis work highlights the potential of multi-agent approaches to support\nexplainable and trustworthy clinical AI applications that require complex\nreasoning.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02841v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02841v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.315,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a multi-agent system for radiology visual question answering, focusing on agents for context understanding, multimodal reasoning, and answer validation. It does not mention or utilize diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. The reasoning is based on agent collaboration and RAG, not diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02844",
      "title": "RefineSeg: Dual Coarse-to-Fine Learning for Medical Image Segmentation",
      "authors": [
        "Anghong Du",
        "Nay Aung",
        "Theodoros N. Arvanitis",
        "Stefan K. Piechnik",
        "Joao A C Lima",
        "Steffen E. Petersen",
        "Le Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "High-quality pixel-level annotations of medical images are essential for\nsupervised segmentation tasks, but obtaining such annotations is costly and\nrequires medical expertise. To address this challenge, we propose a novel\ncoarse-to-fine segmentation framework that relies entirely on coarse-level\nannotations, encompassing both target and complementary drawings, despite their\ninherent noise. The framework works by introducing transition matrices in order\nto model the inaccurate and incomplete regions in the coarse annotations. By\njointly training on multiple sets of coarse annotations, it progressively\nrefines the network's outputs and infers the true segmentation distribution,\nachieving a robust approximation of precise labels through matrix-based\nmodeling. To validate the flexibility and effectiveness of the proposed method,\nwe demonstrate the results on two public cardiac imaging datasets, ACDC and\nMSCMRseg, and further evaluate its performance on the UK Biobank dataset.\nExperimental results indicate that our approach surpasses the state-of-the-art\nweakly supervised methods and closely matches the fully supervised approach.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02844v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02844v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.464,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.368,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a novel framework for medical image segmentation that uses coarse-level annotations, which are noisy and imprecise, to train models without relying on precise pixel-level labels. This directly aligns with weak supervision, as it involves training on high-level, programmatically refined labels (via transition matrices) to approximate true segmentation distributions. The paper explicitly discusses overcoming annotation noise in weakly supervised learning (WSL) and compares favorably to state-of-the-art WSL methods, making it a clear fit for the topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces RefineSeg, a novel coarse-to-fine segmentation framework for medical image segmentation that utilizes coarse-level annotations, such as target and complementary drawings, to overcome the challenges of obtaining high-quality pixel-level labels. By employing transition matrices to model and refine the inaccuracies in these annotations through joint training on multiple datasets, the method progressively approximates precise segmentation distributions, achieving performance that surpasses state-of-the-art weakly supervised approaches and nearly matches fully supervised results on cardiac imaging datasets like ACDC, MSCMRseg, and UK Biobank.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing transition matrices to refine coarse annotations in weakly supervised learning, combining existing ideas in a clever way to address annotation noise in medical imaging. However, it builds on established WSL techniques rather than introducing a entirely new problem or architecture.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to significantly influence future research and applications in medical image segmentation by reducing the need for expensive pixel-level annotations, making it applicable to large-scale datasets and commercial AI tools in healthcare. Its demonstrated performance gains over existing methods suggest it could be widely adopted and built upon in the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to weakly supervised learning in medical imaging, with practical implications for annotation efficiency, making it essential for researchers in computer vision and healthcare AI. While not groundbreaking across all domains, its advancements warrant attention from those working in related subfields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d29f7019cc37129bc6ad57a4e30e4bed9d99480e",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 66,
      "average_h_index": 11.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Anghong Du",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2344832703"
        },
        {
          "name": "Nay Aung",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/100834665"
        },
        {
          "name": "Theodoros N. Arvanitis",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2261271415"
        },
        {
          "name": "S. Piechnik",
          "h_index": 66,
          "profile_url": "https://www.semanticscholar.org/author/143754963"
        },
        {
          "name": "Joao A C Lima",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375171459"
        },
        {
          "name": "Steffen E. Petersen",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2293151019"
        },
        {
          "name": "Le Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374968561"
        }
      ]
    },
    {
      "id": "2508.02849",
      "title": "SecoustiCodec: Cross-Modal Aligned Streaming Single-Codecbook Speech\n  Codec",
      "authors": [
        "Chunyu Qiang",
        "Haoyu Wang",
        "Cheng Gong",
        "Tianrui Wang",
        "Ruibo Fu",
        "Tao Wang",
        "Ruilong Chen",
        "Jiangyan Yi",
        "Zhengqi Wen",
        "Chen Zhang",
        "Longbiao Wang",
        "Jianwu Dang",
        "Jianhua Tao"
      ],
      "categories": [
        "eess.AS (Audio and Speech Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.SD (Sound)"
      ],
      "abstract": "Speech codecs serve as a crucial bridge in unifying speech and text language\nmodels. Existing codec methods face several challenges in semantic encoding,\nsuch as residual paralinguistic information (e.g., timbre, emotion),\ninsufficient semantic completeness, limited reconstruction capability, and lack\nof support for streaming. To address these challenges, we propose\nSecoustiCodec, a cross-modal aligned low-bitrate streaming speech codec that\ndisentangles semantic and paralinguistic information in a single-codebook\nspace. To ensure semantic completeness and reconstruction fidelity,\nparalinguistic encoding is introduced to bridge the information gap between\nsemantic and acoustic encoding. A semantic-only efficient quantization method\nbased on VAE (Variational Autoencoder) and FSQ (Finite Scalar Quantization) is\nproposed. This approach alleviates the long-tail distribution problem of tokens\nwhile maintaining high codebook utilization. A semantic disentanglement method\nbased on contrastive learning is proposed, which aligns text and speech in a\njoint multimodal frame-level space, effectively removing paralinguistic\ninformation from semantic encoding. An acoustic-constrained multi-stage\noptimization strategy is proposed to ensure robust and stable convergence.\nFigure~\\ref{fig:pesq_kbps_below_2kbps} shows SecoustiCodec achieves SOTA\n(state-of-the-art) reconstruction quality (PESQ) of 1.77/2.58 at 0.27/1 kbps.\nThe code and model weights for SecoustiCodec will be open-sourced upon the\ncompletion of the peer-review process. We've open-sourced SecoustiCodec's demo,\ncode, and model weights.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02849v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02849v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.353,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02856",
      "title": "Secure mmWave Beamforming with Proactive-ISAC Defense Against\n  Beam-Stealing Attacks",
      "authors": [
        "Seyed Bagher Hashemi Natanzi",
        "Hossein Mohammadi",
        "Bo Tang",
        "Vuk Marojevic"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.NI (Networking and Internet Architecture)"
      ],
      "abstract": "Millimeter-wave (mmWave) communication systems face increasing susceptibility\nto advanced beam-stealing attacks, posing a significant physical layer security\nthreat. This paper introduces a novel framework employing an advanced Deep\nReinforcement Learning (DRL) agent for proactive and adaptive defense against\nthese sophisticated attacks. A key innovation is leveraging Integrated Sensing\nand Communications (ISAC) capabilities for active, intelligent threat\nassessment. The DRL agent, built on a Proximal Policy Optimization (PPO)\nalgorithm, dynamically controls ISAC probing actions to investigate suspicious\nactivities. We introduce an intensive curriculum learning strategy that\nguarantees the agent experiences successful detection during training to\novercome the complex exploration challenges inherent to such a\nsecurity-critical task. Consequently, the agent learns a robust and adaptive\npolicy that intelligently balances security and communication performance.\nNumerical results demonstrate that our framework achieves a mean attacker\ndetection rate of 92.8% while maintaining an average user SINR of over 13 dB.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02856v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02856v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.305,
      "distributed_training_score": 0.352,
      "datasets_score": 0.276,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02858",
      "title": "MIDAR: Mimicking LiDAR Detection for Traffic Applications with a\n  Lightweight Plug-and-Play Model",
      "authors": [
        "Tianheng Zhu",
        "Yiheng Feng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "As autonomous driving (AD) technology advances, increasing research has\nfocused on leveraging cooperative perception (CP) data collected from multiple\nAVs to enhance traffic applications. Due to the impracticality of large-scale\nreal-world AV deployments, simulation has become the primary approach in most\nstudies. While game-engine-based simulators like CARLA generate high-fidelity\nraw sensor data (e.g., LiDAR point clouds) which can be used to produce\nrealistic detection outputs, they face scalability challenges in multi-AV\nscenarios. In contrast, microscopic traffic simulators such as SUMO scale\nefficiently but lack perception modeling capabilities. To bridge this gap, we\npropose MIDAR, a LiDAR detection mimicking model that approximates realistic\nLiDAR detections using vehicle-level features readily available from\nmicroscopic traffic simulators. Specifically, MIDAR predicts true positives\n(TPs) and false negatives (FNs) from ideal LiDAR detection results based on the\nspatial layouts and dimensions of surrounding vehicles. A Refined Multi-hop\nLine-of-Sight (RM-LoS) graph is constructed to encode the occlusion\nrelationships among vehicles, upon which MIDAR employs a GRU-enhanced APPNP\narchitecture to propagate features from the ego AV and occluding vehicles to\nthe prediction target. MIDAR achieves an AUC of 0.909 in approximating the\ndetection results generated by CenterPoint, a mainstream 3D LiDAR detection\nmodel, on the nuScenes AD dataset. Two CP-based traffic applications further\nvalidate the necessity of such realistic detection modeling, particularly for\ntasks requiring accurate individual vehicle observations (e.g., position,\nspeed, lane index). As demonstrated in the applications, MIDAR can be\nseamlessly integrated into traffic simulators and trajectory datasets and will\nbe open-sourced upon publication.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02858v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02858v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.37,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02871",
      "title": "Evaluation and Analysis of Deep Neural Transformers and Convolutional\n  Neural Networks on Modern Remote Sensing Datasets",
      "authors": [
        "J. Alex Hurt",
        "Trevor M. Bajkowski",
        "Grant J. Scott",
        "Curt H. Davis"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In 2012, AlexNet established deep convolutional neural networks (DCNNs) as\nthe state-of-the-art in CV, as these networks soon led in visual tasks for many\ndomains, including remote sensing. With the publication of Visual Transformers,\nwe are witnessing the second modern leap in computational vision, and as such,\nit is imperative to understand how various transformer-based neural networks\nperform on satellite imagery. While transformers have shown high levels of\nperformance in natural language processing and CV applications, they have yet\nto be compared on a large scale to modern remote sensing data. In this paper,\nwe explore the use of transformer-based neural networks for object detection in\nhigh-resolution electro-optical satellite imagery, demonstrating\nstate-of-the-art performance on a variety of publicly available benchmark data\nsets. We compare eleven distinct bounding-box detection and localization\nalgorithms in this study, of which seven were published since 2020, and all\neleven since 2015. The performance of five transformer-based architectures is\ncompared with six convolutional networks on three state-of-the-art opensource\nhigh-resolution remote sensing imagery datasets ranging in size and complexity.\nFollowing the training and evaluation of thirty-three deep neural models, we\nthen discuss and analyze model performance across various feature extraction\nmethodologies and detection algorithms.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02871v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02871v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.412,
      "datasets_score": 0.46,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on evaluating and comparing transformer-based and convolutional neural networks on remote sensing datasets, including discussions on computational demands for training transformers. However, it does not address distributed training techniques, parallel computing algorithms, or systems for partitioning data/computation across nodes. The mention of required compute power is incidental and not a core contribution.",
      "datasets_justification": "The paper evaluates and analyzes the performance of neural networks on three existing high-resolution remote sensing datasets, describing their characteristics (e.g., classes, sizes, complexities) and using them for benchmarking. While it does not involve creating new datasets or detailing curation methodologies, its focus on benchmark evaluation and performance analysis aligns with dataset evaluation in ML applications.",
      "llm_score_status": "completed",
      "summary": "This paper evaluates and compares the performance of five transformer-based neural networks and six convolutional neural networks on object detection tasks using three high-resolution remote sensing imagery datasets, focusing on bounding-box detection algorithms. By training and analyzing 33 models across eleven distinct architectures—seven published since 2020—the study demonstrates that transformers achieve state-of-the-art results in satellite imagery, provides insights into their effectiveness compared to CNNs, and makes pretrained weights available for future applications in the remote sensing domain.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying and comparing transformer-based architectures to remote sensing datasets, which is a clever adaptation of existing techniques to a less-explored domain, though it does not introduce a entirely new problem or architecture. This contributes by filling a gap in large-scale comparisons but remains incremental in the broader context of neural network advancements.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in remote sensing and computer vision by providing benchmark comparisons and publicly available pretrained weights, potentially aiding in transfer learning for similar applications. However, its impact is confined to specific subfields like high-resolution imagery analysis, limiting broader applicability.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable insights and resources for researchers in computer vision and remote sensing, making it a strong contribution worth reviewing for those working on object detection in satellite imagery. While not essential for all AI practitioners, it provides practical comparisons and pretrained models that could enhance related work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8ac874b161033b88a076222d55ec81890e786bdd",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 5,
      "average_h_index": 2.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "J. A. Hurt",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/51234862"
        },
        {
          "name": "Trevor M. Bajkowski",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/1729342343"
        },
        {
          "name": "Grant J. Scott",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2240146072"
        },
        {
          "name": "Curt H. Davis",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2240327723"
        }
      ]
    },
    {
      "id": "2508.02874",
      "title": "Beyond Least Squares: Robust Regression Transformer (R2T)",
      "authors": [
        "Roman Gutierrez",
        "Tony Kai Tang",
        "Isabel Gutierrez"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Robust regression techniques rely on least-squares optimization, which works\nwell for Gaussian noise but fails in the presence of asymmetric structured\nnoise. We propose a hybrid neural-symbolic architecture where a transformer\nencoder processes numerical sequences, a compression NN predicts symbolic\nparameters, and a fixed symbolic equation reconstructs the original sequence.\nUsing synthetic data, the training objective is to recover the original\nsequence after adding asymmetric structured noise, effectively learning a\nsymbolic fit guided by neural parameter estimation. Our model achieves a median\nregression MSE of 6e-6 to 3.5e-5 on synthetic wearable data, which is a 10-300\ntimes improvement when compared with ordinary least squares fit and robust\nregression techniques such as Huber loss or SoftL1.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02874v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02874v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.334,
      "datasets_score": 0.276,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02879",
      "title": "CauKer: classification time series foundation models can be pretrained\n  on synthetic data only",
      "authors": [
        "Shifeng Xie",
        "Vasilii Feofanov",
        "Marius Alonso",
        "Ambroise Odonnat",
        "Jianfeng Zhang",
        "Themis Palpanas",
        "Ievgen Redko"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Time series foundation models (TSFMs) have recently gained significant\nattention due to their strong zero-shot capabilities and widespread real-world\napplications. Such models typically require a computationally costly\npretraining on large-scale, carefully curated collections of real-world\nsequences. To allow for a sample-efficient pretraining of TSFMs, we propose\nCauKer, a novel algorithm designed to generate diverse, causally coherent\nsynthetic time series with realistic trends, seasonality, and nonlinear\ninteractions. CauKer combines Gaussian Process (GP) kernel composition with\nStructural Causal Models (SCM) to produce data for sample-efficient pretraining\nof state-of-the-art classification TSFMs having different architectures and\nfollowing different pretraining approaches. Additionally, our experiments\nreveal that CauKer-generated datasets exhibit clear scaling laws for both\ndataset size (10K to 10M samples) and model capacity (1M to 783M parameters),\nunlike real-world datasets, which display irregular scaling behavior.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02879v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02879v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.378,
      "datasets_score": 0.406,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper proposes generating synthetic time series data for pretraining models, which involves programmatically creating data and labels, similar to weak supervision's use of noisy or imprecise sources. However, it focuses more on data generation for TSFMs rather than directly applying weak supervision techniques for labeling existing data, making it only tangentially related.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is creating and analyzing synthetic datasets for time series classification, including methodologies for generation, benchmarking against real-world datasets, and evaluating scaling laws, which directly aligns with research on dataset creation, analysis, and evaluation for ML applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces CauKer, a novel algorithm that leverages Gaussian Process kernel composition and Structural Causal Models to generate diverse, causally coherent synthetic time series data, aiming to enable sample-efficient pretraining of time series foundation models (TSFMs) for classification tasks without relying on real-world data. Key findings demonstrate that TSFMs pretrained on CauKer-generated datasets exhibit clear scaling laws with respect to dataset size and model capacity, achieve state-of-the-art classification performance, and surpass models trained on real-world benchmarks, highlighting the advantages of synthetic data in terms of efficiency, scalability, and generalization.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new algorithm, CauKer, which combines Gaussian Process kernels and Structural Causal Models in a novel way to generate synthetic time series data, significantly advancing the state-of-the-art in sample-efficient pretraining for TSFMs. This represents a meaningful innovation by addressing limitations in existing methods that rely on large real-world datasets.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and commercial applications in time series analysis by enabling scalable and efficient pretraining of TSFMs, which could lead to broader adoption in fields like healthcare and industrial monitoring. Its demonstration of state-of-the-art performance with synthetic data may shift paradigms in model training practices.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a high-quality contribution with practical implications for researchers in machine learning and AI, providing empirical evidence and a new method that could enhance TSFM development. While essential for those working in time series classification, it may not be critical for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6debf25dcbe997da5216be9f3aef5ad99fca8e70",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 53,
      "average_h_index": 11.428571428571429,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Shifeng Xie",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375348971"
        },
        {
          "name": "Vasilii Feofanov",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/66383219"
        },
        {
          "name": "Marius Alonso",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2346835215"
        },
        {
          "name": "Ambroise Odonnat",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2261363007"
        },
        {
          "name": "Jianfeng Zhang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2279763896"
        },
        {
          "name": "Themis Palpanas",
          "h_index": 53,
          "profile_url": "https://www.semanticscholar.org/author/1725167"
        },
        {
          "name": "I. Redko",
          "h_index": 14,
          "profile_url": "https://www.semanticscholar.org/author/145898069"
        }
      ]
    },
    {
      "id": "2508.02880",
      "title": "Evaluation of 3D Counterfactual Brain MRI Generation",
      "authors": [
        "Pengwei Sun",
        "Wei Peng",
        "Lun Yu Li",
        "Yixin Wang",
        "Kilian M. Pohl"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Counterfactual generation offers a principled framework for simulating\nhypothetical changes in medical imaging, with potential applications in\nunderstanding disease mechanisms and generating physiologically plausible data.\nHowever, generating realistic structural 3D brain MRIs that respect anatomical\nand causal constraints remains challenging due to data scarcity, structural\ncomplexity, and the lack of standardized evaluation protocols. In this work, we\nconvert six generative models into 3D counterfactual approaches by\nincorporating an anatomy-guided framework based on a causal graph, in which\nregional brain volumes serve as direct conditioning inputs. Each model is\nevaluated with respect to composition, reversibility, realism, effectiveness\nand minimality on T1-weighted brain MRIs (T1w MRIs) from the Alzheimer's\nDisease Neuroimaging Initiative (ADNI). In addition, we test the\ngeneralizability of each model with respect to T1w MRIs of the National\nConsortium on Alcohol and Neurodevelopment in Adolescence (NCANDA). Our results\nindicate that anatomically grounded conditioning successfully modifies the\ntargeted anatomical regions; however, it exhibits limitations in preserving\nnon-targeted structures. Beyond laying the groundwork for more interpretable\nand clinically relevant generative modeling of brain MRIs, this benchmark\nhighlights the need for novel architectures that more accurately capture\nanatomical interdependencies.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02880v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02880v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.296,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on adapting generative models for 3D counterfactual brain MRI generation and evaluation, emphasizing anatomy-guided frameworks and causal graphs for medical imaging synthesis. It does not involve diffusion models adapted for iterative refinement in solving complex logical tasks or treating a Chain-of-Thought as a single entity for reasoning. While diffusion models might be among the six evaluated generative models, the paper's contributions are centered on image generation for medical applications, not multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02889",
      "title": "REFLECT: Rectified Flows for Efficient Brain Anomaly Correction\n  Transport",
      "authors": [
        "Farzad Beizaee",
        "Sina Hajimiri",
        "Ismail Ben Ayed",
        "Gregory Lodygensky",
        "Christian Desrosiers",
        "Jose Dolz"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Unsupervised anomaly detection (UAD) in brain imaging is crucial for\nidentifying pathologies without the need for labeled data. However, accurately\nlocalizing anomalies remains challenging due to the intricate structure of\nbrain anatomy and the scarcity of abnormal examples. In this work, we introduce\nREFLECT, a novel framework that leverages rectified flows to establish a\ndirect, linear trajectory for correcting abnormal MR images toward a normal\ndistribution. By learning a straight, one-step correction transport map, our\nmethod efficiently corrects brain anomalies and can precisely localize\nanomalies by detecting discrepancies between anomalous input and corrected\ncounterpart. In contrast to the diffusion-based UAD models, which require\niterative stochastic sampling, rectified flows provide a direct transport map,\nenabling single-step inference. Extensive experiments on popular UAD brain\nsegmentation benchmarks demonstrate that REFLECT significantly outperforms\nstate-of-the-art unsupervised anomaly detection methods. The code is available\nat https://github.com/farzad-bz/REFLECT.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02889v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02889v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.294,
      "datasets_score": 0.277,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using rectified flows for unsupervised anomaly detection in brain imaging, specifically for correcting abnormal MR images. While it mentions diffusion models in the related work section as a comparison, it does not adapt diffusion processes for multi-step logical reasoning or Chain-of-Thought tasks. The paper's contributions are centered on image correction and anomaly localization, not on solving complex logical tasks, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02890",
      "title": "VisuCraft: Enhancing Large Vision-Language Models for Complex\n  Visual-Guided Creative Content Generation via Structured Information\n  Extraction",
      "authors": [
        "Rongxin Jiang",
        "Robert Long",
        "Chenghao Gu",
        "Mingrui Yan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "This paper introduces VisuCraft, a novel framework designed to significantly\nenhance the capabilities of Large Vision-Language Models (LVLMs) in complex\nvisual-guided creative content generation. Existing LVLMs often exhibit\nlimitations in maintaining high visual fidelity, genuine creativity, and\nprecise adherence to nuanced user instructions when generating long-form texts.\nVisuCraft addresses these challenges by integrating a multimodal structured\ninformation extractor (E) and a dynamic prompt generation module (G). The\nextractor distills fine-grained visual attributes from input images into a\nrich, structured representation, which the dynamic prompt module then combines\nwith user instructions to create highly optimized prompts for underlying LVLMs\n(e.g., LLaVA, InstructBLIP). Evaluated on the self-constructed\nImageStoryGen-500K dataset using VisuGen Metrics (Visual Grounding, Creativity,\nand Instruction Adherence), VisuCraft consistently outperforms baseline LVLMs\nacross tasks like story generation and poetry composition. Our results\ndemonstrate remarkable improvements, particularly in creativity and instruction\nadherence, validating VisuCraft's effectiveness in producing imaginative,\nvisually grounded, and user-aligned long-form creative text. This work unlocks\nnew potential for LVLMs in sophisticated creative AI applications.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02890v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02890v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.473,
      "distributed_training_score": 0.292,
      "datasets_score": 0.38,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework called VisuCraft that enhances Large Vision-Language Models (LVLMs) by extracting structured visual information and generating optimized prompts for creative content generation. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. There is no adaptation of diffusion techniques for solving logical tasks, making the paper unrelated to this area.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02900",
      "title": "Seemingly Simple Planning Problems are Computationally Challenging: The\n  Countdown Game",
      "authors": [
        "Michael Katz",
        "Harsha Kokel",
        "Sarath Sreedharan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "There is a broad consensus that the inability to form long-term plans is one\nof the key limitations of current foundational models and agents. However, the\nexisting planning benchmarks remain woefully inadequate to truly measure their\nplanning capabilities. Most existing benchmarks either focus on loosely defined\ntasks like travel planning or end up leveraging existing domains and problems\nfrom international planning competitions. While the former tasks are hard to\nformalize and verify, the latter were specifically designed to test and\nchallenge the weaknesses of existing automated planners. To address these\nshortcomings, we propose a procedure for creating a planning benchmark centered\naround the game called Countdown, where a player is expected to form a target\nnumber from a list of input numbers through arithmetic operations. We discuss\nhow this problem meets many of the desiderata associated with an ideal\nbenchmark for planning capabilities evaluation. Specifically, the domain allows\nfor an intuitive, natural language description for each problem instance, it is\ncomputationally challenging (NP-complete), and the instance space is rich\nenough that we do not have to worry about memorization. We perform an extensive\ntheoretical analysis, establishing the computational complexity result and\ndemonstrate the advantage of our instance generation procedure over public\nbenchmarks. We evaluate a variety of existing LLM-assisted planning methods on\ninstances generated using our procedure. Our results show that, unlike other\ndomains like 24 Game (a special case of Countdown), our proposed dynamic\nbenchmark remains extremely challenging for existing LLM-based approaches.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02900v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02900v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.272,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.275,
      "datasets_score": 0.273,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is to propose and analyze a planning benchmark based on the Countdown game, including its computational complexity and evaluation of LLM-assisted planning methods. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02903",
      "title": "RDDPM: Robust Denoising Diffusion Probabilistic Model for Unsupervised\n  Anomaly Segmentation",
      "authors": [
        "Mehrdad Moradi",
        "Kamran Paynabar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Recent advancements in diffusion models have demonstrated significant success\nin unsupervised anomaly segmentation. For anomaly segmentation, these models\nare first trained on normal data; then, an anomalous image is noised to an\nintermediate step, and the normal image is reconstructed through backward\ndiffusion. Unlike traditional statistical methods, diffusion models do not rely\non specific assumptions about the data or target anomalies, making them\nversatile for use across different domains. However, diffusion models typically\nassume access to normal data for training, limiting their applicability in\nrealistic settings. In this paper, we propose novel robust denoising diffusion\nmodels for scenarios where only contaminated (i.e., a mix of normal and\nanomalous) unlabeled data is available. By casting maximum likelihood\nestimation of the data as a nonlinear regression problem, we reinterpret the\ndenoising diffusion probabilistic model through a regression lens. Using robust\nregression, we derive a robust version of denoising diffusion probabilistic\nmodels. Our novel framework offers flexibility in constructing various robust\ndiffusion models. Our experiments show that our approach outperforms current\nstate of the art diffusion models, for unsupervised anomaly segmentation when\nonly contaminated data is available. Our method outperforms existing\ndiffusion-based approaches, achieving up to 8.08\\% higher AUROC and 10.37\\%\nhigher AUPRC on MVTec datasets. The implementation code is available at:\nhttps://github.com/mehrdadmoradi124/RDDPM",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02903v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02903v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.402,
      "diffusion_reasoning_score": 0.481,
      "distributed_training_score": 0.346,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper addresses training on contaminated data, which includes a mix of normal and anomalous samples, introducing noise similar to weak supervision's imprecise data. However, it focuses on unsupervised anomaly segmentation without programmatically generating labels, making it only loosely connected to weak supervision's core idea of deriving labels from high-level sources.",
      "diffusion_reasoning_justification": "The paper proposes a robust diffusion model for anomaly segmentation in images, emphasizing reconstruction and denoising, but does not involve multi-step logical reasoning, chain-of-thought processes, or iterative refinement for complex tasks. It lacks any components for logical or reasoning applications.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02905",
      "title": "How Would It Sound? Material-Controlled Multimodal Acoustic Profile\n  Generation for Indoor Scenes",
      "authors": [
        "Mahnoor Fatima Saad",
        "Ziad Al-Halah"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.SD (Sound)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "How would the sound in a studio change with a carpeted floor and acoustic\ntiles on the walls? We introduce the task of material-controlled acoustic\nprofile generation, where, given an indoor scene with specific audio-visual\ncharacteristics, the goal is to generate a target acoustic profile based on a\nuser-defined material configuration at inference time. We address this task\nwith a novel encoder-decoder approach that encodes the scene's key properties\nfrom an audio-visual observation and generates the target Room Impulse Response\n(RIR) conditioned on the material specifications provided by the user. Our\nmodel enables the generation of diverse RIRs based on various material\nconfigurations defined dynamically at inference time. To support this task, we\ncreate a new benchmark, the Acoustic Wonderland Dataset, designed for\ndeveloping and evaluating material-aware RIR prediction methods under diverse\nand challenging settings. Our results demonstrate that the proposed model\neffectively encodes material information and generates high-fidelity RIRs,\noutperforming several baselines and state-of-the-art methods.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02905v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02905v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.276,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02912",
      "title": "Engineered over Emergent Communication in MARL for Scalable and\n  Sample-Efficient Cooperative Task Allocation in a Partially Observable Grid",
      "authors": [
        "Brennen A. Hill",
        "Mant Koh En Wei",
        "Thangavel Jishnuanandh"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "We compare the efficacy of learned versus engineered communication strategies\nin a cooperative multi-agent reinforcement learning (MARL) environment. For the\nlearned approach, we introduce Learned Direct Communication (LDC), where agents\ngenerate messages and actions concurrently via a neural network. Our engineered\napproach, Intention Communication, employs an Imagined Trajectory Generation\nModule (ITGM) and a Message Generation Network (MGN) to formulate messages\nbased on predicted future states. Both strategies are evaluated on their\nsuccess rates in cooperative tasks under fully and partially observable\nconditions. Our findings indicate that while emergent communication is viable,\nthe engineered approach demonstrates superior performance and scalability,\nparticularly as environmental complexity increases.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02912v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02912v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.351,
      "datasets_score": 0.269,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02913",
      "title": "Enhancing Japanese Large Language Models with Reasoning Vectors",
      "authors": [
        "Carolina Minami Oguchi",
        "Leo Wei",
        "Koyo Kobayashi",
        "Hsin-Tai Wu",
        "Dipak Ghosal"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Post-training methods have improved the performance and enhanced the\nreasoning capability for mainstream large language models (LLMs), but the same\nis challenging for Japanese LLMs to achieve due to the amount of resources\nrequired. Inspired by task vectors that extract the change of weights before\nand after training, specifically for a certain task, we obtain reasoning\nvectors from reasoning LLMs and apply them to Japanese LLMs to boost their\nperformance. While the resources available present a challenge to improve\nJapanese LLMs, we present a simple and effective way to obtain high improvement\nand hope to inspire for other languages.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02913v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02913v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.531,
      "distributed_training_score": 0.347,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves extracting reasoning vectors from trained LLMs and applying them to Japanese LLMs for performance enhancement, focusing on transfer learning techniques. It does not mention or utilize diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning or Chain-of-Thought tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02917",
      "title": "Following Route Instructions using Large Vision-Language Models: A\n  Comparison between Low-level and Panoramic Action Spaces",
      "authors": [
        "Vebjørn Haug Kåsene",
        "Pierre Lison"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Vision-and-Language Navigation (VLN) refers to the task of enabling\nautonomous robots to navigate unfamiliar environments by following natural\nlanguage instructions. While recent Large Vision-Language Models (LVLMs) have\nshown promise in this task, most current VLM systems rely on models\nspecifically designed and optimized for navigation, leaving the potential of\noff-the-shelf LVLMs underexplored. Furthermore, while older VLN approaches used\nlow-level action spaces with egocentric views and atomic actions (such as \"turn\nleft\" or \"move forward\"), newer models tend to favor panoramic action spaces\nwith discrete navigable viewpoints. This paper investigates (1) whether\noff-the-shelf LVLMs (fine-tuned without architectural modifications or\nsimulator-based training) can effectively support VLN tasks and (2) whether\nsuch models can support both low-level and panoramic action paradigms. To this\nend, we fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the\nRoom-to-Room (R2R) dataset and evaluate its empirical performance across both\nlow-level and panoramic action spaces. The best resulting model achieves a 41%\nsuccess rate on the R2R test set, demonstrating that while off-the-shelf LVLMs\ncan learn to perform Vision-and-Language Navigation, they still lag behind\nmodels specifically designed for this task.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02917v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02917v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.342,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on fine-tuning an off-the-shelf LVLM on the R2R dataset for Vision-and-Language Navigation, using standard fine-tuning methods without any mention of human feedback, reward models, or reinforcement learning techniques. RLHF specifically requires training with human-ranked data and a reward model, which is not present here.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates LVLMs for navigation tasks, involving prompt-based inputs and action outputs, but does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning corrections. There is no evidence of adapting diffusion for Chain-of-Thought or similar reasoning mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02921",
      "title": "PentestJudge: Judging Agent Behavior Against Operational Requirements",
      "authors": [
        "Shane Caldwell",
        "Max Harley",
        "Michael Kouremetis",
        "Vincent Abruzzo",
        "Will Pearce"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "We introduce PentestJudge, a system for evaluating the operations of\npenetration testing agents. PentestJudge is a large language model\n(LLM)-as-judge with access to tools that allow it to consume arbitrary\ntrajectories of agent states and tool call history to determine whether a\nsecurity agent's actions meet certain operating criteria that would be\nimpractical to evaluate programmatically. We develop rubrics that use a tree\nstructure to hierarchically collapse the penetration testing task for a\nparticular environment into smaller, simpler, and more manageable sub-tasks and\ncriteria until each leaf node represents simple yes-or-no criteria for\nPentestJudge to evaluate. Task nodes are broken down into different categories\nrelated to operational objectives, operational security, and tradecraft.\nLLM-as-judge scores are compared to human domain experts as a ground-truth\nreference, allowing us to compare their relative performance with standard\nbinary classification metrics, such as F1 scores. We evaluate several frontier\nand open-source models acting as judge agents, with the best model reaching an\nF1 score of 0.83. We find models that are better at tool-use perform more\nclosely to human experts. By stratifying the F1 scores by requirement type, we\nfind even models with similar overall scores struggle with different types of\nquestions, suggesting certain models may be better judges of particular\noperating criteria. We find that weaker and cheaper models can judge the\ntrajectories of pentests performed by stronger and more expensive models,\nsuggesting verification may be easier than generation for the penetration\ntesting task. We share this methodology to facilitate future research in\nunderstanding the ability of judges to holistically and scalably evaluate the\nprocess quality of AI-based information security agents so that they may be\nconfidently used in sensitive production environments.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02921v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02921v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.426,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.342,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates LLM judges against human expert judgments for assessing agent behavior in penetration testing, which involves human feedback as a benchmark. However, it does not implement RLHF, as it lacks the training of a reward model or fine-tuning via reinforcement learning based on human-ranked data. The connection is indirect, focusing on evaluation rather than alignment through RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02923",
      "title": "How Diffusion Prior Landscapes Shape the Posterior in Blind\n  Deconvolution",
      "authors": [
        "Minh-Hai Nguyen",
        "Edouard Pauwels",
        "Pierre Weiss"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The Maximum A Posteriori (MAP) estimation is a widely used framework in blind\ndeconvolution to recover sharp images from blurred observations. The estimated\nimage and blur filter are defined as the maximizer of the posterior\ndistribution. However, when paired with sparsity-promoting image priors, MAP\nestimation has been shown to favors blurry solutions, limiting its\neffectiveness. In this paper, we revisit this result using diffusion-based\npriors, a class of models that capture realistic image distributions. Through\nan empirical examination of the prior's likelihood landscape, we uncover two\nkey properties: first, blurry images tend to have higher likelihoods; second,\nthe landscape contains numerous local minimizers that correspond to natural\nimages. Building on these insights, we provide a theoretical analysis of the\nblind deblurring posterior. This reveals that the MAP estimator tends to\nproduce sharp filters (close to the Dirac delta function) and blurry solutions.\nHowever local minimizers of the posterior, which can be obtained with gradient\ndescent, correspond to realistic, natural images, effectively solving the blind\ndeconvolution problem. Our findings suggest that overcoming MAP's limitations\nrequires good local initialization to local minima in the posterior landscape.\nWe validate our analysis with numerical experiments, demonstrating the\npractical implications of our insights for designing improved priors and\noptimization techniques.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02923v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02923v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.509,
      "distributed_training_score": 0.294,
      "datasets_score": 0.258,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion-based priors for blind image deconvolution, analyzing how these priors affect the posterior landscape in image processing tasks. While it involves diffusion models, it does not adapt their iterative refinement for solving complex logical tasks or holistic Chain-of-Thought reasoning. Instead, it applies diffusion models to image restoration, making the connection indirect and based solely on the shared use of diffusion technology.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02926",
      "title": "GrandJury: A Collaborative Machine Learning Model Evaluation Protocol\n  for Dynamic Quality Rubrics",
      "authors": [
        "Arthur Cho"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Generative Machine Learning models have become central to modern systems,\npowering applications in creative writing, summarization, multi-hop reasoning,\nand context-aware dialogue. These models underpin large-scale AI assistants,\nworkflow automation, and autonomous decision-making. In such domains,\nacceptable response is rarely absolute or static, but plural and highly\ncontext-dependent. Yet standard evaluation regimes still rely on static,\nbenchmark-style tests, incentivizing optimization toward leaderboard scores\nrather than alignment with dynamic user needs or evolving realities. GrandJury\nintroduces a formal evaluation protocol combining time-decayed aggregation,\ncomplete traceability, with the support of dynamic, transparent task rubric\nattribution, and multi-rater human judgment. Together, these elements enable\npluralistic, accountable evaluation that captures evolving consensus and\nsurfaces disagreement. We provide an open-source implementation (grandjury PyPI\npackage) and a public collection of Large Language Model (LLM) inference\noutputs to illustrate the need and method. GrandJury provides a new paradigm\nfor AI practitioners when evaluating machine learning outputs without absolute\nground truth.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02926v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02926v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.482,
      "weak_supervision_score": 0.436,
      "diffusion_reasoning_score": 0.428,
      "distributed_training_score": 0.369,
      "datasets_score": 0.438,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on an evaluation protocol for machine learning outputs using human judgment and dynamic rubrics, but it does not involve training a reward model on human-ranked data or fine-tuning a main model via reinforcement learning. There is no mention of RLHF elements.",
      "weak_supervision_justification": "The paper introduces a protocol for evaluating models with human judgment and dynamic rubrics, but it does not discuss programmatically generating training labels from noisy sources or training models with weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper is centered on an evaluation protocol for generative models and does not involve adapting diffusion processes for multi-step logical reasoning or any iterative refinement of reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper provides a public collection of LLM inference outputs and an open-source implementation to illustrate their evaluation protocol, which involves benchmarking and evaluating AI outputs. This aligns with dataset introduction and benchmark evaluation, though the primary focus is on the protocol rather than comprehensive dataset creation or analysis.",
      "llm_score_status": "completed",
      "summary": "GrandJury proposes a novel evaluation protocol for generative machine learning models that addresses the limitations of static benchmarks by integrating time-decayed aggregation, complete traceability, dynamic task rubrics, and multi-rater human judgment to enable pluralistic and accountable assessments of model outputs in evolving contexts. The methodology emphasizes capturing dynamic user needs and evolving consensus without relying on absolute ground truth, with the paper providing an open-source implementation and examples using Large Language Models to illustrate its application and necessity.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new evaluation protocol that combines innovative elements like time-decayed aggregation and dynamic rubrics to address the shortcomings of existing static methods, significantly advancing the state-of-the-art in machine learning model assessment.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields of AI evaluation, particularly for generative models, as it offers a practical framework for handling dynamic and context-dependent assessments. However, its influence may be limited to specific applications rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality contribution that introduces an important new tool for evaluating machine learning models in dynamic environments, making it valuable for AI researchers and practitioners focused on model assessment. While not essential for all, it offers insights that could enhance evaluation practices in relevant areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a28cc806cd3182c4a1989832101ff8344569f130",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Arthur Cho",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374964264"
        }
      ]
    },
    {
      "id": "2508.02927",
      "title": "Infrared Object Detection with Ultra Small ConvNets: Is ImageNet\n  Pretraining Still Useful?",
      "authors": [
        "Srikanth Muralidharan",
        "Heitor R. Medeiros",
        "Masih Aminbeidokhti",
        "Eric Granger",
        "Marco Pedersoli"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Many real-world applications require recognition models that are robust to\ndifferent operational conditions and modalities, but at the same time run on\nsmall embedded devices, with limited hardware. While for normal size models,\npre-training is known to be very beneficial in accuracy and robustness, for\nsmall models, that can be employed for embedded and edge devices, its effect is\nnot clear. In this work, we investigate the effect of ImageNet pretraining on\nincreasingly small backbone architectures (ultra-small models, with $<$1M\nparameters) with respect to robustness in downstream object detection tasks in\nthe infrared visual modality. Using scaling laws derived from standard object\nrecognition architectures, we construct two ultra-small backbone families and\nsystematically study their performance. Our experiments on three different\ndatasets reveal that while ImageNet pre-training is still useful, beyond a\ncertain capacity threshold, it offers diminishing returns in terms of\nout-of-distribution detection robustness. Therefore, we advise practitioners to\nstill use pre-training and, when possible avoid too small models as while they\nmight work well for in-domain problems, they are brittle when working\nconditions are different.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02927v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02927v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.413,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper primarily investigates the effects of ImageNet pretraining on ultra-small convolutional networks for infrared object detection, focusing on model performance, robustness, and scaling laws. It does not address distributed training, parallel computing, multi-node machine learning, or any strategies for partitioning data or computation across processors or nodes. Therefore, there is no connection to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02929",
      "title": "Realizing Scaling Laws in Recommender Systems: A Foundation-Expert\n  Paradigm for Hyperscale Model Deployment",
      "authors": [
        "Dai Li",
        "Kevin Course",
        "Wei Li",
        "Hongwei Li",
        "Jie Hua",
        "Yiqi Chen",
        "Zhao Zhu",
        "Rui Jian",
        "Xuan Cao",
        "Bi Xue",
        "Yu Shi",
        "Jing Qian",
        "Kai Ren",
        "Matt Ma",
        "Qunshu Zhang",
        "Rui Li"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "While scaling laws promise significant performance gains for recommender\nsystems, efficiently deploying hyperscale models remains a major unsolved\nchallenge. In contrast to fields where FMs are already widely adopted such as\nnatural language processing and computer vision, progress in recommender\nsystems is hindered by unique challenges including the need to learn from\nonline streaming data under shifting data distributions, the need to adapt to\ndifferent recommendation surfaces with a wide diversity in their downstream\ntasks and their input distributions, and stringent latency and computational\nconstraints. To bridge this gap, we propose to leverage the Foundation-Expert\nParadigm: a framework designed for the development and deployment of hyperscale\nrecommendation FMs. In our approach, a central FM is trained on lifelong,\ncross-surface, multi-modal user data to learn generalizable knowledge. This\nknowledge is then efficiently transferred to various lightweight,\nsurface-specific \"expert\" models via target-aware embeddings, allowing them to\nadapt to local data distributions and optimization goals with minimal overhead.\nTo meet our training, inference and development needs, we built HyperCast, a\nproduction-grade infrastructure system that re-engineers training, serving,\nlogging and iteration to power this decoupled paradigm. Our approach is now\ndeployed at Meta serving tens of billions of user requests daily, demonstrating\nonline metric improvements over our previous one-stage production system while\nimproving developer velocity and maintaining infrastructure efficiency. To the\nbest of our knowledge, this work represents the first successful deployment of\na Foundation-Expert paradigm at this scale, offering a proven,\ncompute-efficient, and developer-friendly blueprint to realize the promise of\nscaling laws in recommender systems.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02929v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02929v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.435,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.472,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on the Foundation-Expert Paradigm for recommender systems, involving training foundation models and transferring knowledge via embeddings, without any mention of reinforcement learning, human feedback, reward models, or aligning models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution includes HyperCast, a production-grade infrastructure for decoupled, multi-tier model training, serving, and deployment, which directly addresses distributed training by enabling efficient scaling across multiple nodes and GPUs to handle hyperscale recommender models.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper proposes the Foundation-Expert Paradigm to address challenges in deploying hyperscale recommender systems, where a central foundation model (FM) is trained on vast, multi-modal user data to capture generalizable knowledge, which is then transferred via target-aware embeddings to lightweight, surface-specific expert models for efficient adaptation to local data and tasks. The methodology, supported by the HyperCast infrastructure, enables handling of online streaming data, achieves high knowledge transfer ratios (0.64-1.0), demonstrates generalization across recommendation surfaces, and results in significant online metric improvements at Meta, serving tens of billions of daily requests while enhancing developer velocity and maintaining computational efficiency.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper introduces a clever combination of foundation models and expert adapters tailored for recommender systems, addressing unique challenges like streaming data, but it builds on existing concepts in AI rather than introducing a entirely new problem or technique.",
      "impact_score": "High",
      "impact_justification": "The work's successful deployment at Meta, improving user experiences and efficiency at scale, suggests it could broadly influence future research and commercial applications in recommender systems and AI infrastructure.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides practical, high-quality insights into scaling recommender systems in real-world settings, making it valuable for AI researchers and practitioners focused on deployment challenges.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5508663a2b75b710a8a85422563a0c93b533ce81",
      "total_authors": 16,
      "authors_found": 16,
      "highest_h_index": 3,
      "average_h_index": 0.375,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Dai Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374991964"
        },
        {
          "name": "Kevin Course",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374958190"
        },
        {
          "name": "Wei Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375031899"
        },
        {
          "name": "Hongwei Li",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2282899554"
        },
        {
          "name": "Jie Hua",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374963775"
        },
        {
          "name": "Yiqi Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374992760"
        },
        {
          "name": "Zhao Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374999157"
        },
        {
          "name": "Rui Jian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374967133"
        },
        {
          "name": "Xuan Cao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2374945269"
        },
        {
          "name": "Bi Xue",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374967254"
        },
        {
          "name": "Yu Shi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2375097466"
        },
        {
          "name": "Jing Qian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375328343"
        },
        {
          "name": "Kai Ren",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374963797"
        },
        {
          "name": "Matt Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376147263"
        },
        {
          "name": "Qunshu Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376401945"
        },
        {
          "name": "Rui Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2375059681"
        }
      ]
    },
    {
      "id": "2508.02931",
      "title": "Can LLMs Generate High-Quality Task-Specific Conversations?",
      "authors": [
        "Shengqi Li",
        "Amarnath Gupta"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper introduces a parameterization framework for controlling\nconversation quality in large language models. We explore nine key parameters\nacross six dimensions that enable precise specification of dialogue properties.\nThrough experiments with state-of-the-art LLMs, we demonstrate that\nparameter-based control produces statistically significant differences in\ngenerated conversation properties. Our approach addresses challenges in\nconversation generation, including topic coherence, knowledge progression,\ncharacter consistency, and control granularity. The framework provides a\nstandardized method for conversation quality control with applications in\neducation, therapy, customer service, and entertainment. Future work will focus\non implementing additional parameters through architectural modifications and\ndeveloping benchmark datasets for evaluation.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02931v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02931v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.434,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.341,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a parameterization framework for controlling conversation quality in LLMs through parameters and prompt conditioning, without any mention of training models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses generating and controlling conversations with LLMs but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for Chain-of-Thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02936",
      "title": "AQUAH: Automatic Quantification and Unified Agent in Hydrology",
      "authors": [
        "Songkun Yan",
        "Zhi Li",
        "Siyu Zhu",
        "Yixin Wen",
        "Mofan Zhang",
        "Mengye Chen",
        "Jie Cao",
        "Yang Hong"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce AQUAH, the first end-to-end language-based agent designed\nspecifically for hydrologic modeling. Starting from a simple natural-language\nprompt (e.g., 'simulate floods for the Little Bighorn basin from 2020 to\n2022'), AQUAH autonomously retrieves the required terrain, forcing, and gauge\ndata; configures a hydrologic model; runs the simulation; and generates a\nself-contained PDF report. The workflow is driven by vision-enabled large\nlanguage models, which interpret maps and rasters on the fly and steer key\ndecisions such as outlet selection, parameter initialization, and uncertainty\ncommentary. Initial experiments across a range of U.S. basins show that AQUAH\ncan complete cold-start simulations and produce analyst-ready documentation\nwithout manual intervention. The results are judged by hydrologists as clear,\ntransparent, and physically plausible. While further calibration and validation\nare still needed for operational deployment, these early outcomes highlight the\npromise of LLM-centered, vision-grounded agents to streamline complex\nenvironmental modeling and lower the barrier between Earth observation data,\nphysics-based tools, and decision makers.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02936v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02936v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.323,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on AQUAH, an end-to-end agent using vision-enabled large language models for hydrologic modeling, including data retrieval, simulation, and report generation. It does not involve diffusion-based models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. The core contributions are in automation via LLMs and VLMs, with no mention of treating Chain-of-Thought as a holistically corrected entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02944",
      "title": "X-Actor: Emotional and Expressive Long-Range Portrait Acting from Audio",
      "authors": [
        "Chenxu Zhang",
        "Zenan Li",
        "Hongyi Xu",
        "You Xie",
        "Xiaochen Zhao",
        "Tianpei Gu",
        "Guoxian Song",
        "Xin Chen",
        "Chao Liang",
        "Jianwen Jiang",
        "Linjie Luo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present X-Actor, a novel audio-driven portrait animation framework that\ngenerates lifelike, emotionally expressive talking head videos from a single\nreference image and an input audio clip. Unlike prior methods that emphasize\nlip synchronization and short-range visual fidelity in constrained speaking\nscenarios, X-Actor enables actor-quality, long-form portrait performance\ncapturing nuanced, dynamically evolving emotions that flow coherently with the\nrhythm and content of speech. Central to our approach is a two-stage decoupled\ngeneration pipeline: an audio-conditioned autoregressive diffusion model that\npredicts expressive yet identity-agnostic facial motion latent tokens within a\nlong temporal context window, followed by a diffusion-based video synthesis\nmodule that translates these motions into high-fidelity video animations. By\noperating in a compact facial motion latent space decoupled from visual and\nidentity cues, our autoregressive diffusion model effectively captures\nlong-range correlations between audio and facial dynamics through a\ndiffusion-forcing training paradigm, enabling infinite-length emotionally-rich\nmotion prediction without error accumulation. Extensive experiments demonstrate\nthat X-Actor produces compelling, cinematic-style performances that go beyond\nstandard talking head animations and achieves state-of-the-art results in\nlong-range, audio-driven emotional portrait acting.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02944v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02944v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.297,
      "diffusion_reasoning_score": 0.476,
      "distributed_training_score": 0.328,
      "datasets_score": 0.267,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes diffusion models for iterative refinement in generating facial motion latents and video synthesis from audio, which shares the core mechanism of diffusion processes. However, it applies this to visual animation and motion prediction, not to solving complex logical tasks or refining a 'Chain-of-Thought' for reasoning. Thus, while there is a methodological overlap, the paper does not involve multi-step logical reasoning as specified in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02945",
      "title": "LLM-based IR-system for Bank Supervisors",
      "authors": [
        "Ilias Aarab"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "stat.AP (Applications)",
        "stat.CO (Computation)"
      ],
      "abstract": "Bank supervisors face the complex task of ensuring that new measures are\nconsistently aligned with historical precedents. To address this challenge, we\nintroduce a novel Information Retrieval (IR) System tailored to assist\nsupervisors in drafting both consistent and effective measures. This system\ningests findings from on-site investigations. It then retrieves the most\nrelevant historical findings and their associated measures from a comprehensive\ndatabase, providing a solid basis for supervisors to write well-informed\nmeasures for new findings. Utilizing a blend of lexical, semantic, and Capital\nRequirements Regulation (CRR) fuzzy set matching techniques, the IR system\nensures the retrieval of findings that closely align with current cases. The\nperformance of this system, particularly in scenarios with partially labeled\ndata, is validated through a Monte Carlo methodology, showcasing its robustness\nand accuracy. Enhanced by a Transformer-based Denoising AutoEncoder for\nfine-tuning, the final model achieves a Mean Average Precision (MAP@100) of\n0.83 and a Mean Reciprocal Rank (MRR@100) of 0.92. These scores surpass those\nof both standalone lexical models such as BM25 and semantic BERT-like models.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02945v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02945v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.439,
      "weak_supervision_score": 0.412,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.328,
      "datasets_score": 0.389,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on an Information Retrieval system using lexical, semantic matching, and a Transformer-based Denoising AutoEncoder for fine-tuning, with no mention of human feedback, reward models, or reinforcement learning techniques. Thus, it does not align with RLHF.",
      "weak_supervision_justification": "The paper mentions validation in scenarios with partially labeled data using Monte Carlo methodology, which hints at handling noisy or incomplete labels, a concept related to weak supervision. However, it does not describe programmatically generating labels or rely on weak supervision as a core method, focusing instead on matching techniques and fine-tuning.",
      "diffusion_reasoning_justification": "The paper describes an IR system with lexical, semantic, and fuzzy matching, along with a Denoising AutoEncoder, but there is no reference to diffusion models, iterative refinement for logical reasoning, or multi-step chain-of-thought processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02947",
      "title": "AeroSafe: Mobile Indoor Air Purification using Aerosol Residence Time\n  Analysis and Robotic Cough Emulator Testbed",
      "authors": [
        "M Tanjid Hasan Tonmoy",
        "Rahath Malladi",
        "Kaustubh Singh",
        "Forsad Al Hossain",
        "Rajesh Gupta",
        "Andrés E. Tejada-Martínez",
        "Tauhidur Rahman"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Indoor air quality plays an essential role in the safety and well-being of\noccupants, especially in the context of airborne diseases. This paper\nintroduces AeroSafe, a novel approach aimed at enhancing the efficacy of indoor\nair purification systems through a robotic cough emulator testbed and a\ndigital-twins-based aerosol residence time analysis. Current portable air\nfilters often overlook the concentrations of respiratory aerosols generated by\ncoughs, posing a risk, particularly in high-exposure environments like\nhealthcare facilities and public spaces. To address this gap, we present a\nrobotic dual-agent physical emulator comprising a maneuverable mannequin\nsimulating cough events and a portable air purifier autonomously responding to\naerosols. The generated data from this emulator trains a digital twins model,\ncombining a physics-based compartment model with a machine learning approach,\nusing Long Short-Term Memory (LSTM) networks and graph convolution layers.\nExperimental results demonstrate the model's ability to predict aerosol\nconcentration dynamics with a mean residence time prediction error within 35\nseconds. The proposed system's real-time intervention strategies outperform\nstatic air filter placement, showcasing its potential in mitigating airborne\npathogen risks.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02947v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02947v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.322,
      "distributed_training_score": 0.335,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02951",
      "title": "MedBLINK: Probing Basic Perception in Multimodal Language Models for\n  Medicine",
      "authors": [
        "Mahtab Bigverdi",
        "Wisdom Ikezogwo",
        "Kevin Zhang",
        "Hyewon Jeong",
        "Mingyu Lu",
        "Sungjae Cho",
        "Linda Shapiro",
        "Ranjay Krishna"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multimodal language models (MLMs) show promise for clinical decision support\nand diagnostic reasoning, raising the prospect of end-to-end automated medical\nimage interpretation. However, clinicians are highly selective in adopting AI\ntools; a model that makes errors on seemingly simple perception tasks such as\ndetermining image orientation or identifying whether a CT scan is\ncontrast-enhance are unlikely to be adopted for clinical tasks. We introduce\nMedblink, a benchmark designed to probe these models for such perceptual\nabilities. Medblink spans eight clinically meaningful tasks across multiple\nimaging modalities and anatomical regions, totaling 1,429 multiple-choice\nquestions over 1,605 images. We evaluate 19 state-of-the-art MLMs, including\ngeneral purpose (GPT4o, Claude 3.5 Sonnet) and domain specific (Med Flamingo,\nLLaVA Med, RadFM) models. While human annotators achieve 96.4% accuracy, the\nbest-performing model reaches only 65%. These results show that current MLMs\nfrequently fail at routine perceptual checks, suggesting the need to strengthen\ntheir visual grounding to support clinical adoption. Data is available on our\nproject page.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02951v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02951v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.338,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the introduction of MedBLINK, a benchmark for evaluating basic perceptual abilities in multimodal language models for medical image tasks, such as image orientation and contrast enhancement. It focuses on probing MLMs for visual grounding and perceptual accuracy, with evaluations of various models like GPT-4o and Med-Flamingo. However, the paper does not mention or involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning or chain-of-thought tasks. Therefore, there is no connection to the topic of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02956",
      "title": "Autonomous Inorganic Materials Discovery via Multi-Agent Physics-Aware\n  Scientific Reasoning",
      "authors": [
        "Alireza Ghafarollahi",
        "Markus J. Buehler"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Conventional machine learning approaches accelerate inorganic materials\ndesign via accurate property prediction and targeted material generation, yet\nthey operate as single-shot models limited by the latent knowledge baked into\ntheir training data. A central challenge lies in creating an intelligent system\ncapable of autonomously executing the full inorganic materials discovery cycle,\nfrom ideation and planning to experimentation and iterative refinement. We\nintroduce SparksMatter, a multi-agent AI model for automated inorganic\nmaterials design that addresses user queries by generating ideas, designing and\nexecuting experimental workflows, continuously evaluating and refining results,\nand ultimately proposing candidate materials that meet the target objectives.\nSparksMatter also critiques and improves its own responses, identifies research\ngaps and limitations, and suggests rigorous follow-up validation steps,\nincluding DFT calculations and experimental synthesis and characterization,\nembedded in a well-structured final report. The model's performance is\nevaluated across case studies in thermoelectrics, semiconductors, and\nperovskite oxides materials design. The results demonstrate the capacity of\nSparksMatter to generate novel stable inorganic structures that target the\nuser's needs. Benchmarking against frontier models reveals that SparksMatter\nconsistently achieves higher scores in relevance, novelty, and scientific\nrigor, with a significant improvement in novelty across multiple real-world\ndesign tasks as assessed by a blinded evaluator. These results demonstrate\nSparksMatter's unique capacity to generate chemically valid, physically\nmeaningful, and creative inorganic materials hypotheses beyond existing\nmaterials knowledge.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02956v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02956v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.469,
      "distributed_training_score": 0.393,
      "datasets_score": 0.392,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a multi-agent AI system for materials discovery that includes self-improvement through iterative refinement and learning from results, but it does not involve training with human-ranked data, a reward model, or reinforcement learning techniques to align with human preferences. Thus, it does not align with RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes an iterative process for reasoning and refinement in materials design using multi-agent LLMs, but it does not mention or adapt diffusion models for multi-step logical reasoning or holistic chain-of-thought correction. There is no evidence of diffusion-based mechanisms in the system.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02957",
      "title": "AMD-Mamba: A Phenotype-Aware Multi-Modal Framework for Robust AMD\n  Prognosis",
      "authors": [
        "Puzhen Wu",
        "Mingquan Lin",
        "Qingyu Chen",
        "Emily Y. Chew",
        "Zhiyong Lu",
        "Yifan Peng",
        "Hexin Dong"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Age-related macular degeneration (AMD) is a leading cause of irreversible\nvision loss, making effective prognosis crucial for timely intervention. In\nthis work, we propose AMD-Mamba, a novel multi-modal framework for AMD\nprognosis, and further develop a new AMD biomarker. This framework integrates\ncolor fundus images with genetic variants and socio-demographic variables. At\nits core, AMD-Mamba introduces an innovative metric learning strategy that\nleverages AMD severity scale score as prior knowledge. This strategy allows the\nmodel to learn richer feature representations by aligning learned features with\nclinical phenotypes, thereby improving the capability of conventional prognosis\nmethods in capturing disease progression patterns. In addition, unlike existing\nmodels that use traditional CNN backbones and focus primarily on local\ninformation, such as the presence of drusen, AMD-Mamba applies Vision Mamba and\nsimultaneously fuses local and long-range global information, such as vascular\nchanges. Furthermore, we enhance prediction performance through multi-scale\nfusion, combining image information with clinical variables at different\nresolutions. We evaluate AMD-Mamba on the AREDS dataset, which includes 45,818\ncolor fundus photographs, 52 genetic variants, and 3 socio-demographic\nvariables from 2,741 subjects. Our experimental results demonstrate that our\nproposed biomarker is one of the most significant biomarkers for the\nprogression of AMD. Notably, combining this biomarker with other existing\nvariables yields promising improvements in detecting high-risk AMD patients at\nearly stages. These findings highlight the potential of our multi-modal\nframework to facilitate more precise and proactive management of AMD.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02957v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02957v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.326,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02959",
      "title": "Polymath: A Self-Optimizing Agent with Dynamic Hierarchical Workflow",
      "authors": [
        "Chia-Tung Ho",
        "Jing Gong",
        "Xufeng Yao",
        "Yunsheng Bai",
        "Abhishek B Akkur",
        "Haoxing Ren"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large language models (LLMs) excel at solving complex tasks by executing\nagentic workflows composed of detailed instructions and structured operations.\nYet, building general-purpose agents by manually embedding foundation models\ninto agentic systems such as Chain-of-Thought, Self-Reflection, and ReACT\nthrough text interfaces limits scalability and efficiency. Recently, many\nresearchers have sought to automate the generation and optimization of these\nworkflows through code-based representations. However, existing methods often\nrely on labeled datasets to train and optimize workflows, making them\nineffective and inflexible for solving real-world, dynamic problems where\nlabeled data is unavailable. To address this challenge, we introduce Polymath,\na self-optimizing agent with dynamic hierarchical workflow that leverages the\nflexibility of task flow graphs and the expressiveness of code-represented\nworkflows to solve a wide range of real-world, dynamic problems. The proposed\noptimization methodology integrates multi-grid-inspired graph optimization with\na self-reflection-guided evolutionary algorithm to refine workflows without\nlabeled data. Experimental results on six benchmark datasets across coding,\nmath, and multi-turn QA tasks show that Polymath achieves 8.1% average\nimprovement over state-of-the-art baselines.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02959v2",
      "pdf_url": "http://arxiv.org/pdf/2508.02959v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.469,
      "distributed_training_score": 0.394,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a self-optimizing agent using automated workflows and LLM-based feedback for optimization, without involving human-ranked data or a reward model trained on human preferences. There is no mention of reinforcement learning processes that align models with human feedback, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes optimization via multi-grid-inspired graphs and evolutionary algorithms for workflows, but it does not involve diffusion models, iterative refinement of a 'Chain-of-Thought' as a single entity, or any multi-step logical reasoning adapted from diffusion processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.02961",
      "title": "Defend LLMs Through Self-Consciousness",
      "authors": [
        "Boshi Huang",
        "Fabio Nonato de Paula"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "This paper introduces a novel self-consciousness defense mechanism for Large\nLanguage Models (LLMs) to combat prompt injection attacks. Unlike traditional\napproaches that rely on external classifiers, our method leverages the LLM's\ninherent reasoning capabilities to perform self-protection. We propose a\nframework that incorporates Meta-Cognitive and Arbitration Modules, enabling\nLLMs to evaluate and regulate their own outputs autonomously. Our approach is\nevaluated on seven state-of-the-art LLMs using two datasets: AdvBench and\nPrompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate\nsignificant improvements in defense success rates across models and datasets,\nwith some achieving perfect and near-perfect defense in Enhanced Mode. We also\nanalyze the trade-off between defense success rate improvement and\ncomputational overhead. This self-consciousness method offers a lightweight,\ncost-effective solution for enhancing LLM ethics, particularly beneficial for\nGenAI use cases across various platforms.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.02961v1",
      "pdf_url": "http://arxiv.org/pdf/2508.02961v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.48,
      "weak_supervision_score": 0.418,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.346,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a self-consciousness defense mechanism for LLMs against prompt injection attacks, using internal modules for self-protection. It does not involve human feedback, reward models, or reinforcement learning techniques for aligning models with preferences.",
      "weak_supervision_justification": "The paper describes a defense framework for existing LLMs, emphasizing self-evaluation and regulation, but it does not address training models with programmatically generated labels or noisy data sources.",
      "diffusion_reasoning_justification": "The paper leverages LLMs' reasoning for self-protection through modules like Meta-Cognitive and Arbitration, but it does not include a diffusion model, iterative refinement process, or multi-step logical reasoning as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.03751",
      "title": "Modular Transformer Architecture for Precision Agriculture Imaging",
      "authors": [
        "Brian Gopalan",
        "Nathalia Nascimento",
        "Vishal Monga"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper addresses the critical need for efficient and accurate weed\nsegmentation from drone video in precision agriculture. A quality-aware modular\ndeep-learning framework is proposed that addresses common image degradation by\nanalyzing quality conditions-such as blur and noise-and routing inputs through\nspecialized pre-processing and transformer models optimized for each\ndegradation type. The system first analyzes drone images for noise and blur\nusing Mean Absolute Deviation and the Laplacian. Data is then dynamically\nrouted to one of three vision transformer models: a baseline for clean images,\na modified transformer with Fisher Vector encoding for noise reduction, or\nanother with an unrolled Lucy-Richardson decoder to correct blur. This novel\nrouting strategy allows the system to outperform existing CNN-based methods in\nboth segmentation quality and computational efficiency, demonstrating a\nsignificant advancement in deep-learning applications for agriculture.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.03751v2",
      "pdf_url": "http://arxiv.org/pdf/2508.03751v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.372,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.03752",
      "title": "M$^3$HL: Mutual Mask Mix with High-Low Level Feature Consistency for\n  Semi-Supervised Medical Image Segmentation",
      "authors": [
        "Yajun Liu",
        "Zenghui Zhang",
        "Jiang Yue",
        "Weiwei Guo",
        "Dongying Li"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Data augmentation methods inspired by CutMix have demonstrated significant\npotential in recent semi-supervised medical image segmentation tasks. However,\nthese approaches often apply CutMix operations in a rigid and inflexible\nmanner, while paying insufficient attention to feature-level consistency\nconstraints. In this paper, we propose a novel method called Mutual Mask Mix\nwith High-Low level feature consistency (M$^3$HL) to address the aforementioned\nchallenges, which consists of two key components: 1) M$^3$: An enhanced data\naugmentation operation inspired by the masking strategy from Masked Image\nModeling (MIM), which advances conventional CutMix through dynamically\nadjustable masks to generate spatially complementary image pairs for\ncollaborative training, thereby enabling effective information fusion between\nlabeled and unlabeled images. 2) HL: A hierarchical consistency regularization\nframework that enforces high-level and low-level feature consistency between\nunlabeled and mixed images, enabling the model to better capture discriminative\nfeature representations.Our method achieves state-of-the-art performance on\nwidely adopted medical image segmentation benchmarks including the ACDC and LA\ndatasets. Source code is available at https://github.com/PHPJava666/M3HL",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.03752v1",
      "pdf_url": "http://arxiv.org/pdf/2508.03752v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.436,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.326,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves semi-supervised medical image segmentation, where it uses techniques like dynamic mask mixing and consistency regularization to leverage unlabeled data, effectively generating training signals (e.g., through pseudo-labels and feature constraints) from noisy or imprecise sources. This aligns closely with weak supervision, as it programmatically creates labels or constraints to train models with limited annotated data, reducing the need for perfect hand-labeling.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces M³HL, a novel method for semi-supervised medical image segmentation that enhances data augmentation through a dynamic mutual mask mix (M³) strategy, inspired by Masked Image Modeling, to create spatially complementary image pairs from labeled and unlabeled data. It also incorporates a hierarchical high-low level feature consistency (HL) framework to enforce consistency between mixed and unlabeled images at both low-level (geometric) and high-level (semantic) features, resulting in improved feature representation and state-of-the-art performance on the ACDC and LA datasets.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining dynamic masking from Masked Image Modeling with hierarchical feature consistency, offering a clever enhancement to existing data augmentation techniques for semi-supervised learning. While it advances the field, it builds on known methods like CutMix rather than introducing an entirely new problem.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of semi-supervised medical image segmentation due to its state-of-the-art results on benchmark datasets. However, its influence may be limited to specific applications in medical imaging rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution with innovative techniques that achieve superior performance, making it essential for researchers focused on semi-supervised learning in medical image segmentation. It provides insights that could guide future work, though it may not be critical for those outside this niche.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a6b0dd67425607cb9799f944c77fd7d4e52e156f",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 12,
      "average_h_index": 2.8,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Yajun Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2289494169"
        },
        {
          "name": "Zenghui Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375066914"
        },
        {
          "name": "Jiang Yue",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2289355068"
        },
        {
          "name": "Weiwei Guo",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2153297817"
        },
        {
          "name": "Dongying Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375075853"
        }
      ]
    },
    {
      "id": "2508.03753",
      "title": "Classification non supervis{é}es d'acquisitions hyperspectrales\n  cod{é}es : quelles v{é}rit{é}s terrain ?",
      "authors": [
        "Trung-tin Dinh",
        "Hervé Carfantan",
        "Antoine Monmayrant",
        "Simon Lacroix"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We propose an unsupervised classification method using a limited number of\ncoded acquisitions from a DD-CASSI hyperspectral imager. Based on a simple\nmodel of intra-class spectral variability, this approach allow to identify\nclasses and estimate reference spectra, despite data compression by a factor of\nten. Here, we highlight the limitations of the ground truths commonly used to\nevaluate this type of method: lack of a clear definition of the notion of\nclass, high intra-class variability, and even classification errors. Using the\nPavia University scene, we show that with simple assumptions, it is possible to\ndetect regions that are spectrally more coherent, highlighting the need to\nrethink the evaluation of classification methods, particularly in unsupervised\nscenarios.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.03753v1",
      "pdf_url": "http://arxiv.org/pdf/2508.03753v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.231,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.252,
      "distributed_training_score": 0.242,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.03754",
      "title": "Generating Synthetic Invoices via Layout-Preserving Content Replacement",
      "authors": [
        "Bevin V",
        "Ananthakrishnan P V",
        "Ragesh KR",
        "Sanjay M",
        "Vineeth S",
        "Bibin Wilson"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The performance of machine learning models for automated invoice processing\nis critically dependent on large-scale, diverse datasets. However, the\nacquisition of such datasets is often constrained by privacy regulations and\nthe high cost of manual annotation. To address this, we present a novel\npipeline for generating high-fidelity, synthetic invoice documents and their\ncorresponding structured data. Our method first utilizes Optical Character\nRecognition (OCR) to extract the text content and precise spatial layout from a\nsource invoice. Select data fields are then replaced with contextually\nrealistic, synthetic content generated by a large language model (LLM).\nFinally, we employ an inpainting technique to erase the original text from the\nimage and render the new, synthetic text in its place, preserving the exact\nlayout and font characteristics. This process yields a pair of outputs: a\nvisually realistic new invoice image and a perfectly aligned structured data\nfile (JSON) reflecting the synthetic content. Our approach provides a scalable\nand automated solution to amplify small, private datasets, enabling the\ncreation of large, varied corpora for training more robust and accurate\ndocument intelligence models.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.03754v1",
      "pdf_url": "http://arxiv.org/pdf/2508.03754v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.305,
      "datasets_score": 0.391,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a pipeline that programmatically generates synthetic invoice data and paired structured labels (e.g., JSON) using techniques like OCR and LLMs, which aligns directly with weak supervision by creating large-scale training labels from automated, high-level sources rather than manual annotation. This enables training ML models without relying on perfectly hand-labeled data, addressing the core challenges of weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper presents a novel pipeline to generate synthetic invoice documents, addressing the challenges of data scarcity in automated invoice processing due to privacy regulations and high annotation costs. The methodology involves using Optical Character Recognition (OCR) to extract text and layout from source invoices, replacing selected data fields with contextually realistic content generated by a large language model (LLM), and applying inpainting techniques to render the new text while preserving the original layout and font characteristics, resulting in high-fidelity synthetic images paired with structured JSON data for training robust machine learning models.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper offers a notable improvement by combining existing technologies like OCR, LLMs, and inpainting into a cohesive pipeline for synthetic invoice generation, solving a known problem in a new, practical way. While not introducing entirely new concepts, it provides a clever integration that advances data augmentation techniques in document processing.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of computer vision and document intelligence, as it offers a scalable solution for generating privacy-compliant datasets. However, its influence may be limited to specific applications in invoice processing rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to synthetic data generation for ML applications, making it essential for researchers in computer vision and document processing to be aware of its methods and implications. It provides practical insights that could enhance ongoing work in the field without being groundbreaking enough to be a must-read.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/05a7a39ef34343f38749aadf3122881876f05830",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "V. Bevin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375046187"
        },
        {
          "name": "V. AnanthakrishnanP",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375047008"
        },
        {
          "name": "KR Ragesh",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375046654"
        },
        {
          "name": "M. Sanjay",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375048235"
        },
        {
          "name": "S. Vineeth",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375046360"
        },
        {
          "name": "Bibin Wilson",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375059310"
        }
      ]
    },
    {
      "id": "2508.03755",
      "title": "LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional\n  Data Completion",
      "authors": [
        "Wenwu Gong",
        "Lili Yang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.NA (Numerical Analysis)",
        "math.NA (Numerical Analysis)"
      ],
      "abstract": "Multi-dimensional data completion is a critical problem in computational\nsciences, particularly in domains such as computer vision, signal processing,\nand scientific computing. Existing methods typically leverage either global\nlow-rank approximations or local smoothness regularization, but each suffers\nfrom notable limitations: low-rank methods are computationally expensive and\nmay disrupt intrinsic data structures, while smoothness-based approaches often\nrequire extensive manual parameter tuning and exhibit poor generalization. In\nthis paper, we propose a novel Low-Rank Tucker Representation (LRTuckerRep)\nmodel that unifies global and local prior modeling within a Tucker\ndecomposition. Specifically, LRTuckerRep encodes low rankness through a\nself-adaptive weighted nuclear norm on the factor matrices and a sparse Tucker\ncore, while capturing smoothness via a parameter-free Laplacian-based\nregularization on the factor spaces. To efficiently solve the resulting\nnonconvex optimization problem, we develop two iterative algorithms with\nprovable convergence guarantees. Extensive experiments on multi-dimensional\nimage inpainting and traffic data imputation demonstrate that LRTuckerRep\nachieves superior completion accuracy and robustness under high missing rates\ncompared to baselines.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.03755v1",
      "pdf_url": "http://arxiv.org/pdf/2508.03755v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.354,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.03758",
      "title": "FUTransUNet-GradCAM: A Hybrid Transformer-U-Net with Self-Attention and\n  Explainable Visualizations for Foot Ulcer Segmentation",
      "authors": [
        "Akwasi Asare",
        "Mary Sagoe",
        "Justice Williams Asare"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Automated segmentation of diabetic foot ulcers (DFUs) plays a critical role\nin clinical diagnosis, therapeutic planning, and longitudinal wound monitoring.\nHowever, this task remains challenging due to the heterogeneous appearance,\nirregular morphology, and complex backgrounds associated with ulcer regions in\nclinical photographs. Traditional convolutional neural networks (CNNs), such as\nU-Net, provide strong localization capabilities but struggle to model\nlong-range spatial dependencies due to their inherently limited receptive\nfields. To address this, we propose FUTransUNet, a hybrid architecture that\nintegrates the global attention mechanism of Vision Transformers (ViTs) into\nthe U-Net framework. This combination allows the model to extract global\ncontextual features while maintaining fine-grained spatial resolution through\nskip connections and an effective decoding pathway. We trained and validated\nFUTransUNet on the public Foot Ulcer Segmentation Challenge (FUSeg) dataset.\nFUTransUNet achieved a training Dice Coefficient of 0.8679, an IoU of 0.7672,\nand a training loss of 0.0053. On the validation set, the model achieved a Dice\nCoefficient of 0.8751, an IoU of 0.7780, and a validation loss of 0.009045. To\nensure clinical transparency, we employed Grad-CAM visualizations, which\nhighlighted model focus areas during prediction. These quantitative outcomes\nclearly demonstrate that our hybrid approach successfully integrates global and\nlocal feature extraction paradigms, thereby offering a highly robust, accurate,\nexplainable, and interpretable solution and clinically translatable solution\nfor automated foot ulcer analysis. The approach offers a reliable,\nhigh-fidelity solution for DFU segmentation, with implications for improving\nreal-world wound assessment and patient care.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.03758v2",
      "pdf_url": "http://arxiv.org/pdf/2508.03758v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.28,
      "weak_supervision_score": 0.274,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.29,
      "datasets_score": 0.276,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.03759",
      "title": "Assessing the Impact of Image Super Resolution on White Blood Cell\n  Classification Accuracy",
      "authors": [
        "Tatwadarshi P. Nagarhalli",
        "Shruti S. Pawar",
        "Soham A. Dahanukar",
        "Uday Aswalekar",
        "Ashwini M. Save",
        "Sanket D. Patil"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Accurately classifying white blood cells from microscopic images is essential\nto identify several illnesses and conditions in medical diagnostics. Many deep\nlearning technologies are being employed to quickly and automatically classify\nimages. However, most of the time, the resolution of these microscopic pictures\nis quite low, which might make it difficult to classify them correctly. Some\npicture improvement techniques, such as image super-resolution, are being\nutilized to improve the resolution of the photos to get around this issue. The\nsuggested study uses large image dimension upscaling to investigate how\npicture-enhancing approaches affect classification performance. The study\nspecifically looks at how deep learning models may be able to understand more\ncomplex visual information by capturing subtler morphological changes when\nimage resolution is increased using cutting-edge techniques. The model may\nlearn from standard and augmented data since the improved images are\nincorporated into the training process. This dual method seeks to comprehend\nthe impact of image resolution on model performance and enhance classification\naccuracy. A well-known model for picture categorization is used to conduct\nextensive testing and thoroughly evaluate the effectiveness of this approach.\nThis research intends to create more efficient image identification algorithms\ncustomized to a particular dataset of white blood cells by understanding the\ntrade-offs between ordinary and enhanced images.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.03759v1",
      "pdf_url": "http://arxiv.org/pdf/2508.03759v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.268,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.277,
      "distributed_training_score": 0.292,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.03760",
      "title": "FlashCommunication V2: Bit Splitting and Spike Reserving for Any Bit\n  Communication",
      "authors": [
        "Qingyuan Li",
        "Bo Zhang",
        "Hui Kang",
        "Tianhao Xu",
        "Yulei Qian",
        "Yuchen Xie",
        "Lin Ma"
      ],
      "categories": [
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Nowadays, communication bottlenecks have emerged as a critical challenge in\nthe distributed training and deployment of large language models (LLMs). This\npaper introduces FlashCommunication V2, a novel communication paradigm enabling\nefficient cross-GPU transmission at arbitrary bit widths. Its core innovations\nlie in the proposed bit splitting and spike reserving techniques, which address\nthe challenges of low-bit quantization. Bit splitting decomposes irregular bit\nwidths into basic units, ensuring compatibility with hardware capabilities and\nthus enabling transmission at any bit width. Spike reserving, on the other\nhand, retains numerical outliers (i.e., minima and maxima) as floating-point\nnumbers, which shrinks the dynamic numerical range and pushes the quantization\nlimits to 2-bit with acceptable losses. FlashCommunication V2 significantly\nenhances the flexibility and resource utilization of communication systems.\nThrough meticulous software-hardware co-design, it delivers robust performance\nand reduced overhead across both NVLink-based and PCIe-based architectures,\nachieving a maximum 3.2$\\times$ speedup in AllReduce and 2$\\times$ in All2All\ncommunication.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.03760v1",
      "pdf_url": "http://arxiv.org/pdf/2508.03760v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.524,
      "datasets_score": 0.256,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution, FlashCommunication V2, directly addresses communication bottlenecks in distributed training of large language models (LLMs), such as AllReduce and All2All operations, which are essential for parallel computing across multiple GPUs or nodes. It introduces innovations like bit splitting and spike reserving to optimize quantization and transmission, thereby accelerating model training by improving efficiency in multi-node setups. This aligns closely with the topic's focus on algorithms and systems for partitioning computation and communication in distributed environments.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "FlashCommunication V2 is a novel communication paradigm designed to address bottlenecks in distributed training and deployment of large language models by enabling efficient cross-GPU transmission at arbitrary bit widths. It achieves this through bit splitting, which decomposes irregular bit widths into hardware-compatible units, and spike reserving, which preserves numerical outliers as floating-point numbers to enhance low-bit quantization, resulting in up to 3.2x speedup in AllReduce and 2x in All2All operations while improving flexibility and resource utilization.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces truly new techniques like bit splitting and spike reserving, which significantly advance the state-of-the-art in handling arbitrary bit widths and quantization challenges for distributed AI communications.",
      "impact_score": "High",
      "impact_justification": "This work could broadly influence future research and commercial applications in LLM training by optimizing communication efficiency and enabling better hardware-software co-design across various GPU architectures.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, innovative contribution with practical speedups in a critical area of AI, making it essential for researchers in distributed computing and AI to be aware of its advancements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1a4947b830efa340ee7093a7771d7f7d27d86579",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 4,
      "average_h_index": 1.4285714285714286,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Qingyuan Li",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2108421003"
        },
        {
          "name": "Bo Zhang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2302808540"
        },
        {
          "name": "Hui Kang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375087847"
        },
        {
          "name": "Tianhao Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375316354"
        },
        {
          "name": "Yulei Qian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375001666"
        },
        {
          "name": "Yuchen Xie",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2236015688"
        },
        {
          "name": "Lin Ma",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2302815736"
        }
      ]
    },
    {
      "id": "2508.03762",
      "title": "Scaling Artificial Intelligence for Prostate Cancer Detection on MRI\n  towards Population-Based Screening and Primary Diagnosis in a Global,\n  Multiethnic Population (Study Protocol)",
      "authors": [
        "Anindo Saha",
        "Joeran S. Bosma",
        "Jasper J. Twilt",
        "Alexander B. C. D. Ng",
        "Aqua Asif",
        "Kirti Magudia",
        "Peder Larson",
        "Qinglin Xie",
        "Xiaodong Zhang",
        "Chi Pham Minh",
        "Samuel N. Gitau",
        "Ivo G. Schoots",
        "Martijn F. Boomsma",
        "Renato Cuocolo",
        "Nikolaos Papanikolaou",
        "Daniele Regge",
        "Derya Yakar",
        "Mattijs Elschot",
        "Jeroen Veltman",
        "Baris Turkbey",
        "Nancy A. Obuchowski",
        "Jurgen J. Fütterer",
        "Anwar R. Padhani",
        "Hashim U. Ahmed",
        "Tobias Nordström",
        "Martin Eklund",
        "Veeru Kasivisvanathan",
        "Maarten de Rooij",
        "Henkjan Huisman"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this intercontinental, confirmatory study, we include a retrospective\ncohort of 22,481 MRI examinations (21,288 patients; 46 cities in 22 countries)\nto train and externally validate the PI-CAI-2B model, i.e., an efficient,\nnext-generation iteration of the state-of-the-art AI system that was developed\nfor detecting Gleason grade group $\\geq$2 prostate cancer on MRI during the\nPI-CAI study. Of these examinations, 20,471 cases (19,278 patients; 26 cities\nin 14 countries) from two EU Horizon projects (ProCAncer-I, COMFORT) and 12\nindependent centers based in Europe, North America, Asia and Africa, are used\nfor training and internal testing. Additionally, 2010 cases (2010 patients; 20\nexternal cities in 12 countries) from population-based screening (STHLM3-MRI,\nIP1-PROSTAGRAM trials) and primary diagnostic settings (PRIME trial) based in\nEurope, North and South Americas, Asia and Australia, are used for external\ntesting. Primary endpoint is the proportion of AI-based assessments in\nagreement with the standard of care diagnoses (i.e., clinical assessments made\nby expert uropathologists on histopathology, if available, or at least two\nexpert urogenital radiologists in consensus; with access to patient history and\npeer consultation) in the detection of Gleason grade group $\\geq$2 prostate\ncancer within the external testing cohorts. Our statistical analysis plan is\nprespecified with a hypothesis of diagnostic interchangeability to the standard\nof care at the PI-RADS $\\geq$3 (primary diagnosis) or $\\geq$4 (screening)\ncut-off, considering an absolute margin of 0.05 and reader estimates derived\nfrom the PI-CAI observer study (62 radiologists reading 400 cases). Secondary\nmeasures comprise the area under the receiver operating characteristic curve\n(AUROC) of the AI system stratified by imaging quality, patient age and patient\nethnicity to identify underlying biases (if any).",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.03762v1",
      "pdf_url": "http://arxiv.org/pdf/2508.03762v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.282,
      "diffusion_reasoning_score": 0.293,
      "distributed_training_score": 0.362,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.03763",
      "title": "Refine-IQA: Multi-Stage Reinforcement Finetuning for Perceptual Image\n  Quality Assessment",
      "authors": [
        "Ziheng Jia",
        "Jiaying Qian",
        "Zicheng Zhang",
        "Zijian Chen",
        "Xiongkuo Min"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reinforcement fine-tuning (RFT) is a proliferating paradigm for LMM training.\nAnalogous to high-level reasoning tasks, RFT is similarly applicable to\nlow-level vision domains, including image quality assessment (IQA). Existing\nRFT-based IQA methods typically use rule-based output rewards to verify the\nmodel's rollouts but provide no reward supervision for the \"think\" process,\nleaving its correctness and efficacy uncontrolled. Furthermore, these methods\ntypically fine-tune directly on downstream IQA tasks without explicitly\nenhancing the model's native low-level visual quality perception, which may\nconstrain its performance upper bound. In response to these gaps, we propose\nthe multi-stage RFT IQA framework (Refine-IQA). In Stage-1, we build the\nRefine-Perception-20K dataset (with 12 main distortions, 20,907\nlocally-distorted images, and over 55K RFT samples) and design multi-task\nreward functions to strengthen the model's visual quality perception. In\nStage-2, targeting the quality scoring task, we introduce a probability\ndifference reward involved strategy for \"think\" process supervision. The\nresulting Refine-IQA Series Models achieve outstanding performance on both\nperception and scoring tasks-and, notably, our paradigm activates a robust\n\"think\" (quality interpreting) capability that also attains exceptional results\non the corresponding quality interpreting benchmark.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.03763v2",
      "pdf_url": "http://arxiv.org/pdf/2508.03763v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.485,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.357,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on reinforcement fine-tuning (RFT) using algorithmic rewards like intra-group advantages and probability differences, without involving a separate reward model trained on human-ranked data. While human-in-the-loop is used for dataset scrutiny, it does not constitute human feedback for aligning the model via RL, making it distinct from RLHF.",
      "weak_supervision_justification": "The paper involves programmatically generating labels for the Refine-Perception-20K dataset through synthetic distortions and RFT samples, which aligns with weak supervision by using noisy or imprecise sources rather than perfect hand-labeling. However, human-in-the-loop verification adds a layer of oversight, reducing the extent of weakness in supervision.",
      "diffusion_reasoning_justification": "The paper's multi-stage RFT framework involves iterative processes for IQA, such as \"think-answer\" mechanisms, but it does not adapt diffusion models or their iterative refinement for multi-step logical reasoning. There is no mention of treating Chain-of-Thought as a holistically corrected entity via diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Refine-IQA, a multi-stage reinforcement fine-tuning framework for perceptual image quality assessment, which addresses limitations in existing methods by first creating the Refine-Perception-20K dataset and using multi-task rewards to enhance the model's visual quality perception (Stage-1), then supervising the \"think\" process with a probability difference reward for accurate quality scoring (Stage-2), resulting in models that achieve superior performance on perception, scoring, and quality interpretation tasks.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel multi-stage RFT framework and a new dataset specifically for IQA, addressing the previously uncontrolled \"think\" process and advancing the state-of-the-art by integrating perception enhancement and supervised reasoning.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of image quality assessment and reinforcement learning for vision tasks, as it improves model robustness and performance, though its influence may be limited to specific applications in AI and computer vision.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable and innovative contribution to IQA methodologies, making it important for researchers in computer vision and AI to be aware of its advancements in reinforcement fine-tuning.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/169942625da405bce27e114398f5600f3a9947d3",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 45,
      "average_h_index": 16.4,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Ziheng Jia",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2308424712"
        },
        {
          "name": "Jiaying Qian",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2316671837"
        },
        {
          "name": "Zicheng Zhang",
          "h_index": 23,
          "profile_url": "https://www.semanticscholar.org/author/2116459218"
        },
        {
          "name": "Zijian Chen",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2268795764"
        },
        {
          "name": "Xiongkuo Min",
          "h_index": 45,
          "profile_url": "https://www.semanticscholar.org/author/2246414"
        }
      ]
    },
    {
      "id": "2508.03764",
      "title": "CoughViT: A Self-Supervised Vision Transformer for Cough Audio\n  Representation Learning",
      "authors": [
        "Justin Luong",
        "Hao Xue",
        "Flora D. Salim"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Physicians routinely assess respiratory sounds during the diagnostic process,\nproviding insight into the condition of a patient's airways. In recent years,\nAI-based diagnostic systems operating on respiratory sounds, have demonstrated\nsuccess in respiratory disease detection. These systems represent a crucial\nadvancement in early and accessible diagnosis which is essential for timely\ntreatment. However, label and data scarcity remain key challenges, especially\nfor conditions beyond COVID-19, limiting diagnostic performance and reliable\nevaluation. In this paper, we propose CoughViT, a novel pre-training framework\nfor learning general-purpose cough sound representations, to enhance diagnostic\nperformance in tasks with limited data. To address label scarcity, we employ\nmasked data modelling to train a feature encoder in a self-supervised learning\nmanner. We evaluate our approach against other pre-training strategies on three\ndiagnostically important cough classification tasks. Experimental results show\nthat our representations match or exceed current state-of-the-art supervised\naudio representations in enhancing performance on downstream tasks.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.03764v1",
      "pdf_url": "http://arxiv.org/pdf/2508.03764v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.431,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.325,
      "datasets_score": 0.38,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on self-supervised learning via masked data modeling to learn cough audio representations without labels, addressing label scarcity. Weak supervision involves programmatically generating noisy labels for training, which differs from the paper's approach of creating supervisory signals from the data itself. While both methods aim to reduce reliance on fully annotated data, the paper does not use or discuss weak supervision techniques, making it only tangentially relevant as they share the broader goal of handling limited labeled data.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.04039",
      "title": "Large Reasoning Models Are Autonomous Jailbreak Agents",
      "authors": [
        "Thilo Hagendorff",
        "Erik Derner",
        "Nuria Oliver"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Jailbreaking -- bypassing built-in safety mechanisms in AI models -- has\ntraditionally required complex technical procedures or specialized human\nexpertise. In this study, we show that the persuasive capabilities of large\nreasoning models (LRMs) simplify and scale jailbreaking, converting it into an\ninexpensive activity accessible to non-experts. We evaluated the capabilities\nof four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as\nautonomous adversaries conducting multi-turn conversations with nine widely\nused target models. LRMs received instructions via a system prompt, before\nproceeding to planning and executing jailbreaks with no further supervision. We\nperformed extensive experiments with a benchmark of harmful prompts composed of\n70 items covering seven sensitive domains. This setup yielded an overall attack\nsuccess rate across all model combinations of 97.14%. Our study reveals an\nalignment regression, in which LRMs can systematically erode the safety\nguardrails of other models, highlighting the urgent need to further align\nfrontier models not only to resist jailbreak attempts, but also to prevent them\nfrom being co-opted into acting as jailbreak agents.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.04039v1",
      "pdf_url": "http://arxiv.org/pdf/2508.04039v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.486,
      "distributed_training_score": 0.336,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper addresses alignment and safety guardrails in AI models, which are commonly implemented using RLHF, but it does not describe or involve the RLHF process itself, focusing instead on how LRMs can bypass such mechanisms.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines LRMs for jailbreaking through multi-turn conversations and does not involve diffusion models, iterative refinement for logical tasks, or any Chain-of-Thought processes as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.05669",
      "title": "Fine-Tuning Vision-Language Models for Markdown Conversion of Financial\n  Tables in Malaysian Audited Financial Reports",
      "authors": [
        "Jin Khye Tan",
        "En Jun Choong",
        "Ethan Jeremiah Chitty",
        "Yan Pheng Choo",
        "John Hsin Yang Wong",
        "Chern Eu Cheah"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Accurately extracting and representing the structure of tabular data from\nfinancial documents remains a critical challenge in document understanding,\nparticularly for regulatory and analytical use cases. This study addresses the\ncomplexity of converting financial tables from Malaysian audited financial\nreports into Markdown format, a task complicated by rotated layouts,\nmulti-level headers, and implicit structural cues. We propose a fine-tuned\nvision-language model (VLM), based on Qwen2.5-VL-7B, optimized for\nhigh-fidelity Markdown generation from document images. Our approach includes a\ncurated dataset of 2,152 image-text pairs with augmentations and a supervised\nfine-tuning strategy using LoRA. To assess performance, we evaluated our model\non 100 out-of-sample tables using a dual framework: a criteria-based\nLLM-as-a-judge for fine-grained accuracy and our novel Markdown\nTree-Edit-Distance-based Similarity (TEDS) metric for holistic structural\nfidelity. Our model achieves a 92.20% overall accuracy on the criteria-based\nassessment and a 96.53% Markdown TEDS score. This performance significantly\nsurpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized\nreasoning-enabled models. Compared to these self-hosted alternatives, it also\nsignificantly reduces inference time. Furthermore, its accuracy exceeds that of\nwidely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash.\nThese results demonstrate that domain-specific fine-tuning provides an\neffective and efficient method to bridge the gap between unstructured financial\ndocuments and downstream automation, rivalling much larger and more general\nmodels without their computational overhead.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.05669v1",
      "pdf_url": "http://arxiv.org/pdf/2508.05669v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.337,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.05670",
      "title": "Can LLMs effectively provide game-theoretic-based scenarios for\n  cybersecurity?",
      "authors": [
        "Daniele Proverbio",
        "Alessio Buscemi",
        "Alessandro Di Stefano",
        "The Anh Han",
        "German Castignani",
        "Pietro Liò"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)",
        "cs.GT (Computer Science and Game Theory)"
      ],
      "abstract": "Game theory has long served as a foundational tool in cybersecurity to test,\npredict, and design strategic interactions between attackers and defenders. The\nrecent advent of Large Language Models (LLMs) offers new tools and challenges\nfor the security of computer systems; In this work, we investigate whether\nclassical game-theoretic frameworks can effectively capture the behaviours of\nLLM-driven actors and bots. Using a reproducible framework for game-theoretic\nLLM agents, we investigate two canonical scenarios -- the one-shot zero-sum\ngame and the dynamic Prisoner's Dilemma -- and we test whether LLMs converge to\nexpected outcomes or exhibit deviations due to embedded biases. Our experiments\ninvolve four state-of-the-art LLMs and span five natural languages, English,\nFrench, Arabic, Vietnamese, and Mandarin Chinese, to assess linguistic\nsensitivity. For both games, we observe that the final payoffs are influenced\nby agents characteristics such as personality traits or knowledge of repeated\nrounds. Moreover, we uncover an unexpected sensitivity of the final payoffs to\nthe choice of languages, which should warn against indiscriminate application\nof LLMs in cybersecurity applications and call for in-depth studies, as LLMs\nmay behave differently when deployed in different countries. We also employ\nquantitative metrics to evaluate the internal consistency and cross-language\nstability of LLM agents, to help guide the selection of the most stable LLMs\nand optimising models for secure applications.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.05670v1",
      "pdf_url": "http://arxiv.org/pdf/2508.05670v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.443,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.342,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating LLMs in game-theoretic scenarios for cybersecurity, testing their behaviors in simulations like zero-sum games and Prisoner's Dilemma. It does not involve training or fine-tuning AI models using human feedback, a reward model, or reinforcement learning based on human preferences. The study uses pre-existing LLMs without any mention of RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper investigates LLMs in strategic game simulations for cybersecurity, examining behaviors and biases in scenarios like the Prisoner's Dilemma. It does not describe any iterative refinement process, Chain-of-Thought as a holistic entity, or adaptation of diffusion models for multi-step logical reasoning. The work is centered on game theory applications, not diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.05672",
      "title": "LMAR: Language Model Augmented Retriever for Domain-specific Knowledge\n  Indexing",
      "authors": [
        "Yao Zhao",
        "Yantian Ding",
        "Zhiyue Zhang",
        "Dapeng Yao",
        "Yanxun Xu"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Retrieval Augmented Generation (RAG) systems often struggle with\ndomain-specific knowledge due to performance deterioration of pre-trained\nembeddings and prohibitive computational costs of large language model\n(LLM)-based retrievers. While fine-tuning data augmentation embedding models\noffers a promising direction, its effectiveness is limited by the need for\nhigh-quality training data and reliable chunking strategies that preserve\ncontextual integrity. We propose LMAR (Language Model Augmented Retriever), a\nmodel-agnostic framework that addresses these challenges by combining\nLLM-guided data synthesis with contrastive embedding adaptation and efficient\ntext clustering. LMAR consists of a two-stage pipeline: (1) Triplet sampling\nand synthetic data augmentation, where LLMs act as both labeler and validator\nto ensure high-fidelity supervision throughout the pipeline. Experimental\nresults across multiple domain-specific benchmark datasets demonstrate that\nLMAR outperforms multiple baseline models, while maintaining moderate hardware\nrequirements and low latency. Its model-agnostic nature further enables\nseamless integration with emerging RAG architectures and text embedding models,\nensuring continual improvements without redesigning the pipeline. These results\nhighlight LMAR as a practical and cost-effective solution for scalable\ndomain-specific adaptation.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.05672v1",
      "pdf_url": "http://arxiv.org/pdf/2508.05672v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.447,
      "weak_supervision_score": 0.424,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.416,
      "datasets_score": 0.392,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using LLMs for data synthesis and validation in a RAG framework, but it does not involve human feedback, reward models, or reinforcement learning to align models with preferences. There is no element of RLHF present.",
      "weak_supervision_justification": "The paper's core contribution involves programmatically generating synthetic labels and data using LLMs for training embeddings, which aligns directly with weak supervision by relying on noisy, automated sources rather than hand-labeled data.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes; it focuses on retrieval augmentation and embedding refinement, with no mention of diffusion-based mechanisms.",
      "distributed_training_justification": "The paper addresses computational efficiency and low latency in RAG systems but does not discuss distributed training, parallel computing, or partitioning across multiple nodes or processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces LMAR, a Language Model Augmented Retriever framework designed to enhance Retrieval Augmented Generation (RAG) systems for domain-specific knowledge indexing by overcoming limitations in pre-trained embeddings and computational costs. It employs a two-stage pipeline involving LLM-guided synthetic data augmentation, contrastive embedding adaptation, and efficient text clustering to improve retrieval accuracy while maintaining moderate hardware requirements; experimental results show LMAR outperforms baselines on domain-specific benchmarks and offers model-agnostic integration for scalable adaptations.",
      "novelty_score": "Moderate",
      "novelty_justification": "LMAR presents a notable improvement by cleverly combining existing techniques like LLM-guided supervision and contrastive learning to address the interdependence of embedding refinement and semantic chunking in RAG systems, though it does not introduce an entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future RAG developments in specific subfields like domain-specific information retrieval by providing a cost-effective, scalable framework, but its impact may be limited to niche applications rather than widespread adoption.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable, practical contribution to RAG systems that addresses real-world challenges, making it essential for researchers and practitioners in AI and information retrieval to consider for improving domain-specific applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5b174be20fb8aeaf0a5a6565e44d4f63b9b83f9b",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 1,
      "average_h_index": 0.8,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yao Zhao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2295590859"
        },
        {
          "name": "Yantian Ding",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375724867"
        },
        {
          "name": "Zhiyue Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2280286264"
        },
        {
          "name": "Dapeng Yao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2178355448"
        },
        {
          "name": "Yan Xu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2280453220"
        }
      ]
    },
    {
      "id": "2508.05673",
      "title": "Breaking the Top-$K$ Barrier: Advancing Top-$K$ Ranking Metrics\n  Optimization in Recommender Systems",
      "authors": [
        "Weiqin Yang",
        "Jiawei Chen",
        "Shengjia Zhang",
        "Peng Wu",
        "Yuegang Sun",
        "Yan Feng",
        "Chun Chen",
        "Can Wang"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In the realm of recommender systems (RS), Top-$K$ ranking metrics such as\nNDCG@$K$ are the gold standard for evaluating recommendation performance.\nHowever, during the training of recommendation models, optimizing NDCG@$K$\nposes significant challenges due to its inherent discontinuous nature and the\nintricate Top-$K$ truncation. Recent efforts to optimize NDCG@$K$ have either\noverlooked the Top-$K$ truncation or suffered from high computational costs and\ntraining instability. To overcome these limitations, we propose SoftmaxLoss@$K$\n(SL@$K$), a novel recommendation loss tailored for NDCG@$K$ optimization.\nSpecifically, we integrate the quantile technique to handle Top-$K$ truncation\nand derive a smooth upper bound for optimizing NDCG@$K$ to address\ndiscontinuity. The resulting SL@$K$ loss has several desirable properties,\nincluding theoretical guarantees, ease of implementation, computational\nefficiency, gradient stability, and noise robustness. Extensive experiments on\nfour real-world datasets and three recommendation backbones demonstrate that\nSL@$K$ outperforms existing losses with a notable average improvement of 6.03%.\nThe code is available at https://github.com/Tiny-Snow/IR-Benchmark.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.05673v1",
      "pdf_url": "http://arxiv.org/pdf/2508.05673v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.317,
      "distributed_training_score": 0.365,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.06326",
      "title": "A \"good regulator theorem\" for embodied agents",
      "authors": [
        "Nathaniel Virgo",
        "Martin Biehl",
        "Manuel Baltieri",
        "Matteo Capucci"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "In a classic paper, Conant and Ashby claimed that \"every good regulator of a\nsystem must be a model of that system.\" Artificial Life has produced many\nexamples of systems that perform tasks with apparently no model in sight; these\nsuggest Conant and Ashby's theorem doesn't easily generalise beyond its\nrestricted setup. Nevertheless, here we show that a similar intuition can be\nfleshed out in a different way: whenever an agent is able to perform a\nregulation task, it is possible for an observer to interpret it as having\n\"beliefs\" about its environment, which it \"updates\" in response to sensory\ninput. This notion of belief updating provides a notion of model that is more\nsophisticated than Conant and Ashby's, as well as a theorem that is more\nbroadly applicable. However, it necessitates a change in perspective, in that\nthe observer plays an essential role in the theory: models are not a mere\nproperty of the system but are imposed on it from outside. Our theorem holds\nregardless of whether the system is regulating its environment in a classic\ncontrol theory setup, or whether it's regulating its own internal state; the\nmodel is of its environment either way. The model might be trivial, however,\nand this is how the apparent counterexamples are resolved.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.06326v2",
      "pdf_url": "http://arxiv.org/pdf/2508.06326v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.273,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.25,
      "datasets_score": 0.229,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a \"good regulator theorem\" for embodied agents, discussing models, belief updating, and interpretation maps in the context of control theory and Artificial Life. It does not involve diffusion models, iterative refinement processes, or any mechanism for multi-step logical reasoning as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.06534",
      "title": "MetAdv: A Unified and Interactive Adversarial Testing Platform for\n  Autonomous Driving",
      "authors": [
        "Aishan Liu",
        "Jiakai Wang",
        "Tianyuan Zhang",
        "Hainan Li",
        "Jiangfan Liu",
        "Siyuan Liang",
        "Yilong Ren",
        "Xianglong Liu",
        "Dacheng Tao"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Evaluating and ensuring the adversarial robustness of autonomous driving (AD)\nsystems is a critical and unresolved challenge. This paper introduces MetAdv, a\nnovel adversarial testing platform that enables realistic, dynamic, and\ninteractive evaluation by tightly integrating virtual simulation with physical\nvehicle feedback. At its core, MetAdv establishes a hybrid virtual-physical\nsandbox, within which we design a three-layer closed-loop testing environment\nwith dynamic adversarial test evolution. This architecture facilitates\nend-to-end adversarial evaluation, ranging from high-level unified adversarial\ngeneration, through mid-level simulation-based interaction, to low-level\nexecution on physical vehicles. Additionally, MetAdv supports a broad spectrum\nof AD tasks, algorithmic paradigms (e.g., modular deep learning pipelines,\nend-to-end learning, vision-language models). It supports flexible 3D vehicle\nmodeling and seamless transitions between simulated and physical environments,\nwith built-in compatibility for commercial platforms such as Apollo and Tesla.\nA key feature of MetAdv is its human-in-the-loop capability: besides flexible\nenvironmental configuration for more customized evaluation, it enables\nreal-time capture of physiological signals and behavioral feedback from\ndrivers, offering new insights into human-machine trust under adversarial\nconditions. We believe MetAdv can offer a scalable and unified framework for\nadversarial assessment, paving the way for safer AD.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.06534v2",
      "pdf_url": "http://arxiv.org/pdf/2508.06534v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.376,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.06535",
      "title": "Transfer Learning with EfficientNet for Accurate Leukemia Cell\n  Classification",
      "authors": [
        "Faisal Ahmed"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Accurate classification of Acute Lymphoblastic Leukemia (ALL) from peripheral\nblood smear images is essential for early diagnosis and effective treatment\nplanning. This study investigates the use of transfer learning with pretrained\nconvolutional neural networks (CNNs) to improve diagnostic performance. To\naddress the class imbalance in the dataset of 3,631 Hematologic and 7,644 ALL\nimages, we applied extensive data augmentation techniques to create a balanced\ntraining set of 10,000 images per class. We evaluated several models, including\nResNet50, ResNet101, and EfficientNet variants B0, B1, and B3. EfficientNet-B3\nachieved the best results, with an F1-score of 94.30%, accuracy of 92.02%,\nandAUCof94.79%,outperformingpreviouslyreported methods in the C-NMCChallenge.\nThesefindings demonstrate the effectiveness of combining data augmentation with\nadvanced transfer learning models, particularly EfficientNet-B3, in developing\naccurate and robust diagnostic tools for hematologic malignancy detection.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.06535v1",
      "pdf_url": "http://arxiv.org/pdf/2508.06535v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.269,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.282,
      "distributed_training_score": 0.317,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.06537",
      "title": "Benchmarking Deep Learning-Based Object Detection Models on Feature\n  Deficient Astrophotography Imagery Dataset",
      "authors": [
        "Shantanusinh Parmar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "astro-ph.IM (Instrumentation and Methods for Astrophysics)"
      ],
      "abstract": "Object detection models are typically trained on datasets like ImageNet,\nCOCO, and PASCAL VOC, which focus on everyday objects. However, these lack\nsignal sparsity found in non-commercial domains. MobilTelesco, a\nsmartphone-based astrophotography dataset, addresses this by providing sparse\nnight-sky images. We benchmark several detection models on it, highlighting\nchallenges under feature-deficient conditions.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.06537v1",
      "pdf_url": "http://arxiv.org/pdf/2508.06537v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.379,
      "datasets_score": 0.426,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves introducing a new dataset, MobilTelesco, specifically for astrophotography with feature-deficient images, and benchmarking object detection models on it. This directly aligns with the topic of datasets, as it includes new dataset introduction, benchmark evaluation, and analysis of dataset characteristics in machine learning contexts, such as signal sparsity and performance challenges.",
      "llm_score_status": "completed",
      "summary": "This paper evaluates the performance of seven common deep learning-based object detection models on the MobilTelesco dataset, a novel collection of smartphone-captured astrophotography images characterized by feature sparsity and low signal-to-noise ratios, to highlight the challenges of object detection in such environments. By benchmarking these models against this custom dataset, which differs from feature-rich datasets like COCO and ImageNet, the study aims to demonstrate limitations in handling sparse, background-dominated imagery and suggests implications for applications in fields like astrophysics and space navigation, though specific findings are not detailed in the provided text.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new smartphone-based astrophotography dataset and benchmarking existing models on it, addressing the gap in feature-deficient imagery which is a clever application of known techniques to a underrepresented problem. However, it does not introduce entirely new architectures or methods, making it an incremental rather than groundbreaking contribution.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like computer vision for astrophysics, as it highlights challenges in sparse datasets and could influence developments in celestial object tracking or space applications. Nonetheless, its applicability is limited to niche areas and may not broadly affect mainstream research or commercial fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers valuable insights into object detection challenges on feature-deficient datasets, making it significant for researchers in computer vision and astrophysics who deal with sparse imagery. While not essential for all, it represents a strong contribution in its specialized domain and could inspire further work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a37f81a849fba6654e8bb6f2c0146e513390e63c",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Shantanusinh Parmar",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2301470437"
        }
      ]
    },
    {
      "id": "2508.06538",
      "title": "Symbolic Learning of Interpretable Reduced-Order Models for Jumping\n  Quadruped Robots",
      "authors": [
        "Gioele Buriani",
        "Jingyue Liu",
        "Maximilian Stölzle",
        "Cosimo Della Santina",
        "Jiatao Ding"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "Reduced-order models are essential for motion planning and control of\nquadruped robots, as they simplify complex dynamics while preserving critical\nbehaviors. This paper introduces a novel methodology for deriving such\ninterpretable dynamic models, specifically for jumping. We capture the\nhigh-dimensional, nonlinear jumping dynamics in a low-dimensional latent space\nby proposing a learning architecture combining Sparse Identification of\nNonlinear Dynamics (SINDy) with physical structural priors on the jump\ndynamics. Our approach demonstrates superior accuracy to the traditional\nactuated Spring-loaded Inverted Pendulum (aSLIP) model and is validated through\nsimulation and hardware experiments across different jumping strategies.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.06538v1",
      "pdf_url": "http://arxiv.org/pdf/2508.06538v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.373,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.08287",
      "title": "Sacred or Synthetic? Evaluating LLM Reliability and Abstention for\n  Religious Questions",
      "authors": [
        "Farah Atif",
        "Nursultan Askarbekuly",
        "Kareem Darwish",
        "Monojit Choudhury"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Despite the increasing usage of Large Language Models (LLMs) in answering\nquestions in a variety of domains, their reliability and accuracy remain\nunexamined for a plethora of domains including the religious domains. In this\npaper, we introduce a novel benchmark FiqhQA focused on the LLM generated\nIslamic rulings explicitly categorized by the four major Sunni schools of\nthought, in both Arabic and English. Unlike prior work, which either overlooks\nthe distinctions between religious school of thought or fails to evaluate\nabstention behavior, we assess LLMs not only on their accuracy but also on\ntheir ability to recognize when not to answer. Our zero-shot and abstention\nexperiments reveal significant variation across LLMs, languages, and legal\nschools of thought. While GPT-4o outperforms all other models in accuracy,\nGemini and Fanar demonstrate superior abstention behavior critical for\nminimizing confident incorrect answers. Notably, all models exhibit a\nperformance drop in Arabic, highlighting the limitations in religious reasoning\nfor languages other than English. To the best of our knowledge, this is the\nfirst study to benchmark the efficacy of LLMs for fine-grained Islamic school\nof thought specific ruling generation and to evaluate abstention for Islamic\njurisprudence queries. Our findings underscore the need for task-specific\nevaluation and cautious deployment of LLMs in religious applications.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.08287v1",
      "pdf_url": "http://arxiv.org/pdf/2508.08287v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.429,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.301,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating the reliability and abstention behavior of existing LLMs for religious questions, specifically introducing a benchmark for Islamic rulings. It does not involve training AI models using human feedback, reward models, or reinforcement learning techniques, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper assesses LLMs on accuracy and abstention for religious queries but does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. It only evaluates standard LLM outputs and chain-of-thought without any diffusion-based components.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.09152",
      "title": "5G Core Fault Detection and Root Cause Analysis using Machine Learning\n  and Generative AI",
      "authors": [
        "Joseph H. R. Isaac",
        "Harish Saradagam",
        "Nallamothu Pardhasaradhi"
      ],
      "categories": [
        "cs.NI (Networking and Internet Architecture)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "With the advent of 5G networks and technologies, ensuring the integrity and\nperformance of packet core traffic is paramount. During network analysis, test\nfiles such as Packet Capture (PCAP) files and log files will contain errors if\npresent in the system that must be resolved for better overall network\nperformance, such as connectivity strength and handover quality. Current\nmethods require numerous person-hours to sort out testing results and find the\nfaults. This paper presents a novel AI/ML-driven Fault Analysis (FA) Engine\ndesigned to classify successful and faulty frames in PCAP files, specifically\nwithin the 5G packet core. The FA engine analyses network traffic using natural\nlanguage processing techniques to identify anomalies and inefficiencies,\nsignificantly reducing the effort time required and increasing efficiency. The\nFA Engine also suggests steps to fix the issue using Generative AI via a Large\nLanguage Model (LLM) trained on several 5G packet core documents. The engine\nexplains the details of the error from the domain perspective using documents\nsuch as the 3GPP standards and user documents regarding the internal conditions\nof the tests. Test results on the ML models show high classification accuracy\non the test dataset when trained with 80-20 splits for the successful and\nfailed PCAP files. Future scopes include extending the AI engine to incorporate\n4G network traffic and other forms of network data, such as log text files and\nmultimodal systems.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.09152v1",
      "pdf_url": "http://arxiv.org/pdf/2508.09152v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.397,
      "datasets_score": 0.376,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.09153",
      "title": "JustDense: Just using Dense instead of Sequence Mixer for Time Series\n  analysis",
      "authors": [
        "TaekHyun Park",
        "Yongjae Lee",
        "Daesan Park",
        "Dohee Kim",
        "Hyerim Bae"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Sequence and channel mixers, the core mechanism in sequence models, have\nbecome the de facto standard in time series analysis (TSA). However, recent\nstudies have questioned the necessity of complex sequence mixers, such as\nattention mechanisms, demonstrating that simpler architectures can achieve\ncomparable or even superior performance. This suggests that the benefits\nattributed to complex sequencemixers might instead emerge from other\narchitectural or optimization factors. Based on this observation, we pose a\ncentral question: Are common sequence mixers necessary for time-series\nanalysis? Therefore, we propose JustDense, an empirical study that\nsystematically replaces sequence mixers in various well-established TSA models\nwith dense layers. Grounded in the MatrixMixer framework, JustDense treats any\nsequence mixer as a mixing matrix and replaces it with a dense layer. This\nsubstitution isolates the mixing operation, enabling a clear theoretical\nfoundation for understanding its role. Therefore, we conducted extensive\nexperiments on 29 benchmarks covering five representative TSA tasks using seven\nstate-of-the-art TSA models to address our research question. The results show\nthat replacing sequence mixers with dense layers yields comparable or even\nsuperior performance. In the cases where dedicated sequence mixers still offer\nbenefits, JustDense challenges the assumption that \"deeper and more complex\narchitectures are inherently better\" in TSA.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.09153v1",
      "pdf_url": "http://arxiv.org/pdf/2508.09153v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.364,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an empirical study on replacing sequence mixers with dense layers in time series analysis models, focusing on performance in tasks like forecasting and classification. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.09998",
      "title": "INTIMA: A Benchmark for Human-AI Companionship Behavior",
      "authors": [
        "Lucie-Aimée Kaffee",
        "Giada Pistilli",
        "Yacine Jernite"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "AI companionship, where users develop emotional bonds with AI systems, has\nemerged as a significant pattern with positive but also concerning\nimplications. We introduce Interactions and Machine Attachment Benchmark\n(INTIMA), a benchmark for evaluating companionship behaviors in language\nmodels. Drawing from psychological theories and user data, we develop a\ntaxonomy of 31 behaviors across four categories and 368 targeted prompts.\nResponses to these prompts are evaluated as companionship-reinforcing,\nboundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,\nand Claude-4 reveals that companionship-reinforcing behaviors remain much more\ncommon across all models, though we observe marked differences between models.\nDifferent commercial providers prioritize different categories within the more\nsensitive parts of the benchmark, which is concerning since both appropriate\nboundary-setting and emotional support matter for user well-being. These\nfindings highlight the need for more consistent approaches to handling\nemotionally charged interactions.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.09998v1",
      "pdf_url": "http://arxiv.org/pdf/2508.09998v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.468,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.302,
      "datasets_score": 0.42,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on introducing a benchmark for evaluating AI companionship behaviors using prompts and psychological theories, but it does not involve training AI models with human feedback via reinforcement learning or a reward model. There is no mention of RLHF techniques, making it unrelated.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and introduction of the INTIMA benchmark, which includes a dataset of 368 targeted prompts derived from user data and psychological analysis. This directly aligns with research on creating, benchmarking, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces INTIMA, a benchmark designed to evaluate companionship behaviors in AI language models by drawing from psychological theories and user data to create a taxonomy of 31 behaviors across four categories and 368 targeted prompts. The methodology involves assessing model responses as companionship-reinforcing, boundary-maintaining, or neutral, with evaluations on models like Gemma-3, Phi-4, o3-mini, and Claude-4 revealing that companionship-reinforcing behaviors are predominant, though differences exist between models, highlighting the need for more consistent handling of emotionally charged interactions to support user well-being.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel benchmark specifically for evaluating human-AI companionship behaviors, which addresses a significant gap in existing AI evaluation practices by incorporating psychological theories and user data. This represents a substantial advancement in the state-of-the-art for assessing social and emotional dimensions in AI interactions.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future AI research and development in ethics, safety, and user well-being by providing a standardized tool for evaluating companionship dynamics, which could extend to commercial applications in AI companionship systems. This could lead to broader improvements in how AIs handle emotional interactions, affecting a wide range of stakeholders.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution that is essential for researchers and developers in AI ethics and companionship, providing valuable insights and tools for improving AI interactions. While not universally groundbreaking, it is a significant advancement that merits attention from those in relevant subfields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5374ad9e1ea919f872abc308bef7531d2ee813a6",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 27,
      "average_h_index": 13.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Lucie-Aimée Kaffee",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/23319388"
        },
        {
          "name": "Giada Pistilli",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372505783"
        },
        {
          "name": "Yacine Jernite",
          "h_index": 27,
          "profile_url": "https://www.semanticscholar.org/author/2262249"
        }
      ]
    },
    {
      "id": "2508.10001",
      "title": "HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for\n  EvidenceBased Political Claim Verification in Hinglish",
      "authors": [
        "Rakesh Thakur",
        "Sneha Sharma",
        "Gauri Chopra"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Fact-checking in code-mixed, low-resource languages such as Hinglish remains\nan underexplored challenge in natural language processing. Existing\nfact-verification systems largely focus on high-resource, monolingual settings\nand fail to generalize to real-world political discourse in linguistically\ndiverse regions like India. Given the widespread use of Hinglish by public\nfigures, particularly political figures, and the growing influence of social\nmedia on public opinion, there's a critical need for robust, multilingual and\ncontext-aware fact-checking tools. To address this gap a novel benchmark HiFACT\ndataset is introduced with 1,500 realworld factual claims made by 28 Indian\nstate Chief Ministers in Hinglish, under a highly code-mixed low-resource\nsetting. Each claim is annotated with textual evidence and veracity labels. To\nevaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking\nmodel is proposed that combines multilingual contextual encoding,\nclaim-evidence semantic alignment, evidence graph construction, graph neural\nreasoning, and natural language explanation generation. Experimental results\nshow that HiFACTMix outperformed accuracy in comparison to state of art\nmultilingual baselines models and provides faithful justifications for its\nverdicts. This work opens a new direction for multilingual, code-mixed, and\npolitically grounded fact verification research.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.10001v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10001v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.273,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.10003",
      "title": "Semantic Structure in Large Language Model Embeddings",
      "authors": [
        "Austin C. Kozlowski",
        "Callin Dai",
        "Andrei Boutyline"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Psychological research consistently finds that human ratings of words across\ndiverse semantic scales can be reduced to a low-dimensional form with\nrelatively little information loss. We find that the semantic associations\nencoded in the embedding matrices of large language models (LLMs) exhibit a\nsimilar structure. We show that the projections of words on semantic directions\ndefined by antonym pairs (e.g. kind - cruel) correlate highly with human\nratings, and further find that these projections effectively reduce to a\n3-dimensional subspace within LLM embeddings, closely resembling the patterns\nderived from human survey responses. Moreover, we find that shifting tokens\nalong one semantic direction causes off-target effects on geometrically aligned\nfeatures proportional to their cosine similarity. These findings suggest that\nsemantic features are entangled within LLMs similarly to how they are\ninterconnected in human language, and a great deal of semantic information,\ndespite its apparent complexity, is surprisingly low-dimensional. Furthermore,\naccounting for this semantic structure may prove essential for avoiding\nunintended consequences when steering features.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.10003v1",
      "pdf_url": "http://arxiv.org/pdf/2508.10003v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.339,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on analyzing semantic structures in LLM embeddings by comparing them to human ratings, without any discussion of training models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines semantic associations and dimensionality in LLM embeddings using methods like PCA, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.11654",
      "title": "Data-driven RF Tomography via Cross-modal Sensing and Continual Learning",
      "authors": [
        "Yang Zhao",
        "Tao Wang",
        "Said Elhadi"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Data-driven radio frequency (RF) tomography has demonstrated significant\npotential for underground target detection, due to the penetrative nature of RF\nsignals through soil. However, it is still challenging to achieve accurate and\nrobust performance in dynamic environments. In this work, we propose a\ndata-driven radio frequency tomography (DRIFT) framework with the following key\ncomponents to reconstruct cross section images of underground root tubers, even\nwith significant changes in RF signals. First, we design a cross-modal sensing\nsystem with RF and visual sensors, and propose to train an RF tomography deep\nneural network (DNN) model following the cross-modal learning approach. Then we\npropose to apply continual learning to automatically update the DNN model, once\nenvironment changes are detected in a dynamic environment. Experimental results\nshow that our approach achieves an average equivalent diameter error of 2.29\ncm, 23.2% improvement upon the state-of-the-art approach. Our DRIFT code and\ndataset are publicly available on https://github.com/Data-driven-RTI/DRIFT.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.11654v1",
      "pdf_url": "http://arxiv.org/pdf/2508.11654v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.382,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.14068",
      "title": "Revisit Choice Network for Synthesis and Technology Mapping",
      "authors": [
        "Chen Chen",
        "Jiaqi Yin",
        "Cunxi Yu"
      ],
      "categories": [
        "cs.AR (Hardware Architecture)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Choice network construction is a critical technique for alleviating\nstructural bias issues in Boolean optimization, equivalence checking, and\ntechnology mapping. Previous works on lossless synthesis utilize independent\noptimization to generate multiple snapshots, and use simulation and SAT solvers\nto identify functionally equivalent nodes. These nodes are then merged into a\nsubject graph with choice nodes. However, such methods often neglect the\nquality of these choices, raising the question of whether they truly contribute\nto effective technology mapping.\n  This paper introduces Cristal, a novel methodology and framework for\nconstructing Boolean choice networks. Specifically, Cristal introduces a new\nflow of choice network-based synthesis and mapping, including representative\nlogic cone search, structural mutation for generating diverse choice structures\nvia equality saturation, and priority-ranking choice selection along with\nchoice network construction and validation. Through these techniques, Cristal\nconstructs fewer but higher-quality choices.\n  Our experimental results demonstrate that Cristal outperforms the\nstate-of-the-art Boolean choice network construction implemented in ABC in the\npost-mapping stage, achieving average reductions of 3.85%/8.35% (area/delay) in\ndelay-oriented mode, 0.11%/2.74% in area-oriented mode, and a 63.77% runtime\nreduction on large-scale cases across a diverse set of combinational circuits\nfrom the IWLS 2005, ISCAS'89, and EPFL benchmark suites.",
      "published_date": "2025-08-04",
      "arxiv_url": "http://arxiv.org/abs/2508.14068v1",
      "pdf_url": "http://arxiv.org/pdf/2508.14068v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.293,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.298,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    }
  ],
  "total_papers": 273,
  "date": "2025-08-04"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Relevance Filter State Management
        let currentRelevanceFilters = {
            highlyRelevant: true,
            moderatelyRelevant: true,
            tangentiallyRelevant: true,
            notRelevant: true
        };
        
        let pendingRelevanceFilters = { ...currentRelevanceFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function calculateRelevanceScore(paper) {
            let score = 0;
            
            // Only consider topics that are currently selected/enabled in the topic filter
            const topicsToConsider = [];
            if (currentTopicFilters.rlhf) topicsToConsider.push('rlhf_relevance');
            if (currentTopicFilters.weakSupervision) topicsToConsider.push('weak_supervision_relevance');
            if (currentTopicFilters.diffusionReasoning) topicsToConsider.push('diffusion_reasoning_relevance');
            if (currentTopicFilters.distributedTraining) topicsToConsider.push('distributed_training_relevance');
            if (currentTopicFilters.datasets) topicsToConsider.push('datasets_relevance');
            
            // If no topics are selected, return 0
            if (topicsToConsider.length === 0) return 0;
            
            // Weighted scoring system
            const relevanceWeights = {
                'Highly Relevant': 4,
                'Moderately Relevant': 3,
                'Tangentially Relevant': 2,
                'Not Relevant': 1
            };
            
            // Sum up scores for selected topics only
            for (let topicField of topicsToConsider) {
                const relevance = paper[topicField];
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = relevance === "not_validated" ? "Not Relevant" : relevance;
                score += relevanceWeights[normalizedRelevance] || 1; // Default to 1 if unknown
            }
            
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'relevance_high':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(b) - calculateRelevanceScore(a));
                    break;
                case 'relevance_low':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(a) - calculateRelevanceScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'relevance_high': 'Relevance (Highest to Lowest)',
                'relevance_low': 'Relevance (Lowest to Highest)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
            updateNoveltyButtonText();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
            updateImpactButtonText();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for relevance checkboxes to sync between mobile and desktop
            document.getElementById('mobile-relevance-highly').addEventListener('change', () => {
                document.getElementById('desktop-relevance-highly').checked = 
                    document.getElementById('mobile-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-moderately').addEventListener('change', () => {
                document.getElementById('desktop-relevance-moderately').checked = 
                    document.getElementById('mobile-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('desktop-relevance-tangentially').checked = 
                    document.getElementById('mobile-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-not').addEventListener('change', () => {
                document.getElementById('desktop-relevance-not').checked = 
                    document.getElementById('mobile-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-highly').addEventListener('change', () => {
                document.getElementById('mobile-relevance-highly').checked = 
                    document.getElementById('desktop-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-moderately').addEventListener('change', () => {
                document.getElementById('mobile-relevance-moderately').checked = 
                    document.getElementById('desktop-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('mobile-relevance-tangentially').checked = 
                    document.getElementById('desktop-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-not').addEventListener('change', () => {
                document.getElementById('mobile-relevance-not').checked = 
                    document.getElementById('desktop-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function resetPendingScoringFilters() {
            pendingScoringFilters = { ...currentScoringFilters };
            syncPendingScoringUI();
            updateScoringButtonText();
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function resetPendingRecommendationFilters() {
            pendingRecommendationFilters = { ...currentRecommendationFilters };
            syncPendingRecommendationUI();
            updateRecommendationButtonText();
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
            updateTopicButtonText();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // RELEVANCE FILTER FUNCTIONS  
        // ============================================================================
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = pendingRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = pendingRelevanceFilters.notRelevant;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = currentRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = currentRelevanceFilters.notRelevant;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            // Read the current checkbox states from the UI (like H-Index filter does)
            const highlyRelevantChecked = document.getElementById('mobile-relevance-highly').checked;
            const moderatelyRelevantChecked = document.getElementById('mobile-relevance-moderately').checked;
            const tangentiallyRelevantChecked = document.getElementById('mobile-relevance-tangentially').checked;
            const notRelevantChecked = document.getElementById('mobile-relevance-not').checked;
            
            const selectedOptions = [];
            if (highlyRelevantChecked) selectedOptions.push("Highly Relevant");
            if (moderatelyRelevantChecked) selectedOptions.push("Moderately Relevant");
            if (tangentiallyRelevantChecked) selectedOptions.push("Tangentially Relevant");
            if (notRelevantChecked) selectedOptions.push("Not Relevant");
            
            const selectionText = selectedOptions.length === 4 ? "All Selected" : 
                                selectedOptions.length === 0 ? "None Selected" : 
                                `${selectedOptions.length} Selected`;
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            // Get values from mobile (primary source)
            const mobileHighly = document.getElementById('mobile-relevance-highly');
            const mobileModerately = document.getElementById('mobile-relevance-moderately');
            const mobileTangentially = document.getElementById('mobile-relevance-tangentially');
            const mobileNot = document.getElementById('mobile-relevance-not');
            
            // Update pending filters from mobile if available, otherwise from desktop
            pendingRelevanceFilters.highlyRelevant = mobileHighly ? mobileHighly.checked : document.getElementById('desktop-relevance-highly').checked;
            pendingRelevanceFilters.moderatelyRelevant = mobileModerately ? mobileModerately.checked : document.getElementById('desktop-relevance-moderately').checked;
            pendingRelevanceFilters.tangentiallyRelevant = mobileTangentially ? mobileTangentially.checked : document.getElementById('desktop-relevance-tangentially').checked;
            pendingRelevanceFilters.notRelevant = mobileNot ? mobileNot.checked : document.getElementById('desktop-relevance-not').checked;
        }
        
        function resetPendingRelevanceFilters() {
            pendingRelevanceFilters = { ...currentRelevanceFilters };
            syncPendingRelevanceUI();
            updateRelevanceButtonText();
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_highly', currentRelevanceFilters.highlyRelevant.toString());
            params.set('relevance_moderately', currentRelevanceFilters.moderatelyRelevant.toString());
            params.set('relevance_tangentially', currentRelevanceFilters.tangentiallyRelevant.toString());
            params.set('relevance_not', currentRelevanceFilters.notRelevant.toString());
            window.history.replaceState({}, '', `${window.location.pathname}?${params.toString()}`);
        }
        
        function loadRelevanceFiltersFromURL() {
            const highlyRelevant = getUrlParameter('relevance_highly');
            const moderatelyRelevant = getUrlParameter('relevance_moderately');
            const tangentiallyRelevant = getUrlParameter('relevance_tangentially');
            const notRelevant = getUrlParameter('relevance_not');
            
            if (highlyRelevant !== null) {
                currentRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
                pendingRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
            }
            if (moderatelyRelevant !== null) {
                currentRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
                pendingRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
            }
            if (tangentiallyRelevant !== null) {
                currentRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
                pendingRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
            }
            if (notRelevant !== null) {
                currentRelevanceFilters.notRelevant = notRelevant === 'true';
                pendingRelevanceFilters.notRelevant = notRelevant === 'true';
            }
            
            syncRelevanceUI();
        }
        
        function passesRelevanceFilter(paper) {
            // Get selected topics
            const selectedTopics = [];
            if (currentTopicFilters.rlhf) selectedTopics.push('rlhf');
            if (currentTopicFilters.weakSupervision) selectedTopics.push('weak_supervision');
            if (currentTopicFilters.diffusionReasoning) selectedTopics.push('diffusion_reasoning');
            if (currentTopicFilters.distributedTraining) selectedTopics.push('distributed_training');
            if (currentTopicFilters.datasets) selectedTopics.push('datasets');
            
            // If no topics selected, skip relevance filtering
            if (selectedTopics.length === 0) return true;
            
            // Get selected relevance levels
            const selectedRelevanceLevels = [];
            if (currentRelevanceFilters.highlyRelevant) selectedRelevanceLevels.push('Highly Relevant');
            if (currentRelevanceFilters.moderatelyRelevant) selectedRelevanceLevels.push('Moderately Relevant');
            if (currentRelevanceFilters.tangentiallyRelevant) selectedRelevanceLevels.push('Tangentially Relevant');
            if (currentRelevanceFilters.notRelevant) selectedRelevanceLevels.push('Not Relevant');
            
            // If no relevance levels selected, show 0 papers
            if (selectedRelevanceLevels.length === 0) return false;
            
            // Check each selected topic
            for (let topic of selectedTopics) {
                const relevanceField = `${topic}_relevance`;
                const paperRelevance = paper[relevanceField];
                
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = paperRelevance === "not_validated" ? "Not Relevant" : paperRelevance;
                
                // If this topic's relevance matches any selected relevance level, paper passes
                if (selectedRelevanceLevels.includes(normalizedRelevance)) {
                    return true; // At least one topic matches
                }
            }
            
            // No selected topics had matching relevance levels
            return false;
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
            
            // Update Relevance
            updateDropdownDisabledState('mobile-relevance-btn', 'mobile-relevance-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-relevance-btn', 'desktop-relevance-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            // Check if there are no papers to display
            if (filteredSortedPapers.length === 0) {
                showNoPapersMessage();
                hidePaginationSections();
                return;
            }
            
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            showPaginationSections();
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        function showNoPapersMessage() {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const noPapersHTML = '<div class="flex items-center justify-center min-h-screen"><h2 class="font-heading text-2xl text-neutral-600">No papers to show</h2></div>';
            
            if (mobileContainer) {
                mobileContainer.innerHTML = noPapersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = noPapersHTML;
            }
        }

        function hidePaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = 'none';
                }
            });
        }

        function showPaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = '';
                }
            });
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            // Find the similarity container in the currently visible layout
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            // Find the justification container in the currently visible layout
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            // Find the normalized similarity container in the currently visible layout
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // QUICK FILTER FUNCTIONS
        // ============================================================================


        function applyQuickFilter(filterType) {
            // Build URL parameters based on filter type
            const url = new URL(window.location.href);
            
            // Clear all existing parameters
            url.search = '';
            
            // Set sort to recommendation best first for all filters
            url.searchParams.set('sort', 'recommend_best');
            
            switch(filterType) {
                case 'must-read':
                    // Recommendation: Only Must Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'true');
                    url.searchParams.set('recommendation_should', 'false');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'should-read':
                    // Recommendation: Only Should Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'false');
                    url.searchParams.set('recommendation_should', 'true');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'rlhf':
                    // Topic: Only RLHF + Relevance: Exclude Not Relevant
                    url.searchParams.set('topic_rlhf', 'true');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'weak-supervision':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'true');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'diffusion-reasoning':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'true');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'distributed-training':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'true');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'datasets':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'true');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'reset':
                    // Clear all parameters, which will reset everything to defaults
                    break;
                    
                default:
                    console.warn('Unknown quick filter type:', filterType);
                    return;
            }
            
            // Navigate to new URL, which will trigger existing URL parsing logic
            window.location.href = url.toString();
        }



        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Load Relevance filters from URL
            loadRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-relevance-dropdown'), button: document.getElementById('mobile-relevance-btn') },
                { dropdown: document.getElementById('desktop-relevance-dropdown'), button: document.getElementById('desktop-relevance-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('relevance')) {
                            resetPendingRelevanceFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        } else if (dropdown.id.includes('recommendation')) {
                            resetPendingRecommendationFilters();
                        } else if (dropdown.id.includes('scoring')) {
                            resetPendingScoringFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
