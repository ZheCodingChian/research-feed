<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 29 July 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: 'rgba(107, 106, 101, 0.7)' // #6B6A65 with 70% opacity
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 29 July 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100 min-h-screen">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="mobile-quick-must-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="mobile-quick-should-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="mobile-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="mobile-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="mobile-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="mobile-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="mobile-quick-datasets" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="mobile-quick-reset" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-highly" checked>
                                            <label for="mobile-relevance-highly"></label>
                                        </div>
                                        <label for="mobile-relevance-highly" class="text-neutral-10 text-xl font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-moderately" checked>
                                            <label for="mobile-relevance-moderately"></label>
                                        </div>
                                        <label for="mobile-relevance-moderately" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-tangentially" checked>
                                            <label for="mobile-relevance-tangentially"></label>
                                        </div>
                                        <label for="mobile-relevance-tangentially" class="text-neutral-10 text-xl font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-not" checked>
                                            <label for="mobile-relevance-not"></label>
                                        </div>
                                        <label for="mobile-relevance-not" class="text-neutral-10 text-xl font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="desktop-quick-must-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="desktop-quick-should-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="desktop-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="desktop-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="desktop-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="desktop-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="desktop-quick-datasets" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="desktop-quick-reset" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-highly" checked>
                                            <label for="desktop-relevance-highly"></label>
                                        </div>
                                        <label for="desktop-relevance-highly" class="text-neutral-10 text-lg font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-moderately" checked>
                                            <label for="desktop-relevance-moderately"></label>
                                        </div>
                                        <label for="desktop-relevance-moderately" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-tangentially" checked>
                                            <label for="desktop-relevance-tangentially"></label>
                                        </div>
                                        <label for="desktop-relevance-tangentially" class="text-neutral-10 text-lg font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-not" checked>
                                            <label for="desktop-relevance-not"></label>
                                        </div>
                                        <label for="desktop-relevance-not" class="text-neutral-10 text-lg font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 29 July 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl min-h-screen">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2507.21395",
      "title": "Sync-TVA: A Graph-Attention Framework for Multimodal Emotion Recognition\n  with Cross-Modal Fusion",
      "authors": [
        "Zeyu Deng",
        "Yanhui Lu",
        "Jiashu Liao",
        "Shuang Wu",
        "Chongfeng Wei"
      ],
      "categories": [
        "cs.MM (Multimedia)",
        "cs.AI (Artificial Intelligence)",
        "cs.SD (Sound)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Multimodal emotion recognition (MER) is crucial for enabling emotionally\nintelligent systems that perceive and respond to human emotions. However,\nexisting methods suffer from limited cross-modal interaction and imbalanced\ncontributions across modalities. To address these issues, we propose Sync-TVA,\nan end-to-end graph-attention framework featuring modality-specific dynamic\nenhancement and structured cross-modal fusion. Our design incorporates a\ndynamic enhancement module for each modality and constructs heterogeneous\ncross-modal graphs to model semantic relations across text, audio, and visual\nfeatures. A cross-attention fusion mechanism further aligns multimodal cues for\nrobust emotion inference. Experiments on MELD and IEMOCAP demonstrate\nconsistent improvements over state-of-the-art models in both accuracy and\nweighted F1 score, especially under class-imbalanced conditions.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21395v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21395v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.359,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a graph-attention framework for multimodal emotion recognition, emphasizing cross-modal fusion, dynamic enhancement, and attention mechanisms for integrating text, audio, and visual features. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical reasoning tasks. Therefore, there is no connection to the topic of Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21406",
      "title": "Shapley Uncertainty in Natural Language Generation",
      "authors": [
        "Meilin Zhu",
        "Gaojie Jin",
        "Xiaowei Huang",
        "Lijun Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In question-answering tasks, determining when to trust the outputs is crucial\nto the alignment of large language models (LLMs). Kuhn et al. (2023) introduces\nsemantic entropy as a measure of uncertainty, by incorporating linguistic\ninvariances from the same meaning. It primarily relies on setting threshold to\nmeasure the level of semantic equivalence relation. We propose a more nuanced\nframework that extends beyond such thresholding by developing a Shapley-based\nuncertainty metric that captures the continuous nature of semantic\nrelationships. We establish three fundamental properties that characterize\nvalid uncertainty metrics and prove that our Shapley uncertainty satisfies\nthese criteria. Through extensive experiments, we demonstrate that our Shapley\nuncertainty more accurately predicts LLM performance in question-answering and\nother datasets, compared to similar baseline measures.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21406v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21406v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.286,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on developing a Shapley-based uncertainty metric for natural language generation, particularly in question-answering tasks, by incorporating semantic relationships and correlations. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for complex logical tasks or Chain-of-Thought reasoning. Therefore, there is no connection to the topic of Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21407",
      "title": "Graph-Augmented Large Language Model Agents: Current Progress and Future\n  Prospects",
      "authors": [
        "Yixin Liu",
        "Guibin Zhang",
        "Kun Wang",
        "Shiyuan Li",
        "Shirui Pan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Autonomous agents based on large language models (LLMs) have demonstrated\nimpressive capabilities in a wide range of applications, including web\nnavigation, software development, and embodied control. While most LLMs are\nlimited in several key agentic procedures, such as reliable planning, long-term\nmemory, tool management, and multi-agent coordination, graphs can serve as a\npowerful auxiliary structure to enhance structure, continuity, and coordination\nin complex agent workflows. Given the rapid growth and fragmentation of\nresearch on Graph-augmented LLM Agents (GLA), this paper offers a timely and\ncomprehensive overview of recent advances and also highlights key directions\nfor future work. Specifically, we categorize existing GLA methods by their\nprimary functions in LLM agent systems, including planning, memory, and tool\nusage, and then analyze how graphs and graph learning algorithms contribute to\neach. For multi-agent systems, we further discuss how GLA solutions facilitate\nthe orchestration, efficiency optimization, and trustworthiness of MAS.\nFinally, we highlight key future directions to advance this field, from\nimproving structural adaptability to enabling unified, scalable, and multimodal\nGLA systems. We hope this paper can serve as a roadmap for future research on\nGLA and foster a deeper understanding of the role of graphs in LLM agent\nsystems.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21407v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21407v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.4,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on graph-augmented LLM agents, emphasizing enhancements in planning, memory, and tool management through graphs, without any discussion of reinforcement learning, human feedback, reward models, or aligning models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses reasoning in LLM agents using graphs for planning and workflows, but it does not involve diffusion models, iterative refinement processes, or treating chain-of-thought as a holistic entity for multi-step logical correction.",
      "distributed_training_justification": "The paper mentions efficiency in GLA systems, such as using graphs for information access and lightweight graph neural networks, but it does not address distributed training, parallel computing, or strategies for partitioning data/models across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21419",
      "title": "GovRelBench:A Benchmark for Government Domain Relevance",
      "authors": [
        "Haiquan Wang",
        "Yi Chen",
        "Shang Zeng",
        "Yun Bian",
        "Zhe Cui"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Current evaluations of LLMs in the government domain primarily focus on\nsafety considerations in specific scenarios, while the assessment of the\nmodels' own core capabilities, particularly domain relevance, remains\ninsufficient. To address this gap, we propose GovRelBench, a benchmark\nspecifically designed for evaluating the core capabilities of LLMs in the\ngovernment domain. GovRelBench consists of government domain prompts and a\ndedicated evaluation tool, GovRelBERT. During the training process of\nGovRelBERT, we introduce the SoftGovScore method: this method trains a model\nbased on the ModernBERT architecture by converting hard labels to soft scores,\nenabling it to accurately compute the text's government domain relevance score.\nThis work aims to enhance the capability evaluation framework for large models\nin the government domain, providing an effective tool for relevant research and\npractice. Our code and dataset are available at\nhttps://github.com/pan-xi/GovRelBench.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21419v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21419v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.357,
      "datasets_score": 0.427,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on creating a benchmark and evaluation tool for assessing LLMs in the government domain, using methods like SoftGovScore for label conversion. It does not involve human feedback, reward models, or reinforcement learning techniques, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces GovRelBench, which includes a new dataset of government domain prompts, along with methodologies for its creation, benchmarking, and evaluation. This directly aligns with research on creating and benchmarking datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces GovRelBench, a benchmark designed to evaluate the core capabilities of Large Language Models (LLMs) in the government domain by assessing the relevance of their outputs to governmental contexts. It employs a dedicated evaluation tool, GovRelBERT, based on the ModernBERT architecture and trained using the SoftGovScore method, which converts hard labels into soft scores for more precise relevance measurement, addressing a gap in existing evaluations that primarily focus on safety rather than core capabilities.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a new benchmark and evaluation method for LLMs in the government domain, combining existing techniques like ModernBERT with a novel SoftGovScore approach to address an underserved area. However, it builds on established architectures rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of domain-specific AI evaluations, particularly for government applications, due to its open-source availability and targeted focus. Nonetheless, its influence may remain confined to niche areas rather than broadly affecting general AI research or commercial sectors.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a valuable and practical benchmark for evaluating LLMs in the government domain, making it a significant contribution that researchers in AI and public sector applications should be aware of. While not essential for all, it offers useful tools and insights for those working in related areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/51eced1bb3be392fc74ddf4127c7ed5df79cd6e8",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Haiquan Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373399382"
        },
        {
          "name": "Yi Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374434204"
        },
        {
          "name": "Shang Zeng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374306971"
        },
        {
          "name": "Yun Bian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373725710"
        },
        {
          "name": "Zhe Cui",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374093553"
        }
      ]
    },
    {
      "id": "2507.21420",
      "title": "ReGATE: Learning Faster and Better with Fewer Tokens in MLLMs",
      "authors": [
        "Chaoyu Li",
        "Yogesh Kulkarni",
        "Pooyan Fazli"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "The computational cost of training multimodal large language models (MLLMs)\nrapidly increases with the number of tokens involved. Existing efficiency\nmethods primarily target inference and rely on token reduction or merging,\noffering limited benefit during training. In this paper, we propose ReGATE\n(Reference$-$Guided Adaptive Token Elision), an adaptive token pruning method\nfor accelerating MLLM training. Specifically, ReGATE adopts a teacher-student\nframework in which the MLLM being trained serves as the student, and a frozen\nreference large language model (LLM) acts as the teacher. The teacher computes\nper-token reference losses, which are combined with an exponential moving\naverage (EMA) of the student's own difficulty scores. This adaptive\ndifficulty-based scoring enables the selective processing of crucial tokens\nwhile bypassing less informative ones in the forward pass, significantly\nreducing computational overhead. Experiments demonstrate that ReGATE, when\napplied to VideoLLaMA2, matches the peak accuracy of standard training on\nMVBench up to 2$\\times$ faster, using only 35% of the tokens. With additional\ntraining, it even surpasses the baseline on several multimodal benchmarks, all\nwhile reducing the total token count by over 41%. Code and models will be\nreleased soon.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21420v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21420v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.442,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces ReGATE, a method for adaptive token pruning in MLLM training to reduce computational costs, using a teacher-student framework. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical reasoning tasks, such as treating a Chain-of-Thought as a single entity for correction.",
      "distributed_training_justification": "The paper focuses on accelerating MLLM training through selective token processing and a teacher-student setup, which optimizes computation within a single model. It does not discuss distributed training techniques, parallel computing across multiple nodes, or strategies for partitioning data, architecture, or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21423",
      "title": "MapDiffusion: Generative Diffusion for Vectorized Online HD Map\n  Construction and Uncertainty Estimation in Autonomous Driving",
      "authors": [
        "Thomas Monninger",
        "Zihan Zhang",
        "Zhipeng Mo",
        "Md Zafar Anwar",
        "Steffen Staab",
        "Sihao Ding"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Autonomous driving requires an understanding of the static environment from\nsensor data. Learned Bird's-Eye View (BEV) encoders are commonly used to fuse\nmultiple inputs, and a vector decoder predicts a vectorized map representation\nfrom the latent BEV grid. However, traditional map construction models provide\ndeterministic point estimates, failing to capture uncertainty and the inherent\nambiguities of real-world environments, such as occlusions and missing lane\nmarkings. We propose MapDiffusion, a novel generative approach that leverages\nthe diffusion paradigm to learn the full distribution of possible vectorized\nmaps. Instead of predicting a single deterministic output from learned queries,\nMapDiffusion iteratively refines randomly initialized queries, conditioned on a\nBEV latent grid, to generate multiple plausible map samples. This allows\naggregating samples to improve prediction accuracy and deriving uncertainty\nestimates that directly correlate with scene ambiguity. Extensive experiments\non the nuScenes dataset demonstrate that MapDiffusion achieves state-of-the-art\nperformance in online map construction, surpassing the baseline by 5% in\nsingle-sample performance. We further show that aggregating multiple samples\nconsistently improves performance along the ROC curve, validating the benefit\nof distribution modeling. Additionally, our uncertainty estimates are\nsignificantly higher in occluded areas, reinforcing their value in identifying\nregions with ambiguous sensor input. By modeling the full map distribution,\nMapDiffusion enhances the robustness and reliability of online vectorized HD\nmap construction, enabling uncertainty-aware decision-making for autonomous\nvehicles in complex environments.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21423v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21423v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.561,
      "distributed_training_score": 0.362,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for generating and refining vectorized HD maps in autonomous driving, emphasizing uncertainty estimation in environmental perception. While it employs iterative refinement similar to diffusion processes, it does not adapt this for solving complex logical tasks, multi-step reasoning, or treating a 'Chain-of-Thought' as an entity. The core contribution is in map construction, not logical reasoning, so it does not meet the topic's criteria.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21432",
      "title": "Towards Locally Deployable Fine-Tuned Causal Large Language Models for\n  Mode Choice Behaviour",
      "authors": [
        "Tareq Alsaleh",
        "Bilal Farooq"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study investigates the adoption of open-access, locally deployable\ncausal large language models (LLMs) for travel mode choice prediction and\nintroduces LiTransMC, the first fine-tuned causal LLM developed for this task.\nWe systematically benchmark eleven LLMs (1-12B parameters) across three stated\nand revealed preference datasets, testing 396 configurations and generating\nover 79,000 synthetic commuter predictions. Beyond predictive accuracy, we\nevaluate models generated reasoning using BERTopic for topic modelling and a\nnovel Explanation Strength Index, providing the first structured analysis of\nhow LLMs articulate decision factors in alignment with behavioural theory.\nLiTransMC, fine-tuned using parameter efficient and loss masking strategy,\nachieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of\n0.000245, surpassing both untuned local models and larger proprietary systems,\nincluding GPT-4o with advanced persona inference and embedding-based loading,\nwhile also outperforming classical mode choice methods such as discrete choice\nmodels and machine learning classifiers for the same dataset. This dual\nimprovement, i.e., high instant-level accuracy and near-perfect distributional\ncalibration, demonstrates the feasibility of creating specialist, locally\ndeployable LLMs that integrate prediction and interpretability. Through\ncombining structured behavioural prediction with natural language reasoning,\nthis work unlocks the potential for conversational, multi-task transport models\ncapable of supporting agent-based simulations, policy testing, and behavioural\ninsight generation. These findings establish a pathway for transforming general\npurpose LLMs into specialized, explainable tools for transportation research\nand policy formulation, while maintaining privacy, reducing cost, and\nbroadening access through local deployment.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21432v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21432v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.437,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.49,
      "distributed_training_score": 0.361,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes fine-tuning LLMs using parameter-efficient methods like LoRA and loss masking on travel datasets, but it does not involve human feedback, reward models, or reinforcement learning techniques. There is no mention of aligning the model with human preferences through ranked data or RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper analyzes LLM reasoning for mode choice prediction using tools like BERTopic and an Explanation Strength Index, but it does not incorporate diffusion models, iterative refinement processes, or treat chain-of-thought as a holistically corrected entity. No components related to multi-step logical reasoning via diffusion are present.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21433",
      "title": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse",
      "authors": [
        "Kaiwen Chen",
        "Xin Tan",
        "Minchen Yu",
        "Hong Xu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21433v2",
      "pdf_url": "http://arxiv.org/pdf/2507.21433v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.49,
      "distributed_training_score": 0.438,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on memory-efficient inference for Large Reasoning Models through KV cache reuse, emphasizing techniques like collaborative filtering for similar reasoning steps. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for correction, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper's main contribution is optimizing KV cache during inference to reduce memory overhead, with no discussion of distributed training, parallel computing, multi-node setups, or partitioning data/computation across processors. It pertains solely to inference efficiency, not training acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21438",
      "title": "Evo-DKD: Dual-Knowledge Decoding for Autonomous Ontology Evolution in\n  Large Language Models",
      "authors": [
        "Vishal Raman",
        "Vijai Aravindh R"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Ontologies and knowledge graphs require continuous evolution to remain\ncomprehensive and accurate, but manual curation is labor intensive. Large\nLanguage Models (LLMs) possess vast unstructured knowledge but struggle with\nmaintaining structured consistency. We propose Evo-DKD, a novel dual-decoder\nframework for autonomous ontology evolution that combines structured ontology\ntraversal with unstructured text reasoning. Evo-DKD introduces two parallel\ndecoding streams within an LLM: one decoder generates candidate ontology edits\n(e.g., new concepts or relations) while the other produces natural-language\njustifications. A dynamic attention-based gating mechanism coordinates the two\nstreams, deciding at each step how to blend structured and unstructured\nknowledge. Due to GPU constraints, we simulate the dual-decoder behavior using\nprompt-based mode control to approximate coordinated decoding in a\nsingle-stream mode. The system operates in a closed reasoning loop: proposed\nontology edits are validated (via consistency checks and cross-verification\nwith the text explanations) and then injected into the knowledge base, which in\nturn informs subsequent reasoning. We demonstrate Evo-DKD's effectiveness on\nuse cases including healthcare ontology refinement, semantic search\nimprovement, and cultural heritage timeline modeling. Experiments show that\nEvo-DKD outperforms baselines using structured-only or unstructured-only\ndecoding in both precision of ontology updates and downstream task performance.\nWe present quantitative metrics and qualitative examples, confirming the\ncontributions of the dual-decoder design and gating router. Evo-DKD offers a\nnew paradigm for LLM-driven knowledge base maintenance, combining the strengths\nof symbolic and neural reasoning for sustainable ontology evolution.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21438v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21438v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.511,
      "distributed_training_score": 0.377,
      "datasets_score": 0.402,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Not Relevant",
      "rlhf_justification": "The paper focuses on a dual-decoder framework for autonomous ontology evolution using LLMs, without any mention of human feedback, reward models, or reinforcement learning for alignment. It operates independently, so it does not involve RLHF.",
      "weak_supervision_justification": "The paper proposes a method for ontology evolution with LLMs, involving validation and editing, but it does not discuss training models using programmatically generated labels or noisy sources as in weak supervision. The focus is on runtime application, not training paradigms.",
      "diffusion_reasoning_justification": "The paper describes a dual-decoder system with iterative reasoning loops for ontology updates, but it does not adapt diffusion models or involve multi-step logical refinement as a holistic chain-of-thought process. There is no reference to diffusion-based techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper evaluates a framework on specific domains like healthcare and cultural heritage, using existing setups for experiments, but it does not focus on creating, analyzing, benchmarking, or evaluating datasets as its main contribution.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21440",
      "title": "Dual Cross-image Semantic Consistency with Self-aware Pseudo Labeling\n  for Semi-supervised Medical Image Segmentation",
      "authors": [
        "Han Wu",
        "Chong Wang",
        "Zhiming Cui"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Semi-supervised learning has proven highly effective in tackling the\nchallenge of limited labeled training data in medical image segmentation. In\ngeneral, current approaches, which rely on intra-image pixel-wise consistency\ntraining via pseudo-labeling, overlook the consistency at more comprehensive\nsemantic levels (e.g., object region) and suffer from severe discrepancy of\nextracted features resulting from an imbalanced number of labeled and unlabeled\ndata. To overcome these limitations, we present a new \\underline{Du}al\n\\underline{C}ross-\\underline{i}mage \\underline{S}emantic\n\\underline{C}onsistency (DuCiSC) learning framework, for semi-supervised\nmedical image segmentation. Concretely, beyond enforcing pixel-wise semantic\nconsistency, DuCiSC proposes dual paradigms to encourage region-level semantic\nconsistency across: 1) labeled and unlabeled images; and 2) labeled and fused\nimages, by explicitly aligning their prototypes. Relying on the dual paradigms,\nDuCiSC can effectively establish consistent cross-image semantics via prototype\nrepresentations, thereby addressing the feature discrepancy issue. Moreover, we\ndevise a novel self-aware confidence estimation strategy to accurately select\nreliable pseudo labels, allowing for exploiting the training dynamics of\nunlabeled data. Our DuCiSC method is extensively validated on four datasets,\nincluding two popular binary benchmarks in segmenting the left atrium and\npancreas, a multi-class Automatic Cardiac Diagnosis Challenge dataset, and a\nchallenging scenario of segmenting the inferior alveolar nerve that features\ncomplicated anatomical structures, showing superior segmentation results over\nprevious state-of-the-art approaches. Our code is publicly available at\n\\href{https://github.com/ShanghaiTech-IMPACT/DuCiSC}{https://github.com/ShanghaiTech-IMPACT/DuCiSC}.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21440v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21440v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.358,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves semi-supervised medical image segmentation using pseudo-labeling, where labels for unlabeled data are programmatically generated from model predictions. This directly aligns with weak supervision, as it relies on noisy or imprecise sources (e.g., pseudo labels derived from the model's outputs) rather than perfect hand-labeled data. The paper also addresses issues like confirmation bias in pseudo labels, which is a key challenge in weak supervision approaches.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces the DuCiSC framework for semi-supervised medical image segmentation, aiming to overcome limitations in existing methods by enforcing not only pixel-wise consistency but also region-level semantic consistency across labeled, unlabeled, and fused images through prototype alignment. The methodology includes dual paradigms for aligning prototypes to address feature discrepancies and a self-aware confidence estimation strategy for selecting reliable pseudo labels, with extensive validation on four datasets demonstrating superior segmentation performance compared to state-of-the-art approaches.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework with dual cross-image semantic consistency and self-aware pseudo-labeling, significantly advancing the state-of-the-art by addressing overlooked aspects like region-level consistency and feature discrepancies in semi-supervised medical image segmentation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of semi-supervised medical image segmentation due to its innovative techniques and strong empirical results on multiple datasets, though its influence may remain confined to specialized applications in computer vision and medical imaging.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution with novel methods and robust validation, making it essential for researchers focused on semi-supervised learning in medical image segmentation to stay informed on advancements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/eea87563c302c3ac8433173def2d7afd434cb4b1",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 5,
      "average_h_index": 2.6666666666666665,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Han Wu",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2213568699"
        },
        {
          "name": "Chong Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2307136332"
        },
        {
          "name": "Zhiming Cui",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2307767474"
        }
      ]
    },
    {
      "id": "2507.21450",
      "title": "Recursive Visual Imagination and Adaptive Linguistic Grounding for\n  Vision Language Navigation",
      "authors": [
        "Bolei Chen",
        "Jiaxu Kang",
        "Yifei Wang",
        "Ping Zhong",
        "Qi Wu",
        "Jianxin Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Vision Language Navigation (VLN) typically requires agents to navigate to\nspecified objects or remote regions in unknown scenes by obeying linguistic\ncommands. Such tasks require organizing historical visual observations for\nlinguistic grounding, which is critical for long-sequence navigational\ndecisions. However, current agents suffer from overly detailed scene\nrepresentation and ambiguous vision-language alignment, which weaken their\ncomprehension of navigation-friendly high-level scene priors and easily lead to\nbehaviors that violate linguistic commands. To tackle these issues, we propose\na navigation policy by recursively summarizing along-the-way visual\nperceptions, which are adaptively aligned with commands to enhance linguistic\ngrounding. In particular, by structurally modeling historical trajectories as\ncompact neural grids, several Recursive Visual Imagination (RVI) techniques are\nproposed to motivate agents to focus on the regularity of visual transitions\nand semantic scene layouts, instead of dealing with misleading geometric\ndetails. Then, an Adaptive Linguistic Grounding (ALG) technique is proposed to\nalign the learned situational memories with different linguistic components\npurposefully. Such fine-grained semantic matching facilitates the accurate\nanticipation of navigation actions and progress. Our navigation policy\noutperforms the state-of-the-art methods on the challenging VLN-CE and\nObjectNav tasks, showing the superiority of our RVI and ALG techniques for VLN.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21450v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21450v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.49,
      "distributed_training_score": 0.305,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Vision Language Navigation using behavior cloning and supervised learning techniques, without any mention of human feedback, reward models, or reinforcement learning to align with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes Recursive Visual Imagination and Adaptive Linguistic Grounding for navigation, involving sequence modeling and neural grids, but does not incorporate diffusion models, iterative refinement, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21453",
      "title": "Validating Pharmacogenomics Generative Artificial Intelligence Query\n  Prompts Using Retrieval-Augmented Generation (RAG)",
      "authors": [
        "Ashley Rector",
        "Keaton Minor",
        "Kamden Minor",
        "Jeff McCormack",
        "Beth Breeden",
        "Ryan Nowers",
        "Jay Dorris"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study evaluated Sherpa Rx, an artificial intelligence tool leveraging\nlarge language models and retrieval-augmented generation (RAG) for\npharmacogenomics, to validate its performance on key response metrics. Sherpa\nRx integrated Clinical Pharmacogenetics Implementation Consortium (CPIC)\nguidelines with Pharmacogenomics Knowledgebase (PharmGKB) data to generate\ncontextually relevant responses. A dataset (N=260 queries) spanning 26 CPIC\nguidelines was used to evaluate drug-gene interactions, dosing recommendations,\nand therapeutic implications. In Phase 1, only CPIC data was embedded. Phase 2\nadditionally incorporated PharmGKB content. Responses were scored on accuracy,\nrelevance, clarity, completeness (5-point Likert scale), and recall. Wilcoxon\nsigned-rank tests compared accuracy between Phase 1 and Phase 2, and between\nPhase 2 and ChatGPT-4omini. A 20-question quiz assessed the tool's real-world\napplicability against other models. In Phase 1 (N=260), Sherpa Rx demonstrated\nhigh performance of accuracy 4.9, relevance 5.0, clarity 5.0, completeness 4.8,\nand recall 0.99. The subset analysis (N=20) showed improvements in accuracy\n(4.6 vs. 4.4, Phase 2 vs. Phase 1 subset) and completeness (5.0 vs. 4.8).\nChatGPT-4omini performed comparably in relevance (5.0) and clarity (4.9) but\nlagged in accuracy (3.9) and completeness (4.2). Differences in accuracy\nbetween Phase 1 and Phase 2 was not statistically significant. However, Phase 2\nsignificantly outperformed ChatGPT-4omini. On the 20-question quiz, Sherpa Rx\nachieved 90% accuracy, outperforming other models. Integrating additional\nresources like CPIC and PharmGKB with RAG enhances AI accuracy and performance.\nThis study highlights the transformative potential of generative AI like Sherpa\nRx in pharmacogenomics, improving decision-making with accurate, personalized\nresponses.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21453v2",
      "pdf_url": "http://arxiv.org/pdf/2507.21453v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.295,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21455",
      "title": "Boost Self-Supervised Dataset Distillation via Parameterization,\n  Predefined Augmentation, and Approximation",
      "authors": [
        "Sheng-Feng Yu",
        "Jia-Jiun Yao",
        "Wei-Chen Chiu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Although larger datasets are crucial for training large deep models, the\nrapid growth of dataset size has brought a significant challenge in terms of\nconsiderable training costs, which even results in prohibitive computational\nexpenses. Dataset Distillation becomes a popular technique recently to reduce\nthe dataset size via learning a highly compact set of representative exemplars,\nwhere the model trained with these exemplars ideally should have comparable\nperformance with respect to the one trained with the full dataset. While most\nof existing works upon dataset distillation focus on supervised datasets, we\ninstead aim to distill images and their self-supervisedly trained\nrepresentations into a distilled set. This procedure, named as Self-Supervised\nDataset Distillation, effectively extracts rich information from real datasets,\nyielding the distilled sets with enhanced cross-architecture generalizability.\nParticularly, in order to preserve the key characteristics of original dataset\nmore faithfully and compactly, several novel techniques are proposed: 1) we\nintroduce an innovative parameterization upon images and representations via\ndistinct low-dimensional bases, where the base selection for parameterization\nis experimentally shown to play a crucial role; 2) we tackle the instability\ninduced by the randomness of data augmentation -- a key component in\nself-supervised learning but being underestimated in the prior work of\nself-supervised dataset distillation -- by utilizing predetermined\naugmentations; 3) we further leverage a lightweight network to model the\nconnections among the representations of augmented views from the same image,\nleading to more compact pairs of distillation. Extensive experiments conducted\non various datasets validate the superiority of our approach in terms of\ndistillation efficiency, cross-architecture generalization, and transfer\nlearning performance.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21455v2",
      "pdf_url": "http://arxiv.org/pdf/2507.21455v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.439,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.429,
      "datasets_score": 0.391,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on self-supervised dataset distillation, which involves compressing datasets for efficient training without relying on labels. It does not address generating labels from noisy or imprecise sources, a core aspect of weak supervision. Thus, there is no connection to programmatically creating training labels.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is on techniques for dataset distillation to reduce training costs, such as parameterization and predefined augmentations, but it does not discuss parallel computing, multi-node setups, or partitioning data/computation across processors. It is solely about dataset compression, not distributed training methods.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21460",
      "title": "An Angular-Temporal Interaction Network for Light Field Object Tracking\n  in Low-Light Scenes",
      "authors": [
        "Mianzhao Wang",
        "Fan Shi",
        "Xu Cheng",
        "Feifei Zhang",
        "Shengyong Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "High-quality 4D light field representation with efficient angular feature\nmodeling is crucial for scene perception, as it can provide discriminative\nspatial-angular cues to identify moving targets. However, recent developments\nstill struggle to deliver reliable angular modeling in the temporal domain,\nparticularly in complex low-light scenes. In this paper, we propose a novel\nlight field epipolar-plane structure image (ESI) representation that explicitly\ndefines the geometric structure within the light field. By capitalizing on the\nabrupt changes in the angles of light rays within the epipolar plane, this\nrepresentation can enhance visual expression in low-light scenes and reduce\nredundancy in high-dimensional light fields. We further propose an\nangular-temporal interaction network (ATINet) for light field object tracking\nthat learns angular-aware representations from the geometric structural cues\nand angular-temporal interaction cues of light fields. Furthermore, ATINet can\nalso be optimized in a self-supervised manner to enhance the geometric feature\ninteraction across the temporal domain. Finally, we introduce a large-scale\nlight field low-light dataset for object tracking. Extensive experimentation\ndemonstrates that ATINet achieves state-of-the-art performance in single object\ntracking. Furthermore, we extend the proposed method to multiple object\ntracking, which also shows the effectiveness of high-quality light field\nangular-temporal modeling.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21460v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21460v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.327,
      "distributed_training_score": 0.333,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21471",
      "title": "An LLM Driven Agent Framework for Automated Infrared Spectral Multi Task\n  Reasoning",
      "authors": [
        "Zujie Xie",
        "Zixuan Chen",
        "Jiheng Liang",
        "Xiangyang Yu",
        "Ziru Yu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Infrared spectroscopy offers rapid, non destructive measurement of chemical\nand material properties but suffers from high dimensional, overlapping spectral\nbands that challenge conventional chemometric approaches. Emerging large\nlanguage models (LLMs), with their capacity for generalization and reasoning,\noffer promising potential for automating complex scientific workflows. Despite\nthis promise, their application in IR spectral analysis remains largely\nunexplored. This study addresses the critical challenge of achieving accurate,\nautomated infrared spectral interpretation under low-data conditions using an\nLLM-driven framework. We introduce an end-to-end, large language model driven\nagent framework that integrates a structured literature knowledge base,\nautomated spectral preprocessing, feature extraction, and multi task reasoning\nin a unified pipeline. By querying a curated corpus of peer reviewed IR\npublications, the agent selects scientifically validated routines. The selected\nmethods transform each spectrum into low dimensional feature sets, which are\nfed into few shot prompt templates for classification, regression, and anomaly\ndetection. A closed loop, multi turn protocol iteratively appends mispredicted\nsamples to the prompt, enabling dynamic refinement of predictions. Across\ndiverse materials: stamp pad ink, Chinese medicine, Pu'er tea, Citri\nReticulatae Pericarpium and waste water COD datasets, the multi turn LLM\nconsistently outperforms single turn inference, rivaling or exceeding machine\nlearning and deep learning models under low data regimes.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21471v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21471v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.418,
      "diffusion_reasoning_score": 0.475,
      "distributed_training_score": 0.371,
      "datasets_score": 0.381,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper describes an LLM-driven framework that operates under low-data conditions, using few-shot prompt templates and a multi-turn protocol to iteratively refine predictions by appending mispredicted samples. This aligns with weak supervision as it relies on programmatically generated or noisy label refinements rather than extensive hand-labeled data, though it does not explicitly focus on generating large quantities of labels from high-level sources.",
      "diffusion_reasoning_justification": "The paper introduces a multi-turn LLM framework for iterative reasoning in spectral analysis, but it does not involve diffusion models or adapt the iterative refinement process of diffusion to logical tasks. There is no mention of treating a Chain-of-Thought as a holistic entity for correction via diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces an LLM-driven agent framework to automate infrared spectral analysis, addressing challenges like high-dimensional data and overlapping bands under low-data conditions, by integrating a literature knowledge base, spectral preprocessing, feature extraction, and multi-task reasoning in a unified pipeline. The methodology employs a closed-loop, multi-turn protocol that iteratively refines predictions by appending mispredicted samples, and evaluations across various datasets demonstrate that this approach outperforms single-turn inference and rivals traditional machine learning and deep learning models in classification, regression, and anomaly detection tasks.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel application of large language models to infrared spectral analysis, which is largely unexplored, by creating an end-to-end framework that advances the state-of-the-art in automated scientific workflows under low-data conditions. This represents a significant innovation in combining LLMs with chemometrics for multi-task reasoning.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfields of AI-driven scientific analysis and spectroscopy, as it demonstrates practical improvements in low-data regimes. However, its influence may be limited to specific applications rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a strong and valuable contribution by innovatively applying LLMs to a challenging domain, making it important for researchers in AI and chemometrics to be aware of. While not essential for all audiences, it offers insights that could inspire further developments in automated scientific reasoning.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/df35098bf9080a8bceee2ae88d87a849eec366ba",
      "total_authors": 5,
      "authors_found": 4,
      "highest_h_index": 2,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zujie Xie",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zixuan Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374494306"
        },
        {
          "name": "Jiheng Liang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2302210536"
        },
        {
          "name": "Xiangyang Yu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2316670665"
        },
        {
          "name": "Ziru Yu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2112220957"
        }
      ]
    },
    {
      "id": "2507.21474",
      "title": "Hebbian Memory-Augmented Recurrent Networks: Engram Neurons in Deep\n  Learning",
      "authors": [
        "Daniel Szelogowski"
      ],
      "categories": [
        "cs.NE (Neural and Evolutionary Computing)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Despite success across diverse tasks, current artificial recurrent network\narchitectures rely primarily on implicit hidden-state memories, limiting their\ninterpretability and ability to model long-range dependencies. In contrast,\nbiological neural systems employ explicit, associative memory traces (i.e.,\nengrams) strengthened through Hebbian synaptic plasticity and activated\nsparsely during recall. Motivated by these neurobiological insights, we\nintroduce the Engram Neural Network (ENN), a novel recurrent architecture\nincorporating an explicit, differentiable memory matrix with Hebbian plasticity\nand sparse, attention-driven retrieval mechanisms. The ENN explicitly models\nmemory formation and recall through dynamic Hebbian traces, improving\ntransparency and interpretability compared to conventional RNN variants. We\nevaluate the ENN architecture on three canonical benchmarks: MNIST digit\nclassification, CIFAR-10 image sequence modeling, and WikiText-103 language\nmodeling. Our empirical results demonstrate that the ENN achieves accuracy and\ngeneralization performance broadly comparable to classical RNN, GRU, and LSTM\narchitectures, with all models converging to similar accuracy and perplexity on\nthe large-scale WikiText-103 task. At the same time, the ENN offers significant\nenhancements in interpretability through observable memory dynamics. Hebbian\ntrace visualizations further reveal biologically plausible, structured memory\nformation processes, validating the potential of neuroscience-inspired\nmechanisms to inform the development of more interpretable and robust deep\nlearning models.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21474v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21474v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.355,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a Hebbian Memory-Augmented Recurrent Network (ENN) inspired by biological neural systems, focusing on explicit memory, Hebbian plasticity, and sparse retrieval for sequence modeling tasks. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21476",
      "title": "Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with\n  HumorBench",
      "authors": [
        "Reuben Narad",
        "Siddharth Suresh",
        "Jiayi Chen",
        "Pine S. L. Dysart-Bricken",
        "Bob Mankoff",
        "Robert Nowak",
        "Jifan Zhang",
        "Lalit Jain"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present HumorBench, a benchmark designed to evaluate large language\nmodels' (LLMs) ability to reason about and explain sophisticated humor in\ncartoon captions. As reasoning models increasingly saturate existing benchmarks\nin mathematics and science, novel and challenging evaluations of model\nintelligence beyond STEM domains are essential. Reasoning is fundamentally\ninvolved in text-based humor comprehension, requiring the identification of\nconnections between concepts in cartoons/captions and external cultural\nreferences, wordplays, and other mechanisms. HumorBench includes approximately\n300 unique cartoon-caption pairs from the New Yorker Caption Contest and\nCartoonstock.com, with expert-annotated evaluation rubrics identifying\nessential joke elements. LLMs are evaluated based on their explanations towards\nthe humor and abilities in identifying the joke elements. To perform well on\nthis task, models must form and test hypotheses about associations between\nconcepts, potentially backtracking from initial interpretations to arrive at\nthe most plausible explanation. Our extensive benchmarking of current SOTA\nmodels reveals three key insights: (1) LLM progress on STEM reasoning transfers\neffectively to humor comprehension; (2) models trained exclusively on STEM\nreasoning data still perform well on HumorBench, demonstrating strong\ntransferability of reasoning abilities; and (3) test-time scaling by increasing\nthinking token budgets yields mixed results across different models in humor\nreasoning.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21476v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21476v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.283,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces HumorBench, a benchmark for evaluating LLMs' humor comprehension and reasoning abilities, focusing on tasks like explaining jokes and identifying elements. It discusses general reasoning transfer from STEM domains and test-time scaling, but does not mention or involve diffusion-based models, iterative refinement processes, or adapting diffusion for logical tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21479",
      "title": "Capacity-Constrained Continual Learning",
      "authors": [
        "Zheng Wen",
        "Doina Precup",
        "Benjamin Van Roy",
        "Satinder Singh"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.IT (Information Theory)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)",
        "math.IT (Information Theory)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Any agents we can possibly build are subject to capacity constraints, as\nmemory and compute resources are inherently finite. However, comparatively\nlittle attention has been dedicated to understanding how agents with limited\ncapacity should allocate their resources for optimal performance. The goal of\nthis paper is to shed some light on this question by studying a simple yet\nrelevant continual learning problem: the capacity-constrained\nlinear-quadratic-Gaussian (LQG) sequential prediction problem. We derive a\nsolution to this problem under appropriate technical conditions. Moreover, for\nproblems that can be decomposed into a set of sub-problems, we also demonstrate\nhow to optimally allocate capacity across these sub-problems in the steady\nstate. We view the results of this paper as a first step in the systematic\ntheoretical study of learning under capacity constraints.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21479v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21479v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.37,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21482",
      "title": "Improving Task Diversity in Label Efficient Supervised Finetuning of\n  LLMs",
      "authors": [
        "Abhinav Arabelly",
        "Jagrut Nemade",
        "Robert D Nowak",
        "Jifan Zhang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse domains, but developing high-performing models for specialized\napplications often requires substantial human annotation -- a process that is\ntime-consuming, labor-intensive, and expensive. In this paper, we address the\nlabel-efficient learning problem for supervised finetuning (SFT) by leveraging\ntask-diversity as a fundamental principle for effective data selection. This is\nmarkedly different from existing methods based on the prompt-diversity. Our\napproach is based on two key observations: 1) task labels for different prompts\nare often readily available; 2) pre-trained models have significantly varying\nlevels of confidence across tasks. We combine these facts to devise a simple\nyet effective sampling strategy: we select examples across tasks using an\ninverse confidence weighting strategy. This produces models comparable to or\nbetter than those trained with more complex sampling procedures, while being\nsignificantly easier to implement and less computationally intensive. Notably,\nour experimental results demonstrate that this method can achieve better\naccuracy than training on the complete dataset (a 4\\% increase in MMLU score).\nAcross various annotation budgets and two instruction finetuning datasets, our\nalgorithm consistently performs at or above the level of the best existing\nmethods, while reducing annotation costs by up to 80\\%.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21482v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21482v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.47,
      "weak_supervision_score": 0.531,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.427,
      "datasets_score": 0.407,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on supervised finetuning with data selection based on task diversity and confidence weighting, without involving reinforcement learning, reward models, or human feedback for alignment. It relies on annotations for SFT, not RLHF processes.",
      "weak_supervision_justification": "The paper addresses label-efficient learning by strategically selecting data for annotation, which indirectly reduces reliance on fully hand-labeled data. However, it still uses human annotations rather than programmatically generating labels from noisy sources, making it only loosely connected to weak supervision.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for reasoning, or multi-step logical processes. It centers on data selection for supervised finetuning of LLMs, with no components related to diffusion-based approaches.",
      "distributed_training_justification": "The paper's main contribution is a data selection strategy for finetuning, with no discussion of parallel computing, multi-node systems, or partitioning data/computation across processors. It focuses solely on improving SFT efficiency.",
      "datasets_justification": "The paper involves analyzing and selecting subsets from existing datasets (e.g., FLAN, Dolly) for finetuning, which relates to dataset curation and evaluation. However, it does not primarily focus on creating new datasets or comprehensive benchmarking, limiting its relevance.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of label-efficient supervised finetuning for Large Language Models (LLMs) by introducing a data selection strategy based on task diversity, which prioritizes examples from tasks where the pre-trained model exhibits lower confidence using an inverse confidence weighting approach. The methodology leverages readily available task labels to ensure comprehensive coverage while focusing resources on uncertain areas, resulting in superior model performance—such as a 4% increase in MMLU scores—compared to training on full datasets, and achieving this with up to 80% reduction in annotation costs across various budgets and datasets.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining task diversity with inverse confidence weighting for data selection, offering a clever alternative to existing prompt-diversity methods without introducing an entirely new problem or technique. While it advances state-of-the-art efficiency in label-efficient learning, it builds on established concepts rather than creating a groundbreaking innovation.",
      "impact_score": "High",
      "impact_justification": "The work's potential to significantly reduce annotation costs by up to 80% while improving performance could influence a broad range of future research in LLM finetuning and commercial applications, making efficient model development more accessible. This positions it as a valuable contribution that may be widely adopted in the AI community.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a practical and effective method for label-efficient finetuning that outperforms existing approaches, making it a valuable resource for researchers and practitioners in AI and machine learning focused on efficient model training. Its clear benefits in reducing costs and improving outcomes justify its significance without it being essential for all readers.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7fdf8867261a1741bcf284e2d7b83758783d0bc8",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 7,
      "average_h_index": 3.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Abhinav Arabelly",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2153993557"
        },
        {
          "name": "Jagrut Nemade",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2143514103"
        },
        {
          "name": "Robert D Nowak",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2262322578"
        },
        {
          "name": "Jifan Zhang",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2108131127"
        }
      ]
    },
    {
      "id": "2507.21483",
      "title": "NCCR: to Evaluate the Robustness of Neural Networks and Adversarial\n  Examples",
      "authors": [
        "Shi Pu",
        "Fu Song",
        "Wenjie Wang"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Neural networks have received a lot of attention recently, and related\nsecurity issues have come with it. Many studies have shown that neural networks\nare vulnerable to adversarial examples that have been artificially perturbed\nwith modification, which is too small to be distinguishable by human\nperception. Different attacks and defenses have been proposed to solve these\nproblems, but there is little research on evaluating the robustness of neural\nnetworks and their inputs. In this work, we propose a metric called the neuron\ncover change rate (NCCR) to measure the ability of deep learning models to\nresist attacks and the stability of adversarial examples. NCCR monitors\nalterations in the output of specifically chosen neurons when the input is\nperturbed, and networks with a smaller degree of variation are considered to be\nmore robust. The results of the experiment on image recognition and the speaker\nrecognition model show that our metrics can provide a good assessment of the\nrobustness of neural networks or their inputs. It can also be used to detect\nwhether an input is adversarial or not, as adversarial examples are always less\nrobust.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21483v2",
      "pdf_url": "http://arxiv.org/pdf/2507.21483v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.354,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21485",
      "title": "HLSDebugger: Identification and Correction of Logic Bugs in HLS Code\n  with LLM Solutions",
      "authors": [
        "Jing Wang",
        "Shang Liu",
        "Yao Lu",
        "Zhiyao Xie"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "High-level synthesis (HLS) accelerates hardware design by enabling the\nautomatic translation of high-level descriptions into efficient hardware\nimplementations. However, debugging HLS code is a challenging and\nlabor-intensive task, especially for novice circuit designers or software\nengineers without sufficient hardware domain knowledge. The recent emergence of\nLarge Language Models (LLMs) is promising in automating the HLS debugging\nprocess. Despite the great potential, three key challenges persist when\napplying LLMs to HLS logic debugging: 1) High-quality circuit data for training\nLLMs is scarce, posing a significant challenge. 2) Debugging logic bugs in\nhardware is inherently more complex than identifying software bugs with\nexisting golden test cases. 3) The absence of reliable test cases requires\nmulti-tasking solutions, performing both bug identification and correction.\ncomplicates the multi-tasking required for effective HLS debugging. In this\nwork, we propose a customized solution named HLSDebugger to address the\nchallenges. HLSDebugger first generates and releases a large labeled dataset\nwith 300K data samples, targeting HLS logic bugs. The HLSDebugger model adopts\nan encoder-decoder structure, performing bug location identification, bug type\nprediction, and bug correction with the same model. HLSDebugger significantly\noutperforms advanced LLMs like GPT-4 in bug identification and by more than 3x\nin bug correction. It makes a substantial advancement in the exploration of\nautomated debugging of HLS code.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21485v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21485v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.363,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21488",
      "title": "Learning to Imitate with Less: Efficient Individual Behavior Modeling in\n  Chess",
      "authors": [
        "Zhenwei Tang",
        "Difan Jiao",
        "Eric Xue",
        "Reid McIlroy-Young",
        "Jon Kleinberg",
        "Siddhartha Sen",
        "Ashton Anderson"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As humans seek to collaborate with, learn from, and better understand\nartificial intelligence systems, developing AIs that can accurately emulate\nindividual decision-making becomes increasingly important. Chess, a\nlong-standing AI benchmark with precise skill measurement, offers an ideal\ntestbed for human-AI alignment. However, existing approaches to modeling human\nbehavior require prohibitively large amounts of data from each individual,\nmaking them impractical for new or sparsely represented users. In this work, we\nintroduce Maia4All, a framework designed to learn and adapt to individual\ndecision-making styles efficiently, even with limited data. Maia4All achieves\nthis through a two-stage optimization process: (1) an enrichment step, which\nbridges population and individual-level human behavior modeling with a\nprototype-enriched model, and (2) a democratization step, which leverages\nability levels or user prototypes to initialize and refine individual\nembeddings with minimal data. Our experimental results show that Maia4All can\naccurately predict individual moves and profile behavioral patterns with high\nfidelity, establishing a new standard for personalized human-like AI behavior\nmodeling in chess. Maia4All achieves individual human behavior modeling in\nchess with only 20 games, compared to the 5,000 games required previously,\nrepresenting a significant improvement in data efficiency. Our work provides an\nexample of how population AI systems can flexibly adapt to individual users\nusing a prototype-enriched model as a bridge. This approach extends beyond\nchess, as shown in our case study on idiosyncratic LLMs, highlighting its\npotential for broader applications in personalized AI adaptation.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21488v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21488v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.45,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.348,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework for efficient imitation learning in chess, using two-stage fine-tuning on human move data to model individual behavior. It does not involve training a separate reward model on human-ranked data or using reinforcement learning to align AI with human preferences, which are core elements of RLHF. Instead, it relies on supervised adaptation techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21489",
      "title": "Describe, Adapt and Combine: Empowering CLIP Encoders for Open-set 3D\n  Object Retrieval",
      "authors": [
        "Zhichuan Wang",
        "Yang Zhou",
        "Zhe Liu",
        "Rui Yu",
        "Song Bai",
        "Yulong Wang",
        "Xinwei He",
        "Xiang Bai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Open-set 3D object retrieval (3DOR) is an emerging task aiming to retrieve 3D\nobjects of unseen categories beyond the training set. Existing methods\ntypically utilize all modalities (i.e., voxels, point clouds, multi-view\nimages) and train specific backbones before fusion. However, they still\nstruggle to produce generalized representations due to insufficient 3D training\ndata. Being contrastively pre-trained on web-scale image-text pairs, CLIP\ninherently produces generalized representations for a wide range of downstream\ntasks. Building upon it, we present a simple yet effective framework named\nDescribe, Adapt and Combine (DAC) by taking only multi-view images for open-set\n3DOR. DAC innovatively synergizes a CLIP model with a multi-modal large\nlanguage model (MLLM) to learn generalized 3D representations, where the MLLM\nis used for dual purposes. First, it describes the seen category information to\nalign with CLIP's training objective for adaptation during training. Second, it\nprovides external hints about unknown objects complementary to visual cues\nduring inference. To improve the synergy, we introduce an Additive-Bias\nLow-Rank adaptation (AB-LoRA), which alleviates overfitting and further\nenhances the generalization to unseen categories. With only multi-view images,\nDAC significantly surpasses prior arts by an average of +10.01\\% mAP on four\nopen-set 3DOR datasets. Moreover, its generalization is also validated on\nimage-based and cross-dataset setups. Code is available at\nhttps://github.com/wangzhichuan123/DAC.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21489v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21489v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.352,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21500",
      "title": "VN-MTEB: Vietnamese Massive Text Embedding Benchmark",
      "authors": [
        "Loc Pham",
        "Tung Luu",
        "Thu Vo",
        "Minh Nguyen",
        "Viet Hoang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Vietnam ranks among the top countries in terms of both internet traffic and\nonline toxicity. As a result, implementing embedding models for recommendation\nand content control duties in applications is crucial. However, a lack of\nlarge-scale test datasets, both in volume and task diversity, makes it tricky\nfor scientists to effectively evaluate AI models before deploying them in\nreal-world, large-scale projects. To solve this important problem, we introduce\na Vietnamese benchmark, VN-MTEB for embedding models, which we created by\ntranslating a large number of English samples from the Massive Text Embedding\nBenchmark using our new automated framework. We leverage the strengths of large\nlanguage models (LLMs) and cutting-edge embedding models to conduct translation\nand filtering processes to retain high-quality samples, guaranteeing a natural\nflow of language and semantic fidelity while preserving named entity\nrecognition (NER) and code snippets. Our comprehensive benchmark consists of 41\ndatasets from six tasks specifically designed for Vietnamese text embeddings.\nIn our analysis, we find that bigger and more complex models using Rotary\nPositional Embedding outperform those using Absolute Positional Embedding in\nembedding tasks. Datasets are available at HuggingFace:\nhttps://huggingface.co/collections/GreenNode/vn-mteb-68871433f0f7573b8e1a6686",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21500v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21500v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.35,
      "datasets_score": 0.458,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of VN-MTEB, a new benchmark dataset for Vietnamese text embeddings, which involves creating, curating, and evaluating datasets for AI applications. It details methodologies for dataset translation and filtering using LLMs, analyzes dataset quality, and benchmarks models on 41 datasets across six tasks, directly aligning with research on dataset creation, curation, benchmarking, and analysis in machine learning.",
      "llm_score_status": "completed",
      "summary": "This paper introduces VN-MTEB, a comprehensive benchmark for evaluating text embedding models specifically for the Vietnamese language, addressing the scarcity of large-scale datasets in low-resource languages by translating and filtering English samples from the Massive Text Embedding Benchmark using advanced large language models and embedding techniques to ensure high quality and semantic fidelity. The benchmark includes 41 datasets across six tasks such as retrieval, classification, and semantic similarity, and through evaluations, the authors reveal that larger models employing Rotary Positional Embedding outperform those using Absolute Positional Embedding, offering insights into model performance for Vietnamese NLP applications.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting existing benchmarking and translation methods to create a new, automated framework for a low-resource language like Vietnamese, combining ideas in a clever way to address a specific gap. While not introducing a entirely new problem, it advances the state-of-the-art for language-specific benchmarks.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of low-resource language NLP, particularly for Vietnamese, by providing a standardized benchmark that facilitates model evaluations and integrations with existing frameworks like MTEB. However, its influence may be limited to specific linguistic contexts rather than broader commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution by filling a critical gap in benchmarks for low-resource languages, making it essential for researchers in Vietnamese NLP or similar areas to be aware of for advancing their work. While not universally groundbreaking, its practical utility and insights justify reading for relevant audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2e85341d2acc461ec1d5fce33b88484e8048ef5c",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Loc Pham",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373728513"
        },
        {
          "name": "Tung Luu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373726830"
        },
        {
          "name": "Thu Vo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373729032"
        },
        {
          "name": "Minh Nguyen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375351470"
        },
        {
          "name": "Viet Hoang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373726640"
        }
      ]
    },
    {
      "id": "2507.21502",
      "title": "Large Language Models for Supply Chain Decisions",
      "authors": [
        "David Simchi-Levi",
        "Konstantina Mellou",
        "Ishai Menache",
        "Jeevan Pathuri"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Supply Chain Management requires addressing a variety of complex\ndecision-making challenges, from sourcing strategies to planning and execution.\nOver the last few decades, advances in computation and information technologies\nhave enabled the transition from manual, intuition and experience-based\ndecision-making, into more automated and data-driven decisions using a variety\nof tools that apply optimization techniques. These techniques use mathematical\nmethods to improve decision-making.\n  Unfortunately, business planners and executives still need to spend\nconsiderable time and effort to (i) understand and explain the recommendations\ncoming out of these technologies; (ii) analyze various scenarios and answer\nwhat-if questions; and (iii) update the mathematical models used in these tools\nto reflect current business environments. Addressing these challenges requires\ninvolving data science teams and/or the technology providers to explain results\nor make the necessary changes in the technology and hence significantly slows\ndown decision making.\n  Motivated by the recent advances in Large Language Models (LLMs), we report\nhow this disruptive technology can democratize supply chain technology -\nnamely, facilitate the understanding of tools' outcomes, as well as the\ninteraction with supply chain tools without human-in-the-loop. Specifically, we\nreport how we apply LLMs to address the three challenges described above, thus\nsubstantially reducing the time to decision from days and weeks to minutes and\nhours as well as dramatically increasing planners' and executives' productivity\nand impact.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21502v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21502v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.366,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper discusses applying Large Language Models (LLMs) to supply chain management for decision-making, focusing on interaction and understanding of tools. It does not mention or involve weak supervision techniques, such as programmatically generating training labels from noisy sources, in the training or application of models.",
      "diffusion_reasoning_justification": "The paper explores the use of LLMs to address supply chain challenges, including scenario analysis and decision-making, but does not reference diffusion-based models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity. There is no indication of multi-step logical reasoning via diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21503",
      "title": "MoHoBench: Assessing Honesty of Multimodal Large Language Models via\n  Unanswerable Visual Questions",
      "authors": [
        "Yanxu Zhu",
        "Shitong Duan",
        "Xiangxu Zhang",
        "Jitao Sang",
        "Peng Zhang",
        "Tun Lu",
        "Xiao Zhou",
        "Jing Yao",
        "Xiaoyuan Yi",
        "Xing Xie"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recently Multimodal Large Language Models (MLLMs) have achieved considerable\nadvancements in vision-language tasks, yet produce potentially harmful or\nuntrustworthy content. Despite substantial work investigating the\ntrustworthiness of language models, MMLMs' capability to act honestly,\nespecially when faced with visually unanswerable questions, remains largely\nunderexplored. This work presents the first systematic assessment of honesty\nbehaviors across various MLLMs. We ground honesty in models' response behaviors\nto unanswerable visual questions, define four representative types of such\nquestions, and construct MoHoBench, a large-scale MMLM honest benchmark,\nconsisting of 12k+ visual question samples, whose quality is guaranteed by\nmulti-stage filtering and human verification. Using MoHoBench, we benchmarked\nthe honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our\nfindings show that: (1) most models fail to appropriately refuse to answer when\nnecessary, and (2) MMLMs' honesty is not solely a language modeling issue, but\nis deeply influenced by visual information, necessitating the development of\ndedicated methods for multimodal honesty alignment. Therefore, we implemented\ninitial alignment methods using supervised and preference learning to improve\nhonesty behavior, providing a foundation for future work on trustworthy MLLMs.\nOur data and code can be found at https://github.com/DSTTSD/MoHoBench.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21503v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21503v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.298,
      "datasets_score": 0.381,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper implements initial alignment methods using preference learning, which aligns with RLHF by incorporating human preferences to fine-tune models. However, it primarily focuses on benchmarking honesty in MLLMs rather than a full RLHF system, and does not explicitly detail a separate reward model or reinforcement learning process.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for reasoning, or multi-step logical processes; it centers on assessing and improving honesty in MLLMs through benchmarks and alignment techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces MoHoBench, a large-scale benchmark designed to evaluate the honesty of Multimodal Large Language Models (MLLMs) when responding to unanswerable visual questions, by defining four types of such questions and compiling over 12,000 verified samples. The authors benchmark 28 popular MLLMs, revealing that most fail to appropriately refuse answers and that honesty is influenced by visual information, leading them to propose initial alignment methods via supervised and preference learning to enhance trustworthy behavior in MLLMs.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel benchmark and systematic assessment for honesty in MLLMs, addressing an underexplored problem in multimodal AI trustworthiness that advances the state-of-the-art.",
      "impact_score": "Moderate",
      "impact_justification": "The work provides a specialized benchmark and methods that are likely to be cited and built upon in the subfield of trustworthy multimodal AI, influencing research on model alignment and safety.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a high-quality contribution to AI safety and multimodal model evaluation, making it essential for researchers focused on trustworthy AI to be aware of its findings and resources.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/9dc3caf9c466e7667f64c4353cb2b3fb34a183b4",
      "total_authors": 10,
      "authors_found": 10,
      "highest_h_index": 9,
      "average_h_index": 4.1,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Yanxu Zhu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2268499891"
        },
        {
          "name": "Shitong Duan",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2258959896"
        },
        {
          "name": "Xiangxu Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2328024849"
        },
        {
          "name": "Jitao Sang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373728423"
        },
        {
          "name": "Peng Zhang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2290178596"
        },
        {
          "name": "Tun Lu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2284552265"
        },
        {
          "name": "Xiao Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373994142"
        },
        {
          "name": "Jing Yao",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2237129499"
        },
        {
          "name": "Xiaoyuan Yi",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2258961742"
        },
        {
          "name": "Xing Xie",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2289847313"
        }
      ]
    },
    {
      "id": "2507.21504",
      "title": "Evaluation and Benchmarking of LLM Agents: A Survey",
      "authors": [
        "Mahmoud Mohammadi",
        "Yipeng Li",
        "Jane Lo",
        "Wendy Yip"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rise of LLM-based agents has opened new frontiers in AI applications, yet\nevaluating these agents remains a complex and underdeveloped area. This survey\nprovides an in-depth overview of the emerging field of LLM agent evaluation,\nintroducing a two-dimensional taxonomy that organizes existing work along (1)\nevaluation objectives -- what to evaluate, such as agent behavior,\ncapabilities, reliability, and safety -- and (2) evaluation process -- how to\nevaluate, including interaction modes, datasets and benchmarks, metric\ncomputation methods, and tooling. In addition to taxonomy, we highlight\nenterprise-specific challenges, such as role-based access to data, the need for\nreliability guarantees, dynamic and long-horizon interactions, and compliance,\nwhich are often overlooked in current research. We also identify future\nresearch directions, including holistic, more realistic, and scalable\nevaluation. This work aims to bring clarity to the fragmented landscape of\nagent evaluation and provide a framework for systematic assessment, enabling\nresearchers and practitioners to evaluate LLM agents for real-world deployment.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21504v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21504v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.324,
      "datasets_score": 0.414,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper is a survey on evaluating and benchmarking LLM agents, focusing on taxonomies, objectives, and processes for assessment. It does not mention or involve reinforcement learning, human feedback, or techniques for aligning models with preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper discusses datasets and benchmarks as part of the evaluation process in its taxonomy for LLM agents, including how they are used for assessment. However, it does not primarily focus on creating, analyzing, or curating datasets, but rather on their role in broader evaluation frameworks.",
      "llm_score_status": "completed",
      "summary": "This survey paper provides a comprehensive overview of the evaluation and benchmarking of Large Language Model (LLM)-based agents, introducing a two-dimensional taxonomy that categorizes evaluation along objectives—such as agent behavior, capabilities, reliability, and safety—and processes—including interaction modes, datasets, benchmarks, metrics, and tooling. It highlights enterprise-specific challenges like role-based data access, reliability guarantees, long-horizon interactions, and compliance, while outlining future research directions to foster more holistic, realistic, and scalable assessments, aiming to guide researchers and practitioners toward effective real-world deployment of LLM agents.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new two-dimensional taxonomy for organizing LLM agent evaluation, which cleverly combines and structures existing ideas to address a fragmented field. While it advances the state-of-the-art in evaluation frameworks, it primarily builds on known problems rather than introducing a entirely new one.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of AI agent evaluation, providing a useful framework for researchers and practitioners. However, its influence may be limited to specific applications in enterprise settings and not extend broadly to other areas of AI.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, structured contribution to the understanding of LLM agent evaluation, making it valuable for those working in AI research and development. It is not essential for all readers but is recommended for specialists in agent evaluation to stay informed on key challenges and frameworks.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a56efef88a8eb94d9c9704f279c254c1bf4a88ab",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Mahmoud Mohammadi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373735728"
        },
        {
          "name": "Yipeng Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373743367"
        },
        {
          "name": "Jane Lo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373735713"
        },
        {
          "name": "Wendy Yip",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373733971"
        }
      ]
    },
    {
      "id": "2507.21506",
      "title": "Decision Transformer-Based Drone Trajectory Planning with Dynamic\n  Safety-Efficiency Trade-Offs",
      "authors": [
        "Chang-Hun Ji",
        "SiWoon Song",
        "Youn-Hee Han",
        "SungTae Moon"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "A drone trajectory planner should be able to dynamically adjust the\nsafety-efficiency trade-off according to varying mission requirements in\nunknown environments. Although traditional polynomial-based planners offer\ncomputational efficiency and smooth trajectory generation, they require expert\nknowledge to tune multiple parameters to adjust this trade-off. Moreover, even\nwith careful tuning, the resulting adjustment may fail to achieve the desired\ntrade-off. Similarly, although reinforcement learning-based planners are\nadaptable in unknown environments, they do not explicitly address the\nsafety-efficiency trade-off. To overcome this limitation, we introduce a\nDecision Transformer-based trajectory planner that leverages a single\nparameter, Return-to-Go (RTG), as a \\emph{temperature parameter} to dynamically\nadjust the safety-efficiency trade-off. In our framework, since RTG intuitively\nmeasures the safety and efficiency of a trajectory, RTG tuning does not require\nexpert knowledge. We validate our approach using Gazebo simulations in both\nstructured grid and unstructured random environments. The experimental results\ndemonstrate that our planner can dynamically adjust the safety-efficiency\ntrade-off by simply tuning the RTG parameter. Furthermore, our planner\noutperforms existing baseline methods across various RTG settings, generating\nsafer trajectories when tuned for safety and more efficient trajectories when\ntuned for efficiency. Real-world experiments further confirm the reliability\nand practicality of our proposed planner.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21506v2",
      "pdf_url": "http://arxiv.org/pdf/2507.21506v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.299,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.375,
      "datasets_score": 0.254,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a Decision Transformer-based drone trajectory planner that uses Return-to-Go (RTG) to adjust safety-efficiency trade-offs. While it employs reinforcement learning concepts, it does not involve human feedback, such as training a separate reward model on human-ranked data or fine-tuning based on human preferences. The system relies on simulated data and a programmatically designed reward function, which does not align with the definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21507",
      "title": "VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly\n  Grounding and Understanding",
      "authors": [
        "Shibo Gao",
        "Peipei Yang",
        "Yangyang Liu",
        "Yi Chen",
        "Han Zhu",
        "Xuyao Zhang",
        "Linlin Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Video Anomaly Detection (VAD) aims to identify anomalous events in videos and\naccurately determine their time intervals. Current VAD methods mainly fall into\ntwo categories: traditional DNN-based approaches that focus on temporal\nlocalization, and LLM-based approaches that emphasize semantic understanding.\nBoth anomaly understanding and grounding are essential for comprehensive video\nanomaly detection and can complement each other. However, no existing model or\ndataset supports both tasks simultaneously. To address this, we introduce VAGU\n(Video Anomaly Grounding and Understanding), the first benchmark to integrate\nboth tasks. Each VAGU instance includes annotations for anomaly category,\nsemantic explanation, precise temporal grounding and Video QA. We also provide\nmultiple-choice Video QA for objective evaluation. Based on this dataset, we\npropose Glance then Scrutinize (GtS), a training-free framework guided by\ntextual prompts. The framework first enables coarse localization of\nhigh-probability anomalous regions, followed by detailed anomaly interpretation\nand temporal boundary refinement. Additionally, we propose the JeAUG metric,\nwhich jointly evaluates semantic interpretability and temporal precision,\novercoming the limitations of traditional metrics. Extensive experiments verify\nthe effectiveness of our benchmark, framework, and evaluation metric.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21507v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21507v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.342,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21513",
      "title": "What Does it Mean for a Neural Network to Learn a \"World Model\"?",
      "authors": [
        "Kenneth Li",
        "Fernanda Viégas",
        "Martin Wattenberg"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "We propose a set of precise criteria for saying a neural net learns and uses\na \"world model.\" The goal is to give an operational meaning to terms that are\noften used informally, in order to provide a common language for experimental\ninvestigation. We focus specifically on the idea of representing a latent\n\"state space\" of the world, leaving modeling the effect of actions to future\nwork. Our definition is based on ideas from the linear probing literature, and\nformalizes the notion of a computation that factors through a representation of\nthe data generation process. An essential addition to the definition is a set\nof conditions to check that such a \"world model\" is not a trivial consequence\nof the neural net's data or task.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21513v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21513v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.477,
      "distributed_training_score": 0.348,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses neural networks and world models in the context of reinforcement learning concepts, such as representing state spaces, but it does not involve human feedback, reward models, or the specific RLHF process of fine-tuning models with human-ranked data. The reference to reinforcement learning is indirect and not central to the paper's main contribution.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on defining criteria for neural networks to learn world models using concepts like linear probing and homomorphic images, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. There is no component related to diffusion-based methods for tasks like Chain-of-Thought processing.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21516",
      "title": "ST-DAI: Single-shot 2.5D Spatial Transcriptomics with Intra-Sample\n  Domain Adaptive Imputation for Cost-efficient 3D Reconstruction",
      "authors": [
        "Jiahe Qian",
        "Yaoyu Fang",
        "Xinkun Wang",
        "Lee A. Cooper",
        "Bo Zhou"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "For 3D spatial transcriptomics (ST), the high per-section acquisition cost of\nfully sampling every tissue section remains a significant challenge. Although\nrecent approaches predict gene expression from histology images, these methods\nrequire large external datasets, which leads to high-cost and suffers from\nsubstantial domain discrepancies that lead to poor generalization on new\nsamples. In this work, we introduce ST-DAI, a single-shot framework for 3D ST\nthat couples a cost-efficient 2.5D sampling scheme with an intra-sample\ndomain-adaptive imputation framework. First, in the cost-efficient 2.5D\nsampling stage, one reference section (central section) is fully sampled while\nother sections (adjacent sections) is sparsely sampled, thereby capturing\nvolumetric context at significantly reduced experimental cost. Second, we\npropose a single-shot 3D imputation learning method that allows us to generate\nfully sampled 3D ST from this cost-efficient 2.5D ST scheme, using only\nsample-specific training. We observe position misalignment and domain\ndiscrepancy between sections. To address those issues, we adopt a pipeline that\nfirst aligns the central section to the adjacent section, thereafter generates\ndense pseudo-supervision on the central section, and then performs Fast\nMulti-Domain Refinement (FMDR), which adapts the network to the domain of the\nadjacent section while fine-tuning only a few parameters through the use of\nParameter-Efficient Domain-Alignment Layers (PDLs). During this refinement, a\nConfidence Score Generator (CSG) reweights the pseudo-labels according to their\nestimated reliability, thereby directing imputation toward trustworthy regions.\nOur experimental results demonstrate that ST-DAI achieves gene expression\nprediction performance comparable to fully sampled approaches while\nsubstantially reducing the measurement burden.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21516v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21516v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.402,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a framework for cost-efficient 3D spatial transcriptomics using 2.5D sampling and intra-sample domain-adaptive imputation, focusing on gene expression prediction and domain adaptation. It does not discuss distributed training, parallel computing, multi-node machine learning, or any methods for partitioning data or computation across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21518",
      "title": "ST-GDance: Long-Term and Collision-Free Group Choreography from Music",
      "authors": [
        "Jing Xu",
        "Weiqiang Wang",
        "Cunjian Chen",
        "Jun Liu",
        "Qiuhong Ke"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Group dance generation from music has broad applications in film, gaming, and\nanimation production. However, it requires synchronizing multiple dancers while\nmaintaining spatial coordination. As the number of dancers and sequence length\nincrease, this task faces higher computational complexity and a greater risk of\nmotion collisions. Existing methods often struggle to model dense\nspatial-temporal interactions, leading to scalability issues and multi-dancer\ncollisions. To address these challenges, we propose ST-GDance, a novel\nframework that decouples spatial and temporal dependencies to optimize\nlong-term and collision-free group choreography. We employ lightweight graph\nconvolutions for distance-aware spatial modeling and accelerated sparse\nattention for efficient temporal modeling. This design significantly reduces\ncomputational costs while ensuring smooth and collision-free interactions.\nExperiments on the AIOZ-GDance dataset demonstrate that ST-GDance outperforms\nstate-of-the-art baselines, particularly in generating long and coherent group\ndance sequences. Project page: https://yilliajing.github.io/ST-GDance-Website/.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21518v2",
      "pdf_url": "http://arxiv.org/pdf/2507.21518v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.278,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.369,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on generating long-term, collision-free group dance sequences from music using a novel framework that decouples spatial and temporal dependencies. While it references diffusion-based models like TCDiff and DiT for motion generation, it adapts these primarily for efficient choreography tasks, not for solving complex logical tasks or treating a Chain-of-Thought as a single entity for iterative reasoning. The core contributions involve graph convolutions and sparse attention for dance coordination, which do not align with diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21521",
      "title": "Optimizing Active Learning in Vision-Language Models via\n  Parameter-Efficient Uncertainty Calibration",
      "authors": [
        "Athmanarayanan Lakshmi Narayanan",
        "Amrutha Machireddy",
        "Ranganath Krishnan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Active Learning (AL) has emerged as a powerful approach for minimizing\nlabeling costs by selectively sampling the most informative data for neural\nnetwork model development. Effective AL for large-scale vision-language models\nnecessitates addressing challenges in uncertainty estimation and efficient\nsampling given the vast number of parameters involved. In this work, we\nintroduce a novel parameter-efficient learning methodology that incorporates\nuncertainty calibration loss within the AL framework. We propose a\ndifferentiable loss function that promotes uncertainty calibration for\neffectively selecting fewer and most informative data samples for fine-tuning.\nThrough extensive experiments across several datasets and vision backbones, we\ndemonstrate that our solution can match and exceed the performance of complex\nfeature-based sampling techniques while being computationally very efficient.\nAdditionally, we investigate the efficacy of Prompt learning versus Low-rank\nadaptation (LoRA) in sample selection, providing a detailed comparative\nanalysis of these methods in the context of efficient AL.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21521v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21521v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.426,
      "weak_supervision_score": 0.443,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.357,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Active Learning for Vision-Language Models, emphasizing uncertainty calibration and efficient sampling for labeling. It does not involve training a reward model on human-ranked data or using reinforcement learning to align models with human preferences, which are key elements of RLHF.",
      "weak_supervision_justification": "The paper's main contribution is a method for selecting informative data samples in Active Learning, relying on human or manual labeling processes. It does not address programmatically generating labels from noisy or imprecise sources, which defines Weak Supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21524",
      "title": "Large Language Models for Wireless Communications: From Adaptation to\n  Autonomy",
      "authors": [
        "Le Liang",
        "Hao Ye",
        "Yucheng Sheng",
        "Ouya Wang",
        "Jiacheng Wang",
        "Shi Jin",
        "Geoffrey Ye Li"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.IT (Information Theory)",
        "math.IT (Information Theory)"
      ],
      "abstract": "The emergence of large language models (LLMs) has revolutionized artificial\nintelligence, offering unprecedented capabilities in reasoning, generalization,\nand zero-shot learning. These strengths open new frontiers in wireless\ncommunications, where increasing complexity and dynamics demand intelligent and\nadaptive solutions. This article explores the role of LLMs in transforming\nwireless systems across three key directions: adapting pretrained LLMs for core\ncommunication tasks, developing wireless-specific foundation models to balance\nversatility and efficiency, and enabling agentic LLMs with autonomous reasoning\nand coordination capabilities. We highlight recent advances, practical case\nstudies, and the unique benefits of LLM-based approaches over traditional\nmethods. Finally, we outline open challenges and research opportunities,\nincluding multimodal fusion, collaboration with lightweight models, and\nself-improving capabilities, charting a path toward intelligent, adaptive, and\nautonomous wireless networks of the future.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21524v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21524v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.434,
      "weak_supervision_score": 0.402,
      "diffusion_reasoning_score": 0.467,
      "distributed_training_score": 0.418,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on adapting LLMs for wireless communications tasks, such as beam prediction and autonomous reasoning, but does not mention training models with human feedback, reward models, or reinforcement learning based on human preferences.",
      "weak_supervision_justification": "The paper discusses adapting pretrained LLMs and developing wireless-specific models using large-scale data, but it does not address programmatically generating labels from noisy or imprecise sources, nor does it emphasize weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper explores LLMs for tasks like reasoning in wireless systems, but it does not involve diffusion models, iterative refinement processes, or treating reasoning paths as entities for multi-step correction.",
      "distributed_training_justification": "The paper mentions challenges with LLMs' computational intensity and efficiency in wireless contexts, but it does not discuss distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21529",
      "title": "Chain-of-Cooking:Cooking Process Visualization via Bidirectional\n  Chain-of-Thought Guidance",
      "authors": [
        "Mengling Xu",
        "Ming Tao",
        "Bing-Kun Bao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Cooking process visualization is a promising task in the intersection of\nimage generation and food analysis, which aims to generate an image for each\ncooking step of a recipe. However, most existing works focus on generating\nimages of finished foods based on the given recipes, and face two challenges to\nvisualize the cooking process. First, the appearance of ingredients changes\nvariously across cooking steps, it is difficult to generate the correct\nappearances of foods that match the textual description, leading to semantic\ninconsistency. Second, the current step might depend on the operations of\nprevious step, it is crucial to maintain the contextual coherence of images in\nsequential order. In this work, we present a cooking process visualization\nmodel, called Chain-of-Cooking. Specifically, to generate correct appearances\nof ingredients, we present a Dynamic Patch Selection Module to retrieve\npreviously generated image patches as references, which are most related to\ncurrent textual contents. Furthermore, to enhance the coherence and keep the\nrational order of generated images, we propose a Semantic Evolution Module and\na Bidirectional Chain-of-Thought (CoT) Guidance. To better utilize the\nsemantics of previous texts, the Semantic Evolution Module establishes the\nsemantical association between latent prompts and current cooking step, and\nmerges it with the latent features. Then the CoT Guidance updates the merged\nfeatures to guide the current cooking step remain coherent with the previous\nstep. Moreover, we construct a dataset named CookViz, consisting of\nintermediate image-text pairs for the cooking process. Quantitative and\nqualitative experiments show that our method outperforms existing methods in\ngenerating coherent and semantic consistent cooking process.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21529v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21529v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.492,
      "distributed_training_score": 0.28,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Moderately Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a diffusion model with Bidirectional Chain-of-Thought (CoT) Guidance to maintain coherence in sequential image generation for cooking processes, which involves iterative refinement across steps. This aligns somewhat with adapting diffusion for multi-step processes, as it treats the sequence as a connected entity for correction. However, the focus is on image generation and visual coherence rather than solving complex logical reasoning tasks, lacking a primary emphasis on holistic reasoning paths. Thus, it is moderately relevant but not a direct match.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper, titled \"Chain-of-Cooking,\" proposes a novel model for cooking process visualization that generates images for each step of a recipe, addressing challenges such as semantic inconsistency in ingredient appearances and contextual coherence across steps. It introduces key components including a Dynamic Patch Selection Module to retrieve relevant image patches from previous steps, a Semantic Evolution Module to establish semantic associations between steps, and Bidirectional Chain-of-Thought Guidance to ensure rational sequence and coherence, while also constructing a new dataset called CookViz; experiments show superior performance in generating semantically consistent and coherent image sequences compared to existing methods.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing techniques like patch selection and chain-of-thought mechanisms in a new way to tackle specific challenges in cooking process visualization, though it builds on known problems in image generation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of computer vision for procedural tasks like food analysis, potentially influencing applications in cooking education, but its applicability remains niche.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable contributions with innovative modules and a new dataset that advance cooking process visualization, making it essential for researchers in computer vision and food-related AI to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1543fa7cc26f57f35a0594300089c1505b7aec82",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 4,
      "average_h_index": 1.6666666666666667,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Mengling Xu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2292659423"
        },
        {
          "name": "Ming Tao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372589703"
        },
        {
          "name": "Bing-Kun Bao",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2261750290"
        }
      ]
    },
    {
      "id": "2507.21530",
      "title": "Suppressing Gradient Conflict for Generalizable Deepfake Detection",
      "authors": [
        "Ming-Hui Liu",
        "Harry Cheng",
        "Xin Luo",
        "Xin-Shun Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Robust deepfake detection models must be capable of generalizing to\never-evolving manipulation techniques beyond training data. A promising\nstrategy is to augment the training data with online synthesized fake images\ncontaining broadly generalizable artifacts. However, in the context of deepfake\ndetection, it is surprising that jointly training on both original and online\nsynthesized forgeries may result in degraded performance. This contradicts the\ncommon belief that incorporating more source-domain data should enhance\ndetection accuracy. Through empirical analysis, we trace this degradation to\ngradient conflicts during backpropagation which force a trade-off between\nsource domain accuracy and target domain generalization. To overcome this\nissue, we propose a Conflict-Suppressed Deepfake Detection (CS-DFD) framework\nthat explicitly mitigates the gradient conflict via two synergistic modules.\nFirst, an Update Vector Search (UVS) module searches for an alternative update\nvector near the initial gradient vector to reconcile the disparities of the\noriginal and online synthesized forgeries. By further transforming the search\nprocess into an extremum optimization problem, UVS yields the uniquely update\nvector, which maximizes the simultaneous loss reductions for each data type.\nSecond, a Conflict Gradient Reduction (CGR) module enforces a low-conflict\nfeature embedding space through a novel Conflict Descent Loss. This loss\npenalizes misaligned gradient directions and guides the learning of\nrepresentations with aligned, non-conflicting gradients. The synergy of UVS and\nCGR alleviates gradient interference in both parameter optimization and\nrepresentation learning. Experiments on multiple deepfake benchmarks\ndemonstrate that CS-DFD achieves state-of-the-art performance in both in-domain\ndetection accuracy and cross-domain generalization.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21530v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21530v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.4,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a framework for suppressing gradient conflicts in deepfake detection training, focusing on techniques like Update Vector Search and Conflict Gradient Reduction to improve model generalization. It does not involve distributed training, parallel computing, or strategies for partitioning data/computation across multiple processors or nodes, making it unrelated to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21532",
      "title": "Automatic Classification of User Requirements from Online Feedback -- A\n  Replication Study",
      "authors": [
        "Meet Bhatt",
        "Nic Boilard",
        "Muhammad Rehan Chaudhary",
        "Cole Thompson",
        "Jacob Idoko",
        "Aakash Sorathiya",
        "Gouri Ginde"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Natural language processing (NLP) techniques have been widely applied in the\nrequirements engineering (RE) field to support tasks such as classification and\nambiguity detection. Although RE research is rooted in empirical investigation,\nit has paid limited attention to replicating NLP for RE (NLP4RE) studies. The\nrapidly advancing realm of NLP is creating new opportunities for efficient,\nmachine-assisted workflows, which can bring new perspectives and results to the\nforefront. Thus, we replicate and extend a previous NLP4RE study (baseline),\n\"Classifying User Requirements from Online Feedback in Small Dataset\nEnvironments using Deep Learning\", which evaluated different deep learning\nmodels for requirement classification from user reviews. We reproduced the\noriginal results using publicly released source code, thereby helping to\nstrengthen the external validity of the baseline study. We then extended the\nsetup by evaluating model performance on an external dataset and comparing\nresults to a GPT-4o zero-shot classifier. Furthermore, we prepared the\nreplication study ID-card for the baseline study, important for evaluating\nreplication readiness. Results showed diverse reproducibility levels across\ndifferent models, with Naive Bayes demonstrating perfect reproducibility. In\ncontrast, BERT and other models showed mixed results. Our findings revealed\nthat baseline deep learning models, BERT and ELMo, exhibited good\ngeneralization capabilities on an external dataset, and GPT-4o showed\nperformance comparable to traditional baseline machine learning models.\nAdditionally, our assessment confirmed the baseline study's replication\nreadiness; however missing environment setup files would have further enhanced\nreadiness. We include this missing information in our replication package and\nprovide the replication study ID-card for our study to further encourage and\nsupport the replication of our study.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21532v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21532v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.357,
      "datasets_score": 0.413,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on replicating and extending an NLP study for classifying user requirements using deep learning models and a zero-shot classifier like GPT-4o. It does not involve training a reward model on human-ranked data or using reinforcement learning to align AI models with human preferences, which are core to RLHF. Therefore, there is no connection to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper involves evaluating and analyzing datasets for NLP-based requirement classification, including reproducing results on original datasets, testing on an external dataset for generalizability, and providing statistical details (e.g., sample sizes, distributions). This aligns with dataset evaluation and benchmarking in ML contexts. However, the primary focus is on replication and model performance, not on creating or curating new datasets, making it only moderately relevant.",
      "llm_score_status": "completed",
      "summary": "This paper replicates and extends a previous study on classifying user requirements from online feedback using deep learning models, aiming to assess reproducibility, generalizability, and the potential of GPT-4o as a zero-shot classifier. The authors reproduced the original results using provided source code, evaluated performance on an external dataset, compared models like BERT and ELMo with GPT-4o, and assessed replication readiness, finding varied reproducibility across models, good generalization for baseline models, comparable performance from GPT-4o, and overall confirmation of the baseline study's replicability with minor enhancements.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by replicating and extending an existing study with new experiments involving GPT-4o and replication readiness assessments, offering a clever combination of ideas rather than introducing a truly new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of NLP for requirements engineering, as it promotes reproducibility and provides practical extensions that could influence future replication studies in software engineering.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a high-quality contribution by advancing replication practices in NLP4RE, making it valuable for researchers in software engineering and AI to understand generalizability and ethical aspects of empirical studies.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7aae692f5db3d5aa2ae6af6f9ab06851ae46af06",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 4,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Meet Bhatt",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373722225"
        },
        {
          "name": "Nic Boilard",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373721465"
        },
        {
          "name": "Muhammad Rehan Chaudhary",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373721739"
        },
        {
          "name": "Cole Thompson",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374182417"
        },
        {
          "name": "Jacob Idoko",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2312204234"
        },
        {
          "name": "Aakash Sorathiya",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2312209268"
        },
        {
          "name": "Gouri Ginde",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2248804137"
        }
      ]
    },
    {
      "id": "2507.21533",
      "title": "Model Predictive Adversarial Imitation Learning for Planning from\n  Observation",
      "authors": [
        "Tyler Han",
        "Yanda Bao",
        "Bhaumik Mehta",
        "Gabriel Guo",
        "Anubhav Vishwakarma",
        "Emily Kang",
        "Sanghun Jung",
        "Rosario Scalise",
        "Jason Zhou",
        "Bryan Xu",
        "Byron Boots"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Human demonstration data is often ambiguous and incomplete, motivating\nimitation learning approaches that also exhibit reliable planning behavior. A\ncommon paradigm to perform planning-from-demonstration involves learning a\nreward function via Inverse Reinforcement Learning (IRL) then deploying this\nreward via Model Predictive Control (MPC). Towards unifying these methods, we\nderive a replacement of the policy in IRL with a planning-based agent. With\nconnections to Adversarial Imitation Learning, this formulation enables\nend-to-end interactive learning of planners from observation-only\ndemonstrations. In addition to benefits in interpretability, complexity, and\nsafety, we study and observe significant improvements on sample efficiency,\nout-of-distribution generalization, and robustness. The study includes\nevaluations in both simulated control benchmarks and real-world navigation\nexperiments using few-to-single observation-only demonstrations.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21533v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21533v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.434,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.333,
      "datasets_score": 0.258,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Model Predictive Adversarial Imitation Learning (MPAIL), which learns from observation-only demonstrations, potentially including human ones, to infer rewards and enable planning. While this involves learning from human-like data to align behavior, it does not align with RLHF's core elements: training a separate reward model on human-ranked data and using it to fine-tune a model via reinforcement learning. Instead, it emphasizes imitation learning via adversarial methods and MPC, making the connection indirect.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21540",
      "title": "PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM\n  Jailbreaking",
      "authors": [
        "Quanchen Zou",
        "Zonghao Ying",
        "Moyang Chen",
        "Wenzhuo Xu",
        "Yisong Xiao",
        "Yakai Li",
        "Deyue Zhang",
        "Dongdong Yang",
        "Zhao Liu",
        "Xiangzheng Zhang"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The increasing sophistication of large vision-language models (LVLMs) has\nbeen accompanied by advances in safety alignment mechanisms designed to prevent\nharmful content generation. However, these defenses remain vulnerable to\nsophisticated adversarial attacks. Existing jailbreak methods typically rely on\ndirect and semantically explicit prompts, overlooking subtle vulnerabilities in\nhow LVLMs compose information over multiple reasoning steps. In this paper, we\npropose a novel and effective jailbreak framework inspired by Return-Oriented\nProgramming (ROP) techniques from software security. Our approach decomposes a\nharmful instruction into a sequence of individually benign visual gadgets. A\ncarefully engineered textual prompt directs the sequence of inputs, prompting\nthe model to integrate the benign visual gadgets through its reasoning process\nto produce a coherent and harmful output. This makes the malicious intent\nemergent and difficult to detect from any single component. We validate our\nmethod through extensive experiments on established benchmarks including\nSafeBench and MM-SafetyBench, targeting popular LVLMs. Results show that our\napproach consistently and substantially outperforms existing baselines on\nstate-of-the-art models, achieving near-perfect attack success rates (over 0.90\non SafeBench) and improving ASR by up to 0.39. Our findings reveal a critical\nand underexplored vulnerability that exploits the compositional reasoning\nabilities of LVLMs, highlighting the urgent need for defenses that secure the\nentire reasoning process.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21540v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21540v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.283,
      "datasets_score": 0.25,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a jailbreak framework for LVLMs inspired by Return-Oriented Programming, which decomposes harmful instructions into benign visual gadgets and uses prompts to exploit multi-step reasoning. It does not involve diffusion models, iterative refinement processes, or adapting diffusion for logical tasks, focusing instead on adversarial attacks rather than diffusion-based reasoning mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21541",
      "title": "Sun sensor calibration algorithms: A systematic mapping and survey",
      "authors": [
        "Michael Herman",
        "Olivia J. Pinon Fischer",
        "Dimitri N. Mavris"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "astro-ph.IM (Instrumentation and Methods for Astrophysics)"
      ],
      "abstract": "Attitude sensors determine the spacecraft attitude through the sensing of an\nastronomical object, field or other phenomena. The Sun and fixed stars are the\ntwo primary astronomical sensing objects. Attitude sensors are critical\ncomponents for the survival and knowledge improvement of spacecraft. Of these,\nsun sensors are the most common and important sensor for spacecraft attitude\ndetermination. The sun sensor measures the Sun vector in spacecraft\ncoordinates. The sun sensor calibration process is particularly difficult due\nto the complex nature of the uncertainties involved. The uncertainties are\nsmall, difficult to observe, and vary spatio-temporally over the lifecycle of\nthe sensor. In addition, the sensors are affected by numerous sources of\nuncertainties, including manufacturing, electrical, environmental, and\ninterference sources. This motivates the development of advanced calibration\nalgorithms to minimize uncertainty over the sensor lifecycle and improve\naccuracy. Although modeling and calibration techniques for sun sensors have\nbeen explored extensively in the literature over the past two decades, there is\ncurrently no resource that consolidates and systematically reviews this body of\nwork. The present review proposes a systematic mapping of sun sensor modeling\nand calibration algorithms across a breadth of sensor configurations. It\nspecifically provides a comprehensive survey of each methodology, along with an\nanalysis of research gaps and recommendations for future directions in sun\nsensor modeling and calibration techniques.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21541v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21541v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.292,
      "weak_supervision_score": 0.277,
      "diffusion_reasoning_score": 0.276,
      "distributed_training_score": 0.256,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21555",
      "title": "Multi-View Reconstruction with Global Context for 3D Anomaly Detection",
      "authors": [
        "Yihan Sun",
        "Yuqi Cheng",
        "Yunkang Cao",
        "Yuxin Zhang",
        "Weiming Shen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D anomaly detection is critical in industrial quality inspection. While\nexisting methods achieve notable progress, their performance degrades in\nhigh-precision 3D anomaly detection due to insufficient global information. To\naddress this, we propose Multi-View Reconstruction (MVR), a method that\nlosslessly converts high-resolution point clouds into multi-view images and\nemploys a reconstruction-based anomaly detection framework to enhance global\ninformation learning. Extensive experiments demonstrate the effectiveness of\nMVR, achieving 89.6\\% object-wise AU-ROC and 95.7\\% point-wise AU-ROC on the\nReal3D-AD benchmark.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21555v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21555v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.348,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21567",
      "title": "RelMap: Enhancing Online Map Construction with Class-Aware Spatial\n  Relation and Semantic Priors",
      "authors": [
        "Tianhui Cai",
        "Yun Zhang",
        "Zewei Zhou",
        "Zhiyu Huang",
        "Jiaqi Ma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Online high-definition (HD) map construction plays an increasingly important\nrole in scaling autonomous driving systems. Transformer-based methods have\nbecome prevalent in online HD map construction; however, existing approaches\noften neglect the inherent spatial and semantic relationships among map\nelements, which limits their accuracy and generalization. To address this, we\npropose RelMap, an end-to-end framework that enhances online map construction\nby incorporating spatial relations and semantic priors. We introduce a\nClass-aware Spatial Relation Prior, which explicitly encodes relative\npositional dependencies between map elements using a learnable class-aware\nrelation encoder. Additionally, we propose a Mixture-of-Experts (MoE)-based\nSemantic Prior, which routes features to class-specific experts based on\npredicted class probabilities, refining instance feature decoding. Our method\nis compatible with both single-frame and temporal perception backbones,\nachieving state-of-the-art performance on both the nuScenes and Argoverse 2\ndatasets.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21567v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21567v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.37,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an end-to-end framework for online HD map construction using Transformer-based methods, incorporating spatial relations and semantic priors. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21571",
      "title": "Finding Uncommon Ground: A Human-Centered Model for Extrospective\n  Explanations",
      "authors": [
        "Laura Spillner",
        "Nima Zargham",
        "Mihai Pomarlan",
        "Robert Porzel",
        "Rainer Malaka"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "The need for explanations in AI has, by and large, been driven by the desire\nto increase the transparency of black-box machine learning models. However,\nsuch explanations, which focus on the internal mechanisms that lead to a\nspecific output, are often unsuitable for non-experts. To facilitate a\nhuman-centered perspective on AI explanations, agents need to focus on\nindividuals and their preferences as well as the context in which the\nexplanations are given. This paper proposes a personalized approach to\nexplanation, where the agent tailors the information provided to the user based\non what is most likely pertinent to them. We propose a model of the agent's\nworldview that also serves as a personal and dynamic memory of its previous\ninteractions with the same user, based on which the artificial agent can\nestimate what part of its knowledge is most likely new information to the user.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21571v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21571v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.463,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.276,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses personalized, human-centered explanations for AI systems, incorporating user preferences and past interactions to tailor explanations. However, it does not involve training or fine-tuning models using human feedback via reinforcement learning, such as creating a reward model or aligning AI with human-ranked data. Thus, it lacks the core elements of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on developing a model for extrospective explanations that consider user context and interactions, but it does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. There is no component related to treating a Chain-of-Thought as an entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21573",
      "title": "LinDeps: A Fine-tuning Free Post-Pruning Method to Remove Layer-Wise\n  Linear Dependencies with Guaranteed Performance Preservation",
      "authors": [
        "Maxim Henry",
        "Adrien Deliège",
        "Anthony Cioppa",
        "Marc Van Droogenbroeck"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Convolutional Neural Networks (CNN) are widely used in many computer vision\ntasks. Yet, their increasing size and complexity pose significant challenges\nfor efficient deployment on resource-constrained platforms. Hence, network\npruning has emerged as an effective way of reducing the size and computational\nrequirements of neural networks by removing redundant or unimportant\nparameters. However, a fundamental challenge with pruning consists in optimally\nremoving redundancies without degrading performance. Most existing pruning\ntechniques overlook structural dependencies across feature maps within a layer,\nresulting in suboptimal pruning decisions. In this work, we introduce LinDeps,\na novel post-pruning method, i.e., a pruning method that can be applied on top\nof any pruning technique, which systematically identifies and removes redundant\nfilters via linear dependency analysis. Particularly, LinDeps applies pivoted\nQR decomposition to feature maps to detect and prune linearly dependent\nfilters. Then, a novel signal recovery mechanism adjusts the next layer's\nkernels to preserve compatibility and performance without requiring any\nfine-tuning. Our experiments on CIFAR-10 and ImageNet with VGG and ResNet\nbackbones demonstrate that LinDeps improves compression rates of existing\npruning techniques while preserving performances, leading to a new state of the\nart in CNN pruning. We also benchmark LinDeps in low-resource setups where no\nretraining can be performed, which shows significant pruning improvements and\ninference speedups over a state-of-the-art method. LinDeps therefore\nconstitutes an essential add-on for any current or future pruning technique.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21573v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21573v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.399,
      "datasets_score": 0.276,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21584",
      "title": "TARS: MinMax Token-Adaptive Preference Strategy for MLLM Hallucination\n  Reduction",
      "authors": [
        "Kejia Zhang",
        "Keda Tao",
        "Zhiming Luo",
        "Chang Liu",
        "Jiasheng Tang",
        "Huan Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal large language models (MLLMs) enable vision-language reasoning,\nyet often generate plausible outputs that are factually incorrect or visually\nungrounded, thereby compromising their reliability. Direct preference\noptimization (DPO) is a common strategy for correcting hallucinations by\naligning model outputs with human preferences. Existing DPO strategies\ntypically treat hallucination-related preferences as fixed targets, relying on\nstatic supervision signals during training. This approach tends to overfit to\nsuperficial linguistic cues in preference data, leading to distributional\nrigidity and spurious correlations that impair grounding in causally relevant\nvisual information. To overcome this limitation, we propose TARS, a\ntoken-adaptive preference strategy that reformulates DPO as a min-max\noptimization problem. TARS maximizes token-level distributional shifts under\nsemantic constraints to simulate alignment uncertainty, and simultaneously\nminimizes the expected preference loss under these controlled perturbations.\nThis joint objective preserves causal grounding while mitigating overfitting to\npreference patterns, thereby reducing hallucinations in multimodal reasoning.\nWe evaluate TARS on multiple hallucination benchmarks and find consistently\nstrong performance. Using only 4.8k preference samples and no expert feedback,\nTARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition\nvalue from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on\nseveral key metrics.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21584v3",
      "pdf_url": "http://arxiv.org/pdf/2507.21584v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.512,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.397,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses Direct Preference Optimization (DPO), which aligns models with preferences similar to RLHF concepts, but it does not use a separate reward model or reinforcement learning. Instead, it relies on preference samples without human feedback, making it related but not a direct implementation of RLHF.",
      "weak_supervision_justification": "TARS uses programmatically generated perturbations on tokens to simulate variations in preference data, aligning with weak supervision by relying on noisy or imprecise sources for training signals, rather than fully hand-labeled data.",
      "diffusion_reasoning_justification": "The paper focuses on a min-max optimization for preference strategy in MLLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces TARS, a token-adaptive preference strategy that reformulates direct preference optimization (DPO) as a min-max optimization problem to reduce hallucinations in multimodal large language models (MLLMs). By maximizing token-level distributional shifts under semantic constraints while minimizing preference loss, TARS enhances visual grounding and mitigates overfitting to superficial cues, resulting in significant reductions in hallucination rates from 26.4% to 13.2% using only 4.8k preference samples, and achieving performance on par with GPT-4o on key benchmarks.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by reformulating DPO as a min-max optimization problem with token-level adaptations, significantly advancing the state-of-the-art in hallucination reduction for MLLMs.",
      "impact_score": "High",
      "impact_justification": "This work could influence a wide range of future research and commercial applications in multimodal AI by improving model reliability and efficiency, as demonstrated by its strong benchmark performance matching advanced models like GPT-4o.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to AI research by addressing a critical issue in MLLMs, making it essential for those working in computer vision and multimodal systems to be aware of its insights and methods.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d1a33bf398116a76f87975e08b3a7619c70034a4",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 3,
      "average_h_index": 1.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Kejia Zhang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2343651756"
        },
        {
          "name": "Keda Tao",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2331856915"
        },
        {
          "name": "Zhiming Luo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375759072"
        },
        {
          "name": "Chang Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373825133"
        },
        {
          "name": "Jiasheng Tang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2343873525"
        },
        {
          "name": "Huan Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2331889370"
        }
      ]
    },
    {
      "id": "2507.21585",
      "title": "SafeDriveRAG: Towards Safe Autonomous Driving with Knowledge Graph-based\n  Retrieval-Augmented Generation",
      "authors": [
        "Hao Ye",
        "Mengshi Qi",
        "Zhaohong Liu",
        "Liang Liu",
        "Huadong Ma"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this work, we study how vision-language models (VLMs) can be utilized to\nenhance the safety for the autonomous driving system, including perception,\nsituational understanding, and path planning. However, existing research has\nlargely overlooked the evaluation of these models in traffic safety-critical\ndriving scenarios. To bridge this gap, we create the benchmark (SafeDrive228K)\nand propose a new baseline based on VLM with knowledge graph-based\nretrieval-augmented generation (SafeDriveRAG) for visual question answering\n(VQA). Specifically, we introduce SafeDrive228K, the first large-scale\nmultimodal question-answering benchmark comprising 228K examples across 18\nsub-tasks. This benchmark encompasses a diverse range of traffic safety\nqueries, from traffic accidents and corner cases to common safety knowledge,\nenabling a thorough assessment of the comprehension and reasoning abilities of\nthe models. Furthermore, we propose a plug-and-play multimodal knowledge\ngraph-based retrieval-augmented generation approach that employs a novel\nmulti-scale subgraph retrieval algorithm for efficient information retrieval.\nBy incorporating traffic safety guidelines collected from the Internet, this\nframework further enhances the model's capacity to handle safety-critical\nsituations. Finally, we conduct comprehensive evaluations on five mainstream\nVLMs to assess their reliability in safety-sensitive driving tasks.\nExperimental results demonstrate that integrating RAG significantly improves\nperformance, achieving a +4.73% gain in Traffic Accidents tasks, +8.79% in\nCorner Cases tasks and +14.57% in Traffic Safety Commonsense across five\nmainstream VLMs, underscoring the potential of our proposed benchmark and\nmethodology for advancing research in traffic safety. Our source code and data\nare available at https://github.com/Lumos0507/SafeDriveRAG.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21585v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21585v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.443,
      "distributed_training_score": 0.371,
      "datasets_score": 0.42,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on vision-language models (VLMs) enhanced with retrieval-augmented generation (RAG) using knowledge graphs for autonomous driving safety. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for reasoning. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces a new large-scale benchmark, SafeDrive228K, comprising 228K examples for evaluating VLMs in autonomous driving safety. It details dataset creation, curation across sub-tasks like traffic accidents and corner cases, and benchmarking of models, directly aligning with research on datasets for ML and AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the safety challenges in autonomous driving by introducing SafeDrive228K, a comprehensive benchmark with 228K examples across 18 sub-tasks focused on traffic accidents, corner cases, and safety commonsense, to evaluate vision-language models (VLMs). It proposes SafeDriveRAG, a novel knowledge graph-based retrieval-augmented generation method that enhances VLMs' performance in safety-critical scenarios through efficient multi-scale subgraph retrieval, demonstrating significant improvements such as +4.73% in traffic accidents and +14.57% in safety commonsense across mainstream VLMs.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing retrieval-augmented generation techniques with knowledge graphs specifically for autonomous driving safety, addressing an underexplored area in traffic safety evaluation. While not introducing entirely new concepts, it cleverly adapts and integrates them to solve known problems in a new context.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of AI for autonomous driving, as the benchmark and method provide practical tools for improving safety in real-world applications. However, its influence may be limited to specialized areas rather than broadly across all AI research.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution by offering a new benchmark and method that advance safety in autonomous driving, making it valuable for researchers focused on AI applications in transportation. It is not essential for all readers but highly relevant for those in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a3f26c6774444c1b3e80a090f8dbde9a92d065c6",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 26,
      "average_h_index": 6.8,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Hao Ye",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2340568082"
        },
        {
          "name": "Mengshi Qi",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2263678904"
        },
        {
          "name": "Zhaohong Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373983846"
        },
        {
          "name": "Liang Liu",
          "h_index": 26,
          "profile_url": "https://www.semanticscholar.org/author/40353343"
        },
        {
          "name": "Huadong Ma",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2268633085"
        }
      ]
    },
    {
      "id": "2507.21587",
      "title": "Emerging Trends in Pseudo-Label Refinement for Weakly Supervised\n  Semantic Segmentation with Image-Level Supervision",
      "authors": [
        "Zheyuan Zhang",
        "Wang Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Unlike fully supervised semantic segmentation, weakly supervised semantic\nsegmentation (WSSS) relies on weaker forms of supervision to perform dense\nprediction tasks. Among the various types of weak supervision, WSSS with image\nlevel annotations is considered both the most challenging and the most\npractical, attracting significant research attention. Therefore, in this\nreview, we focus on WSSS with image level annotations. Additionally, this\nreview concentrates on mainstream research directions, deliberately omitting\nless influential branches.\n  Given the rapid development of new methods and the limitations of existing\nsurveys in capturing recent trends, there is a pressing need for an updated and\ncomprehensive review. Our goal is to fill this gap by synthesizing the latest\nadvancements and state-of-the-art techniques in WSSS with image level labels.\n  Basically, we provide a comprehensive review of recent advancements in WSSS\nwith image level labels, categorizing existing methods based on the types and\nlevels of additional supervision involved. We also examine the challenges of\napplying advanced methods to domain specific datasets in WSSS,a topic that\nremains underexplored. Finally, we discuss the current challenges, evaluate the\nlimitations of existing approaches, and outline several promising directions\nfor future research. This review is intended for researchers who are already\nfamiliar with the fundamental concepts of WSSS and are seeking to deepen their\nunderstanding of current advances and methodological innovations.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21587v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21587v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.536,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.32,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on Weakly Supervised Semantic Segmentation (WSSS) with image-level annotations, which directly embodies weak supervision by using high-level, noisy, or imprecise labels (e.g., image-level tags) to generate pseudo-labels for dense prediction tasks. Its main contributions, such as reviewing methods that refine pseudo-labels and categorizing them into internal and external supervision strategies, align closely with the topic's definition of programmatically generating labels from imperfect sources, rather than relying on hand-labeled data. This makes the paper a core example of advancements in weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper provides a comprehensive review of recent advancements in weakly supervised semantic segmentation (WSSS) using image-level annotations, focusing on pseudo-label refinement techniques to address issues like co-occurrence and partial object coverage in class activation maps (CAMs). It categorizes methods into external and internal supervision strategies, examines challenges in applying these to domain-specific datasets, evaluates limitations of existing approaches, and outlines promising future research directions to improve CAM generation and overall segmentation performance.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new taxonomy for categorizing WSSS methods into internal and external supervision strategies, synthesizing recent trends in a way that updates and organizes existing knowledge without introducing entirely new problems or techniques.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the specific subfield of computer vision and WSSS, as it consolidates recent advancements and suggests future directions, though its influence may be limited to researchers already engaged in this area.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This is a high-quality review that offers significant value for researchers in WSSS by providing an updated synthesis of methods and challenges, making it essential for staying informed on the topic without being groundbreaking enough to be a must-read.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b0f12ec4a0425561296c9b27e10a50053b67dfd0",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 1,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zheyuan Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2333311532"
        },
        {
          "name": "Wang Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2177256876"
        }
      ]
    },
    {
      "id": "2507.21588",
      "title": "Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual\n  Multi-Task Incremental Learning",
      "authors": [
        "Jiong Yin",
        "Liang Li",
        "Jiehua Zhang",
        "Yuhan Gao",
        "Chenggang Yan",
        "Xichun Sheng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Audio-visual multi-task incremental learning aims to continuously learn from\nmultiple audio-visual tasks without the need for joint training on all tasks.\nThe challenge of the problem is how to preserve the old task knowledge while\nfacilitating the learning of new task with previous experiences. To address\nthese challenges, we introduce a three-stage Progressive Homeostatic and\nPlastic audio-visual prompt (PHP) method. In the shallow phase, we design the\ntask-shared modality aggregating adapter to foster cross-task and cross-modal\naudio-visual representation learning to enhance shared understanding between\ntasks. In the middle phase, we propose the task-specific modality-shared\ndynamic generating adapter, which constructs prompts that are tailored to\nindividual tasks while remaining general across modalities, which balances the\nmodels ability to retain knowledge against forgetting with its potential for\nversatile multi-task transferability. In the deep phase, we introduce the\ntask-specific modality-independent prompts to further refine the understand\nability by targeting individual information for each task and modality. By\nincorporating these three phases, PHP retains task-specific prompts while\nadapting shared parameters for new tasks to effectively balance knowledge\nsharing and specificity. Our method achieves SOTA performance in different\norders of four tasks (AVE, AVVP, AVS and AVQA). Our code can be available at\nhttps://github.com/ENJOY-Yin-jiong/PHP.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21588v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21588v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.35,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is on audio-visual multi-task incremental learning, focusing on prompt tuning methods to mitigate catastrophic forgetting and enhance knowledge sharing across tasks and modalities. It does not involve reinforcement learning, human feedback, reward models, or any mechanism for aligning AI with human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21589",
      "title": "Exploring the Link Between Bayesian Inference and Embodied Intelligence:\n  Toward Open Physical-World Embodied AI Systems",
      "authors": [
        "Bin Liu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Embodied intelligence posits that cognitive capabilities fundamentally emerge\nfrom - and are shaped by - an agent's real-time sensorimotor interactions with\nits environment. Such adaptive behavior inherently requires continuous\ninference under uncertainty. Bayesian statistics offers a principled\nprobabilistic framework to address this challenge by representing knowledge as\nprobability distributions and updating beliefs in response to new evidence. The\ncore computational processes underlying embodied intelligence - including\nperception, action selection, learning, and even higher-level cognition - can\nbe effectively understood and modeled as forms of Bayesian inference. Despite\nthe deep conceptual connection between Bayesian statistics and embodied\nintelligence, Bayesian principles have not been widely or explicitly applied in\ntoday's embodied intelligence systems. In this work, we examine both Bayesian\nand contemporary embodied intelligence approaches through two fundamental\nlenses: search and learning - the two central themes in modern AI, as\nhighlighted in Rich Sutton's influential essay \"The Bitter Lesson\". This\nanalysis sheds light on why Bayesian inference has not played a central role in\nthe development of modern embodied intelligence. At the same time, it reveals\nthat current embodied intelligence systems remain largely confined to\nclosed-physical-world environments, and highlights the potential for Bayesian\nmethods to play a key role in extending these systems toward truly open\nphysical-world embodied intelligence.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21589v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21589v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.449,
      "distributed_training_score": 0.289,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an analysis of the connection between Bayesian inference and embodied intelligence, emphasizing probabilistic frameworks for perception, action, and learning in AI systems. It does not discuss diffusion models, iterative refinement processes, Chain-of-Thought reasoning, or any multi-step logical tasks adapted from diffusion techniques. Therefore, there is no overlap with diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21591",
      "title": "Hierarchical Graph Neural Network for Compressed Speech Steganalysis",
      "authors": [
        "Mustapha Hemis",
        "Hamza Kheddar",
        "Mohamed Chahine Ghanem",
        "Bachir Boudraa"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.SD (Sound)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Steganalysis methods based on deep learning (DL) often struggle with\ncomputational complexity and challenges in generalizing across different\ndatasets. Incorporating a graph neural network (GNN) into steganalysis schemes\nenables the leveraging of relational data for improved detection accuracy and\nadaptability. This paper presents the first application of a Graph Neural\nNetwork (GNN), specifically the GraphSAGE architecture, for steganalysis of\ncompressed voice over IP (VoIP) speech streams. The method involves\nstraightforward graph construction from VoIP streams and employs GraphSAGE to\ncapture hierarchical steganalysis information, including both fine grained\ndetails and high level patterns, thereby achieving high detection accuracy.\nExperimental results demonstrate that the developed approach performs well in\nuncovering quantization index modulation (QIM)-based steganographic patterns in\nVoIP signals. It achieves detection accuracy exceeding 98 percent even for\nshort 0.5 second samples, and 95.17 percent accuracy under challenging\nconditions with low embedding rates, representing an improvement of 2.8 percent\nover the best performing state of the art methods. Furthermore, the model\nexhibits superior efficiency, with an average detection time as low as 0.016\nseconds for 0.5-second samples an improvement of 0.003 seconds. This makes it\nefficient for online steganalysis tasks, providing a superior balance between\ndetection accuracy and efficiency under the constraint of short samples with\nlow embedding rates.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21591v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21591v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.268,
      "weak_supervision_score": 0.306,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.293,
      "datasets_score": 0.273,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21600",
      "title": "Locally Controlled Face Aging with Latent Diffusion Models",
      "authors": [
        "Lais Isabelle Alves dos Santos",
        "Julien Despois",
        "Thibaut Chauffier",
        "Sileye O. Ba",
        "Giovanni Palma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present a novel approach to face aging that addresses the limitations of\ncurrent methods which treat aging as a global, homogeneous process. Existing\ntechniques using GANs and diffusion models often condition generation on a\nreference image and target age, neglecting that facial regions age\nheterogeneously due to both intrinsic chronological factors and extrinsic\nelements like sun exposure. Our method leverages latent diffusion models to\nselectively age specific facial regions using local aging signs. This approach\nprovides significantly finer-grained control over the generation process,\nenabling more realistic and personalized aging. We employ a latent diffusion\nrefiner to seamlessly blend these locally aged regions, ensuring a globally\nconsistent and natural-looking synthesis. Experimental results demonstrate that\nour method effectively achieves three key criteria for successful face aging:\nrobust identity preservation, high-fidelity and realistic imagery, and a\nnatural, controllable aging progression.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21600v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21600v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.509,
      "distributed_training_score": 0.315,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the use of latent diffusion models for fine-grained face aging in images, focusing on selective aging of facial regions and blending for realistic synthesis. It does not involve adapting diffusion models for solving complex logical tasks, multi-step reasoning, or treating a Chain-of-Thought as an entity. The application is purely generative for visual outputs, not reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21606",
      "title": "Decoupled Spatio-Temporal Consistency Learning for Self-Supervised\n  Tracking",
      "authors": [
        "Yaozong Zheng",
        "Bineng Zhong",
        "Qihua Liang",
        "Ning Li",
        "Shuxiang Song"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The success of visual tracking has been largely driven by datasets with\nmanual box annotations. However, these box annotations require tremendous human\neffort, limiting the scale and diversity of existing tracking datasets. In this\nwork, we present a novel Self-Supervised Tracking framework named\n\\textbf{{\\tracker}}, designed to eliminate the need of box annotations.\nSpecifically, a decoupled spatio-temporal consistency training framework is\nproposed to learn rich target information across timestamps through global\nspatial localization and local temporal association. This allows for the\nsimulation of appearance and motion variations of instances in real-world\nscenarios. Furthermore, an instance contrastive loss is designed to learn\ninstance-level correspondences from a multi-view perspective, offering robust\ninstance supervision without additional labels. This new design paradigm\nenables {\\tracker} to effectively learn generic tracking representations in a\nself-supervised manner, while reducing reliance on extensive box annotations.\nExtensive experiments on nine benchmark datasets demonstrate that {\\tracker}\nsurpasses \\textit{SOTA} self-supervised tracking methods, achieving an\nimprovement of more than 25.3\\%, 20.4\\%, and 14.8\\% in AUC (AO) score on the\nGOT10K, LaSOT, TrackingNet datasets, respectively. Code:\nhttps://github.com/GXNU-ZhongLab/SSTrack.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21606v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21606v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.331,
      "distributed_training_score": 0.362,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21608",
      "title": "Semantic Segmentation of iPS Cells: Case Study on Model Complexity in\n  Biomedical Imaging",
      "authors": [
        "Maoquan Zhang",
        "Bisser Raytchev",
        "Xiujuan Sun"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Medical image segmentation requires not only accuracy but also robustness\nunder challenging imaging conditions. In this study, we show that a carefully\nconfigured DeepLabv3 model can achieve high performance in segmenting induced\npluripotent stem (iPS) cell colonies, and, under our experimental conditions,\noutperforms large-scale foundation models such as SAM2 and its medical variant\nMedSAM2 without structural modifications. These results suggest that, for\nspecialized tasks characterized by subtle, low-contrast boundaries, increased\nmodel complexity does not necessarily translate to better performance. Our work\nrevisits the assumption that ever-larger and more generalized architectures are\nalways preferable, and provides evidence that appropriately adapted, simpler\nmodels may offer strong accuracy and practical reliability in domain-specific\nbiomedical applications. We also offer an open-source implementation that\nincludes strategies for small datasets and domain-specific encoding, with the\naim of supporting further advances in semantic segmentation for regenerative\nmedicine and related fields.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21608v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21608v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.372,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21610",
      "title": "Research Challenges and Progress in the End-to-End V2X Cooperative\n  Autonomous Driving Competition",
      "authors": [
        "Ruiyang Hao",
        "Haibao Yu",
        "Jiaru Zhong",
        "Chuanye Wang",
        "Jiahao Wang",
        "Yiming Kan",
        "Wenxian Yang",
        "Siqi Fan",
        "Huilin Yin",
        "Jianing Qiu",
        "Yao Mu",
        "Jiankai Sun",
        "Li Chen",
        "Walter Zimmer",
        "Dandan Zhang",
        "Shanghang Zhang",
        "Mac Schwager",
        "Wei Huang",
        "Xiaobo Zhang",
        "Ping Luo",
        "Zaiqing Nie"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "With the rapid advancement of autonomous driving technology,\nvehicle-to-everything (V2X) communication has emerged as a key enabler for\nextending perception range and enhancing driving safety by providing visibility\nbeyond the line of sight. However, integrating multi-source sensor data from\nboth ego-vehicles and infrastructure under real-world constraints, such as\nlimited communication bandwidth and dynamic environments, presents significant\ntechnical challenges. To facilitate research in this area, we organized the\nEnd-to-End Autonomous Driving through V2X Cooperation Challenge, which features\ntwo tracks: cooperative temporal perception and cooperative end-to-end\nplanning. Built on the UniV2X framework and the V2X-Seq-SPD dataset, the\nchallenge attracted participation from over 30 teams worldwide and established\na unified benchmark for evaluating cooperative driving systems. This paper\ndescribes the design and outcomes of the challenge, highlights key research\nproblems including bandwidth-aware fusion, robust multi-agent planning, and\nheterogeneous sensor integration, and analyzes emerging technical trends among\ntop-performing solutions. By addressing practical constraints in communication\nand data fusion, the challenge contributes to the development of scalable and\nreliable V2X-cooperative autonomous driving systems.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21610v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21610v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.391,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21611",
      "title": "Wind Turbine Feature Detection Using Deep Learning and Synthetic Data",
      "authors": [
        "Arash Shahirpour",
        "Jakob Gebler",
        "Manuel Sanders",
        "Tim Reuscher"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "For the autonomous drone-based inspection of wind turbine (WT) blades,\naccurate detection of the WT and its key features is essential for safe drone\npositioning and collision avoidance. Existing deep learning methods typically\nrely on manually labeled real-world images, which limits both the quantity and\nthe diversity of training datasets in terms of weather conditions, lighting,\nturbine types, and image complexity. In this paper, we propose a method to\ngenerate synthetic training data that allows controlled variation of visual and\nenvironmental factors, increasing the diversity and hence creating challenging\nlearning scenarios. Furthermore, we train a YOLOv11 feature detection network\nsolely on synthetic WT images with a modified loss function, to detect WTs and\ntheir key features within an image. The resulting network is evaluated both\nusing synthetic images and a set of real-world WT images and shows promising\nperformance across both synthetic and real-world data, achieving a Pose\nmAP50-95 of 0.97 on real images never seen during training.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21611v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21611v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.396,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.387,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21619",
      "title": "EMIT: Enhancing MLLMs for Industrial Anomaly Detection via\n  Difficulty-Aware GRPO",
      "authors": [
        "Wei Guan",
        "Jun Lan",
        "Jian Cao",
        "Hao Tan",
        "Huijia Zhu",
        "Weiqiang Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Industrial anomaly detection (IAD) plays a crucial role in maintaining the\nsafety and reliability of manufacturing systems. While multimodal large\nlanguage models (MLLMs) show strong vision-language reasoning abilities, their\neffectiveness in IAD remains limited without domain-specific adaptation. In\nthis work, we propose EMIT, a unified framework that enhances MLLMs for IAD via\ndifficulty-aware group relative policy optimization (GRPO). EMIT constructs a\nmulti-task IAD dataset and utilizes GPT-generated object text descriptions to\ncompensate for missing defective images. For few-shot anomaly detection, it\nintegrates a soft prompt and heatmap-guided contrastive embeddings derived from\npatch-level comparisons. To better handle difficult data samples, i.e., cases\nwhere the MLLM struggles to generate correct answers, we propose a\ndifficulty-aware GRPO that extends the original GRPO by incorporating a\nresponse resampling strategy to ensure the inclusion of correct answers in the\nsampled responses, as well as an advantage reweighting mechanism to strengthen\nlearning from such difficult data samples. Extensive experiments on the MMAD\nbenchmark demonstrate that EMIT significantly enhances the IAD performance of\nMLLMs, achieving an average improvement of 7.77\\% over the base model\n(InternVL3-8B) across seven tasks.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21619v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21619v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.413,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.393,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses Group Relative Policy Optimization (GRPO), a reinforcement learning-like method for aligning models via interactive feedback, but it does not involve human preferences, a reward model trained on human-ranked data, or explicit human feedback. Instead, it focuses on automated response resampling and advantage reweighting, making it only loosely connected to RLHF.",
      "weak_supervision_justification": "The paper employs weak supervision by using GPT-4 to programmatically generate object descriptive text as substitutes for missing defective images, allowing training without perfectly hand-labeled data. This aligns directly with weak supervision's core idea of deriving labels from noisy or high-level sources.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for Chain-of-Thought tasks. It focuses on enhancing MLLMs for anomaly detection using GRPO and other techniques, with no components related to diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces EMIT, a framework designed to enhance Multimodal Large Language Models (MLLMs) for Industrial Anomaly Detection (IAD) by incorporating difficulty-aware Group Relative Policy Optimization (GRPO), which addresses challenges in handling difficult data samples through response resampling and advantage reweighting. The methodology involves constructing a multi-task IAD dataset, using GPT-generated descriptions for missing defective images, integrating soft prompts and heatmap-guided contrastive embeddings for few-shot detection, and fine-tuning the model in two stages; experiments on the MMAD benchmark demonstrate that EMIT significantly improves the base model's performance by an average of 7.77% across seven tasks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement to existing GRPO by introducing difficulty-aware mechanisms, which cleverly combines and adapts prior techniques for better handling of challenging IAD scenarios, though it does not introduce an entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in adapting MLLMs for specialized tasks like IAD, given its demonstrated performance gains and potential for industrial applications, but its scope is primarily confined to this subfield.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution with practical enhancements to MLLMs for IAD, supported by solid experimental results, making it important for researchers in computer vision and anomaly detection to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d3ab1d17f5fe568d0fb2942eb986ca44aa8305c0",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 6,
      "average_h_index": 2.3333333333333335,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Wei Guan",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2192215874"
        },
        {
          "name": "Jun Lan",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2158820459"
        },
        {
          "name": "Jian Cao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374011685"
        },
        {
          "name": "Hao Tan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374401963"
        },
        {
          "name": "Huijia Zhu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2267508970"
        },
        {
          "name": "Weiqiang Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2289900569"
        }
      ]
    },
    {
      "id": "2507.21627",
      "title": "GuidPaint: Class-Guided Image Inpainting with Diffusion Models",
      "authors": [
        "Qimin Wang",
        "Xinda Liu",
        "Guohua Geng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In recent years, diffusion models have been widely adopted for image\ninpainting tasks due to their powerful generative capabilities, achieving\nimpressive results. Existing multimodal inpainting methods based on diffusion\nmodels often require architectural modifications and retraining, resulting in\nhigh computational cost. In contrast, context-aware diffusion inpainting\nmethods leverage the model's inherent priors to adjust intermediate denoising\nsteps, enabling high-quality inpainting without additional training and\nsignificantly reducing computation. However, these methods lack fine-grained\ncontrol over the masked regions, often leading to semantically inconsistent or\nvisually implausible content. To address this issue, we propose GuidPaint, a\ntraining-free, class-guided image inpainting framework. By incorporating\nclassifier guidance into the denoising process, GuidPaint enables precise\ncontrol over intermediate generations within the masked areas, ensuring both\nsemantic consistency and visual realism. Furthermore, it integrates stochastic\nand deterministic sampling, allowing users to select preferred intermediate\nresults and deterministically refine them. Experimental results demonstrate\nthat GuidPaint achieves clear improvements over existing context-aware\ninpainting methods in both qualitative and quantitative evaluations.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21627v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21627v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.508,
      "distributed_training_score": 0.324,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a class-guided framework for image inpainting using diffusion models, focusing on improving visual generation and control in masked regions without retraining. It does not involve adapting diffusion models for multi-step logical reasoning, such as treating a Chain-of-Thought as an entity for holistic correction in complex logical tasks. The iterative process here is limited to image denoising and refinement, not reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21631",
      "title": "\"Teammates, Am I Clear?\": Analysing Legible Behaviours in Teams",
      "authors": [
        "Miguel Faria",
        "Francisco S. Melo",
        "Ana Paiva"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "In this paper we investigate the notion of legibility in sequential\ndecision-making in the context of teams and teamwork. There have been works\nthat extend the notion of legibility to sequential decision making, for\ndeterministic and for stochastic scenarios. However, these works focus on one\nagent interacting with one human, foregoing the benefits of having legible\ndecision making in teams of agents or in team configurations with humans. In\nthis work we propose an extension of legible decision-making to multi-agent\nsettings that improves the performance of agents working in collaboration. We\nshowcase the performance of legible decision making in team scenarios using our\nproposed extension in multi-agent benchmark scenarios. We show that a team with\na legible agent is able to outperform a team composed solely of agents with\nstandard optimal behaviour.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21631v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21631v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.302,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on extending legible decision-making to multi-agent settings for improved team performance in collaborative tasks, using scenarios from RL literature like LB-Foraging and Pursuit-Evasion. While it references RL contexts, it does not involve human feedback for training a reward model or fine-tuning AI models based on human preferences, which are core to RLHF. Thus, it is only loosely related through the broader RL framework.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21636",
      "title": "StaffPro: an LLM Agent for Joint Staffing and Profiling",
      "authors": [
        "Alessio Maritan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language model (LLM) agents integrate pre-trained LLMs with modular\nalgorithmic components and have shown remarkable reasoning and decision-making\nabilities. In this work, we investigate their use for two tightly intertwined\nchallenges in workforce management: staffing, i.e., the assignment and\nscheduling of tasks to workers, which may require team formation; and\nprofiling, i.e., the continuous estimation of workers' skills, preferences, and\nother latent attributes from unstructured data. We cast these problems in a\nformal mathematical framework that links scheduling decisions to latent feature\nestimation, and we introduce StaffPro, an LLM agent that addresses staffing and\nprofiling jointly. Differently from existing staffing solutions, StaffPro\nallows expressing optimization objectives using natural language, accepts\ntextual task descriptions and provides high flexibility. StaffPro interacts\ndirectly with humans by establishing a continuous human-agent feedback loop,\nensuring natural and intuitive use. By analyzing human feedback, our agent\ncontinuously estimates the latent features of workers, realizing life-long\nworker profiling and ensuring optimal staffing performance over time. A\nconsulting firm simulation example demonstrates that StaffPro successfully\nestimates workers' attributes and generates high quality schedules. With its\ninnovative design, StaffPro offers a robust, interpretable, and human-centric\nsolution for automated personnel management.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21636v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21636v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.445,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.334,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes an LLM agent that uses human feedback to estimate workers' attributes and improve staffing decisions over time, creating a feedback loop for continuous learning. This shares the concept of incorporating human input for system enhancement, which is a high-level idea in RLHF. However, the paper does not involve training a separate reward model on human-ranked data or using reinforcement learning algorithms to fine-tune the main model, focusing instead on direct analysis of feedback for profiling rather than the core RLHF methodology.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21637",
      "title": "Self-Aware Safety Augmentation: Leveraging Internal Semantic\n  Understanding to Enhance Safety in Vision-Language Models",
      "authors": [
        "Wanying Wang",
        "Zeyu Ma",
        "Han Zheng",
        "Xin Tan",
        "Mingang Chen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large vision-language models (LVLMs) are vulnerable to harmful input compared\nto their language-only backbones. We investigated this vulnerability by\nexploring LVLMs internal dynamics, framing their inherent safety understanding\nin terms of three key capabilities. Specifically, we define these capabilities\nas safety perception, semantic understanding, and alignment for linguistic\nexpression, and experimentally pinpointed their primary locations within the\nmodel architecture. The results indicate that safety perception often emerges\nbefore comprehensive semantic understanding, leading to the reduction in\nsafety. Motivated by these findings, we propose \\textbf{Self-Aware Safety\nAugmentation (SASA)}, a technique that projects informative semantic\nrepresentations from intermediate layers onto earlier safety-oriented layers.\nThis approach leverages the model's inherent semantic understanding to enhance\nsafety recognition without fine-tuning. Then, we employ linear probing to\narticulate the model's internal semantic comprehension to detect the risk\nbefore the generation process. Extensive experiments on various datasets and\ntasks demonstrate that SASA significantly improves the safety of LVLMs, with\nminimal impact on the utility.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21637v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21637v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.465,
      "weak_supervision_score": 0.426,
      "diffusion_reasoning_score": 0.449,
      "distributed_training_score": 0.359,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses existing methods like RLHF for safety alignment but does not use or contribute to RLHF in its main approach. SASA is a tuning-free technique that relies on internal model dynamics, without involving human feedback, reward models, or reinforcement learning.",
      "weak_supervision_justification": "The paper does not involve training models with programmatically generated or noisy labels. Instead, it focuses on analyzing and augmenting internal representations in LVLMs without any form of supervision or label generation.",
      "diffusion_reasoning_justification": "The paper does not mention or utilize diffusion models, iterative refinement for reasoning, or multi-step logical processes. It centers on safety enhancement in vision-language models through internal semantic projections, with no connection to diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21638",
      "title": "Assistax: A Hardware-Accelerated Reinforcement Learning Benchmark for\n  Assistive Robotics",
      "authors": [
        "Leonard Hinckeldey",
        "Elliot Fosong",
        "Elle Miller",
        "Rimvydas Rubavicius",
        "Trevor McInroe",
        "Patricia Wollstadt",
        "Christiane B. Wiebel-Herboth",
        "Subramanian Ramamoorthy",
        "Stefano V. Albrecht"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.RO (Robotics)"
      ],
      "abstract": "The development of reinforcement learning (RL) algorithms has been largely\ndriven by ambitious challenge tasks and benchmarks. Games have dominated RL\nbenchmarks because they present relevant challenges, are inexpensive to run and\neasy to understand. While games such as Go and Atari have led to many\nbreakthroughs, they often do not directly translate to real-world embodied\napplications. In recognising the need to diversify RL benchmarks and addressing\ncomplexities that arise in embodied interaction scenarios, we introduce\nAssistax: an open-source benchmark designed to address challenges arising in\nassistive robotics tasks. Assistax uses JAX's hardware acceleration for\nsignificant speed-ups for learning in physics-based simulations. In terms of\nopen-loop wall-clock time, Assistax runs up to $370\\times$ faster when\nvectorising training runs compared to CPU-based alternatives. Assistax\nconceptualises the interaction between an assistive robot and an active human\npatient using multi-agent RL to train a population of diverse partner agents\nagainst which an embodied robotic agent's zero-shot coordination capabilities\ncan be tested. Extensive evaluation and hyperparameter tuning for popular\ncontinuous control RL and MARL algorithms provide reliable baselines and\nestablish Assistax as a practical benchmark for advancing RL research for\nassistive robotics. The code is available at:\nhttps://github.com/assistive-autonomy/assistax.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21638v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21638v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.436,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.302,
      "distributed_training_score": 0.39,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the introduction of Assistax, a hardware-accelerated benchmark for reinforcement learning in assistive robotics, emphasizing multi-agent RL, zero-shot coordination, and simulations for tasks like human-robot interaction. It does not involve human feedback mechanisms, such as training reward models on human-ranked data or fine-tuning models based on human preferences, which are essential for RLHF. Therefore, the paper focuses on general RL advancements rather than alignment with human feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21640",
      "title": "GUARD-CAN: Graph-Understanding and Recurrent Architecture for CAN\n  Anomaly Detection",
      "authors": [
        "Hyeong Seon Kim",
        "Huy Kang Kim"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Modern in-vehicle networks face various cyber threats due to the lack of\nencryption and authentication in the Controller Area Network (CAN). To address\nthis security issue, this paper presents GUARD-CAN, an anomaly detection\nframework that combines graph-based representation learning with time-series\nmodeling. GUARD-CAN splits CAN messages into fixed-length windows and converts\neach window into a graph that preserves message order. To detect anomalies in\nthe timeaware and structure-aware context at the same window, GUARD-CAN takes\nadvantage of the overcomplete Autoencoder (AE) and Graph Convolutional Network\n(GCN) to generate graph embedding vectors. The model groups these vectors into\nsequences and feeds them into the Gated Recurrent Unit (GRU) to detect temporal\nanomaly patterns across the graphs. GUARD-CAN performs anomaly detection at\nboth the sequence level and the window level, and this allows multi-perspective\nperformance evaluation. The model also verifies the importance of window size\nselection through an analysis based on Shannon entropy. As a result, GUARD-CAN\nshows that the proposed model detects four types of CAN attacks (flooding,\nfuzzing, replay and spoofing attacks) effectively without relying on complex\nfeature engineering.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21640v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21640v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.282,
      "weak_supervision_score": 0.291,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.308,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21649",
      "title": "The Evolution of Video Anomaly Detection: A Unified Framework from DNN\n  to MLLM",
      "authors": [
        "Shibo Gao",
        "Peipei Yang",
        "Haiyang Guo",
        "Yangyang Liu",
        "Yi Chen",
        "Shuai Li",
        "Han Zhu",
        "Jian Xu",
        "Xu-Yao Zhang",
        "Linlin Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Video anomaly detection (VAD) aims to identify and ground anomalous behaviors\nor events in videos, serving as a core technology in the fields of intelligent\nsurveillance and public safety. With the advancement of deep learning, the\ncontinuous evolution of deep model architectures has driven innovation in VAD\nmethodologies, significantly enhancing feature representation and scene\nadaptability, thereby improving algorithm generalization and expanding\napplication boundaries. More importantly, the rapid development of multi-modal\nlarge language (MLLMs) and large language models (LLMs) has introduced new\nopportunities and challenges to the VAD field. Under the support of MLLMs and\nLLMs, VAD has undergone significant transformations in terms of data\nannotation, input modalities, model architectures, and task objectives. The\nsurge in publications and the evolution of tasks have created an urgent need\nfor systematic reviews of recent advancements. This paper presents the first\ncomprehensive survey analyzing VAD methods based on MLLMs and LLMs, providing\nan in-depth discussion of the changes occurring in the VAD field in the era of\nlarge models and their underlying causes. Additionally, this paper proposes a\nunified framework that encompasses both deep neural network (DNN)-based and\nLLM-based VAD methods, offering a thorough analysis of the new VAD paradigms\nempowered by LLMs, constructing a classification system, and comparing their\nstrengths and weaknesses. Building on this foundation, this paper focuses on\ncurrent VAD methods based on MLLMs/LLMs. Finally, based on the trajectory of\ntechnological advancements and existing bottlenecks, this paper distills key\nchallenges and outlines future research directions, offering guidance for the\nVAD community.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21649v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21649v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.36,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21653",
      "title": "DGP: A Dual-Granularity Prompting Framework for Fraud Detection with\n  Graph-Enhanced LLMs",
      "authors": [
        "Yuan Li",
        "Jun Hu",
        "Bryan Hooi",
        "Bingsheng He",
        "Cheng Chen"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Real-world fraud detection applications benefit from graph learning\ntechniques that jointly exploit node features, often rich in textual data, and\ngraph structural information. Recently, Graph-Enhanced LLMs emerge as a\npromising graph learning approach that converts graph information into prompts,\nexploiting LLMs' ability to reason over both textual and structural\ninformation. Among them, text-only prompting, which converts graph information\nto prompts consisting solely of text tokens, offers a solution that relies only\non LLM tuning without requiring additional graph-specific encoders. However,\ntext-only prompting struggles on heterogeneous fraud-detection graphs:\nmulti-hop relations expand exponentially with each additional hop, leading to\nrapidly growing neighborhoods associated with dense textual information. These\nneighborhoods may overwhelm the model with long, irrelevant content in the\nprompt and suppress key signals from the target node, thereby degrading\nperformance. To address this challenge, we propose Dual Granularity Prompting\n(DGP), which mitigates information overload by preserving fine-grained textual\ndetails for the target node while summarizing neighbor information into\ncoarse-grained text prompts. DGP introduces tailored summarization strategies\nfor different data modalities, bi-level semantic abstraction for textual fields\nand statistical aggregation for numerical features, enabling effective\ncompression of verbose neighbor content into concise, informative prompts.\nExperiments across public and industrial datasets demonstrate that DGP operates\nwithin a manageable token budget while improving fraud detection performance by\nup to 6.8% (AUPRC) over state-of-the-art methods, showing the potential of\nGraph-Enhanced LLMs for fraud detection.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21653v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21653v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.367,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes a prompting framework for fraud detection using graph-enhanced LLMs, focusing on data summarization and prompt design. It does not involve human feedback, reward models, or reinforcement learning for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses graph-to-prompt techniques for LLMs in fraud detection, with no reference to diffusion models, iterative refinement processes, or multi-step logical reasoning as a holistic entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21654",
      "title": "AI Literacy as a Key Driver of User Experience in AI-Powered Assessment:\n  Insights from Socratic Mind",
      "authors": [
        "Meryem Yilmaz Soylu",
        "Jeonghyun Lee",
        "Jui-Tse Hung",
        "Christopher Zhang Cui",
        "David A. Joyner"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As Artificial Intelligence (AI) tools become increasingly embedded in higher\neducation, understanding how students interact with these systems is essential\nto supporting effective learning. This study examines how students' AI literacy\nand prior exposure to AI technologies shape their perceptions of Socratic Mind,\nan interactive AI-powered formative assessment tool. Drawing on\nSelf-Determination Theory and user experience research, we analyze\nrelationships among AI literacy, perceived usability, satisfaction, engagement,\nand perceived learning effectiveness. Data from 309 undergraduates in Computer\nScience and Business courses were collected through validated surveys. Partial\nleast squares structural equation modeling showed that AI literacy - especially\nself-efficacy, conceptual understanding, and application skills - significantly\npredicts usability, satisfaction, and engagement. Usability and satisfaction,\nin turn, strongly predict perceived learning effectiveness, while prior AI\nexposure showed no significant effect. These findings highlight that AI\nliteracy, rather than exposure alone, shapes student experiences. Designers\nshould integrate adaptive guidance and user-centered features to support\ndiverse literacy levels, fostering inclusive, motivating, and effective\nAI-based learning environments.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21654v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21654v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.322,
      "distributed_training_score": 0.23,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is on AI literacy's impact on user experience in an educational AI tool, using surveys and structural equation modeling to analyze perceptions like usability and engagement. It does not involve reinforcement learning, human feedback for training AI models, or aligning models with human preferences, which are core to RLHF. Thus, there is no connection to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21664",
      "title": "Can the current trends of AI handle a full course of mathematics?",
      "authors": [
        "Mariam Alsayyad",
        "Fayadh Kadhem"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)",
        "math.HO (History and Overview)"
      ],
      "abstract": "This paper addresses the question of how able the current trends of\nArtificial Intelligence (AI) are in managing to take the responsibility of a\nfull course of mathematics at a college level. The study evaluates this ability\nin four significant aspects, namely, creating a course syllabus, presenting\nselected material, answering student questions, and creating an assessment. It\nshows that even though the AI is strong in some important parts like\norganization and accuracy, there are still some human aspects that are far away\nfrom the current abilities of AI. There is still a hidden emotional part, even\nin science, that cannot be fulfilled by the AI in its current state. This paper\nsuggests some recommendations to integrate the human and AI potentials to\ncreate better outcomes in terms of reaching the target of creating a full\ncourse of mathematics, at a university level, as best as possible.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21664v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21664v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.291,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21665",
      "title": "Automated Detection of Antarctic Benthic Organisms in High-Resolution In\n  Situ Imagery to Aid Biodiversity Monitoring",
      "authors": [
        "Cameron Trotter",
        "Huw Griffiths",
        "Tasnuva Ming Khan",
        "Rowan Whittle"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Monitoring benthic biodiversity in Antarctica is vital for understanding\necological change in response to climate-driven pressures. This work is\ntypically performed using high-resolution imagery captured in situ, though\nmanual annotation of such data remains laborious and specialised, impeding\nlarge-scale analysis. We present a tailored object detection framework for\nidentifying and classifying Antarctic benthic organisms in high-resolution\ntowed camera imagery, alongside the first public computer vision dataset for\nbenthic biodiversity monitoring in the Weddell Sea. Our approach addresses key\nchallenges associated with marine ecological imagery, including limited\nannotated data, variable object sizes, and complex seafloor structure. The\nproposed framework combines resolution-preserving patching, spatial data\naugmentation, fine-tuning, and postprocessing via Slicing Aided Hyper\nInference. We benchmark multiple object detection architectures and demonstrate\nstrong performance in detecting medium and large organisms across 25\nfine-grained morphotypes, significantly more than other works in this area.\nDetection of small and rare taxa remains a challenge, reflecting limitations in\ncurrent detection architectures. Our framework provides a scalable foundation\nfor future machine-assisted in situ benthic biodiversity monitoring research.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21665v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21665v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.294,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.313,
      "distributed_training_score": 0.342,
      "datasets_score": 0.381,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21684",
      "title": "diffSPH: Differentiable Smoothed Particle Hydrodynamics for Adjoint\n  Optimization and Machine Learning",
      "authors": [
        "Rene Winchenbach",
        "Nils Thuerey"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We present diffSPH, a novel open-source differentiable Smoothed Particle\nHydrodynamics (SPH) framework developed entirely in PyTorch with GPU\nacceleration. diffSPH is designed centrally around differentiation to\nfacilitate optimization and machine learning (ML) applications in Computational\nFluid Dynamics~(CFD), including training neural networks and the development of\nhybrid models. Its differentiable SPH core, and schemes for compressible (with\nshock capturing and multi-phase flows), weakly compressible (with boundary\nhandling and free-surface flows), and incompressible physics, enable a broad\nrange of application areas. We demonstrate the framework's unique capabilities\nthrough several applications, including addressing particle shifting via a\nnovel, target-oriented approach by minimizing physical and regularization loss\nterms, a task often intractable in traditional solvers. Further examples\ninclude optimizing initial conditions and physical parameters to match target\ntrajectories, shape optimization, implementing a solver-in-the-loop setup to\nemulate higher-order integration, and demonstrating gradient propagation\nthrough hundreds of full simulation steps. Prioritizing readability, usability,\nand extensibility, this work offers a foundational platform for the CFD\ncommunity to develop and deploy novel neural networks and adjoint optimization\napplications.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21684v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21684v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.402,
      "datasets_score": 0.263,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a differentiable Smoothed Particle Hydrodynamics (SPH) framework for optimization and machine learning in Computational Fluid Dynamics (CFD), emphasizing gradient-based methods for simulations. It does not involve diffusion models, iterative refinement for logical tasks, or treating a chain-of-thought as an entity for holistic correction. There is no component related to multi-step logical reasoning using diffusion processes, making it unrelated to this topic.",
      "distributed_training_justification": "The paper develops diffSPH in PyTorch with GPU acceleration, which involves parallel computing on GPUs, potentially overlapping with distributed training concepts. However, it does not primarily address algorithms for partitioning data or computation across multiple nodes, multi-node machine learning, or strategies for accelerating training in distributed environments. Its focus is on differentiability for CFD applications, not on distributed training innovations.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21690",
      "title": "APT: Improving Diffusion Models for High Resolution Image Generation\n  with Adaptive Path Tracing",
      "authors": [
        "Sangmin Han",
        "Jinho Jeong",
        "Jinwoo Kim",
        "Seon Joo Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Latent Diffusion Models (LDMs) are generally trained at fixed resolutions,\nlimiting their capability when scaling up to high-resolution images. While\ntraining-based approaches address this limitation by training on\nhigh-resolution datasets, they require large amounts of data and considerable\ncomputational resources, making them less practical. Consequently,\ntraining-free methods, particularly patch-based approaches, have become a\npopular alternative. These methods divide an image into patches and fuse the\ndenoising paths of each patch, showing strong performance on high-resolution\ngeneration. However, we observe two critical issues for patch-based approaches,\nwhich we call ``patch-level distribution shift\" and ``increased patch\nmonotonicity.\" To address these issues, we propose Adaptive Path Tracing (APT),\na framework that combines Statistical Matching to ensure patch distributions\nremain consistent in upsampled latents and Scale-aware Scheduling to deal with\nthe patch monotonicity. As a result, APT produces clearer and more refined\ndetails in high-resolution images. In addition, APT enables a shortcut\ndenoising process, resulting in faster sampling with minimal quality\ndegradation. Our experimental results confirm that APT produces more detailed\noutputs with improved inference speed, providing a practical approach to\nhigh-resolution image generation.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21690v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21690v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.577,
      "distributed_training_score": 0.402,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on improving diffusion models for high-resolution image generation, specifically addressing issues in patch-based approaches. It does not involve adapting diffusion processes for multi-step logical reasoning, Chain-of-Thought, or solving complex logical tasks, which are central to this topic.",
      "distributed_training_justification": "The paper discusses challenges with computational resources for training diffusion models but does not propose or evaluate methods for distributed training, parallel computing, or multi-node machine learning. It centers on training-free techniques for image generation, not on partitioning data, architecture, or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21693",
      "title": "MultiAIGCD: A Comprehensive dataset for AI Generated Code Detection\n  Covering Multiple Languages, Models,Prompts, and Scenarios",
      "authors": [
        "Basak Demirok",
        "Mucahid Kutlu",
        "Selin Mergen"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As large language models (LLMs) rapidly advance, their role in code\ngeneration has expanded significantly. While this offers streamlined\ndevelopment, it also creates concerns in areas like education and job\ninterviews. Consequently, developing robust systems to detect AI-generated code\nis imperative to maintain academic integrity and ensure fairness in hiring\nprocesses. In this study, we introduce MultiAIGCD, a dataset for AI-generated\ncode detection for Python, Java, and Go. From the CodeNet dataset's problem\ndefinitions and human-authored codes, we generate several code samples in Java,\nPython, and Go with six different LLMs and three different prompts. This\ngeneration process covered three key usage scenarios: (i) generating code from\nproblem descriptions, (ii) fixing runtime errors in human-written code, and\n(iii) correcting incorrect outputs. Overall, MultiAIGCD consists of 121,271\nAI-generated and 32,148 human-written code snippets. We also benchmark three\nstate-of-the-art AI-generated code detection models and assess their\nperformance in various test scenarios such as cross-model and cross-language.\nWe share our dataset and codes to support research in this field.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21693v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21693v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.418,
      "diffusion_reasoning_score": 0.431,
      "distributed_training_score": 0.398,
      "datasets_score": 0.523,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves programmatically generating AI-generated code samples, which could indirectly relate to weak supervision by creating labels without hand-labeling, but it does not focus on training models with noisy or imprecise sources. The main emphasis is on dataset creation and detection benchmarking, not weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper discusses LLMs for code generation and detection but does not mention or involve diffusion-based models, iterative refinement for reasoning, or any multi-step logical processes adapted from diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's primary contribution is the introduction, curation, and benchmarking of the MultiAIGCD dataset for AI-generated code detection, directly aligning with research on creating, analyzing, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces MultiAIGCD, a comprehensive dataset designed for detecting AI-generated code in Python, Java, and Go, comprising 121,271 AI-generated and 32,148 human-written code snippets generated from 800 programming problems using six large language models (LLMs) and three prompting strategies across three scenarios: code generation from problem definitions, fixing runtime errors, and correcting incorrect outputs. The authors evaluate three state-of-the-art detection models, revealing that OpenAI's ADA embeddings perform best overall, though accuracy declines in cross-language and error-fixing scenarios, highlighting the dataset's utility for advancing research in AI code detection.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a comprehensive dataset that combines multiple languages, models, prompts, and scenarios for AI-generated code detection, building on existing ideas rather than introducing a entirely new problem. While the core concept of detection exists, the dataset's breadth provides a clever expansion for systematic research.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI-generated code detection, as it provides a standardized dataset that can facilitate future comparisons and improvements in software engineering and AI ethics. However, its influence may be limited to specific research areas rather than broader commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a valuable contribution by offering a new dataset and benchmarks that are essential for researchers in AI and software engineering, making it important for those working in code detection to be aware of. While not groundbreaking, its practical utility and shared resources justify reading for relevant audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/96a3a5b5f98e68ff9c767e3500e3fb6e1f969baf",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 12,
      "average_h_index": 5.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Basak Demirok",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2315434756"
        },
        {
          "name": "Mucahid Kutlu",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/38330039"
        },
        {
          "name": "Selin Mergen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2315434890"
        }
      ]
    },
    {
      "id": "2507.21694",
      "title": "A Multi-Agent Generative AI Framework for IC Module-Level Verification\n  Automation",
      "authors": [
        "Wenbo Liu",
        "Forbes Hou",
        "Jon Zhang",
        "Hong Liu",
        "Allen Lei"
      ],
      "categories": [
        "cs.AR (Hardware Architecture)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As large language models demonstrate enormous potential in the field of\nElectronic Design Automation (EDA), generative AI-assisted chip design is\nattracting widespread attention from academia and industry. Although these\ntechnologies have made preliminary progress in tasks such as code generation,\ntheir application in chip verification -- a critical bottleneck in the chip\ndevelopment cycle -- remains at an exploratory stage. This paper proposes an\ninnovative Multi-Agent Verification Framework (MAVF) aimed at addressing the\nlimitations of current single-LLM approaches in complex verification tasks. Our\nframework builds an automated transformation system from design specifications\nto testbench through the collaborative work of multiple specialized agents,\nincluding specification parsing, verification strategy generation, and code\nimplementation. Through verification experiments on multiple chip modules of\nvarying complexity, results show that MAVF significantly outperforms\ntraditional manual methods and single-dialogue generative AI approaches in\nverification document parsing and generation, as well as automated testbench\ngeneration. This research opens new directions for exploring generative AI\napplications in verification automation, potentially providing effective\napproaches to solving the most challenging bottleneck issues in chip design.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21694v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21694v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.364,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a Multi-Agent Verification Framework using generative AI for chip verification tasks, involving agents for specification parsing, strategy generation, and code implementation. However, it does not mention diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. The framework relies on standard generative AI approaches with LLMs, lacking any clear component for diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21695",
      "title": "Towards a Large Physics Benchmark",
      "authors": [
        "Kristian G. Barman",
        "Sascha Caron",
        "Faegheh Hasibi",
        "Eugene Shalugin",
        "Yoris Marcet",
        "Johannes Otte",
        "Henk W. de Regt",
        "Merijn Moody"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "hep-ph (High Energy Physics - Phenomenology)"
      ],
      "abstract": "We introduce a benchmark framework developed by and for the scientific\ncommunity to evaluate, monitor and steer large language model development in\nfundamental physics. Building on philosophical concepts of scientific\nunderstanding and creativity, we develop a scoring system in which each\nquestion is scored by an expert for its correctness, difficulty, and surprise.\nThe questions are of three forms: (i) multiple-choice questions for conceptual\nunderstanding, (ii) analytical problems requiring mathematical derivation, and\n(iii) openended tasks requiring complex problem solving. Our current dataset\ncontains diverse set of examples, including a machine learning challenge to\nclassify high-energy physics events, such as the four top quark signal. To\nensure continued relevance, we propose a living benchmark, where physicists\ncontribute questions, for instance alongside new publications. We invite\ncontributions via: http://www.physicsbenchmarks.org/. We hope that this\nbenchmark will enable a targeted AI development that can make a meaningful\ncontribution to fundamental physics research.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21695v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21695v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.394,
      "datasets_score": 0.443,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on developing a benchmark for evaluating LLMs in physics, including human-in-the-loop for question creation and scoring, but does not involve training AI models using reinforcement learning with a reward model based on human feedback. Thus, it does not align with RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. It centers on benchmark development for LLMs in physics, with no connection to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is creating and evaluating a new benchmark dataset for LLMs in physics, including question curation, expert evaluation, and community contributions, which directly aligns with research on dataset creation, benchmarking, and analysis for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces a benchmark framework for evaluating large language models (LLMs) in fundamental physics, focusing on scientific understanding and creativity by drawing from philosophical concepts. It employs a methodology involving expert-scored questions across three formats—multiple-choice for conceptual understanding, analytical problems for mathematical derivation, and open-ended tasks for complex problem-solving—while presenting a dataset with diverse examples, such as classifying high-energy physics events, and proposing a living, community-driven benchmark to guide AI development and contributions to physics research.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by integrating philosophical ideas with existing benchmarking techniques to address gaps in evaluating LLMs for physics, offering a clever combination rather than a completely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "This work is likely to be cited and built upon within the AI and physics subfields, as it provides a targeted tool for advancing LLM capabilities in scientific reasoning and encourages community involvement.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution by filling a niche in LLM evaluation for physics, making it essential for researchers in AI and fundamental physics to engage with for its practical and philosophical insights.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/17c6d46e8688b604779ed9001b112fc197f3b36c",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 18,
      "average_h_index": 3.25,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "K. G. Barman",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2050272783"
        },
        {
          "name": "Sascha Caron",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2308272042"
        },
        {
          "name": "Faegheh Hasibi",
          "h_index": 18,
          "profile_url": "https://www.semanticscholar.org/author/1951737"
        },
        {
          "name": "Eugene Shalugin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373722327"
        },
        {
          "name": "Yoris Marcet",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373722332"
        },
        {
          "name": "Johannes Otte",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373721534"
        },
        {
          "name": "H. Regt",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2085335917"
        },
        {
          "name": "Merijn Moody",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373722290"
        }
      ]
    },
    {
      "id": "2507.21705",
      "title": "Unrolling Dynamic Programming via Graph Filters",
      "authors": [
        "Sergio Rozada",
        "Samuel Rey",
        "Gonzalo Mateos",
        "Antonio G. Marques"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Dynamic programming (DP) is a fundamental tool used across many engineering\nfields. The main goal of DP is to solve Bellman's optimality equations for a\ngiven Markov decision process (MDP). Standard methods like policy iteration\nexploit the fixed-point nature of these equations to solve them iteratively.\nHowever, these algorithms can be computationally expensive when the\nstate-action space is large or when the problem involves long-term\ndependencies. Here we propose a new approach that unrolls and truncates policy\niterations into a learnable parametric model dubbed BellNet, which we train to\nminimize the so-termed Bellman error from random value function\ninitializations. Viewing the transition probability matrix of the MDP as the\nadjacency of a weighted directed graph, we draw insights from graph signal\nprocessing to interpret (and compactly re-parameterize) BellNet as a cascade of\nnonlinear graph filters. This fresh look facilitates a concise, transferable,\nand unifying representation of policy and value iteration, with an explicit\nhandle on complexity during inference. Preliminary experiments conducted in a\ngrid-like environment demonstrate that BellNet can effectively approximate\noptimal policies in a fraction of the iterations required by classical methods.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21705v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21705v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.382,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves unrolling dynamic programming algorithms into a learnable model using graph filters for Markov decision processes, focusing on efficiency in policy and value iteration. While it employs iterative processes, it does not adapt the iterative refinement mechanism of diffusion models for multi-step logical reasoning or treat a Chain-of-Thought as a holistic entity for correction, as required by the topic. Thus, there is no clear connection to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21706",
      "title": "EnTao-GPM: DNA Foundation Model for Predicting the Germline Pathogenic\n  Mutations",
      "authors": [
        "Zekai Lin",
        "Haoran Sun",
        "Yucheng Guo",
        "Yujie Yang",
        "Yanwen Wang",
        "Bozhen Hu",
        "Chonghang Ye",
        "Qirong Yang",
        "Fan Zhong",
        "Xiaoming Zhang",
        "Lei Liu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Distinguishing pathogenic mutations from benign polymorphisms remains a\ncritical challenge in precision medicine. EnTao-GPM, developed by Fudan\nUniversity and BioMap, addresses this through three innovations: (1)\nCross-species targeted pre-training on disease-relevant mammalian genomes\n(human, pig, mouse), leveraging evolutionary conservation to enhance\ninterpretation of pathogenic motifs, particularly in non-coding regions; (2)\nGermline mutation specialization via fine-tuning on ClinVar and HGMD, improving\naccuracy for both SNVs and non-SNVs; (3) Interpretable clinical framework\nintegrating DNA sequence embeddings with LLM-based statistical explanations to\nprovide actionable insights. Validated against ClinVar, EnTao-GPM demonstrates\nsuperior accuracy in mutation classification. It revolutionizes genetic testing\nby enabling faster, more accurate, and accessible interpretation for clinical\ndiagnostics (e.g., variant assessment, risk identification, personalized\ntreatment) and research, advancing personalized medicine.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21706v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21706v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.33,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21715",
      "title": "Impact of Underwater Image Enhancement on Feature Matching",
      "authors": [
        "Jason M. Summers",
        "Mark W. Jones"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce local matching stability and furthest matchable frame as\nquantitative measures for evaluating the success of underwater image\nenhancement. This enhancement process addresses visual degradation caused by\nlight absorption, scattering, marine growth, and debris. Enhanced imagery plays\na critical role in downstream tasks such as path detection and autonomous\nnavigation for underwater vehicles, relying on robust feature extraction and\nframe matching. To assess the impact of enhancement techniques on\nframe-matching performance, we propose a novel evaluation framework tailored to\nunderwater environments. Through metric-based analysis, we identify strengths\nand limitations of existing approaches and pinpoint gaps in their assessment of\nreal-world applicability. By incorporating a practical matching strategy, our\nframework offers a robust, context-aware benchmark for comparing enhancement\nmethods. Finally, we demonstrate how visual improvements affect the performance\nof a complete real-world algorithm -- Simultaneous Localization and Mapping\n(SLAM) -- reinforcing the framework's relevance to operational underwater\nscenarios.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21715v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21715v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.322,
      "distributed_training_score": 0.24,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21723",
      "title": "Detection Transformers Under the Knife: A Neuroscience-Inspired Approach\n  to Ablations",
      "authors": [
        "Nils Hütten",
        "Florian Hölken",
        "Hasan Tercan",
        "Tobias Meisen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In recent years, Explainable AI has gained traction as an approach to\nenhancing model interpretability and transparency, particularly in complex\nmodels such as detection transformers. Despite rapid advancements, a\nsubstantial research gap remains in understanding the distinct roles of\ninternal components - knowledge that is essential for improving transparency\nand efficiency. Inspired by neuroscientific ablation studies, which investigate\nthe functions of brain regions through selective impairment, we systematically\nanalyze the impact of ablating key components in three state-of-the-art\ndetection transformer models: Detection transformer (DETR), deformable\ndetection transformer (DDETR), and DETR with improved denoising anchor boxes\n(DINO). The ablations target query embeddings, encoder and decoder multi-head\nself-attentions (MHSA) as well as decoder multi-head cross-attention (MHCA)\nlayers. We evaluate the effects of these ablations on the performance metrics\ngIoU and F1-score, quantifying effects on both the classification and\nregression sub-tasks on the COCO dataset. To facilitate reproducibility and\nfuture research, we publicly release the DeepDissect library. Our findings\nreveal model-specific resilience patterns: while DETR is particularly sensitive\nto ablations in encoder MHSA and decoder MHCA, DDETR's multi-scale deformable\nattention enhances robustness, and DINO exhibits the greatest resilience due to\nits look-forward twice update rule, which helps distributing knowledge across\nblocks. These insights also expose structural redundancies, particularly in\nDDETR's and DINO's decoder MHCA layers, highlighting opportunities for model\nsimplification without sacrificing performance. This study advances XAI for\nDETRs by clarifying the contributions of internal components to model\nperformance, offering insights to optimize and improve transparency and\nefficiency in critical applications.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21723v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21723v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.467,
      "distributed_training_score": 0.388,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on ablation studies for detection transformers in object detection, using neuroscience-inspired methods to analyze internal components. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21727",
      "title": "GDAIP: A Graph-Based Domain Adaptive Framework for Individual Brain\n  Parcellation",
      "authors": [
        "Jianfei Zhu",
        "Haiqi Zhu",
        "Shaohui Liu",
        "Feng Jiang",
        "Baichun Wei",
        "Chunzhi Yi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent deep learning approaches have shown promise in learning such\nindividual brain parcellations from functional magnetic resonance imaging\n(fMRI). However, most existing methods assume consistent data distributions\nacross domains and struggle with domain shifts inherent to real-world\ncross-dataset scenarios. To address this challenge, we proposed Graph Domain\nAdaptation for Individual Parcellation (GDAIP), a novel framework that\nintegrates Graph Attention Networks (GAT) with Minimax Entropy (MME)-based\ndomain adaptation. We construct cross-dataset brain graphs at both the group\nand individual levels. By leveraging semi-supervised training and adversarial\noptimization of the prediction entropy on unlabeled vertices from target brain\ngraph, the reference atlas is adapted from the group-level brain graph to the\nindividual brain graph, enabling individual parcellation under cross-dataset\nsettings. We evaluated our method using parcellation visualization, Dice\ncoefficient, and functional homogeneity. Experimental results demonstrate that\nGDAIP produces individual parcellations with topologically plausible\nboundaries, strong cross-session consistency, and ability of reflecting\nfunctional organization.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21727v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21727v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.402,
      "datasets_score": 0.387,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on graph-based domain adaptation using Graph Attention Networks (GAT) and Minimax Entropy for brain parcellation from fMRI data. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought correction.",
      "distributed_training_justification": "The paper describes a framework for individual brain parcellation and domain adaptation but does not address distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes or processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21732",
      "title": "SAMITE: Position Prompted SAM2 with Calibrated Memory for Visual Object\n  Tracking",
      "authors": [
        "Qianxiong Xu",
        "Lanyun Zhu",
        "Chenxi Liu",
        "Guosheng Lin",
        "Cheng Long",
        "Ziyue Li",
        "Rui Zhao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Visual Object Tracking (VOT) is widely used in applications like autonomous\ndriving to continuously track targets in videos. Existing methods can be\nroughly categorized into template matching and autoregressive methods, where\nthe former usually neglects the temporal dependencies across frames and the\nlatter tends to get biased towards the object categories during training,\nshowing weak generalizability to unseen classes. To address these issues, some\nmethods propose to adapt the video foundation model SAM2 for VOT, where the\ntracking results of each frame would be encoded as memory for conditioning the\nrest of frames in an autoregressive manner. Nevertheless, existing methods fail\nto overcome the challenges of object occlusions and distractions, and do not\nhave any measures to intercept the propagation of tracking errors. To tackle\nthem, we present a SAMITE model, built upon SAM2 with additional modules,\nincluding: (1) Prototypical Memory Bank: We propose to quantify the\nfeature-wise and position-wise correctness of each frame's tracking results,\nand select the best frames to condition subsequent frames. As the features of\noccluded and distracting objects are feature-wise and position-wise inaccurate,\ntheir scores would naturally be lower and thus can be filtered to intercept\nerror propagation; (2) Positional Prompt Generator: To further reduce the\nimpacts of distractors, we propose to generate positional mask prompts to\nprovide explicit positional clues for the target, leading to more accurate\ntracking. Extensive experiments have been conducted on six benchmarks, showing\nthe superiority of SAMITE. The code is available at\nhttps://github.com/Sam1224/SAMITE.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21732v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21732v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.293,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21738",
      "title": "Zero-Shot Machine Unlearning with Proxy Adversarial Data Generation",
      "authors": [
        "Huiqiang Chen",
        "Tianqing Zhu",
        "Xin Yu",
        "Wanlei Zhou"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Machine unlearning aims to remove the influence of specific samples from a\ntrained model. A key challenge in this process is over-unlearning, where the\nmodel's performance on the remaining data significantly drops due to the change\nin the model's parameters. Existing unlearning algorithms depend on the\nremaining data to prevent this issue. As such, these methods are inapplicable\nin a more practical scenario, where only the unlearning samples are available\n(i.e., zero-shot unlearning). This paper presents a novel framework, ZS-PAG, to\nfill this gap. Our approach offers three key innovations: (1) we approximate\nthe inaccessible remaining data by generating adversarial samples; (2)\nleveraging the generated samples, we pinpoint a specific subspace to perform\nthe unlearning process, therefore preventing over-unlearning in the challenging\nzero-shot scenario; and (3) we consider the influence of the unlearning process\non the remaining samples and design an influence-based pseudo-labeling\nstrategy. As a result, our method further improves the model's performance\nafter unlearning. The proposed method holds a theoretical guarantee, and\nexperiments on various benchmarks validate the effectiveness and superiority of\nour proposed method over several baselines.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21738v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21738v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.475,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.402,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves generating and optimizing pseudo-labels for unlearning samples using adversarial data and influence functions, which shares some similarities with weak supervision's use of programmatically generated labels. However, the core focus is on machine unlearning rather than training models with noisy or imprecise labels, making the connection indirect.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on machine unlearning techniques, such as adversarial data generation and subspace projection, with no mention of distributed training, parallel computing, or partitioning data across multiple nodes or processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21741",
      "title": "MAGE: Multimodal Alignment and Generation Enhancement via Bridging\n  Visual and Semantic Spaces",
      "authors": [
        "Shaojun E",
        "Yuchen Yang",
        "Jiaheng Wu",
        "Yan Zhang",
        "Tiejun Zhao",
        "Ziyan Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "In the latest advancements in multimodal learning, effectively addressing the\nspatial and semantic losses of visual data after encoding remains a critical\nchallenge. This is because the performance of large multimodal models is\npositively correlated with the coupling between visual encoders and large\nlanguage models. Existing approaches often face issues such as vector gaps or\nsemantic disparities, resulting in information loss during the propagation\nprocess. To address these issues, we propose MAGE (Multimodal Alignment and\nGeneration Enhancement), a novel framework that bridges the semantic spaces of\nvision and text through an innovative alignment mechanism. By introducing the\nIntelligent Alignment Network (IAN), MAGE achieves dimensional and semantic\nalignment. To reduce the gap between synonymous heterogeneous data, we employ a\ntraining strategy that combines cross-entropy and mean squared error,\nsignificantly enhancing the alignment effect. Moreover, to enhance MAGE's\n\"Any-to-Any\" capability, we developed a fine-tuning dataset for multimodal\ntool-calling instructions to expand the model's output capability boundaries.\nFinally, our proposed multimodal large model architecture, MAGE, achieved\nsignificantly better performance compared to similar works across various\nevaluation benchmarks, including MME, MMBench, and SEED. Complete code and\nappendix are available at: https://github.com/GTCOM-NLP/MAGE.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21741v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21741v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.381,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for multimodal alignment and generation enhancement, focusing on bridging visual and semantic spaces using an Intelligent Alignment Network, a dual-loss training strategy, and a dataset for tool-calling. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as defined by the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21742",
      "title": "Adversarial Reconstruction Feedback for Robust Fine-grained\n  Generalization",
      "authors": [
        "Shijie Wang",
        "Jian Shi",
        "Haojie Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Existing fine-grained image retrieval (FGIR) methods predominantly rely on\nsupervision from predefined categories to learn discriminative representations\nfor retrieving fine-grained objects. However, they inadvertently introduce\ncategory-specific semantics into the retrieval representation, creating\nsemantic dependencies on predefined classes that critically hinder\ngeneralization to unseen categories. To tackle this, we propose AdvRF, a novel\nadversarial reconstruction feedback framework aimed at learning\ncategory-agnostic discrepancy representations. Specifically, AdvRF reformulates\nFGIR as a visual discrepancy reconstruction task via synergizing category-aware\ndiscrepancy localization from retrieval models with category-agnostic feature\nlearning from reconstruction models. The reconstruction model exposes residual\ndiscrepancies overlooked by the retrieval model, forcing it to improve\nlocalization accuracy, while the refined signals from the retrieval model guide\nthe reconstruction model to improve its reconstruction ability. Consequently,\nthe retrieval model localizes visual differences, while the reconstruction\nmodel encodes these differences into category-agnostic representations. This\nrepresentation is then transferred to the retrieval model through knowledge\ndistillation for efficient deployment. Quantitative and qualitative evaluations\ndemonstrate that our AdvRF achieves impressive performance on both widely-used\nfine-grained and coarse-grained datasets.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21742v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21742v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.349,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework for fine-grained image retrieval using adversarial training between a retrieval model and a reconstruction model, inspired by GANs. It involves alternating optimization to learn category-agnostic representations, with no mention of human feedback, reward models, or reinforcement learning techniques. RLHF specifically requires human-ranked data and reinforcement-based fine-tuning, which are absent here.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21745",
      "title": "Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable\n  Rewards",
      "authors": [
        "Aybora Koksal",
        "A. Aydin Alatan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in large language and vision-language models have enabled\nstrong reasoning capabilities, yet they remain impractical for specialized\ndomains like remote sensing, where annotated data is scarce and expensive. We\npresent the first few-shot reinforcement learning with verifiable reward (RLVR)\nframework for satellite imagery that eliminates the need for caption\nsupervision--relying solely on lightweight, rule-based binary or IoU-based\nrewards. Adapting the \"1-shot RLVR\" paradigm from language models to\nvision-language models, we employ policy-gradient optimization with as few as\none curated example to align model outputs for satellite reasoning tasks.\nComprehensive experiments across multiple remote sensing benchmarks--including\nclassification, visual question answering, and grounding--show that even a\nsingle example yields substantial improvements over the base model. Scaling to\n128 examples matches or exceeds models trained on thousands of annotated\nsamples. While the extreme one-shot setting can induce mild, task-specific\noverfitting, our approach consistently demonstrates robust generalization and\nefficiency across diverse tasks. Further, we find that prompt design and loss\nweighting significantly influence training stability and final accuracy. Our\nmethod enables cost-effective and data-efficient development of\ndomain-specialist vision-language reasoning models, offering a pragmatic recipe\nfor data-scarce fields: start from a compact VLM, curate a handful of\nreward-checkable cases, and train via RLVR.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21745v2",
      "pdf_url": "http://arxiv.org/pdf/2507.21745v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.467,
      "weak_supervision_score": 0.415,
      "diffusion_reasoning_score": 0.466,
      "distributed_training_score": 0.351,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Reinforcement Learning with Verifiable Rewards (RLVR) using rule-based binary or IoU-based rewards, which are automated and not derived from human preferences or rankings. It explicitly avoids human feedback, relying instead on programmatic verification, so it does not align with RLHF.",
      "weak_supervision_justification": "The paper employs a few-shot RLVR approach with minimal, programmatically generated rewards (e.g., binary or IoU-based) from curated examples, reducing the need for extensive hand-labeled data. This partially overlaps with weak supervision by using noisy or imprecise sources for training signals, though it is specifically tailored to reinforcement learning rather than broader weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper describes a reinforcement learning framework for vision-language models but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined. It focuses on policy-gradient optimization with verifiable rewards, with no mention of adapting diffusion for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces a few-shot reinforcement learning with verifiable reward (RLVR) framework for vision-language models applied to satellite imagery, aiming to enable effective reasoning tasks like classification, visual question answering, and grounding without the need for extensive annotated captions. By using policy-gradient optimization with as few as one curated example and binary or IoU-based rewards, the method achieves substantial performance improvements over base models, scales effectively to match supervised baselines with far fewer examples, and demonstrates robust generalization while highlighting the importance of prompt design and loss weighting for training stability.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel adaptation of the RLVR paradigm from language models to vision-language models for remote sensing, addressing a new problem in a data-scarce domain and significantly advancing the state-of-the-art by eliminating the need for caption supervision.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in remote sensing and few-shot learning by providing a cost-effective method for model training, though its applicability is primarily within specific subfields rather than broadly across all AI domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality and innovative contribution to vision-language models in specialized areas like remote sensing, making it valuable for researchers in AI and data-efficient learning, though it may not be essential for those outside these fields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6cd0e918e5c5fad702dfd71e5ee4762a4c3c076c",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 10,
      "average_h_index": 6.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Aybora Koksal",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/51219713"
        },
        {
          "name": "A. Alatan",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/143768962"
        }
      ]
    },
    {
      "id": "2507.21752",
      "title": "SAT-Based Bounded Fitting for the Description Logic ALC",
      "authors": [
        "Maurice Funk",
        "Jean Christoph Jung",
        "Tom Voellmer"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Bounded fitting is a general paradigm for learning logical formulas from\npositive and negative data examples, that has received considerable interest\nrecently. We investigate bounded fitting for the description logic ALC and its\nsyntactic fragments. We show that the underlying size-restricted fitting\nproblem is NP-complete for all studied fragments, even in the special case of a\nsingle positive and a single negative example. By design, bounded fitting comes\nwith probabilistic guarantees in Valiant's PAC learning framework. In contrast,\nwe show that other classes of algorithms for learning ALC concepts do not\nprovide such guarantees. Finally, we present an implementation of bounded\nfitting in ALC and its fragments based on a SAT solver. We discuss\noptimizations and compare our implementation to other concept learning tools.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21752v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21752v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.248,
      "datasets_score": 0.255,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21753",
      "title": "Towards a rigorous evaluation of RAG systems: the challenge of due\n  diligence",
      "authors": [
        "Grégoire Martinon",
        "Alexandra Lorenzo de Brionne",
        "Jérôme Bohard",
        "Antoine Lojou",
        "Damien Hervault",
        "Nicolas J-B. Brunel"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "stat.AP (Applications)"
      ],
      "abstract": "The rise of generative AI, has driven significant advancements in high-risk\nsectors like healthcare and finance. The Retrieval-Augmented Generation (RAG)\narchitecture, combining language models (LLMs) with search engines, is\nparticularly notable for its ability to generate responses from document\ncorpora. Despite its potential, the reliability of RAG systems in critical\ncontexts remains a concern, with issues such as hallucinations persisting. This\nstudy evaluates a RAG system used in due diligence for an investment fund. We\npropose a robust evaluation protocol combining human annotations and LLM-Judge\nannotations to identify system failures, like hallucinations, off-topic, failed\ncitations, and abstentions. Inspired by the Prediction Powered Inference (PPI)\nmethod, we achieve precise performance measurements with statistical\nguarantees. We provide a comprehensive dataset for further analysis. Our\ncontributions aim to enhance the reliability and scalability of RAG systems\nevaluation protocols in industrial applications.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21753v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21753v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.426,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.31,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating RAG systems using a hybrid protocol of human annotations and LLM-Judge for reliability assessment, but it does not involve training or fine-tuning an AI model with a reward model based on human-ranked data via reinforcement learning. Human feedback here is used solely for evaluation, not for alignment or RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses evaluation protocols for RAG systems and does not mention or adapt diffusion models for iterative refinement in multi-step logical reasoning. There is no component involving a Chain-of-Thought as a single entity for holistic correction, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21756",
      "title": "LiteFat: Lightweight Spatio-Temporal Graph Learning for Real-Time Driver\n  Fatigue Detection",
      "authors": [
        "Jing Ren",
        "Suyu Ma",
        "Hong Jia",
        "Xiwei Xu",
        "Ivan Lee",
        "Haytham Fayek",
        "Xiaodong Li",
        "Feng Xia"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Detecting driver fatigue is critical for road safety, as drowsy driving\nremains a leading cause of traffic accidents. Many existing solutions rely on\ncomputationally demanding deep learning models, which result in high latency\nand are unsuitable for embedded robotic devices with limited resources (such as\nintelligent vehicles/cars) where rapid detection is necessary to prevent\naccidents. This paper introduces LiteFat, a lightweight spatio-temporal graph\nlearning model designed to detect driver fatigue efficiently while maintaining\nhigh accuracy and low computational demands. LiteFat involves converting\nstreaming video data into spatio-temporal graphs (STG) using facial landmark\ndetection, which focuses on key motion patterns and reduces unnecessary data\nprocessing. LiteFat uses MobileNet to extract facial features and create a\nfeature matrix for the STG. A lightweight spatio-temporal graph neural network\nis then employed to identify signs of fatigue with minimal processing and low\nlatency. Experimental results on benchmark datasets show that LiteFat performs\ncompetitively while significantly decreasing computational complexity and\nlatency as compared to current state-of-the-art methods. This work enables the\ndevelopment of real-time, resource-efficient human fatigue detection systems\nthat can be implemented upon embedded robotic devices.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21756v2",
      "pdf_url": "http://arxiv.org/pdf/2507.21756v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.379,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21761",
      "title": "MOR-VIT: Efficient Vision Transformer with Mixture-of-Recursions",
      "authors": [
        "YiZhou Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision Transformers (ViTs) have achieved remarkable success in image\nrecognition, yet standard ViT architectures are hampered by substantial\nparameter redundancy and high computational cost, limiting their practical\ndeployment. While recent efforts on efficient ViTs primarily focus on static\nmodel compression or token-level sparsification, they remain constrained by\nfixed computational depth for all tokens. In this work, we present MoR-ViT, a\nnovel vision transformer framework that, for the first time, incorporates a\ntoken-level dynamic recursion mechanism inspired by the Mixture-of-Recursions\n(MoR) paradigm. This approach enables each token to adaptively determine its\nprocessing depth, yielding a flexible and input-dependent allocation of\ncomputational resources. Extensive experiments on ImageNet-1K and transfer\nbenchmarks demonstrate that MoR-ViT not only achieves state-of-the-art accuracy\nwith up to 70% parameter reduction and 2.5x inference acceleration, but also\noutperforms leading efficient ViT baselines such as DynamicViT and TinyViT\nunder comparable conditions. These results establish dynamic recursion as an\neffective strategy for efficient vision transformers and open new avenues for\nscalable and deployable deep learning models in real-world scenarios.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21761v2",
      "pdf_url": "http://arxiv.org/pdf/2507.21761v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.39,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of an efficient Vision Transformer (MoR-ViT) that uses a dynamic recursion mechanism to optimize computational resources for image recognition tasks. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21763",
      "title": "Learning Kinetic Monte Carlo stochastic dynamics with Deep Generative\n  Adversarial Networks",
      "authors": [
        "Daniele Lanzoni",
        "Olivier Pierre-Louis",
        "Roberto Bergamaschini",
        "Francesco Montalenti"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We show that Generative Adversarial Networks (GANs) may be fruitfully\nexploited to learn stochastic dynamics, surrogating traditional models while\ncapturing thermal fluctuations. Specifically, we showcase the application to a\ntwo-dimensional, many-particle system, focusing on surface-step fluctuations\nand on the related time-dependent roughness. After the construction of a\ndataset based on Kinetic Monte Carlo simulations, a conditional GAN is trained\nto propagate stochastically the state of the system in time, allowing the\ngeneration of new sequences with a reduced computational cost. Modifications\nwith respect to standard GANs, which facilitate convergence and increase\naccuracy, are discussed. The trained network is demonstrated to quantitatively\nreproduce equilibrium and kinetic properties, including scaling laws, with\ndeviations of a few percent from the exact value. Extrapolation limits and\nfuture perspectives are critically discussed.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21763v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21763v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.37,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves using Generative Adversarial Networks (GANs) to learn and simulate stochastic dynamics in physical systems, based on Kinetic Monte Carlo data. It does not employ diffusion models or their iterative refinement processes for multi-step logical reasoning or Chain-of-Thought tasks. Although the introduction references a prior work using diffusion models for similar dynamics, the paper itself focuses on GANs and physical simulations, with no components related to logical reasoning or diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21770",
      "title": "Proposing a Semantic Movie Recommendation System Enhanced by ChatGPT's\n  NLP Results",
      "authors": [
        "Ali Fallahi",
        "Azam Bastanfard",
        "Amineh Amini",
        "Hadi Saboohi"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The importance of recommender systems on the web has grown, especially in the\nmovie industry, with a vast selection of options to watch. To assist users in\ntraversing available items and finding relevant results, recommender systems\nanalyze operational data and investigate users' tastes and habits. Providing\nhighly individualized suggestions can boost user engagement and satisfaction,\nwhich is one of the fundamental goals of the movie industry, significantly in\nonline platforms. According to recent studies and research, using\nknowledge-based techniques and considering the semantic ideas of the textual\ndata is a suitable way to get more appropriate results. This study provides a\nnew method for building a knowledge graph based on semantic information. It\nuses the ChatGPT, as a large language model, to assess the brief descriptions\nof movies and extract their tone of voice. Results indicated that using the\nproposed method may significantly enhance accuracy rather than employing the\nexplicit genres supplied by the publishers.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21770v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21770v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.261,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21778",
      "title": "AU-LLM: Micro-Expression Action Unit Detection via Enhanced LLM-Based\n  Feature Fusion",
      "authors": [
        "Zhishu Liu",
        "Kaishen Yuan",
        "Bo Zhao",
        "Yong Xu",
        "Zitong Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The detection of micro-expression Action Units (AUs) is a formidable\nchallenge in affective computing, pivotal for decoding subtle, involuntary\nhuman emotions. While Large Language Models (LLMs) demonstrate profound\nreasoning abilities, their application to the fine-grained, low-intensity\ndomain of micro-expression AU detection remains unexplored. This paper pioneers\nthis direction by introducing \\textbf{AU-LLM}, a novel framework that for the\nfirst time uses LLM to detect AUs in micro-expression datasets with subtle\nintensities and the scarcity of data. We specifically address the critical\nvision-language semantic gap, the \\textbf{Enhanced Fusion Projector (EFP)}. The\nEFP employs a Multi-Layer Perceptron (MLP) to intelligently fuse mid-level\n(local texture) and high-level (global semantics) visual features from a\nspecialized 3D-CNN backbone into a single, information-dense token. This\ncompact representation effectively empowers the LLM to perform nuanced\nreasoning over subtle facial muscle movements.Through extensive evaluations on\nthe benchmark CASME II and SAMM datasets, including stringent\nLeave-One-Subject-Out (LOSO) and cross-domain protocols, AU-LLM establishes a\nnew state-of-the-art, validating the significant potential and robustness of\nLLM-based reasoning for micro-expression analysis. The codes are available at\nhttps://github.com/ZS-liu-JLU/AU-LLMs.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21778v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21778v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.33,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using LLMs for micro-expression AU detection with feature fusion and LoRA for adaptation, but it does not involve human feedback, reward models, or reinforcement learning techniques. There is no mention of aligning models with human preferences through ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs LLMs for reasoning over visual features in AU detection, but it does not use diffusion models, iterative refinement processes, or treat Chain-of-Thought as an entity for multi-step correction. There is no component for diffusion-based logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21786",
      "title": "MSGCoOp: Multiple Semantic-Guided Context Optimization for Few-Shot\n  Learning",
      "authors": [
        "Zhaolong Wang",
        "Tongfeng Sun",
        "Mingzheng Du",
        "Yachao Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision-language pre-trained models (VLMs) such as CLIP have demonstrated\nremarkable zero-shot generalization, and prompt learning has emerged as an\nefficient alternative to full fine-tuning. However, existing methods often\nstruggle with generalization to novel classes, a phenomenon attributed to\noverfitting on seen classes and forgetting general knowledge. Furthermore,\nrecent approaches that improve generalization often introduce complex\narchitectures or heavy computational overhead. In this paper, we propose a\nMultiple Semantic-Guided Context Optimization (MSGCoOp) framework to enhance\nfew-shot generalization while maintaining computational efficiency. Our\napproach leverages an ensemble of parallel learnable context vectors to capture\ndiverse semantic aspects. To enrich these prompts, we introduce a semantic\nguidance mechanism that aligns them with comprehensive class descriptions\nautomatically generated by a Large Language Model (LLM). Furthermore, a\ndiversity regularization loss encourages the prompts to learn complementary and\northogonal features, preventing them from collapsing into redundant\nrepresentations. Extensive experiments on 11 benchmark datasets show that\nMSGCoOp significantly improves performance on base-to-novel generalization,\nachieving an average harmonic mean improvement of 1.10\\% over the strong KgCoOp\nbaseline. Our method also demonstrates enhanced robustness in cross-domain\ngeneralization tasks. Our code is avaliable at:\n\\href{https://github.com/Rain-Bus/MSGCoOp}{https://github.com/Rain-Bus/MSGCoOp}.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21786v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21786v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.389,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for few-shot learning in vision-language models, focusing on prompt optimization with semantic guidance and diversity regularization. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21790",
      "title": "Can large language models assist choice modelling? Insights into\n  prompting strategies and current models capabilities",
      "authors": [
        "Georges Sfeir",
        "Gabriel Nova",
        "Stephane Hess",
        "Sander van Cranenburgh"
      ],
      "categories": [
        "econ.EM (Econometrics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) are widely used to support various workflows\nacross different disciplines, yet their potential in choice modelling remains\nrelatively unexplored. This work examines the potential of LLMs as assistive\nagents in the specification and, where technically feasible, estimation of\nMultinomial Logit models. We implement a systematic experimental framework\ninvolving thirteen versions of six leading LLMs (ChatGPT, Claude, DeepSeek,\nGemini, Gemma, and Llama) evaluated under five experimental configurations.\nThese configurations vary along three dimensions: modelling goal (suggesting\nvs. suggesting and estimating MNLs); prompting strategy (Zero-Shot vs.\nChain-of-Thoughts); and information availability (full dataset vs. data\ndictionary only). Each LLM-suggested specification is implemented, estimated,\nand evaluated based on goodness-of-fit metrics, behavioural plausibility, and\nmodel complexity. Findings reveal that proprietary LLMs can generate valid and\nbehaviourally sound utility specifications, particularly when guided by\nstructured prompts. Open-weight models such as Llama and Gemma struggled to\nproduce meaningful specifications. Claude 4 Sonnet consistently produced the\nbest-fitting and most complex models, while GPT models suggested models with\nrobust and stable modelling outcomes. Some LLMs performed better when provided\nwith just data dictionary, suggesting that limiting raw data access may enhance\ninternal reasoning capabilities. Among all LLMs, GPT o3 was uniquely capable of\ncorrectly estimating its own specifications by executing self-generated code.\nOverall, the results demonstrate both the promise and current limitations of\nLLMs as assistive agents in choice modelling, not only for model specification\nbut also for supporting modelling decision and estimation, and provide\npractical guidance for integrating these tools into choice modellers'\nworkflows.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21790v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21790v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.467,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.48,
      "distributed_training_score": 0.324,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using existing LLMs for choice modelling tasks, such as model specification and estimation, through prompting strategies. It does not discuss training, fine-tuning, or aligning AI models with human preferences via RLHF. While some LLMs mentioned (e.g., ChatGPT) may have been developed using RLHF, the paper's main contribution is not related to RLHF systems or their mechanisms.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates prompting strategies like Chain-of-Thoughts, which involves multi-step reasoning, but it does not adapt or use diffusion models for iterative refinement of reasoning paths. While Chain-of-Thoughts shares the concept of step-by-step logical processing, there is no mention of diffusion-based processes or holistic correction mechanisms as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21792",
      "title": "Hybrid Causal Identification and Causal Mechanism Clustering",
      "authors": [
        "Saixiong Liu",
        "Yuhua Qian",
        "Jue Li",
        "Honghong Cheng",
        "Feijiang Li"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Bivariate causal direction identification is a fundamental and vital problem\nin the causal inference field. Among binary causal methods, most methods based\non additive noise only use one single causal mechanism to construct a causal\nmodel. In the real world, observations are always collected in different\nenvironments with heterogeneous causal relationships. Therefore, on observation\ndata, this paper proposes a Mixture Conditional Variational Causal Inference\nmodel (MCVCI) to infer heterogeneous causality. Specifically, according to the\nidentifiability of the Hybrid Additive Noise Model (HANM), MCVCI combines the\nsuperior fitting capabilities of the Gaussian mixture model and the neural\nnetwork and elegantly uses the likelihoods obtained from the probabilistic\nbounds of the mixture conditional variational auto-encoder as causal decision\ncriteria. Moreover, we model the casual heterogeneity into cluster numbers and\npropose the Mixture Conditional Variational Causal Clustering (MCVCC) method,\nwhich can reveal causal mechanism expression. Compared with state-of-the-art\nmethods, the comprehensive best performance demonstrates the effectiveness of\nthe methods proposed in this paper on several simulated and real data.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21792v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21792v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.315,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on hybrid causal identification and causal mechanism clustering using models like Mixture Conditional Variational Causal Inference (MCVCI) and variational auto-encoders. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21794",
      "title": "Distribution-Based Masked Medical Vision-Language Model Using Structured\n  Reports",
      "authors": [
        "Shreyank N Gowda",
        "Ruichi Zhang",
        "Xiao Gu",
        "Ying Weng",
        "Lu Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Medical image-language pre-training aims to align medical images with\nclinically relevant text to improve model performance on various downstream\ntasks. However, existing models often struggle with the variability and\nambiguity inherent in medical data, limiting their ability to capture nuanced\nclinical information and uncertainty. This work introduces an uncertainty-aware\nmedical image-text pre-training model that enhances generalization capabilities\nin medical image analysis. Building on previous methods and focusing on Chest\nX-Rays, our approach utilizes structured text reports generated by a large\nlanguage model (LLM) to augment image data with clinically relevant context.\nThese reports begin with a definition of the disease, followed by the\n`appearance' section to highlight critical regions of interest, and finally\n`observations' and `verdicts' that ground model predictions in clinical\nsemantics. By modeling both inter- and intra-modal uncertainty, our framework\ncaptures the inherent ambiguity in medical images and text, yielding improved\nrepresentations and performance on downstream tasks. Our model demonstrates\nsignificant advances in medical image-text pre-training, obtaining\nstate-of-the-art performance on multiple downstream tasks.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21794v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21794v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.458,
      "distributed_training_score": 0.39,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper employs a large language model to programmatically generate structured text reports from original reports, which aligns with weak supervision by using noisy or imprecise sources for training labels instead of perfect hand-annotation. This enhances data for pre-training, though it's not the primary focus of the method.",
      "diffusion_reasoning_justification": "The paper introduces Distribution-based Masked Image-Language Modeling, which deals with uncertainty in medical data but does not involve diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces an uncertainty-aware medical image-text pre-training model for Chest X-Rays, utilizing Distribution-based Masked Image-Language Modeling (D-MLM) to capture inter- and intra-modal uncertainties and enhance generalization in medical image analysis. By augmenting image data with structured text reports generated by a large language model, which include disease definitions, critical region highlights, observations, and verdicts, the model aligns images with clinically relevant semantics, achieving state-of-the-art performance on multiple downstream tasks.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique, D-MLM, which models uncertainty in medical image-text pre-training for the first time in this context, significantly advancing the state-of-the-art by addressing inherent ambiguities in medical data.",
      "impact_score": "High",
      "impact_justification": "The work could influence future research and commercial applications in medical image analysis by providing more robust representations that handle uncertainty, potentially leading to better clinical outcomes and broader adoption in healthcare AI.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper represents a strong and valuable contribution to medical AI with innovative methods and demonstrated improvements, making it essential for researchers in computer vision and healthcare to be aware of for advancing their work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e13ee6df1eaf77a7b355913bfe42ebace0f7c3f3",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 12,
      "average_h_index": 3.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Shreyank N. Gowda",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/152957752"
        },
        {
          "name": "Ruichi Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2373751699"
        },
        {
          "name": "Xiao Gu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2353559277"
        },
        {
          "name": "Ying Weng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2373723586"
        },
        {
          "name": "Lu Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2374278780"
        }
      ]
    },
    {
      "id": "2507.21796",
      "title": "MoDeSuite: Robot Learning Task Suite for Benchmarking Mobile\n  Manipulation with Deformable Objects",
      "authors": [
        "Yuying Zhang",
        "Kevin Sebastian Luck",
        "Francesco Verdoja",
        "Ville Kyrki",
        "Joni Pajarinen"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Mobile manipulation is a critical capability for robots operating in diverse,\nreal-world environments. However, manipulating deformable objects and materials\nremains a major challenge for existing robot learning algorithms. While various\nbenchmarks have been proposed to evaluate manipulation strategies with rigid\nobjects, there is still a notable lack of standardized benchmarks that address\nmobile manipulation tasks involving deformable objects.\n  To address this gap, we introduce MoDeSuite, the first Mobile Manipulation\nDeformable Object task suite, designed specifically for robot learning.\nMoDeSuite consists of eight distinct mobile manipulation tasks covering both\nelastic objects and deformable objects, each presenting a unique challenge\ninspired by real-world robot applications. Success in these tasks requires\neffective collaboration between the robot's base and manipulator, as well as\nthe ability to exploit the deformability of the objects. To evaluate and\ndemonstrate the use of the proposed benchmark, we train two state-of-the-art\nreinforcement learning algorithms and two imitation learning algorithms,\nhighlighting the difficulties encountered and showing their performance in\nsimulation. Furthermore, we demonstrate the practical relevance of the suite by\ndeploying the trained policies directly into the real world with the Spot\nrobot, showcasing the potential for sim-to-real transfer. We expect that\nMoDeSuite will open a novel research domain in mobile manipulation involving\ndeformable objects. Find more details, code, and videos at\nhttps://sites.google.com/view/modesuite/home.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21796v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21796v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.336,
      "distributed_training_score": 0.342,
      "datasets_score": 0.39,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21799",
      "title": "Unlocking Interpretability for RF Sensing: A Complex-Valued White-Box\n  Transformer",
      "authors": [
        "Xie Zhang",
        "Yina Wang",
        "Chenshu Wu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The empirical success of deep learning has spurred its application to the\nradio-frequency (RF) domain, leading to significant advances in Deep Wireless\nSensing (DWS). However, most existing DWS models function as black boxes with\nlimited interpretability, which hampers their generalizability and raises\nconcerns in security-sensitive physical applications. In this work, inspired by\nthe remarkable advances of white-box transformers, we present RF-CRATE, the\nfirst mathematically interpretable deep network architecture for RF sensing,\ngrounded in the principles of complex sparse rate reduction. To accommodate the\nunique RF signals, we conduct non-trivial theoretical derivations that extend\nthe original real-valued white-box transformer to the complex domain. By\nleveraging the CR-Calculus framework, we successfully construct a fully\ncomplex-valued white-box transformer with theoretically derived self-attention\nand residual multi-layer perceptron modules. Furthermore, to improve the\nmodel's ability to extract discriminative features from limited wireless data,\nwe introduce Subspace Regularization, a novel regularization strategy that\nenhances feature diversity, resulting in an average performance improvement of\n19.98% across multiple sensing tasks. We extensively evaluate RF-CRATE against\nseven baselines with multiple public and self-collected datasets involving\ndifferent RF signals. The results show that RF-CRATE achieves performance on\npar with thoroughly engineered black-box models, while offering full\nmathematical interpretability. More importantly, by extending CRATE to the\ncomplex domain, RF-CRATE yields substantial improvements, achieving an average\nclassification gain of 5.08% and reducing regression error by 10.34% across\ndiverse sensing tasks compared to CRATE. RF-CRATE is fully open-sourced at:\nhttps://github.com/rfcrate/RF_CRATE.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21799v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21799v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.356,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21802",
      "title": "MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE",
      "authors": [
        "Junzhe Li",
        "Yutao Cui",
        "Tao Huang",
        "Yinping Ma",
        "Chun Fan",
        "Miles Yang",
        "Zhao Zhong"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Although GRPO substantially enhances flow matching models in human preference\nalignment of image generation, methods such as FlowGRPO still exhibit\ninefficiency due to the necessity of sampling and optimizing over all denoising\nsteps specified by the Markov Decision Process (MDP). In this paper, we propose\n$\\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed\nsampling strategies through the integration of stochastic differential\nequations (SDE) and ordinary differential equations (ODE). This streamlines the\noptimization process within the MDP to improve efficiency and boost\nperformance. Specifically, MixGRPO introduces a sliding window mechanism, using\nSDE sampling and GRPO-guided optimization only within the window, while\napplying ODE sampling outside. This design confines sampling randomness to the\ntime-steps within the window, thereby reducing the optimization overhead, and\nallowing for more focused gradient updates to accelerate convergence.\nAdditionally, as time-steps beyond the sliding window are not involved in\noptimization, higher-order solvers are supported for sampling. So we present a\nfaster variant, termed $\\textbf{MixGRPO-Flash}$, which further improves\ntraining efficiency while achieving comparable performance. MixGRPO exhibits\nsubstantial gains across multiple dimensions of human preference alignment,\noutperforming DanceGRPO in both effectiveness and efficiency, with nearly 50%\nlower training time. Notably, MixGRPO-Flash further reduces training time by\n71%. Codes and models are available at\n$\\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21802v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21802v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.399,
      "datasets_score": 0.246,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves enhancing flow matching models using GRPO, a policy optimization technique within RLHF, to align image generation with human preferences. It explicitly references RLHF strategies, reward models from human-ranked data, and optimization processes that fine-tune models based on these rewards, making it directly applicable to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on improving efficiency in diffusion-based image generation through mixed ODE-SDE strategies, but it does not involve adapting diffusion for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks. It is centered on generative tasks, not reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces MixGRPO, a novel framework to enhance the efficiency of Group Relative Policy Optimization (GRPO) in flow matching models for image generation, particularly in aligning with human preferences. By integrating Stochastic Differential Equations (SDE) for sampling and optimization within a sliding window and Ordinary Differential Equations (ODE) outside it, the method reduces computational overhead by focusing randomness and gradient updates on a subset of denoising steps, while a faster variant, MixGRPO-Flash, employs higher-order solvers for further acceleration. Experimental results show MixGRPO outperforms DanceGRPO with improved human preference metrics, such as increasing ImageReward from 1.088 to 1.629, and achieves up to 50% reduction in training time, with MixGRPO-Flash reducing it by 71%.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing SDE and ODE techniques with a sliding window mechanism to improve GRPO efficiency, offering a notable advancement in optimizing flow matching models without introducing an entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in efficient training for generative models and human preference alignment in AI and computer vision, as its efficiency gains could be adopted in similar subfields, though its applicability may remain niche.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a strong and practical contribution to improving training efficiency in generative AI, making it valuable for researchers in computer vision and reinforcement learning from human feedback.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/20309e3e9934742961623b1acf459bf247954016",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Junzhe Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373749736"
        },
        {
          "name": "Yutao Cui",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374078222"
        },
        {
          "name": "Tao Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373746469"
        },
        {
          "name": "Yinping Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373913439"
        },
        {
          "name": "Chun Fan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375323401"
        },
        {
          "name": "Miles Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373751034"
        },
        {
          "name": "Zhao Zhong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375371350"
        }
      ]
    },
    {
      "id": "2507.21809",
      "title": "HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D\n  Worlds from Words or Pixels",
      "authors": [
        "HunyuanWorld Team",
        "Zhenwei Wang",
        "Yuhao Liu",
        "Junta Wu",
        "Zixiao Gu",
        "Haoyuan Wang",
        "Xuhui Zuo",
        "Tianyu Huang",
        "Wenhuan Li",
        "Sheng Zhang",
        "Yihang Lian",
        "Yulin Tsai",
        "Lifu Wang",
        "Sicong Liu",
        "Puhua Jiang",
        "Xianghui Yang",
        "Dongyuan Guo",
        "Yixuan Tang",
        "Xinyue Mao",
        "Jiaao Yu",
        "Junlin Yu",
        "Jihong Zhang",
        "Meng Chen",
        "Liang Dong",
        "Yiwen Jia",
        "Chao Zhang",
        "Yonghao Tan",
        "Hao Zhang",
        "Zheng Ye",
        "Peng He",
        "Runzhou Wu",
        "Minghui Chen",
        "Zhan Li",
        "Wangchen Qin",
        "Lei Wang",
        "Yifu Sun",
        "Lin Niu",
        "Xiang Yuan",
        "Xiaofeng Yang",
        "Yingping He",
        "Jie Xiao",
        "Yangyu Tao",
        "Jianchen Zhu",
        "Jinbao Xue",
        "Kai Liu",
        "Chongqing Zhao",
        "Xinming Wu",
        "Tian Liu",
        "Peng Chen",
        "Di Wang",
        "Yuhong Liu",
        "Linus",
        "Jie Jiang",
        "Tengfei Wang",
        "Chunchao Guo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Creating immersive and playable 3D worlds from texts or images remains a\nfundamental challenge in computer vision and graphics. Existing world\ngeneration approaches typically fall into two categories: video-based methods\nthat offer rich diversity but lack 3D consistency and rendering efficiency, and\n3D-based methods that provide geometric consistency but struggle with limited\ntraining data and memory-inefficient representations. To address these\nlimitations, we present HunyuanWorld 1.0, a novel framework that combines the\nbest of both worlds for generating immersive, explorable, and interactive 3D\nscenes from text and image conditions. Our approach features three key\nadvantages: 1) 360{\\deg} immersive experiences via panoramic world proxies; 2)\nmesh export capabilities for seamless compatibility with existing computer\ngraphics pipelines; 3) disentangled object representations for augmented\ninteractivity. The core of our framework is a semantically layered 3D mesh\nrepresentation that leverages panoramic images as 360{\\deg} world proxies for\nsemantic-aware world decomposition and reconstruction, enabling the generation\nof diverse 3D worlds. Extensive experiments demonstrate that our method\nachieves state-of-the-art performance in generating coherent, explorable, and\ninteractive 3D worlds while enabling versatile applications in virtual reality,\nphysical simulation, game development, and interactive content creation.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21809v2",
      "pdf_url": "http://arxiv.org/pdf/2507.21809v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.319,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses the use of video diffusion models for generating 3D worlds, such as in panoramic image generation and world exploration, which involves iterative refinement processes typical of diffusion models. However, it focuses on visual and spatial generation rather than adapting diffusion for multi-step logical reasoning tasks, like treating a Chain-of-Thought as a single entity for holistic correction in complex logical problems. Thus, while diffusion is a component, it is not applied to reasoning as specified.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21820",
      "title": "Anyone Can Jailbreak: Prompt-Based Attacks on LLMs and T2Is",
      "authors": [
        "Ahmed B Mustafa",
        "Zihan Ye",
        "Yang Lu",
        "Michael P Pound",
        "Shreyank N Gowda"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Despite significant advancements in alignment and content moderation, large\nlanguage models (LLMs) and text-to-image (T2I) systems remain vulnerable to\nprompt-based attacks known as jailbreaks. Unlike traditional adversarial\nexamples requiring expert knowledge, many of today's jailbreaks are low-effort,\nhigh-impact crafted by everyday users with nothing more than cleverly worded\nprompts. This paper presents a systems-style investigation into how non-experts\nreliably circumvent safety mechanisms through techniques such as multi-turn\nnarrative escalation, lexical camouflage, implication chaining, fictional\nimpersonation, and subtle semantic edits. We propose a unified taxonomy of\nprompt-level jailbreak strategies spanning both text-output and T2I models,\ngrounded in empirical case studies across popular APIs. Our analysis reveals\nthat every stage of the moderation pipeline, from input filtering to output\nvalidation, can be bypassed with accessible strategies. We conclude by\nhighlighting the urgent need for context-aware defenses that reflect the ease\nwith which these jailbreaks can be reproduced in real-world settings.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21820v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21820v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.309,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses how LLMs are safeguarded through alignment fine-tuning, which may involve RLHF, but its main focus is on prompt-based attacks and jailbreaks, not on the RLHF process itself. It does not evaluate, propose, or analyze RLHF systems or human feedback mechanisms.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines jailbreaks on text-to-image (T2I) models, which might use diffusion processes for image generation, but it does not address adapting diffusion for multi-step logical reasoning or iterative refinement of Chain-of-Thought. The core contribution is on attack strategies, not diffusion-based reasoning techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21823",
      "title": "An Agentic AI for a New Paradigm in Business Process Development",
      "authors": [
        "Mohammad Azarijafari",
        "Luisa Mich",
        "Michele Missikoff"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Artificial Intelligence agents represent the next major revolution in the\ncontinuous technological evolution of industrial automation. In this paper, we\nintroduce a new approach for business process design and development that\nleverages the capabilities of Agentic AI. Departing from the traditional\ntask-based approach to business process design, we propose an agent-based\nmethod, where agents contribute to the achievement of business goals,\nidentified by a set of business objects. When a single agent cannot fulfill a\ngoal, we have a merge goal that can be achieved through the collaboration of\nmultiple agents. The proposed model leads to a more modular and intelligent\nbusiness process development by organizing it around goals, objects, and\nagents. As a result, this approach enables flexible and context-aware\nautomation in dynamic industrial environments.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21823v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21823v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.297,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21830",
      "title": "DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series\n  Forecasting Framework",
      "authors": [
        "Kuiye Ding",
        "Fanda Fan",
        "Yao Wang",
        "Ruijie jian",
        "Xiaorui Wang",
        "Luqi Gong",
        "Yishan Jiang",
        "Chunjie Luo an Jianfeng Zhan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multivariate Time Series Forecasting plays a key role in many applications.\nRecent works have explored using Large Language Models for MTSF to take\nadvantage of their reasoning abilities. However, many methods treat LLMs as\nend-to-end forecasters, which often leads to a loss of numerical precision and\nforces LLMs to handle patterns beyond their intended design. Alternatively,\nmethods that attempt to align textual and time series modalities within latent\nspace frequently encounter alignment difficulty. In this paper, we propose to\ntreat LLMs not as standalone forecasters, but as semantic guidance modules\nwithin a dual-stream framework. We propose DualSG, a dual-stream framework that\nprovides explicit semantic guidance, where LLMs act as Semantic Guides to\nrefine rather than replace traditional predictions. As part of DualSG, we\nintroduce Time Series Caption, an explicit prompt format that summarizes trend\npatterns in natural language and provides interpretable context for LLMs,\nrather than relying on implicit alignment between text and time series in the\nlatent space. We also design a caption-guided fusion module that explicitly\nmodels inter-variable relationships while reducing noise and computation.\nExperiments on real-world datasets from diverse domains show that DualSG\nconsistently outperforms 15 state-of-the-art baselines, demonstrating the value\nof explicitly combining numerical forecasting with semantic guidance.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21830v2",
      "pdf_url": "http://arxiv.org/pdf/2507.21830v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.365,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a dual-stream framework for multivariate time series forecasting that uses LLMs for semantic guidance, including components like Time Series Caption and fusion modules. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21831",
      "title": "Introducing HALC: A general pipeline for finding optimal prompting\n  strategies for automated coding with LLMs in the computational social\n  sciences",
      "authors": [
        "Andreas Reich",
        "Claudia Thoms",
        "Tobias Schrimpf"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "LLMs are seeing widespread use for task automation, including automated\ncoding in the social sciences. However, even though researchers have proposed\ndifferent prompting strategies, their effectiveness varies across LLMs and\ntasks. Often trial and error practices are still widespread. We propose\nHALC$-$a general pipeline that allows for the systematic and reliable\nconstruction of optimal prompts for any given coding task and model, permitting\nthe integration of any prompting strategy deemed relevant. To investigate LLM\ncoding and validate our pipeline, we sent a total of 1,512 individual prompts\nto our local LLMs in over two million requests. We test prompting strategies\nand LLM task performance based on few expert codings (ground truth). When\ncompared to these expert codings, we find prompts that code reliably for single\nvariables (${\\alpha}$climate = .76; ${\\alpha}$movement = .78) and across two\nvariables (${\\alpha}$climate = .71; ${\\alpha}$movement = .74) using the LLM\nMistral NeMo. Our prompting strategies are set up in a way that aligns the LLM\nto our codebook$-$we are not optimizing our codebook for LLM friendliness. Our\npaper provides insights into the effectiveness of different prompting\nstrategies, crucial influencing factors, and the identification of reliable\nprompts for each coding task and model.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21831v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21831v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.477,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.306,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes a pipeline for optimizing prompts for LLMs in automated coding, using expert codings for evaluation, but it does not involve training a reward model on human-ranked data or fine-tuning models via reinforcement learning. There is no mention of RLHF processes.",
      "weak_supervision_justification": "The paper relies on expert codings as ground truth to assess prompting strategies, rather than programmatically generating noisy or imprecise labels for training models, which is the essence of weak supervision. It focuses on prompt optimization, not label generation for model training.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21833",
      "title": "Analysis of Fourier Neural Operators via Effective Field Theory",
      "authors": [
        "Taeyoung Kim"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Fourier Neural Operators (FNOs) have emerged as leading surrogates for\nhigh-dimensional partial-differential equations, yet their stability,\ngeneralization and frequency behavior lack a principled explanation. We present\nthe first systematic effective-field-theory analysis of FNOs in an\ninfinite-dimensional function space, deriving closed recursion relations for\nthe layer kernel and four-point vertex and then examining three practically\nimportant settings-analytic activations, scale-invariant cases and\narchitectures with residual connections. The theory shows that nonlinear\nactivations inevitably couple frequency inputs to high-frequency modes that are\notherwise discarded by spectral truncation, and experiments confirm this\nfrequency transfer. For wide networks we obtain explicit criticality conditions\non the weight-initialization ensemble that keep small input perturbations to\nhave uniform scale across depth, and empirical tests validate these\npredictions. Taken together, our results quantify how nonlinearity enables\nneural operators to capture non-trivial features, supply criteria for\nhyper-parameter selection via criticality analysis, and explain why\nscale-invariant activations and residual connections enhance feature learning\nin FNOs.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21833v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21833v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.267,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.347,
      "datasets_score": 0.239,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21839",
      "title": "Against racing to AGI: Cooperation, deterrence, and catastrophic risks",
      "authors": [
        "Leonard Dung",
        "Max Hellrigel-Holderbaum"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "AGI Racing is the view that it is in the self-interest of major actors in AI\ndevelopment, especially powerful nations, to accelerate their frontier AI\ndevelopment to build highly capable AI, especially artificial general\nintelligence (AGI), before competitors have a chance. We argue against AGI\nRacing. First, the downsides of racing to AGI are much higher than portrayed by\nthis view. Racing to AGI would substantially increase catastrophic risks from\nAI, including nuclear instability, and undermine the prospects of technical AI\nsafety research to be effective. Second, the expected benefits of racing may be\nlower than proponents of AGI Racing hold. In particular, it is questionable\nwhether winning the race enables complete domination over losers. Third,\ninternational cooperation and coordination, and perhaps carefully crafted\ndeterrence measures, constitute viable alternatives to racing to AGI which have\nmuch smaller risks and promise to deliver most of the benefits that racing to\nAGI is supposed to provide. Hence, racing to AGI is not in anyone's\nself-interest as other actions, particularly incentivizing and seeking\ninternational cooperation around AI issues, are preferable.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21839v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21839v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.241,
      "diffusion_reasoning_score": 0.289,
      "distributed_training_score": 0.327,
      "datasets_score": 0.258,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21844",
      "title": "Cross-Architecture Distillation Made Simple with Redundancy Suppression",
      "authors": [
        "Weijia Zhang",
        "Yuehao Liu",
        "Wu Ran",
        "Chao Ma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We describe a simple method for cross-architecture knowledge distillation,\nwhere the knowledge transfer is cast into a redundant information suppression\nformulation. Existing methods introduce sophisticated modules,\narchitecture-tailored designs, and excessive parameters, which impair their\nefficiency and applicability. We propose to extract the architecture-agnostic\nknowledge in heterogeneous representations by reducing the redundant\narchitecture-exclusive information. To this end, we present a simple redundancy\nsuppression distillation (RSD) loss, which comprises cross-architecture\ninvariance maximisation and feature decorrelation objectives. To prevent the\nstudent from entirely losing its architecture-specific capabilities, we further\ndesign a lightweight module that decouples the RSD objective from the student's\ninternal representations. Our method is devoid of the architecture-specific\ndesigns and complex operations in the pioneering method of OFA. It outperforms\nOFA on CIFAR-100 and ImageNet-1k benchmarks with only a fraction of their\nparameter overhead, which highlights its potential as a simple and strong\nbaseline to the cross-architecture distillation community.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21844v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21844v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.416,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a method for cross-architecture knowledge distillation, focusing on transferring knowledge between different model architectures by suppressing redundant information. It does not involve distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors, which are central to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21846",
      "title": "Probabilistic Active Goal Recognition",
      "authors": [
        "Chenyuan Zhang",
        "Cristian Rojas Cardenas",
        "Hamid Rezatofighi",
        "Mor Vered",
        "Buser Say"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.SC (Symbolic Computation)"
      ],
      "abstract": "In multi-agent environments, effective interaction hinges on understanding\nthe beliefs and intentions of other agents. While prior work on goal\nrecognition has largely treated the observer as a passive reasoner, Active Goal\nRecognition (AGR) focuses on strategically gathering information to reduce\nuncertainty. We adopt a probabilistic framework for Active Goal Recognition and\npropose an integrated solution that combines a joint belief update mechanism\nwith a Monte Carlo Tree Search (MCTS) algorithm, allowing the observer to plan\nefficiently and infer the actor's hidden goal without requiring domain-specific\nknowledge. Through comprehensive empirical evaluation in a grid-based domain,\nwe show that our joint belief update significantly outperforms passive goal\nrecognition, and that our domain-independent MCTS performs comparably to our\nstrong domain-specific greedy baseline. These results establish our solution as\na practical and robust framework for goal inference, advancing the field toward\nmore interactive and adaptive multi-agent systems.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21846v2",
      "pdf_url": "http://arxiv.org/pdf/2507.21846v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.298,
      "datasets_score": 0.267,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Probabilistic Active Goal Recognition using POMDP and MCTS for goal inference in multi-agent systems, with no mention of human feedback, preference learning, or training a reward model based on human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes MCTS for planning and belief updates in goal recognition, but does not involve diffusion models, iterative refinement of reasoning paths, or any multi-step logical processes based on diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21848",
      "title": "EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for\n  Advantage Diversity",
      "authors": [
        "Xingjian Zhang",
        "Siwei Wen",
        "Wenjun Wu",
        "Lei Huang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have made remarkable progress in enhancing\nstep-by-step reasoning through reinforcement learning. However, the Group\nRelative Policy Optimization (GRPO) algorithm, which relies on sparse reward\nrules, often encounters the issue of identical rewards within groups, leading\nto the advantage collapse problem. Existing works typically address this\nchallenge from two perspectives: enforcing model reflection to enhance response\ndiversity, and introducing internal feedback to augment the training signal\n(advantage). In this work, we begin by analyzing the limitations of model\nreflection and investigating the policy entropy of responses at the\nfine-grained sample level. Based on our experimental findings, we propose the\nEDGE-GRPO algorithm, which adopts \\textbf{E}ntropy-\\textbf{D}riven Advantage\nand \\textbf{G}uided \\textbf{E}rror Correction to effectively mitigate the\nproblem of advantage collapse. Extensive experiments on several main reasoning\nbenchmarks demonstrate the effectiveness and superiority of our approach. It is\navailable at https://github.com/ZhangXJ199/EDGE-GRPO.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21848v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21848v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.506,
      "distributed_training_score": 0.412,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on improving the GRPO algorithm, a reinforcement learning method that uses reward models and advantages for training LLMs, which shares conceptual similarities with RLHF. However, it relies on rule-based or sparse rewards rather than human-ranked data, so it does not fully align with RLHF's core emphasis on human feedback for reward modeling.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses enhancements to GRPO for LLM reasoning but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "The paper addresses algorithmic improvements to GRPO for reasoning tasks and does not mention distributed training, parallel computing, or partitioning computations across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the advantage collapse problem in the Group Relative Policy Optimization (GRPO) algorithm for large language models by proposing the EDGE-GRPO method, which integrates Entropy-Driven Advantage (EDA) to enhance advantage diversity based on policy entropy and Guided Error Correction (GEC) to improve response diversity. Through analysis of existing limitations and extensive experiments on reasoning benchmarks, the authors demonstrate that EDGE-GRPO achieves over 20% performance improvement compared to vanilla GRPO, validating its effectiveness in mitigating issues like identical rewards and enhancing overall reasoning capabilities with fewer training samples.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by cleverly combining entropy-driven advantages and guided error correction to address the advantage collapse in GRPO, offering a new way to enhance policy optimization in existing reinforcement learning frameworks for LLMs.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI reasoning models, as it demonstrates practical improvements in efficiency and performance with fewer samples, potentially influencing future developments in reinforcement learning for LLMs.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to reinforcement learning techniques for LLMs, making it essential for researchers in AI reasoning to be aware of its innovative approaches and empirical results.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/526fca4c3076b533f118dfd702f107d9bc6bc65d",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 4,
      "average_h_index": 3.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Xingjian Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2302364463"
        },
        {
          "name": "Siwei Wen",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2333960287"
        },
        {
          "name": "Wenjun Wu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2334017812"
        },
        {
          "name": "Lei Huang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2333958581"
        }
      ]
    },
    {
      "id": "2507.21857",
      "title": "Unleashing the Power of Motion and Depth: A Selective Fusion Strategy\n  for RGB-D Video Salient Object Detection",
      "authors": [
        "Jiahao He",
        "Daerji Suolang",
        "Keren Fu",
        "Qijun Zhao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Applying salient object detection (SOD) to RGB-D videos is an emerging task\ncalled RGB-D VSOD and has recently gained increasing interest, due to\nconsiderable performance gains of incorporating motion and depth and that RGB-D\nvideos can be easily captured now in daily life. Existing RGB-D VSOD models\nhave different attempts to derive motion cues, in which extracting motion\ninformation explicitly from optical flow appears to be a more effective and\npromising alternative. Despite this, there remains a key issue that how to\neffectively utilize optical flow and depth to assist the RGB modality in SOD.\nPrevious methods always treat optical flow and depth equally with respect to\nmodel designs, without explicitly considering their unequal contributions in\nindividual scenarios, limiting the potential of motion and depth. To address\nthis issue and unleash the power of motion and depth, we propose a novel\nselective cross-modal fusion framework (SMFNet) for RGB-D VSOD, incorporating a\npixel-level selective fusion strategy (PSF) that achieves optimal fusion of\noptical flow and depth based on their actual contributions. Besides, we propose\na multi-dimensional selective attention module (MSAM) to integrate the fused\nfeatures derived from PSF with the remaining RGB modality at multiple\ndimensions, effectively enhancing feature representation to generate refined\nfeatures. We conduct comprehensive evaluation of SMFNet against 19\nstate-of-the-art models on both RDVS and DVisal datasets, making the evaluation\nthe most comprehensive RGB-D VSOD benchmark up to date, and it also\ndemonstrates the superiority of SMFNet over other models. Meanwhile, evaluation\non five video benchmark datasets incorporating synthetic depth validates the\nefficacy of SMFNet as well. Our code and benchmark results are made publicly\navailable at https://github.com/Jia-hao999/SMFNet.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21857v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21857v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.315,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on RGB-D video salient object detection, proposing a selective fusion strategy for motion and depth features using techniques like optical flow and attention modules. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21858",
      "title": "Low-Cost Test-Time Adaptation for Robust Video Editing",
      "authors": [
        "Jianhui Wang",
        "Yinda Chen",
        "Yangfan He",
        "Xinyuan Song",
        "Yi Xin",
        "Dapeng Zhang",
        "Zhongwei Wan",
        "Bin Li",
        "Rongchao Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Video editing is a critical component of content creation that transforms raw\nfootage into coherent works aligned with specific visual and narrative\nobjectives. Existing approaches face two major challenges: temporal\ninconsistencies due to failure in capturing complex motion patterns, and\noverfitting to simple prompts arising from limitations in UNet backbone\narchitectures. While learning-based methods can enhance editing quality, they\ntypically demand substantial computational resources and are constrained by the\nscarcity of high-quality annotated data. In this paper, we present Vid-TTA, a\nlightweight test-time adaptation framework that personalizes optimization for\neach test video during inference through self-supervised auxiliary tasks. Our\napproach incorporates a motion-aware frame reconstruction mechanism that\nidentifies and preserves crucial movement regions, alongside a prompt\nperturbation and reconstruction strategy that strengthens model robustness to\ndiverse textual descriptions. These innovations are orchestrated by a\nmeta-learning driven dynamic loss balancing mechanism that adaptively adjusts\nthe optimization process based on video characteristics. Extensive experiments\ndemonstrate that Vid-TTA significantly improves video temporal consistency and\nmitigates prompt overfitting while maintaining low computational overhead,\noffering a plug-and-play performance boost for existing video editing models.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21858v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21858v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.334,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21863",
      "title": "VidFuncta: Towards Generalizable Neural Representations for Ultrasound\n  Videos",
      "authors": [
        "Julia Wolleb",
        "Florentin Bieder",
        "Paul Friedrich",
        "Hemant D. Tagare",
        "Xenophon Papademetris"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Ultrasound is widely used in clinical care, yet standard deep learning\nmethods often struggle with full video analysis due to non-standardized\nacquisition and operator bias. We offer a new perspective on ultrasound video\nanalysis through implicit neural representations (INRs). We build on Functa, an\nINR framework in which each image is represented by a modulation vector that\nconditions a shared neural network. However, its extension to the temporal\ndomain of medical videos remains unexplored. To address this gap, we propose\nVidFuncta, a novel framework that leverages Functa to encode variable-length\nultrasound videos into compact, time-resolved representations. VidFuncta\ndisentangles each video into a static video-specific vector and a sequence of\ntime-dependent modulation vectors, capturing both temporal dynamics and\ndataset-level redundancies. Our method outperforms 2D and 3D baselines on video\nreconstruction and enables downstream tasks to directly operate on the learned\n1D modulation vectors. We validate VidFuncta on three public ultrasound video\ndatasets -- cardiac, lung, and breast -- and evaluate its downstream\nperformance on ejection fraction prediction, B-line detection, and breast\nlesion classification. These results highlight the potential of VidFuncta as a\ngeneralizable and efficient representation framework for ultrasound videos. Our\ncode is publicly available under\nhttps://github.com/JuliaWolleb/VidFuncta_public.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21863v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21863v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.279,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.286,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21872",
      "title": "MultiEditor: Controllable Multimodal Object Editing for Driving\n  Scenarios Using 3D Gaussian Splatting Priors",
      "authors": [
        "Shouyi Lu",
        "Zihan Lin",
        "Chao Lu",
        "Huanran Wang",
        "Guirong Zhuo",
        "Lianqing Zheng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Autonomous driving systems rely heavily on multimodal perception data to\nunderstand complex environments. However, the long-tailed distribution of\nreal-world data hinders generalization, especially for rare but safety-critical\nvehicle categories. To address this challenge, we propose MultiEditor, a\ndual-branch latent diffusion framework designed to edit images and LiDAR point\nclouds in driving scenarios jointly. At the core of our approach is introducing\n3D Gaussian Splatting (3DGS) as a structural and appearance prior for target\nobjects. Leveraging this prior, we design a multi-level appearance control\nmechanism--comprising pixel-level pasting, semantic-level guidance, and\nmulti-branch refinement--to achieve high-fidelity reconstruction across\nmodalities. We further propose a depth-guided deformable cross-modality\ncondition module that adaptively enables mutual guidance between modalities\nusing 3DGS-rendered depth, significantly enhancing cross-modality consistency.\nExtensive experiments demonstrate that MultiEditor achieves superior\nperformance in visual and geometric fidelity, editing controllability, and\ncross-modality consistency. Furthermore, generating rare-category vehicle data\nwith MultiEditor substantially enhances the detection accuracy of perception\nmodels on underrepresented classes.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21872v3",
      "pdf_url": "http://arxiv.org/pdf/2507.21872v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.366,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using Latent Diffusion Models for editing images and LiDAR point clouds in autonomous driving scenarios, emphasizing generative tasks like data synthesis and multimodal consistency. It does not involve adapting the iterative refinement process of diffusion models for multi-step logical reasoning, such as treating a Chain-of-Thought as a single entity for holistic correction. Instead, the diffusion components are applied to visual and geometric data manipulation, lacking any clear element of solving complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21873",
      "title": "A Neuro-Symbolic Approach for Probabilistic Reasoning on Graph Data",
      "authors": [
        "Raffaele Pojer",
        "Andrea Passerini",
        "Kim G. Larsen",
        "Manfred Jaeger"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Graph neural networks (GNNs) excel at predictive tasks on graph-structured\ndata but often lack the ability to incorporate symbolic domain knowledge and\nperform general reasoning. Relational Bayesian Networks (RBNs), in contrast,\nenable fully generative probabilistic modeling over graph-like structures and\nsupport rich symbolic knowledge and probabilistic inference. This paper\npresents a neuro-symbolic framework that seamlessly integrates GNNs into RBNs,\ncombining the learning strength of GNNs with the flexible reasoning\ncapabilities of RBNs.\n  We develop two implementations of this integration: one compiles GNNs\ndirectly into the native RBN language, while the other maintains the GNN as an\nexternal component. Both approaches preserve the semantics and computational\nproperties of GNNs while fully aligning with the RBN modeling paradigm. We also\npropose a maximum a-posteriori (MAP) inference method for these neuro-symbolic\nmodels.\n  To demonstrate the framework's versatility, we apply it to two distinct\nproblems. First, we transform a GNN for node classification into a collective\nclassification model that explicitly models homo- and heterophilic label\npatterns, substantially improving accuracy. Second, we introduce a\nmulti-objective network optimization problem in environmental planning, where\nMAP inference supports complex decision-making. Both applications include new\npublicly available benchmark datasets.\n  This work introduces a powerful and coherent neuro-symbolic approach to graph\ndata, bridging learning and reasoning in ways that enable novel applications\nand improved performance across diverse tasks.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21873v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21873v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.485,
      "distributed_training_score": 0.359,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a neuro-symbolic framework integrating GNNs with RBNs for probabilistic reasoning on graph data, focusing on symbolic knowledge incorporation and inference tasks like node classification. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning via a Chain-of-Thought approach as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21875",
      "title": "Tiny-BioMoE: a Lightweight Embedding Model for Biosignal Analysis",
      "authors": [
        "Stefanos Gkikas",
        "Ioannis Kyprakis",
        "Manolis Tsiknakis"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Pain is a complex and pervasive condition that affects a significant portion\nof the population. Accurate and consistent assessment is essential for\nindividuals suffering from pain, as well as for developing effective management\nstrategies in a healthcare system. Automatic pain assessment systems enable\ncontinuous monitoring, support clinical decision-making, and help minimize\npatient distress while mitigating the risk of functional deterioration.\nLeveraging physiological signals offers objective and precise insights into a\nperson's state, and their integration in a multimodal framework can further\nenhance system performance. This study has been submitted to the \\textit{Second\nMultimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The\nproposed approach introduces \\textit{Tiny-BioMoE}, a lightweight pretrained\nembedding model for biosignal analysis. Trained on $4.4$ million biosignal\nimage representations and consisting of only $7.3$ million parameters, it\nserves as an effective tool for extracting high-quality embeddings for\ndownstream tasks. Extensive experiments involving electrodermal activity, blood\nvolume pulse, respiratory signals, peripheral oxygen saturation, and their\ncombinations highlight the model's effectiveness across diverse modalities in\nautomatic pain recognition tasks. \\textit{\\textcolor{blue}{The model's\narchitecture (code) and weights are available at\nhttps://github.com/GkikasStefanos/Tiny-BioMoE.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21875v5",
      "pdf_url": "http://arxiv.org/pdf/2507.21875v5",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.347,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21881",
      "title": "Multi-Representation Diagrams for Pain Recognition: Integrating Various\n  Electrodermal Activity Signals into a Single Image",
      "authors": [
        "Stefanos Gkikas",
        "Ioannis Kyprakis",
        "Manolis Tsiknakis"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Pain is a multifaceted phenomenon that affects a substantial portion of the\npopulation. Reliable and consistent evaluation benefits those experiencing pain\nand underpins the development of effective and advanced management strategies.\nAutomatic pain-assessment systems deliver continuous monitoring, inform\nclinical decision-making, and aim to reduce distress while preventing\nfunctional decline. By incorporating physiological signals, these systems\nprovide objective, accurate insights into an individual's condition. This study\nhas been submitted to the \\textit{Second Multimodal Sensing Grand Challenge for\nNext-Gen Pain Assessment (AI4PAIN)}. The proposed method introduces a pipeline\nthat leverages electrodermal activity signals as input modality. Multiple\nrepresentations of the signal are created and visualized as waveforms, and they\nare jointly visualized within a single multi-representation diagram. Extensive\nexperiments incorporating various processing and filtering techniques, along\nwith multiple representation combinations, demonstrate the effectiveness of the\nproposed approach. It consistently yields comparable, and in several cases\nsuperior, results to traditional fusion methods, establishing it as a robust\nalternative for integrating different signal representations or modalities.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21881v4",
      "pdf_url": "http://arxiv.org/pdf/2507.21881v4",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.285,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21882",
      "title": "The Impact of Foundational Models on Patient-Centric e-Health Systems",
      "authors": [
        "Elmira Onagh",
        "Alireza Davoodi",
        "Maleknaz Nayebi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "As Artificial Intelligence (AI) becomes increasingly embedded in healthcare\ntechnologies, understanding the maturity of AI in patient-centric applications\nis critical for evaluating its trustworthiness, transparency, and real-world\nimpact. In this study, we investigate the integration and maturity of AI\nfeature integration in 116 patient-centric healthcare applications. Using Large\nLanguage Models (LLMs), we extracted key functional features, which are then\ncategorized into different stages of the Gartner AI maturity model. Our results\nshow that over 86.21\\% of applications remain at the early stages of AI\nintegration, while only 13.79% demonstrate advanced AI integration.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21882v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21882v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.313,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is an analysis of AI maturity in patient-centric healthcare applications using Large Language Models (LLMs) for feature extraction and categorization based on the Gartner AI maturity model. It does not involve reinforcement learning, human feedback for training models, or any related techniques such as creating a reward model from human-ranked data. Therefore, there is no direct or indirect connection to Reinforcement Learning from Human Feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21886",
      "title": "Efficient Pain Recognition via Respiration Signals: A Single\n  Cross-Attention Transformer Multi-Window Fusion Pipeline",
      "authors": [
        "Stefanos Gkikas",
        "Ioannis Kyprakis",
        "Manolis Tsiknakis"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "Pain is a complex condition affecting a large portion of the population.\nAccurate and consistent evaluation is essential for individuals experiencing\npain, and it supports the development of effective and advanced management\nstrategies. Automatic pain assessment systems provide continuous monitoring and\nsupport clinical decision-making, aiming to reduce distress and prevent\nfunctional decline. This study has been submitted to the \\textit{Second\nMultimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The\nproposed method introduces a pipeline that leverages respiration as the input\nsignal and incorporates a highly efficient cross-attention transformer\nalongside a multi-windowing strategy. Extensive experiments demonstrate that\nrespiration is a valuable physiological modality for pain assessment. Moreover,\nexperiments revealed that compact and efficient models, when properly\noptimized, can achieve strong performance, often surpassing larger\ncounterparts. The proposed multi-window approach effectively captures both\nshort-term and long-term features, as well as global characteristics, thereby\nenhancing the model's representational capacity.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21886v4",
      "pdf_url": "http://arxiv.org/pdf/2507.21886v4",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.332,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21888",
      "title": "CAPE: A CLIP-Aware Pointing Ensemble of Complementary Heatmap Cues for\n  Embodied Reference Understanding",
      "authors": [
        "Fevziye Irem Eyiokur",
        "Dogucan Yaman",
        "Hazım Kemal Ekenel",
        "Alexander Waibel"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We address the problem of Embodied Reference Understanding, which involves\npredicting the object that a person in the scene is referring to through both\npointing gesture and language. Accurately identifying the referent requires\nmultimodal understanding: integrating textual instructions, visual pointing,\nand scene context. However, existing methods often struggle to effectively\nleverage visual clues for disambiguation. We also observe that, while the\nreferent is often aligned with the head-to-fingertip line, it occasionally\naligns more closely with the wrist-to-fingertip line. Therefore, relying on a\nsingle line assumption can be overly simplistic and may lead to suboptimal\nperformance. To address this, we propose a dual-model framework, where one\nmodel learns from the head-to-fingertip direction and the other from the\nwrist-to-fingertip direction. We further introduce a Gaussian ray heatmap\nrepresentation of these lines and use them as input to provide a strong\nsupervisory signal that encourages the model to better attend to pointing cues.\nTo combine the strengths of both models, we present the CLIP-Aware Pointing\nEnsemble module, which performs a hybrid ensemble based on CLIP features.\nAdditionally, we propose an object center prediction head as an auxiliary task\nto further enhance referent localization. We validate our approach through\nextensive experiments and analysis on the benchmark YouRefIt dataset, achieving\nan improvement of approximately 4 mAP at the 0.25 IoU threshold.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21888v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21888v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.295,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21890",
      "title": "Data-driven quantum Koopman method for simulating nonlinear dynamics",
      "authors": [
        "Baoyang Zhang",
        "Zhen Lu",
        "Yaomin Zhao",
        "Yue Yang"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Quantum computation offers potential exponential speedups for simulating\ncertain physical systems, but its application to nonlinear dynamics is\ninherently constrained by the requirement of unitary evolution. We propose the\nquantum Koopman method (QKM), a data-driven framework that bridges this gap\nthrough transforming nonlinear dynamics into linear unitary evolution in\nhigher-dimensional observable spaces. Leveraging the Koopman operator theory to\nachieve a global linearization, our approach maps system states into a\nhierarchy of Hilbert spaces using a deep autoencoder. Within the linearized\nembedding spaces, the state representation is decomposed into modulus and phase\ncomponents, and the evolution is governed by a set of unitary Koopman operators\nthat act exclusively on the phase. These operators are constructed from\ndiagonal Hamiltonians with coefficients learned from data, a structure designed\nfor efficient implementation on quantum hardware. This architecture enables\ndirect multi-step prediction, and the operator's computational complexity\nscales logarithmically with the observable space dimension. The QKM is\nvalidated across diverse nonlinear systems. Its predictions maintain relative\nerrors below 6% for reaction-diffusion systems and shear flows, and capture key\nstatistics in 2D turbulence. This work establishes a practical pathway for\nquantum-accelerated simulation of nonlinear phenomena, exploring a framework\nbuilt on the synergy between deep learning for global linearization and quantum\nalgorithms for unitary dynamics evolution.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21890v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21890v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.263,
      "weak_supervision_score": 0.287,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.311,
      "datasets_score": 0.249,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21893",
      "title": "Aether Weaver: Multimodal Affective Narrative Co-Generation with Dynamic\n  Scene Graphs",
      "authors": [
        "Saeed Ghorbani"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce Aether Weaver, a novel, integrated framework for multimodal\nnarrative co-generation that overcomes limitations of sequential text-to-visual\npipelines. Our system concurrently synthesizes textual narratives, dynamic\nscene graph representations, visual scenes, and affective soundscapes, driven\nby a tightly integrated, co-generation mechanism. At its core, the Narrator, a\nlarge language model, generates narrative text and multimodal prompts, while\nthe Director acts as a dynamic scene graph manager, and analyzes the text to\nbuild and maintain a structured representation of the story's world, ensuring\nspatio-temporal and relational consistency for visual rendering and subsequent\nnarrative generation. Additionally, a Narrative Arc Controller guides the\nhigh-level story structure, influencing multimodal affective consistency,\nfurther complemented by an Affective Tone Mapper that ensures congruent\nemotional expression across all modalities. Through qualitative evaluations on\na diverse set of narrative prompts encompassing various genres, we demonstrate\nthat Aether Weaver significantly enhances narrative depth, visual fidelity, and\nemotional resonance compared to cascaded baseline approaches. This integrated\nframework provides a robust platform for rapid creative prototyping and\nimmersive storytelling experiences.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21893v2",
      "pdf_url": "http://arxiv.org/pdf/2507.21893v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.486,
      "distributed_training_score": 0.314,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces Aether Weaver, a framework for concurrent multimodal narrative generation using components like a large language model, dynamic scene graphs, and affective mapping. It does not involve diffusion models, iterative refinement processes, or adaptations for complex logical reasoning tasks. There is no component for multi-step logical reasoning using diffusion, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21899",
      "title": "LLM-based Content Classification Approach for GitHub Repositories by the\n  README Files",
      "authors": [
        "Malik Uzair Mehmood",
        "Shahid Hussain",
        "Wen Li Wang",
        "Muhammad Usama Malik"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "GitHub is the world's most popular platform for storing, sharing, and\nmanaging code. Every GitHub repository has a README file associated with it.\nThe README files should contain project-related information as per the\nrecommendations of GitHub to support the usage and improvement of repositories.\nHowever, GitHub repository owners sometimes neglected these recommendations.\nThis prevents a GitHub repository from reaching its full potential. This\nresearch posits that the comprehensiveness of a GitHub repository's README file\nsignificantly influences its adoption and utilization, with a lack of detail\npotentially hindering its full potential for widespread engagement and impact\nwithin the research community. Large Language Models (LLMs) have shown great\nperformance in many text-based tasks including text classification, text\ngeneration, text summarization and text translation. In this study, an approach\nis developed to fine-tune LLMs for automatically classifying different sections\nof GitHub README files. Three encoder-only LLMs are utilized, including BERT,\nDistilBERT and RoBERTa. These pre-trained models are then fine-tuned based on a\ngold-standard dataset consisting of 4226 README file sections. This approach\noutperforms current state-of-the-art methods and has achieved an overall F1\nscore of 0.98. Moreover, we have also investigated the use of\nParameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation\n(LoRA) and shown an economical alternative to full fine-tuning without\ncompromising much performance. The results demonstrate the potential of using\nLLMs in designing an automatic classifier for categorizing the content of\nGitHub README files. Consequently, this study contributes to the development of\nautomated tools for GitHub repositories to improve their identifications and\npotential usages.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21899v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21899v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.329,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21905",
      "title": "Evaluating Deepfake Detectors in the Wild",
      "authors": [
        "Viacheslav Pirogov",
        "Maksim Artemev"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Deepfakes powered by advanced machine learning models present a significant\nand evolving threat to identity verification and the authenticity of digital\nmedia. Although numerous detectors have been developed to address this problem,\ntheir effectiveness has yet to be tested when applied to real-world data. In\nthis work we evaluate modern deepfake detectors, introducing a novel testing\nprocedure designed to mimic real-world scenarios for deepfake detection. Using\nstate-of-the-art deepfake generation methods, we create a comprehensive dataset\ncontaining more than 500,000 high-quality deepfake images. Our analysis shows\nthat detecting deepfakes still remains a challenging task. The evaluation shows\nthat in fewer than half of the deepfake detectors tested achieved an AUC score\ngreater than 60%, with the lowest being 50%. We demonstrate that basic image\nmanipulations, such as JPEG compression or image enhancement, can significantly\nreduce model performance. All code and data are publicly available at\nhttps://github.com/SumSubstance/Deepfake-Detectors-in-the-Wild.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21905v2",
      "pdf_url": "http://arxiv.org/pdf/2507.21905v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.365,
      "datasets_score": 0.426,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes creating a new dataset of over 500,000 high-quality deepfake images using state-of-the-art generation methods, which aligns directly with dataset creation and curation for AI applications. It also involves benchmarking and evaluating this dataset in real-world scenarios for deepfake detection, fitting the criteria for dataset analysis and benchmark evaluation. This makes the paper highly relevant to the topic.",
      "llm_score_status": "completed",
      "summary": "This paper evaluates modern deepfake detectors by introducing a novel testing procedure that mimics real-world scenarios, using state-of-the-art deepfake generation methods to create a dataset of over 500,000 high-quality deepfake images. The authors assess various detectors, revealing that fewer than half achieve an AUC score above 60%, and demonstrate that simple image manipulations like JPEG compression significantly degrade performance, with all code and data made publicly available to facilitate further research.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a novel testing procedure and a comprehensive dataset to evaluate deepfake detectors in real-world conditions, cleverly combining existing ideas to address the limitations of current benchmarks.",
      "impact_score": "Moderate",
      "impact_justification": "This work is likely to be cited and built upon within the subfield of deepfake detection and AI security, as it provides a new dataset and highlights practical challenges that could guide future improvements.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "It represents a strong contribution by exposing weaknesses in deepfake detectors and offering public resources, making it valuable for researchers in computer vision and AI to stay informed on real-world applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3fa08c73cd1ac5e03e839a65492a42506b2a8295",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Viacheslav Pirogov",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373724519"
        },
        {
          "name": "Maksim Artemev",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373722397"
        }
      ]
    },
    {
      "id": "2507.21912",
      "title": "Predict Patient Self-reported Race from Skin Histological Images",
      "authors": [
        "Shengjia Chen",
        "Ruchika Verma",
        "Kevin Clare",
        "Jannes Jegminat",
        "Eugenia Alleva",
        "Kuan-lin Huang",
        "Brandon Veremis",
        "Thomas Fuchs",
        "Gabriele Campanella"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CE (Computational Engineering, Finance, and Science)"
      ],
      "abstract": "Artificial Intelligence (AI) has demonstrated success in computational\npathology (CPath) for disease detection, biomarker classification, and\nprognosis prediction. However, its potential to learn unintended demographic\nbiases, particularly those related to social determinants of health, remains\nunderstudied. This study investigates whether deep learning models can predict\nself-reported race from digitized dermatopathology slides and identifies\npotential morphological shortcuts. Using a multisite dataset with a racially\ndiverse population, we apply an attention-based mechanism to uncover\nrace-associated morphological features. After evaluating three dataset curation\nstrategies to control for confounding factors, the final experiment showed that\nWhite and Black demographic groups retained high prediction performance (AUC:\n0.799, 0.762), while overall performance dropped to 0.663. Attention analysis\nrevealed the epidermis as a key predictive feature, with significant\nperformance declines when these regions were removed. These findings highlight\nthe need for careful data curation and bias mitigation to ensure equitable AI\ndeployment in pathology. Code available at:\nhttps://github.com/sinai-computational-pathology/CPath_SAIF.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21912v2",
      "pdf_url": "http://arxiv.org/pdf/2507.21912v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.314,
      "datasets_score": 0.376,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21917",
      "title": "ArtSeek: Deep artwork understanding via multimodal in-context reasoning\n  and late interaction retrieval",
      "authors": [
        "Nicola Fanelli",
        "Gennaro Vessio",
        "Giovanna Castellano"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Analyzing digitized artworks presents unique challenges, requiring not only\nvisual interpretation but also a deep understanding of rich artistic,\ncontextual, and historical knowledge. We introduce ArtSeek, a multimodal\nframework for art analysis that combines multimodal large language models with\nretrieval-augmented generation. Unlike prior work, our pipeline relies only on\nimage input, enabling applicability to artworks without links to Wikidata or\nWikipedia-common in most digitized collections. ArtSeek integrates three key\ncomponents: an intelligent multimodal retrieval module based on late\ninteraction retrieval, a contrastive multitask classification network for\npredicting artist, genre, style, media, and tags, and an agentic reasoning\nstrategy enabled through in-context examples for complex visual question\nanswering and artwork explanation via Qwen2.5-VL. Central to this approach is\nWikiFragments, a Wikipedia-scale dataset of image-text fragments curated to\nsupport knowledge-grounded multimodal reasoning. Our framework achieves\nstate-of-the-art results on multiple benchmarks, including a +8.4% F1\nimprovement in style classification over GraphCLIP and a +7.1 BLEU@1 gain in\ncaptioning on ArtPedia. Qualitative analyses show that ArtSeek can interpret\nvisual motifs, infer historical context, and retrieve relevant knowledge, even\nfor obscure works. Though focused on visual arts, our approach generalizes to\nother domains requiring external knowledge, supporting scalable multimodal AI\nresearch. Both the dataset and the source code will be made publicly available\nat https://github.com/cilabuniba/artseek.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21917v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21917v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.47,
      "distributed_training_score": 0.343,
      "datasets_score": 0.414,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on multimodal retrieval, classification, and in-context reasoning using models like Qwen2.5-VL, but it does not involve diffusion models or iterative refinement processes for logical tasks. There is no mention of treating a Chain-of-Thought as a holistically refined entity, making this topic unrelated.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces WikiFragments, a new Wikipedia-scale dataset of image-text fragments for knowledge-grounded multimodal reasoning, and details its curation methodology. It also discusses benchmarking and making the dataset publicly available, aligning directly with research on creating and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "ArtSeek is a multimodal framework designed for deep artwork analysis that uses only image inputs to overcome the limitations of prior systems reliant on external metadata. It integrates a multimodal retrieval module with late interaction retrieval and the newly curated WikiFragments dataset, a contrastive multitask classification network for predicting attributes like artist and style, and an agentic reasoning component using Qwen2.5-VL for visual question answering and explanations. The framework achieves state-of-the-art performance, including an 8.4% F1 improvement in style classification over GraphCLIP and a 7.1 BLEU@1 gain in captioning on ArtPedia, while demonstrating capabilities in interpreting visual motifs, inferring historical contexts, and generalizing to other knowledge-intensive domains.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel framework, ArtSeek, that combines multimodal reasoning with retrieval-augmented generation and creates a new dataset, WikiFragments, to address the challenge of analyzing unannotated artworks, significantly advancing the state-of-the-art in art understanding. This approach represents a true innovation by enabling deep interpretation without reliance on external links, which is a departure from existing methods.",
      "impact_score": "High",
      "impact_justification": "The work has high potential impact as it could influence future research in multimodal AI and commercial applications in art analysis, given its open-source release and generalization to other domains requiring contextual knowledge. Its state-of-the-art results and ability to handle obscure artworks make it likely to be widely cited and built upon in computer vision and AI fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper is a high-quality contribution with innovative techniques and strong empirical results that advance art analysis, making it valuable for researchers in computer vision and multimodal AI. While essential for those in the field, it may not be critical for a broader audience outside of specialized domains.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/072d3ee5861dce8977ff5345697047889ec0a013",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 18,
      "average_h_index": 8.666666666666666,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Nicola Fanelli",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2284072604"
        },
        {
          "name": "G. Vessio",
          "h_index": 18,
          "profile_url": "https://www.semanticscholar.org/author/2101805"
        },
        {
          "name": "Giovanna Castellano",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2258156875"
        }
      ]
    },
    {
      "id": "2507.21919",
      "title": "Training language models to be warm and empathetic makes them less\n  reliable and more sycophantic",
      "authors": [
        "Lujain Ibrahim",
        "Franziska Sofia Hafner",
        "Luc Rocher"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Artificial intelligence (AI) developers are increasingly building language\nmodels with warm and empathetic personas that millions of people now use for\nadvice, therapy, and companionship. Here, we show how this creates a\nsignificant trade-off: optimizing language models for warmth undermines their\nreliability, especially when users express vulnerability. We conducted\ncontrolled experiments on five language models of varying sizes and\narchitectures, training them to produce warmer, more empathetic responses, then\nevaluating them on safety-critical tasks. Warm models showed substantially\nhigher error rates (+10 to +30 percentage points) than their original\ncounterparts, promoting conspiracy theories, providing incorrect factual\ninformation, and offering problematic medical advice. They were also\nsignificantly more likely to validate incorrect user beliefs, particularly when\nuser messages expressed sadness. Importantly, these effects were consistent\nacross different model architectures, and occurred despite preserved\nperformance on standard benchmarks, revealing systematic risks that current\nevaluation practices may fail to detect. As human-like AI systems are deployed\nat an unprecedented scale, our findings indicate a need to rethink how we\ndevelop and oversee these systems that are reshaping human relationships and\nsocial interaction.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21919v2",
      "pdf_url": "http://arxiv.org/pdf/2507.21919v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.503,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.351,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on supervised fine-tuning to train models for warmer responses, without involving a reward model, human-ranked data, or reinforcement learning. RLHF specifically requires these elements, which are not present.",
      "weak_supervision_justification": "The paper employs standard supervised fine-tuning with presumably curated data for training, but does not mention programmatically generating labels from noisy or imprecise sources, which is central to weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21922",
      "title": "SwinECAT: A Transformer-based fundus disease classification model with\n  Shifted Window Attention and Efficient Channel Attention",
      "authors": [
        "Peiran Gu",
        "Teng Yao",
        "Mengshen He",
        "Fuhao Duan",
        "Feiyan Liu",
        "RenYuan Peng",
        "Bao Ge"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In recent years, artificial intelligence has been increasingly applied in the\nfield of medical imaging. Among these applications, fundus image analysis\npresents special challenges, including small lesion areas in certain fundus\ndiseases and subtle inter-disease differences, which can lead to reduced\nprediction accuracy and overfitting in the models. To address these challenges,\nthis paper proposes the Transformer-based model SwinECAT, which combines the\nShifted Window (Swin) Attention with the Efficient Channel Attention (ECA)\nAttention. SwinECAT leverages the Swin Attention mechanism in the Swin\nTransformer backbone to effectively capture local spatial structures and\nlong-range dependencies within fundus images. The lightweight ECA mechanism is\nincorporated to guide the SwinECAT's attention toward critical feature\nchannels, enabling more discriminative feature representation. In contrast to\nprevious studies that typically classify fundus images into 4 to 6 categories,\nthis work expands fundus disease classification to 9 distinct types, thereby\nenhancing the granularity of diagnosis. We evaluate our method on the Eye\nDisease Image Dataset (EDID) containing 16,140 fundus images for 9-category\nclassification. Experimental results demonstrate that SwinECAT achieves 88.29\\%\naccuracy, with weighted F1-score of 0.88 and macro F1-score of 0.90. The\nclassification results of our proposed model SwinECAT significantly outperform\nthe baseline Swin Transformer and multiple compared baseline models. To our\nknowledge, this represents the highest reported performance for 9-category\nclassification on this public dataset.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21922v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21922v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.264,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.346,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21924",
      "title": "MMAT-1M: A Large Reasoning Dataset for Multimodal Agent Tuning",
      "authors": [
        "Tianhong Gao",
        "Yannian Fu",
        "Weiqun Wu",
        "Haixiao Yue",
        "Shanshan Liu",
        "Gang Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Large Language Models (LLMs), enhanced through agent tuning, have\ndemonstrated remarkable capabilities in Chain-of-Thought (CoT) and tool\nutilization, significantly surpassing the performance of standalone models.\nHowever, the multimodal domain still lacks a large-scale, high-quality agent\ntuning dataset to unlock the full potential of multimodal large language\nmodels. To bridge this gap, we introduce MMAT-1M, the first million-scale\nmultimodal agent tuning dataset designed to support CoT, reflection, and\ndynamic tool usage. Our dataset is constructed through a novel four-stage data\nengine: 1) We first curate publicly available multimodal datasets containing\nquestion-answer pairs; 2) Then, leveraging GPT-4o, we generate rationales for\nthe original question-answer pairs and dynamically integrate API calls and\nRetrieval Augmented Generation (RAG) information through a multi-turn paradigm;\n3) Furthermore, we refine the rationales through reflection to ensure logical\nconsistency and accuracy, creating a multi-turn dialogue dataset with both\nRationale and Reflection (RR); 4) Finally, to enhance efficiency, we optionally\ncompress multi-turn dialogues into a One-turn Rationale and Reflection (ORR)\nformat. By fine-tuning open-source multimodal models on the MMAT-1M, we observe\nsignificant performance gains. For instance, the InternVL2.5-8B-RR model\nachieves an average improvement of 2.7% across eight public benchmarks and 8.8%\non the RAG benchmark Dyn-VQA, demonstrating the dataset's effectiveness in\nenhancing multimodal reasoning and tool-based capabilities. The dataset is\npublicly available at https://github.com/VIS-MPU-Agent/MMAT-1M.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21924v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21924v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.449,
      "distributed_training_score": 0.36,
      "datasets_score": 0.471,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on creating a dataset for multimodal agent tuning using Chain-of-Thought reasoning and reflection, but it does not involve diffusion models or adapt iterative refinement processes from diffusion for logical tasks. There is no mention of treating reasoning paths as entities for holistic correction via diffusion-like steps.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of MMAT-1M, a new million-scale dataset for multimodal agent tuning, along with its curation methodology, refinement processes, and evaluation on benchmarks. This directly aligns with research on creating, analyzing, and benchmarking datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces MMAT-1M, a million-scale dataset designed for tuning multimodal agents to enhance Chain-of-Thought reasoning, reflection, and dynamic tool usage in multimodal large language models. It employs a four-stage data synthesis process—curating existing datasets, generating rationales with GPT-4o including API calls and RAG, refining through reflection for accuracy, and optionally compressing into a one-turn format—and demonstrates significant performance improvements, such as a 2.7% average gain across benchmarks and 8.8% on Dyn-VQA, when fine-tuning models like InternVL2.5-8B.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new million-scale dataset and a novel four-stage data engine with reflection mechanisms, significantly advancing the state-of-the-art in multimodal agent tuning by addressing key shortcomings in existing datasets.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of multimodal AI, as it provides a publicly available dataset that enhances model performance on reasoning and tool-based tasks. However, its influence may be limited to specific applications rather than broader commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution by introducing a high-quality dataset that addresses critical gaps in multimodal agent tuning, making it essential for researchers in computer vision and AI to be aware of for advancing their work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/aefcbc059c85337d94d62c7e659db8b32e94ca87",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 6,
      "average_h_index": 1.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Tianhong Gao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374460261"
        },
        {
          "name": "Yannian Fu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374400532"
        },
        {
          "name": "Weiqun Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374180847"
        },
        {
          "name": "Haixiao Yue",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/40069452"
        },
        {
          "name": "Shanshan Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354278625"
        },
        {
          "name": "Gang Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373766858"
        }
      ]
    },
    {
      "id": "2507.21928",
      "title": "Vibe Coding as a Reconfiguration of Intent Mediation in Software\n  Development: Definition, Implications, and Research Agenda",
      "authors": [
        "Christian Meske",
        "Tobias Hermanns",
        "Esther von der Weiden",
        "Kai-Uwe Loser",
        "Thorsten Berger"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Software development is undergoing a fundamental transformation as vibe\ncoding becomes widespread, with large portions of contemporary codebases now\nbeing AI-generated. The disconnect between rapid adoption and limited\nconceptual understanding highlights the need for an inquiry into this emerging\nparadigm. Drawing on an intent perspective and historical analysis, we define\nvibe coding as a software development paradigm where humans and generative AI\nengage in collaborative flow to co-create software artifacts through natural\nlanguage dialogue, shifting the mediation of developer intent from\ndeterministic instruction to probabilistic inference. By intent mediation, we\nrefer to the fundamental process through which developers translate their\nconceptual goals into representations that computational systems can execute.\nOur results show that vibe coding reconfigures cognitive work by redistributing\nepistemic labor between humans and machines, shifting the expertise in the\nsoftware development process away from traditional areas such as design or\ntechnical implementation toward collaborative orchestration. We identify key\nopportunities, including democratization, acceleration, and systemic leverage,\nalongside risks, such as black box codebases, responsibility gaps, and\necosystem bias. We conclude with a research agenda spanning human-,\ntechnology-, and organization-centered directions to guide future\ninvestigations of this paradigm.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21928v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21928v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.286,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on \"vibe coding\" as a collaborative paradigm between humans and generative AI in software development, emphasizing intent mediation and cognitive redistribution. It does not discuss diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic. There is no mention of adapting diffusion for complex tasks or treating Chain-of-Thought as an entity, making the paper unrelated to Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21929",
      "title": "Libra: Large Chinese-based Safeguard for AI Content",
      "authors": [
        "Ziyang Chen",
        "Huimu Yu",
        "Xing Wu",
        "Dongqin Liu",
        "Songlin Hu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) excel in text understanding and generation but\nraise significant safety and ethical concerns in high-stakes applications. To\nmitigate these risks, we present Libra-Guard, a cutting-edge safeguard system\ndesigned to enhance the safety of Chinese-based LLMs. Leveraging a two-stage\ncurriculum training pipeline, Libra-Guard enhances data efficiency by employing\nguard pretraining on synthetic samples, followed by fine-tuning on\nhigh-quality, real-world data, thereby significantly reducing reliance on\nmanual annotations. To enable rigorous safety evaluations, we also introduce\nLibra-Test, the first benchmark specifically designed to evaluate the\neffectiveness of safeguard systems for Chinese content. It covers seven\ncritical harm scenarios and includes over 5,700 samples annotated by domain\nexperts. Experiments show that Libra-Guard achieves 86.79% accuracy,\noutperforming Qwen2.5-14B-Instruct (74.33%) and ShieldLM-Qwen-14B-Chat\n(65.69%), and nearing closed-source models like Claude-3.5-Sonnet and GPT-4o.\nThese contributions establish a robust framework for advancing the safety\ngovernance of Chinese LLMs and represent a tentative step toward developing\nsafer, more reliable Chinese AI systems.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21929v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21929v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.479,
      "weak_supervision_score": 0.427,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.406,
      "datasets_score": 0.392,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a two-stage curriculum training pipeline for Libra-Guard, involving pretraining on synthetic data and fine-tuning on real-world data, without any mention of reinforcement learning, human feedback, or a reward model. It emphasizes supervised training to enhance safety, not alignment via RLHF.",
      "weak_supervision_justification": "The paper's main contribution involves generating large-scale synthetic data programmatically and using it alongside real-world data for training Libra-Guard, which reduces reliance on manual annotations. This directly aligns with weak supervision by leveraging noisy or programmatically derived labels to train models efficiently.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper does not discuss distributed training, parallel computing, or multi-node systems for accelerating model training. It focuses on the training pipeline and data strategies for Libra-Guard, without any reference to partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Libra-Guard, a safeguard system designed to enhance the safety of Chinese-based large language models (LLMs) by employing a two-stage curriculum training pipeline that uses synthetic data for pretraining and real-world data for fine-tuning, thereby reducing the need for manual annotations. It also presents Libra-Test, the first benchmark for evaluating safeguards on Chinese content across seven harm scenarios with over 5,700 expert-annotated samples, and demonstrates that Libra-Guard achieves 86.79% accuracy, outperforming existing open-source models and approaching the performance of leading closed-source ones, thus advancing safety governance for Chinese LLMs.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting existing safeguard techniques to Chinese LLMs through a clever two-stage curriculum training approach with synthetic data, addressing a gap in language-specific safety measures. While it builds on prior work like LlamaGuard, it innovates by focusing on a underrepresented language context rather than introducing an entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI safety for non-English languages, particularly Chinese LLMs, due to the introduction of a specialized benchmark and effective safeguard system. However, its influence may remain confined to this niche rather than broadly affecting general AI research or commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to AI safety for Chinese content, making it essential for researchers in that area to understand its methodologies and benchmark. While not groundbreaking across all AI fields, its targeted advancements warrant attention for those focused on ethical AI development.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e93a58bdc30d36f342ecb83586e6152f565aa015",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 5,
      "average_h_index": 1.2,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ziyang Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374494298"
        },
        {
          "name": "Huimu Yu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2324016522"
        },
        {
          "name": "Xing Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374192010"
        },
        {
          "name": "Dongqin Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2350508589"
        },
        {
          "name": "Songlin Hu",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2257376973"
        }
      ]
    },
    {
      "id": "2507.21931",
      "title": "Post-Training Large Language Models via Reinforcement Learning from\n  Self-Feedback",
      "authors": [
        "Carel van Niekerk",
        "Renato Vukovic",
        "Benjamin Matthias Ruppik",
        "Hsien-chin Lin",
        "Milica Gašić"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) often produce plausible but poorly-calibrated\nanswers, limiting their reliability on reasoning-intensive tasks. We present\nReinforcement Learning from Self-Feedback (RLSF), a post-training stage that\nuses the model's own confidence as an intrinsic reward, mimicking how humans\nlearn in the absence of external feedback. After a frozen LLM generates several\nchain-of-thought solutions, we define and compute the confidence of each final\nanswer span and rank the traces accordingly. These synthetic preferences are\nthen used to fine-tune the policy with standard preference optimization,\nsimilar to RLHF yet requiring no human labels, gold answers, or externally\ncurated rewards.\n  RLSF simultaneously (i) refines the model's probability estimates --\nrestoring well-behaved calibration -- and (ii) strengthens step-by-step\nreasoning, yielding improved performance on arithmetic reasoning and\nmultiple-choice question answering.\n  By turning a model's own uncertainty into useful self-feedback, RLSF affirms\nreinforcement learning on intrinsic model behaviour as a principled and\ndata-efficient component of the LLM post-training pipeline and warrents further\nresearch in intrinsic rewards for LLM post-training.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21931v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21931v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.571,
      "weak_supervision_score": 0.428,
      "diffusion_reasoning_score": 0.487,
      "distributed_training_score": 0.353,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes a method similar to RLHF, using preference optimization for fine-tuning, but it relies on the model's self-feedback (intrinsic rewards from confidence) rather than human-ranked data. This makes it conceptually related but not directly aligned with RLHF, as no human feedback is involved.",
      "weak_supervision_justification": "The paper generates synthetic preferences programmatically from the model's own confidence scores, which serves as a noisy, intrinsic source for training labels, aligning with weak supervision's use of imprecise, automated labeling without hand-annotated data. However, it focuses more on reinforcement learning than on broad weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for reasoning; it instead uses chain-of-thought generation and reinforcement learning based on self-feedback, with no mention of adapting diffusion mechanisms for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces Reinforcement Learning from Self-Feedback (RLSF), a post-training method for large language models that leverages the model's own confidence in generated chain-of-thought solutions as an intrinsic reward signal to improve calibration and reasoning. By generating multiple candidate answers, ranking them based on confidence, and using this synthetic preference data for fine-tuning via preference optimization, RLSF enhances the model's probability estimates and performance on tasks like arithmetic reasoning and multiple-choice question answering without requiring external feedback, while affirming its potential as a data-efficient component in LLM post-training pipelines.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by cleverly combining existing techniques like reinforcement learning from human feedback and chain-of-thought prompting to create a self-feedback mechanism, advancing how LLMs can be fine-tuned without external rewards.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in LLM post-training by demonstrating a data-efficient approach to improving calibration and reasoning, potentially leading to broader adoption in subfields focused on AI efficiency and reliability.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides valuable insights into self-supervised reinforcement learning for LLMs, offering practical methods and empirical evidence that are significant for researchers working on model calibration and reasoning enhancements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d38c397873c4e93125a3be97ec645d01e9e4a1e1",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 10,
      "average_h_index": 6.6,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Carel van Niekerk",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/113869710"
        },
        {
          "name": "Renato Vukovic",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2182290500"
        },
        {
          "name": "Benjamin Matthias Ruppik",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/1666232939"
        },
        {
          "name": "Hsien-Chin Lin",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2116102483"
        },
        {
          "name": "Milica Gavsi'c",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/1676892968"
        }
      ]
    },
    {
      "id": "2507.21945",
      "title": "Attention-Driven Multimodal Alignment for Long-term Action Quality\n  Assessment",
      "authors": [
        "Xin Wang",
        "Peng-Jie Li",
        "Yuan-Yuan Shen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Long-term action quality assessment (AQA) focuses on evaluating the quality\nof human activities in videos lasting up to several minutes. This task plays an\nimportant role in the automated evaluation of artistic sports such as rhythmic\ngymnastics and figure skating, where both accurate motion execution and\ntemporal synchronization with background music are essential for performance\nassessment. However, existing methods predominantly fall into two categories:\nunimodal approaches that rely solely on visual features, which are inadequate\nfor modeling multimodal cues like music; and multimodal approaches that\ntypically employ simple feature-level contrastive fusion, overlooking deep\ncross-modal collaboration and temporal dynamics. As a result, they struggle to\ncapture complex interactions between modalities and fail to accurately track\ncritical performance changes throughout extended sequences. To address these\nchallenges, we propose the Long-term Multimodal Attention Consistency Network\n(LMAC-Net). LMAC-Net introduces a multimodal attention consistency mechanism to\nexplicitly align multimodal features, enabling stable integration of visual and\naudio information and enhancing feature representations. Specifically, we\nintroduce a multimodal local query encoder module to capture temporal semantics\nand cross-modal relations, and use a two-level score evaluation for\ninterpretable results. In addition, attention-based and regression-based losses\nare applied to jointly optimize multimodal alignment and score fusion.\nExperiments conducted on the RG and Fis-V datasets demonstrate that LMAC-Net\nsignificantly outperforms existing methods, validating the effectiveness of our\nproposed approach.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21945v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21945v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.309,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21947",
      "title": "Enhancing Generalization in Data-free Quantization via Mixup-class\n  Prompting",
      "authors": [
        "Jiwoong Park",
        "Chaeun Lee",
        "Yongseok Choi",
        "Sein Park",
        "Deokki Hong",
        "Jungwook Choi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Post-training quantization (PTQ) improves efficiency but struggles with\nlimited calibration data, especially under privacy constraints. Data-free\nquantization (DFQ) mitigates this by generating synthetic images using\ngenerative models such as generative adversarial networks (GANs) and\ntext-conditioned latent diffusion models (LDMs), while applying existing PTQ\nalgorithms. However, the relationship between generated synthetic images and\nthe generalizability of the quantized model during PTQ remains underexplored.\nWithout investigating this relationship, synthetic images generated by previous\nprompt engineering methods based on single-class prompts suffer from issues\nsuch as polysemy, leading to performance degradation. We propose\n\\textbf{mixup-class prompt}, a mixup-based text prompting strategy that fuses\nmultiple class labels at the text prompt level to generate diverse, robust\nsynthetic data. This approach enhances generalization, and improves\noptimization stability in PTQ. We provide quantitative insights through\ngradient norm and generalization error analysis. Experiments on convolutional\nneural networks (CNNs) and vision transformers (ViTs) show that our method\nconsistently outperforms state-of-the-art DFQ methods like GenQ. Furthermore,\nit pushes the performance boundary in extremely low-bit scenarios, achieving\nnew state-of-the-art accuracy in challenging 2-bit weight, 4-bit activation\n(W2A4) quantization.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21947v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21947v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.425,
      "diffusion_reasoning_score": 0.456,
      "distributed_training_score": 0.425,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper uses synthetic data generated from generative models, which involves programmatically created data similar to weak supervision sources. However, it focuses on data generation for post-training quantization calibration, not on training models with noisy or imprecise labels, making the connection indirect.",
      "diffusion_reasoning_justification": "The paper employs latent diffusion models for generating synthetic images in quantization, but it does not adapt diffusion processes for multi-step logical reasoning or chain-of-thought tasks; it is solely for data generation, not reasoning.",
      "distributed_training_justification": "The paper does not address distributed training, parallel computing, or partitioning computations across nodes; it focuses exclusively on quantization techniques and synthetic data generation.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21949",
      "title": "Contrast-Prior Enhanced Duality for Mask-Free Shadow Removal",
      "authors": [
        "Jiyu Wu",
        "Yifan Liu",
        "Jiancheng Huang",
        "Mingfu Yan",
        "Shifeng Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Existing shadow removal methods often rely on shadow masks, which are\nchallenging to acquire in real-world scenarios. Exploring intrinsic image cues,\nsuch as local contrast information, presents a potential alternative for\nguiding shadow removal in the absence of explicit masks. However, the cue's\ninherent ambiguity becomes a critical limitation in complex scenes, where it\ncan fail to distinguish true shadows from low-reflectance objects and intricate\nbackground textures. To address this motivation, we propose the Adaptive Gated\nDual-Branch Attention (AGBA) mechanism. AGBA dynamically filters and re-weighs\nthe contrast prior to effectively disentangle shadow features from confounding\nvisual elements. Furthermore, to tackle the persistent challenge of restoring\nsoft shadow boundaries and fine-grained details, we introduce a diffusion-based\nFrequency-Contrast Fusion Network (FCFN) that leverages high-frequency and\ncontrast cues to guide the generative process. Extensive experiments\ndemonstrate that our method achieves state-of-the-art results among mask-free\napproaches while maintaining competitive performance relative to mask-based\nmethods.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21949v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21949v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.314,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21953",
      "title": "MapAgent: Trajectory-Constructed Memory-Augmented Planning for Mobile\n  Task Automation",
      "authors": [
        "Yi Kong",
        "Dianxi Shi",
        "Guoli Yang",
        "Zhang ke-di",
        "Chenlin Huang",
        "Xiaopeng Li",
        "Songchang Jin"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The recent advancement of autonomous agents powered by Large Language Models\n(LLMs) has demonstrated significant potential for automating tasks on mobile\ndevices through graphical user interfaces (GUIs). Despite initial progress,\nthese agents still face challenges when handling complex real-world tasks.\nThese challenges arise from a lack of knowledge about real-life mobile\napplications in LLM-based agents, which may lead to ineffective task planning\nand even cause hallucinations. To address these challenges, we propose a novel\nLLM-based agent framework called MapAgent that leverages memory constructed\nfrom historical trajectories to augment current task planning. Specifically, we\nfirst propose a trajectory-based memory mechanism that transforms task\nexecution trajectories into a reusable and structured page-memory database.\nEach page within a trajectory is extracted as a compact yet comprehensive\nsnapshot, capturing both its UI layout and functional context. Secondly, we\nintroduce a coarse-to-fine task planning approach that retrieves relevant pages\nfrom the memory database based on similarity and injects them into the LLM\nplanner to compensate for potential deficiencies in understanding real-world\napp scenarios, thereby achieving more informed and context-aware task planning.\nFinally, planned tasks are transformed into executable actions through a task\nexecutor supported by a dual-LLM architecture, ensuring effective tracking of\ntask progress. Experimental results in real-world scenarios demonstrate that\nMapAgent achieves superior performance to existing methods. The code will be\nopen-sourced to support further research.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21953v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21953v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.433,
      "distributed_training_score": 0.346,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces MapAgent, an LLM-based framework for mobile task automation that uses memory from historical trajectories for task planning and execution. It emphasizes trajectory-based memory, coarse-to-fine planning, and a dual-LLM architecture, but does not involve diffusion models, iterative refinement of reasoning paths, or any adaptation of diffusion processes for logical tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21954",
      "title": "Fine-Tuning Code Language Models to Detect Cross-Language Bugs",
      "authors": [
        "Zengyang Li",
        "Yimeng Li",
        "Binbin Huang",
        "Peng Liang",
        "Ran Mo",
        "Hui Liu",
        "Yutao Ma"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multilingual programming, which involves using multiple programming languages\n(PLs) in a single project, is increasingly common due to its benefits. However,\nit introduces cross-language bugs (CLBs), which arise from interactions between\ndifferent PLs and are difficult to detect by single-language bug detection\ntools. This paper investigates the potential of pre-trained code language\nmodels (CodeLMs) in CLB detection. We developed CLCFinder, a cross-language\ncode identification tool, and constructed a CLB dataset involving three PL\ncombinations (Python-C/C++, Java-C/C++, and Python-Java) with nine interaction\ntypes. We fine-tuned 13 CodeLMs on this dataset and evaluated their\nperformance, analyzing the effects of dataset size, token sequence length, and\ncode comments. Results show that all CodeLMs performed poorly before\nfine-tuning, but exhibited varying degrees of performance improvement after\nfine-tuning, with UniXcoder-base achieving the best F1 score (0.7407). Notably,\nsmall fine-tuned CodeLMs tended to performe better than large ones. CodeLMs\nfine-tuned on single-language bug datasets performed poorly on CLB detection,\ndemonstrating the distinction between CLBs and single-language bugs.\nAdditionally, increasing the fine-tuning dataset size significantly improved\nperformance, while longer token sequences did not necessarily improve the model\nperformance. The impact of code comments varied across models. Some fine-tuned\nCodeLMs' performance was improved, while others showed degraded performance.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21954v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21954v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.368,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21959",
      "title": "Mitigating Spurious Correlations in Weakly Supervised Semantic\n  Segmentation via Cross-architecture Consistency Regularization",
      "authors": [
        "Zheyuan Zhang",
        "Yen-chia Hsu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Scarcity of pixel-level labels is a significant challenge in practical\nscenarios. In specific domains like industrial smoke, acquiring such detailed\nannotations is particularly difficult and often requires expert knowledge. To\nalleviate this, weakly supervised semantic segmentation (WSSS) has emerged as a\npromising approach. However, due to the supervision gap and inherent bias in\nmodels trained with only image level labels, existing WSSS methods suffer from\nlimitations such as incomplete foreground coverage, inaccurate object\nboundaries, and spurious correlations, especially in our domain, where\nemissions are always spatially coupled with chimneys.\n  Previous solutions typically rely on additional priors or external knowledge\nto mitigate these issues, but they often lack scalability and fail to address\nthe model's inherent bias toward co-occurring context. To address this, we\npropose a novel WSSS framework that directly targets the co-occurrence problem\nwithout relying on external supervision. Unlike prior methods that adopt a\nsingle network, we employ a teacher-student framework that combines CNNs and\nViTs. We introduce a knowledge transfer loss that enforces cross-architecture\nconsistency by aligning internal representations. Additionally, we incorporate\npost-processing techniques to address partial coverage and further improve\npseudo mask quality.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21959v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21959v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.44,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.354,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a novel framework for Weakly Supervised Semantic Segmentation (WSSS) that uses image-level labels to generate pseudo masks via class activation maps (CAMs), directly embodying weak supervision by relying on high-level, noisy annotations rather than precise pixel-level labels. It addresses challenges in WSSS, such as spurious correlations, without additional supervision, aligning with the topic's definition of programmatically generating training labels from imprecise sources.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper tackles the challenges of weakly supervised semantic segmentation (WSSS), particularly spurious correlations in domains like industrial smoke segmentation, by proposing a novel teacher-student framework that combines CNNs and ViTs. The methodology enforces cross-architecture consistency through a knowledge transfer loss to align internal representations and mitigate biases without additional supervision, while incorporating post-processing techniques to improve pseudo mask quality; evaluations on a custom dataset demonstrate enhanced segmentation accuracy and the complementary strengths of different network architectures.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by cleverly combining CNNs and ViTs in a teacher-student framework with cross-architecture consistency regularization to address spurious correlations in WSSS, offering a new way to enhance existing methods without external supervision. However, it builds on established concepts like teacher-student paradigms and consistency learning rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of weakly supervised semantic segmentation, especially for domains with co-occurring features, as it provides a practical framework that could inspire similar approaches in computer vision tasks. Nonetheless, its influence may be limited to specific applications like industrial smoke detection rather than broadly across all AI research.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with innovative insights into mitigating biases in WSSS, making it valuable for researchers in computer vision focused on weakly supervised methods. While not essential for all, it provides practical advancements that could inform ongoing work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0fb294abdefe9e65b438ae26abf2a48a15a5f993",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 1,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zheyuan Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2333311532"
        },
        {
          "name": "Yen-chia Hsu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374077180"
        }
      ]
    },
    {
      "id": "2507.21960",
      "title": "PanoSplatt3R: Leveraging Perspective Pretraining for Generalized Unposed\n  Wide-Baseline Panorama Reconstruction",
      "authors": [
        "Jiahui Ren",
        "Mochu Xiang",
        "Jiajun Zhu",
        "Yuchao Dai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Wide-baseline panorama reconstruction has emerged as a highly effective and\npivotal approach for not only achieving geometric reconstruction of the\nsurrounding 3D environment, but also generating highly realistic and immersive\nnovel views. Although existing methods have shown remarkable performance across\nvarious benchmarks, they are predominantly reliant on accurate pose\ninformation. In real-world scenarios, the acquisition of precise pose often\nrequires additional computational resources and is highly susceptible to noise.\nThese limitations hinder the broad applicability and practicality of such\nmethods. In this paper, we present PanoSplatt3R, an unposed wide-baseline\npanorama reconstruction method. We extend and adapt the foundational\nreconstruction pretrainings from the perspective domain to the panoramic\ndomain, thus enabling powerful generalization capabilities. To ensure a\nseamless and efficient domain-transfer process, we introduce RoPE rolling that\nspans rolled coordinates in rotary positional embeddings across different\nattention heads, maintaining a minimal modification to RoPE's mechanism, while\nmodeling the horizontal periodicity of panorama images. Comprehensive\nexperiments demonstrate that PanoSplatt3R, even in the absence of pose\ninformation, significantly outperforms current state-of-the-art methods. This\nsuperiority is evident in both the generation of high-quality novel views and\nthe accuracy of depth estimation, thereby showcasing its great potential for\npractical applications. Project page: https://npucvr.github.io/PanoSplatt3R",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21960v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21960v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.361,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21964",
      "title": "Thou Shalt Not Prompt: Zero-Shot Human Activity Recognition in Smart\n  Homes via Language Modeling of Sensor Data & Activities",
      "authors": [
        "Sourish Gunesh Dhekane",
        "Thomas Ploetz"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Developing zero-shot human activity recognition (HAR) methods is a critical\ndirection in smart home research -- considering its impact on making HAR\nsystems work across smart homes having diverse sensing modalities, layouts, and\nactivities of interest. The state-of-the-art solutions along this direction are\nbased on generating natural language descriptions of the sensor data and\nfeeding it via a carefully crafted prompt to the LLM to perform classification.\nDespite their performance guarantees, such ``prompt-the-LLM'' approaches carry\nseveral risks, including privacy invasion, reliance on an external service, and\ninconsistent predictions due to version changes, making a case for alternative\nzero-shot HAR methods that do not require prompting the LLMs. In this paper, we\npropose one such solution that models sensor data and activities using natural\nlanguage, leveraging its embeddings to perform zero-shot classification and\nthereby bypassing the need to prompt the LLMs for activity predictions. The\nimpact of our work lies in presenting a detailed case study on six datasets,\nhighlighting how language modeling can bolster HAR systems in zero-shot\nrecognition.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21964v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21964v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.435,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.338,
      "datasets_score": 0.405,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on zero-shot human activity recognition using language modeling of sensor data, with no mention of reinforcement learning, human feedback, or fine-tuning models based on human preferences. It does not involve training a reward model or any RLHF mechanisms.",
      "weak_supervision_justification": "The paper's zero-shot approach reduces reliance on large annotated datasets, aligning somewhat with weak supervision's goal of using minimal or noisy labels. However, it primarily uses pre-trained language embeddings for classification without programmatically generating training labels, so it does not directly employ weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning. It relies on static language embeddings and similarity metrics for classification, with no components related to diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper extensively evaluates its method on six diverse datasets (e.g., Aruba, Milan), including benchmarking against baselines and providing guidelines for new environments. This contributes to dataset analysis and benchmarking in AI, though the primary focus is on the HAR method rather than dataset creation or curation.",
      "llm_score_status": "completed",
      "summary": "This paper introduces a zero-shot human activity recognition (HAR) method for smart homes that avoids prompting large language models (LLMs) by using language modeling to generate embeddings from text summaries of sensor data and descriptions of activity labels, enabling classification via similarity metrics. The approach addresses privacy, reliability, and dependency issues associated with LLM-based methods, demonstrating comparable performance to state-of-the-art baselines across six diverse datasets, strong results in few-shot scenarios, and providing guidelines for optimal implementation.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting language modeling for zero-shot HAR without external LLMs, offering a clever alternative to existing prompt-based techniques while addressing their shortcomings.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence HAR research in smart homes, particularly in privacy-sensitive areas like healthcare, by providing a reliable alternative that can be built upon in specific subfields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution with practical implications for HAR systems, making it essential for researchers in AI and machine learning focused on smart home applications due to its empirical evaluations and guidelines.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/00b0bb3bf725d13e5ddd16343264a84f13e2828a",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 4,
      "average_h_index": 2.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Sourish Gunesh Dhekane",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/80667186"
        },
        {
          "name": "Thomas Ploetz",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2366855925"
        }
      ]
    },
    {
      "id": "2507.21968",
      "title": "A Deep Learning Pipeline Using Synthetic Data to Improve Interpretation\n  of Paper ECG Images",
      "authors": [
        "Xiaoyu Wang",
        "Ramesh Nadarajah",
        "Zhiqiang Zhang",
        "David Wong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Cardiovascular diseases (CVDs) are the leading global cause of death, and\nearly detection is essential to improve patient outcomes. Electrocardiograms\n(ECGs), especially 12-lead ECGs, play a key role in the identification of CVDs.\nThese are routinely interpreted by human experts, a process that is\ntime-consuming and requires expert knowledge. Historical research in this area\nhas focused on automatic ECG interpretation from digital signals, with recent\ndeep learning approaches achieving strong results. In practice, however, most\nECG data in clinical practice are stored or shared in image form. To bridge\nthis gap, we propose a deep learning framework designed specifically to\nclassify paper-like ECG images into five main diagnostic categories. Our method\nwas the winning entry to the 2024 British Heart Foundation Open Data Science\nChallenge. It addresses two main challenges of paper ECG classification: visual\nnoise (e.g., shadows or creases) and the need to detect fine-detailed waveform\npatterns. We propose a pre-processing pipeline that reduces visual noise and a\ntwo-stage fine-tuning strategy: the model is first fine-tuned on synthetic and\nexternal ECG image datasets to learn domain-specific features, and then further\nfine-tuned on the target dataset to enhance disease-specific recognition. We\nadopt the ConvNeXt architecture as the backbone of our model. Our method\nachieved AUROC scores of 0.9688 on the public validation set and 0.9677 on the\nprivate test set of the British Heart Foundation Open Data Science Challenge,\nhighlighting its potential as a practical tool for automated ECG interpretation\nin clinical workflows.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21968v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21968v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.347,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21971",
      "title": "EIFNet: Leveraging Event-Image Fusion for Robust Semantic Segmentation",
      "authors": [
        "Zhijiang Li",
        "Haoran He"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Event-based semantic segmentation explores the potential of event cameras,\nwhich offer high dynamic range and fine temporal resolution, to achieve robust\nscene understanding in challenging environments. Despite these advantages, the\ntask remains difficult due to two main challenges: extracting reliable features\nfrom sparse and noisy event streams, and effectively fusing them with dense,\nsemantically rich image data that differ in structure and representation. To\naddress these issues, we propose EIFNet, a multi-modal fusion network that\ncombines the strengths of both event and frame-based inputs. The network\nincludes an Adaptive Event Feature Refinement Module (AEFRM), which improves\nevent representations through multi-scale activity modeling and spatial\nattention. In addition, we introduce a Modality-Adaptive Recalibration Module\n(MARM) and a Multi-Head Attention Gated Fusion Module (MGFM), which align and\nintegrate features across modalities using attention mechanisms and gated\nfusion strategies. Experiments on DDD17-Semantic and DSEC-Semantic datasets\nshow that EIFNet achieves state-of-the-art performance, demonstrating its\neffectiveness in event-based semantic segmentation.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21971v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21971v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.32,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on event-based semantic segmentation using multi-modal fusion techniques, such as attention mechanisms and feature refinement modules, to integrate event and image data. It does not involve diffusion models, iterative refinement processes for logical reasoning, or any adaptation of diffusion for solving complex logical tasks like Chain-of-Thought reasoning. Therefore, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21974",
      "title": "Reasoning Language Models for Root Cause Analysis in 5G Wireless\n  Networks",
      "authors": [
        "Mohamed Sana",
        "Nicola Piovesan",
        "Antonio De Domenico",
        "Yibin Kang",
        "Haozhe Zhang",
        "Merouane Debbah",
        "Fadhel Ayed"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.NI (Networking and Internet Architecture)"
      ],
      "abstract": "Root Cause Analysis (RCA) in mobile networks remains a challenging task due\nto the need for interpretability, domain expertise, and causal reasoning. In\nthis work, we propose a lightweight framework that leverages Large Language\nModels (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of\nannotated troubleshooting problems designed to benchmark RCA capabilities. Our\nevaluation reveals that existing open-source reasoning LLMs struggle with these\nproblems, underscoring the need for domain-specific adaptation. To address this\nissue, we propose a two-stage training methodology that combines supervised\nfine-tuning with reinforcement learning to improve the accuracy and reasoning\nquality of LLMs. The proposed approach fine-tunes a series of RCA models to\nintegrate domain knowledge and generate structured, multi-step diagnostic\nexplanations, improving both interpretability and effectiveness. Extensive\nexperiments across multiple LLM sizes show significant performance gains over\nstate-of-the-art reasoning and non-reasoning models, including strong\ngeneralization to randomized test variants. These results demonstrate the\npromise of domain-adapted, reasoning-enhanced LLMs for practical and\nexplainable RCA in network operation and management.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21974v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21974v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.526,
      "distributed_training_score": 0.389,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning (specifically GRPO) in its second training stage to improve LLMs for RCA, which involves reward-based optimization. However, it does not mention human feedback, a separate reward model trained on human-ranked data, or alignment with human preferences, making it only loosely related to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on LLMs with supervised fine-tuning and reinforcement learning for RCA, including Chain-of-Thought traces, but does not involve diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21976",
      "title": "The Effect of Compression Techniques on Large Multimodal Language Models\n  in the Medical Domain",
      "authors": [
        "Tanvir Ahmed Khan",
        "Aranya Saha",
        "Ismam Nur Swapnil",
        "Mohammad Ariful Haque"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) hold huge potential for usage in the\nmedical domain, but their computational costs necessitate efficient compression\ntechniques. This paper evaluates the impact of structural pruning and\nactivation-aware quantization on a fine-tuned LLAVA model for medical\napplications. We propose a novel layer selection method for pruning, analyze\ndifferent quantization techniques, and assess the performance trade-offs in a\nprune-SFT-quantize pipeline. Our proposed method enables MLLMs with 7B\nparameters to run within 4 GB of VRAM, reducing memory usage by 70% while\nachieving 4% higher model performance compared to traditional pruning and\nquantization techniques in the same compression ratio.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21976v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21976v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.393,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on compression techniques, such as structural pruning and quantization, for Multimodal Large Language Models (MLLMs) in the medical domain. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning. As a result, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21977",
      "title": "Motion Matters: Motion-guided Modulation Network for Skeleton-based\n  Micro-Action Recognition",
      "authors": [
        "Jihao Gu",
        "Kun Li",
        "Fei Wang",
        "Yanyan Wei",
        "Zhiliang Wu",
        "Hehe Fan",
        "Meng Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Micro-Actions (MAs) are an important form of non-verbal communication in\nsocial interactions, with potential applications in human emotional analysis.\nHowever, existing methods in Micro-Action Recognition often overlook the\ninherent subtle changes in MAs, which limits the accuracy of distinguishing MAs\nwith subtle changes. To address this issue, we present a novel Motion-guided\nModulation Network (MMN) that implicitly captures and modulates subtle motion\ncues to enhance spatial-temporal representation learning. Specifically, we\nintroduce a Motion-guided Skeletal Modulation module (MSM) to inject motion\ncues at the skeletal level, acting as a control signal to guide spatial\nrepresentation modeling. In parallel, we design a Motion-guided Temporal\nModulation module (MTM) to incorporate motion information at the frame level,\nfacilitating the modeling of holistic motion patterns in micro-actions.\nFinally, we propose a motion consistency learning strategy to aggregate the\nmotion cues from multi-scale features for micro-action classification.\nExperimental results on the Micro-Action 52 and iMiGUE datasets demonstrate\nthat MMN achieves state-of-the-art performance in skeleton-based micro-action\nrecognition, underscoring the importance of explicitly modeling subtle motion\ncues. The code will be available at https://github.com/momiji-bit/MMN.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21977v3",
      "pdf_url": "http://arxiv.org/pdf/2507.21977v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.287,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21985",
      "title": "ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models",
      "authors": [
        "Hyun Jun Yook",
        "Ga San Jhun",
        "Jae Hyun Cho",
        "Min Jeon",
        "Donghyun Kim",
        "Tae Hyung Kim",
        "Youn Kyu Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Machine unlearning (MU) removes specific data points or concepts from deep\nlearning models to enhance privacy and prevent sensitive content generation.\nAdversarial prompts can exploit unlearned models to generate content containing\nremoved concepts, posing a significant security risk. However, existing\nadversarial attack methods still face challenges in generating content that\naligns with an attacker's intent while incurring high computational costs to\nidentify successful prompts. To address these challenges, we propose ZIUM, a\nZero-shot Intent-aware adversarial attack on Unlearned Models, which enables\nthe flexible customization of target attack images to reflect an attacker's\nintent. Additionally, ZIUM supports zero-shot adversarial attacks without\nrequiring further optimization for previously attacked unlearned concepts. The\nevaluation across various MU scenarios demonstrated ZIUM's effectiveness in\nsuccessfully customizing content based on user-intent prompts while achieving a\nsuperior attack success rate compared to existing methods. Moreover, its\nzero-shot adversarial attack significantly reduces the attack time for\npreviously attacked unlearned concepts.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21985v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21985v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.32,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21990",
      "title": "ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical\n  Knowledge",
      "authors": [
        "Zihan Zhao",
        "Bo Chen",
        "Ziping Wan",
        "Lu Chen",
        "Xuanze Lin",
        "Shiyang Yu",
        "Situo Zhang",
        "Da Ma",
        "Zichen Zhu",
        "Danyang Zhang",
        "Huayang Wang",
        "Zhongyang Dai",
        "Liyang Wen",
        "Xin Chen",
        "Kai Yu"
      ],
      "categories": [
        "cs.CE (Computational Engineering, Finance, and Science)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While large language models (LLMs) have achieved impressive progress, their\napplication in scientific domains such as chemistry remains hindered by shallow\ndomain understanding and limited reasoning capabilities. In this work, we focus\non the specific field of chemistry and develop a Chemical Reasoner LLM,\nChemDFM-R. We first construct a comprehensive dataset of atomized knowledge\npoints to enhance the model's understanding of the fundamental principles and\nlogical structure of chemistry. Then, we propose a mix-sourced distillation\nstrategy that integrates expert-curated knowledge with general-domain reasoning\nskills, followed by domain-specific reinforcement learning to enhance chemical\nreasoning. Experiments on diverse chemical benchmarks demonstrate that\nChemDFM-R achieves cutting-edge performance while providing interpretable,\nrationale-driven outputs. Further case studies illustrate how explicit\nreasoning chains significantly improve the reliability, transparency, and\npractical utility of the model in real-world human-AI collaboration scenarios.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21990v2",
      "pdf_url": "http://arxiv.org/pdf/2507.21990v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.565,
      "distributed_training_score": 0.377,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes domain-specific reinforcement learning to enhance chemical reasoning, but it does not mention training a reward model on human-ranked data or aligning the model with human preferences, which are essential components of RLHF. The reinforcement learning appears to be based on domain-specific data without human feedback involvement.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on constructing a chemical reasoner LLM using knowledge distillation and reinforcement learning, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.21992",
      "title": "Teach Me to Trick: Exploring Adversarial Transferability via Knowledge\n  Distillation",
      "authors": [
        "Siddhartha Pradhan",
        "Shikshya Shiwakoti",
        "Neha Bathuri"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We investigate whether knowledge distillation (KD) from multiple\nheterogeneous teacher models can enhance the generation of transferable\nadversarial examples. A lightweight student model is trained using two KD\nstrategies: curriculum-based switching and joint optimization, with ResNet50\nand DenseNet-161 as teachers. The trained student is then used to generate\nadversarial examples using FG, FGS, and PGD attacks, which are evaluated\nagainst a black-box target model (GoogLeNet). Our results show that student\nmodels distilled from multiple teachers achieve attack success rates comparable\nto ensemble-based baselines, while reducing adversarial example generation time\nby up to a factor of six. An ablation study further reveals that lower\ntemperature settings and the inclusion of hard-label supervision significantly\nenhance transferability. These findings suggest that KD can serve not only as a\nmodel compression technique but also as a powerful tool for improving the\nefficiency and effectiveness of black-box adversarial attacks.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.21992v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21992v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.409,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on using knowledge distillation from multiple teachers to generate transferable adversarial examples, focusing on model compression and adversarial attacks. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as it centers on neural network vulnerabilities rather than reasoning mechanisms.",
      "distributed_training_justification": "The paper explores knowledge distillation strategies for training a student model to improve adversarial example generation, but it does not discuss distributed training, parallel computing, or multi-node machine learning. There is no mention of partitioning data, architecture, or computation across processors or nodes for acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22000",
      "title": "Staining and locking computer vision models without retraining",
      "authors": [
        "Oliver J. Sutton",
        "Qinghua Zhou",
        "George Leete",
        "Alexander N. Gorban",
        "Ivan Y. Tyukin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We introduce new methods of staining and locking computer vision models, to\nprotect their owners' intellectual property. Staining, also known as\nwatermarking, embeds secret behaviour into a model which can later be used to\nidentify it, while locking aims to make a model unusable unless a secret\ntrigger is inserted into input images. Unlike existing methods, our algorithms\ncan be used to stain and lock pre-trained models without requiring fine-tuning\nor retraining, and come with provable, computable guarantees bounding their\nworst-case false positive rates. The stain and lock are implemented by directly\nmodifying a small number of the model's weights and have minimal impact on the\n(unlocked) model's performance. Locked models are unlocked by inserting a small\n`trigger patch' into the corner of the input image. We present experimental\nresults showing the efficacy of our methods and demonstrating their practical\nperformance on a variety of computer vision models.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22000v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22000v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.359,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22002",
      "title": "Bridging Synthetic and Real-World Domains: A Human-in-the-Loop\n  Weakly-Supervised Framework for Industrial Toxic Emission Segmentation",
      "authors": [
        "Yida Tao",
        "Yen-Chia Hsu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Industrial smoke segmentation is critical for air-quality monitoring and\nenvironmental protection but is often hampered by the high cost and scarcity of\npixel-level annotations in real-world settings. We introduce CEDANet, a\nhuman-in-the-loop, class-aware domain adaptation framework that uniquely\nintegrates weak, citizen-provided video-level labels with adversarial feature\nalignment. Specifically, we refine pseudo-labels generated by a source-trained\nsegmentation model using citizen votes, and employ class-specific domain\ndiscriminators to transfer rich source-domain representations to the industrial\ndomain. Comprehensive experiments on SMOKE5K and custom IJmond datasets\ndemonstrate that CEDANet achieves an F1-score of 0.414 and a smoke-class IoU of\n0.261 with citizen feedback, vastly outperforming the baseline model, which\nscored 0.083 and 0.043 respectively. This represents a five-fold increase in\nF1-score and a six-fold increase in smoke-class IoU. Notably, CEDANet with\ncitizen-constrained pseudo-labels achieves performance comparable to the same\narchitecture trained on limited 100 fully annotated images with F1-score of\n0.418 and IoU of 0.264, demonstrating its ability to reach small-sampled fully\nsupervised-level accuracy without target-domain annotations. Our research\nvalidates the scalability and cost-efficiency of combining citizen science with\nweakly supervised domain adaptation, offering a practical solution for complex,\ndata-scarce environmental monitoring applications.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22002v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22002v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.481,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.34,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, CEDANet, directly employs weak supervision by using citizen-provided video-level labels, which are noisy and high-level, to generate and refine pseudo-labels for training a segmentation model. This aligns with the definition of weak supervision, as it programmatically leverages imprecise sources to achieve performance comparable to fully supervised methods, without relying on pixel-level annotations.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces CEDANet, a human-in-the-loop, weakly-supervised framework for industrial smoke segmentation that addresses the scarcity of pixel-level annotations by integrating citizen-provided video-level labels with adversarial feature alignment. It refines pseudo-labels using citizen feedback and employs class-specific domain discriminators to adapt from synthetic to real-world domains, achieving significant performance gains on datasets like SMOKE5K and IJmond, with an F1-score of 0.414 and smoke-class IoU of 0.261, outperforming baselines and matching fully supervised models with limited data.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of human-in-the-loop feedback and existing domain adaptation techniques to address annotation scarcity in environmental monitoring, offering a notable improvement over standard methods without introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like computer vision for environmental applications, as it demonstrates scalable, cost-efficient solutions for data-scarce real-world problems, though its influence may remain niche.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a strong, practical contribution to weakly supervised learning and domain adaptation in AI, making it valuable for researchers in environmental monitoring and computer vision.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5f01013c464377acd3e33fb88d6f2b96816a02ff",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yida Tao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374096600"
        },
        {
          "name": "Yen-chia Hsu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374077180"
        }
      ]
    },
    {
      "id": "2507.22003",
      "title": "See Different, Think Better: Visual Variations Mitigating Hallucinations\n  in LVLMs",
      "authors": [
        "Ziyun Dai",
        "Xiaoqiang Li",
        "Shaohua Zhang",
        "Yuanchen Wu",
        "Jide Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in visual understanding and multimodal reasoning. However, LVLMs\nfrequently exhibit hallucination phenomena, manifesting as the generated\ntextual responses that demonstrate inconsistencies with the provided visual\ncontent. Existing hallucination mitigation methods are predominantly\ntext-centric, the challenges of visual-semantic alignment significantly limit\ntheir effectiveness, especially when confronted with fine-grained visual\nunderstanding scenarios. To this end, this paper presents ViHallu, a\nVision-Centric Hallucination mitigation framework that enhances visual-semantic\nalignment through Visual Variation Image Generation and Visual Instruction\nConstruction. ViHallu introduces visual variation images with controllable\nvisual alterations while maintaining the overall image structure. These images,\ncombined with carefully constructed visual instructions, enable LVLMs to better\nunderstand fine-grained visual content through fine-tuning, allowing models to\nmore precisely capture the correspondence between visual content and text,\nthereby enhancing visual-semantic alignment. Extensive experiments on multiple\nbenchmarks show that ViHallu effectively enhances models' fine-grained visual\nunderstanding while significantly reducing hallucination tendencies.\nFurthermore, we release ViHallu-Instruction, a visual instruction dataset\nspecifically designed for hallucination mitigation and visual-semantic\nalignment. Code is available at https://github.com/oliviadzy/ViHallu.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22003v2",
      "pdf_url": "http://arxiv.org/pdf/2507.22003v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.304,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for mitigating hallucinations in Large Vision-Language Models (LVLMs) by generating visual variation images and constructing visual instructions to improve visual-semantic alignment. It does not involve adapting the iterative refinement process of diffusion models for multi-step logical reasoning or treating a Chain-of-Thought as a single entity for holistic correction. While image generation might implicitly use techniques like diffusion, the paper does not specify or focus on diffusion for reasoning tasks, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22008",
      "title": "VeS: Teaching Pixels to Listen Without Supervision",
      "authors": [
        "Sajay Raj"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent dense audio-visual (AV) models achieve impressive retrieval and\nemergent localization, but almost all evidence comes from English-centric,\ncaption-rich web video. It is unclear whether these objectives survive in\nlow-resource, code-switched, and noisy multilingual settings that typify\ndeveloping regions. We show they do**-**and that the choice of aggregation\nfunction becomes even more critical. Using a multilingual subset of Project\nVaani spanning dozens of Indian languages and dialectal variants, we compare\nthree contrastive objectives: (i) a global mean-pooled loss (CLIP-style), (ii)\na dense max-mean token matcher (DenseAV-style), and (iii) a simple hybrid\n(motivated by frozen-vision alignment strategies). The dense objective delivers\na +59% relative R@1 (Audio Visual) improvement over global pooling and\nsubstantially lower mean/median ranks, while consistently producing sharp\nzero-shot localization heatmaps of spoken objects-despite keeping the vision\nbackbone entirely frozen (no LoRA / partial fine-tuning). Our results\ndemonstrate that dense token routing is not a luxury of high-resource English\ncorpora; it is more decisive when annotations and acoustic cleanliness are\nscarce. We release the codebase and trained models.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22008v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22008v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.426,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.369,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves self-supervised audio-visual learning using contrastive objectives on noisy, multilingual data without text or spatial supervision. This aligns closely with weak supervision, as it programmatically generates training signals (e.g., positive and negative pairs via contrastive losses) from high-level, imprecise sources like audio-visual alignments, rather than relying on hand-labeled data. The use of dense objectives in low-resource settings further demonstrates weak supervision principles by effectively handling scarce and noisy annotations.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper, titled \"VeS: Teaching Pixels to Listen Without Supervision,\" investigates the effectiveness of contrastive objectives for self-supervised audio-visual learning in low-resource, multilingual, and noisy environments, using a subset of the Project Vaani dataset featuring dozens of Indian languages. The methodology involves comparing three approaches—a global mean-pooled loss (CLIP-style), a dense max-mean token-matching loss (DenseAV-style), and a hybrid—while keeping the vision backbone frozen, revealing that the dense objective achieves a 59% relative improvement in audio-visual retrieval (R@1) and produces sharp zero-shot localization heatmaps, underscoring its superiority in annotation-scarce settings.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying and comparing existing contrastive objectives in a new, underrepresented multilingual and noisy context, demonstrating a clever adaptation rather than a wholly new technique.",
      "impact_score": "High",
      "impact_justification": "The work addresses critical gaps in audio-visual learning for low-resource settings and releases accessible code, potentially influencing future research and applications in diverse linguistic environments.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides significant practical insights and improvements in self-supervised learning for multilingual contexts, making it a valuable contribution for researchers in computer vision and audio-visual processing.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2cfd86a726619f04dab34866fe112bb50a3527fd",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Sajay Raj",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373733488"
        }
      ]
    },
    {
      "id": "2507.22009",
      "title": "PHAX: A Structured Argumentation Framework for User-Centered Explainable\n  AI in Public Health and Biomedical Sciences",
      "authors": [
        "Bahar İlgen",
        "Akshat Dubey",
        "Georges Hattab"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Ensuring transparency and trust in AI-driven public health and biomedical\nsciences systems requires more than accurate predictions-it demands\nexplanations that are clear, contextual, and socially accountable. While\nexplainable AI (XAI) has advanced in areas like feature attribution and model\ninterpretability, most methods still lack the structure and adaptability needed\nfor diverse health stakeholders, including clinicians, policymakers, and the\ngeneral public. We introduce PHAX-a Public Health Argumentation and\neXplainability framework-that leverages structured argumentation to generate\nhuman-centered explanations for AI outputs. PHAX is a multi-layer architecture\ncombining defeasible reasoning, adaptive natural language techniques, and user\nmodeling to produce context-aware, audience-specific justifications. More\nspecifically, we show how argumentation enhances explainability by supporting\nAI-driven decision-making, justifying recommendations, and enabling interactive\ndialogues across user types. We demonstrate the applicability of PHAX through\nuse cases such as medical term simplification, patient-clinician communication,\nand policy justification. In particular, we show how simplification decisions\ncan be modeled as argument chains and personalized based on user\nexpertise-enhancing both interpretability and trust. By aligning formal\nreasoning methods with communicative demands, PHAX contributes to a broader\nvision of transparent, human-centered AI in public health.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22009v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22009v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.297,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.259,
      "datasets_score": 0.279,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces PHAX, a framework for generating user-adaptive explanations using structured argumentation and NLP, but it does not involve training models with human-ranked data or using reinforcement learning to align AI with human preferences. There is no mention of RLHF elements like a reward model or iterative fine-tuning based on human feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on structured argumentation and reasoning chains for explanations in AI outputs, but it does not adapt diffusion models or their iterative refinement process for multi-step logical tasks. There is no indication of treating a 'Chain-of-Thought' as a holistically corrected entity using diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22010",
      "title": "Exploring the Stratified Space Structure of an RL Game with the Volume\n  Growth Transform",
      "authors": [
        "Justin Curry",
        "Brennan Lagasse",
        "Ngoc B. Lam",
        "Gregory Cox",
        "David Rosenbluth",
        "Alberto Speranzon"
      ],
      "categories": [
        "math.AT (Algebraic Topology)",
        "cs.AI (Artificial Intelligence)",
        "cs.CG (Computational Geometry)",
        "cs.LG (Machine Learning)",
        "math.DG (Differential Geometry)"
      ],
      "abstract": "In this work, we explore the structure of the embedding space of a\ntransformer model trained for playing a particular reinforcement learning (RL)\ngame. Specifically, we investigate how a transformer-based Proximal Policy\nOptimization (PPO) model embeds visual inputs in a simple environment where an\nagent must collect \"coins\" while avoiding dynamic obstacles consisting of\n\"spotlights.\" By adapting Robinson et al.'s study of the volume growth\ntransform for LLMs to the RL setting, we find that the token embedding space\nfor our visual coin collecting game is also not a manifold, and is better\nmodeled as a stratified space, where local dimension can vary from point to\npoint. We further strengthen Robinson's method by proving that fairly general\nvolume growth curves can be realized by stratified spaces. Finally, we carry\nout an analysis that suggests that as an RL agent acts, its latent\nrepresentation alternates between periods of low local dimension, while\nfollowing a fixed sub-strategy, and bursts of high local dimension, where the\nagent achieves a sub-goal (e.g., collecting an object) or where the\nenvironmental complexity increases (e.g., more obstacles appear). Consequently,\nour work suggests that the distribution of dimensions in a stratified latent\nspace may provide a new geometric indicator of complexity for RL games.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22010v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22010v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.349,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is analyzing the geometric structure of latent spaces in a reinforcement learning (RL) game using volume growth transforms, focusing on whether the space is a manifold or stratified. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22017",
      "title": "Cyst-X: AI-Powered Pancreatic Cancer Risk Prediction from Multicenter\n  MRI in Centralized and Federated Learning",
      "authors": [
        "Hongyi Pan",
        "Gorkem Durak",
        "Elif Keles",
        "Deniz Seyithanoglu",
        "Zheyuan Zhang",
        "Alpay Medetalibeyoglu",
        "Halil Ertugrul Aktas",
        "Andrea Mia Bejar",
        "Ziliang Hong",
        "Yavuz Taktak",
        "Gulbiz Dagoglu Kartal",
        "Mehmet Sukru Erturk",
        "Timurhan Cebeci",
        "Maria Jaramillo Gonzalez",
        "Yury Velichko",
        "Lili Zhao",
        "Emil Agarunov",
        "Federica Proietto Salanitri",
        "Concetto Spampinato",
        "Pallavi Tiwari",
        "Ziyue Xu",
        "Sachin Jambawalikar",
        "Ivo G. Schoots",
        "Marco J. Bruno",
        "Chenchang Huang",
        "Candice Bolan",
        "Tamas Gonda",
        "Frank H. Miller",
        "Rajesh N. Keswani",
        "Michael B. Wallace",
        "Ulas Bagci"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Pancreatic cancer is projected to become the second-deadliest malignancy in\nWestern countries by 2030, highlighting the urgent need for better early\ndetection. Intraductal papillary mucinous neoplasms (IPMNs), key precursors to\npancreatic cancer, are challenging to assess with current guidelines, often\nleading to unnecessary surgeries or missed malignancies. We present Cyst-X, an\nAI framework that predicts IPMN malignancy using multicenter MRI data,\nleveraging MRI's superior soft tissue contrast over CT. Trained on 723 T1- and\n738 T2-weighted scans from 764 patients across seven institutions, our models\n(AUC=0.82) significantly outperform both Kyoto guidelines (AUC=0.75) and expert\nradiologists. The AI-derived imaging features align with known clinical markers\nand offer biologically meaningful insights. We also demonstrate strong\nperformance in a federated learning setting, enabling collaborative training\nwithout sharing patient data. To promote privacy-preserving AI development and\nimprove IPMN risk stratification, the Cyst-X dataset is released as the first\nlarge-scale, multi-center pancreatic cysts MRI dataset.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22017v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22017v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.288,
      "distributed_training_score": 0.372,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22020",
      "title": "XAI for Point Cloud Data using Perturbations based on Meaningful\n  Segmentation",
      "authors": [
        "Raju Ningappa Mulawade",
        "Christoph Garth",
        "Alexander Wiebel"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We propose a novel segmentation-based explainable artificial intelligence\n(XAI) method for neural networks working on point cloud classification. As one\nbuilding block of this method, we propose a novel point-shifting mechanism to\nintroduce perturbations in point cloud data. Recently, AI has seen an\nexponential growth. Hence, it is important to understand the decision-making\nprocess of AI algorithms when they are applied in critical areas. Our work\nfocuses on explaining AI algorithms that classify point cloud data. An\nimportant aspect of the methods used for explaining AI algorithms is their\nability to produce explanations that are easy for humans to understand. This\nallows them to analyze the AI algorithms better and make appropriate decisions\nbased on that analysis. Therefore, in this work, we intend to generate\nmeaningful explanations that can be easily interpreted by humans. The point\ncloud data we consider represents 3D objects such as cars, guitars, and\nlaptops. We make use of point cloud segmentation models to generate\nexplanations for the working of classification models. The segments are used to\nintroduce perturbations into the input point cloud data and generate saliency\nmaps. The perturbations are introduced using the novel point-shifting mechanism\nproposed in this work which ensures that the shifted points no longer influence\nthe output of the classification algorithm. In contrast to previous methods,\nthe segments used by our method are meaningful, i.e. humans can easily\ninterpret the meaning of the segments. Thus, the benefit of our method over\nother methods is its ability to produce more meaningful saliency maps. We\ncompare our method with the use of classical clustering algorithms to generate\nexplanations. We also analyze the saliency maps generated for example inputs\nusing our method to demonstrate the usefulness of the method in generating\nmeaningful explanations.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22020v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22020v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.328,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a segmentation-based explainable AI (XAI) method for point cloud classification, introducing perturbations via a novel point-shifting mechanism to generate saliency maps. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. There is no mention of treating a 'Chain-of-Thought' as an entity or any diffusion-related components, making it entirely unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22024",
      "title": "Cardiac-CLIP: A Vision-Language Foundation Model for 3D Cardiac CT\n  Images",
      "authors": [
        "Yutao Hu",
        "Ying Zheng",
        "Shumei Miao",
        "Xiaolei Zhang",
        "Jiahao Xia",
        "Yaolei Qi",
        "Yiyang Zhang",
        "Yuting He",
        "Qian Chen",
        "Jing Ye",
        "Hongyan Qiao",
        "Xiuhua Hu",
        "Lei Xu",
        "Jiayin Zhang",
        "Hui Liu",
        "Minwen Zheng",
        "Yining Wang",
        "Daimin Zhang",
        "Ji Zhang",
        "Wenqi Shao",
        "Yun Liu",
        "Longjiang Zhang",
        "Guanyu Yang"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Foundation models have demonstrated remarkable potential in medical domain.\nHowever, their application to complex cardiovascular diagnostics remains\nunderexplored. In this paper, we present Cardiac-CLIP, a multi-modal foundation\nmodel designed for 3D cardiac CT images. Cardiac-CLIP is developed through a\ntwo-stage pre-training strategy. The first stage employs a 3D masked\nautoencoder (MAE) to perform self-supervised representation learning from\nlarge-scale unlabeled volumetric data, enabling the visual encoder to capture\nrich anatomical and contextual features. In the second stage, contrastive\nlearning is introduced to align visual and textual representations,\nfacilitating cross-modal understanding. To support the pre-training, we collect\n16641 real clinical CT scans, supplemented by 114k publicly available data.\nMeanwhile, we standardize free-text radiology reports into unified templates\nand construct the pathology vectors according to diagnostic attributes, based\non which the soft-label matrix is generated to supervise the contrastive\nlearning process. On the other hand, to comprehensively evaluate the\neffectiveness of Cardiac-CLIP, we collect 6,722 real-clinical data from 12\nindependent institutions, along with the open-source data to construct the\nevaluation dataset. Specifically, Cardiac-CLIP is comprehensively evaluated\nacross multiple tasks, including cardiovascular abnormality classification,\ninformation retrieval and clinical analysis. Experimental results demonstrate\nthat Cardiac-CLIP achieves state-of-the-art performance across various\ndownstream tasks in both internal and external data. Particularly, Cardiac-CLIP\nexhibits great effectiveness in supporting complex clinical tasks such as the\nprospective prediction of acute coronary syndrome, which is notoriously\ndifficult in real-world scenarios.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22024v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22024v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.342,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22025",
      "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and\n  Precise Inference-Time Grounding",
      "authors": [
        "Shuquan Lian",
        "Yuhang Wu",
        "Jia Ma",
        "Yifan Ding",
        "Zihan Song",
        "Bingqi Chen",
        "Xiawu Zheng",
        "Hui Li"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The emergence of Multimodal Large Language Models (MLLMs) has driven\nsignificant advances in Graphical User Interface (GUI) agent capabilities.\nNevertheless, existing GUI agent training and inference techniques still suffer\nfrom a dilemma for reasoning designs, ineffective reward, and visual noise. To\naddress these issues, we introduce UI-AGILE for enhancing GUI agents at both\ntraining and inference. For training, we propose a suite of improvements to the\nSupervised Fine-Tuning (SFT) process: 1) a continuous reward function to\nincentivize high-precision grounding; 2) a ``Simple Thinking'' reward to\nbalance planning with speed and grounding accuracy; and 3) a cropping-based\nresampling strategy to mitigate the sparse reward problem and improve learning\non complex tasks. For inference, we present decomposed grounding with selection\nto dramatically improve grounding accuracy on high-resolution displays by\nbreaking the image into smaller, manageable parts. Experiments show that\nUI-AGILE achieves the state-of-the-art grounding performance on two benchmarks\nScreenSpot-Pro and ScreenSpot-v2 while it also exhibits strong general agent\ncapabilities. For instance, using both our training and inference enhancement\nmethods brings 23\\% grounding accuracy improvement over the best baseline on\nScreenSpot-Pro. We provide the code in https://github.com/KDEGroup/UI-AGILE.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22025v3",
      "pdf_url": "http://arxiv.org/pdf/2507.22025v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.451,
      "weak_supervision_score": 0.42,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.347,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses Reinforcement Fine-Tuning (RFT) with reward functions to improve GUI agents, which involves reinforcement learning concepts. However, it does not mention training a reward model on human-ranked data or using human feedback, focusing instead on programmatically designed rewards like continuous grounding rewards. Thus, it is only tangentially related to RLHF.",
      "weak_supervision_justification": "The paper employs techniques like cropping-based resampling to dynamically adjust training samples and mitigate sparse rewards, which aligns with weak supervision by programmatically generating or modifying labels from noisy sources rather than relying solely on hand-labeled data. While not the core focus, this approach shares principles of weak supervision in training GUI agents.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for reasoning tasks. It focuses on reinforcement learning enhancements, reward designs, and inference techniques for GUI agents, with no mention of multi-step logical reasoning via diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "UI-AGILE is a framework aimed at enhancing Graphical User Interface (GUI) agents by addressing limitations in training and inference using Multimodal Large Language Models (MLLMs). It introduces training improvements such as a continuous reward function for precise grounding, a \"Simple Thinking\" reward to balance planning and accuracy, and cropping-based resampling to mitigate sparse rewards; for inference, it proposes decomposed grounding with selection to improve accuracy on high-resolution screens. Experimental results show that UI-AGILE achieves state-of-the-art performance on benchmarks like ScreenSpot-Pro and ScreenSpot-v2, with a 23% improvement in grounding accuracy over the best baselines, while demonstrating strong general agent capabilities.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing reinforcement learning techniques with innovative reward designs and inference strategies, effectively solving known problems in GUI agents. While it advances the state-of-the-art, it builds on prior methods rather than introducing a entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfields of AI, computer vision, and language processing due to its practical enhancements for GUI agents. However, its influence may be confined to specific applications in interface automation rather than broadly transformative across all AI domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides high-quality contributions with empirical evidence of significant improvements, making it valuable for researchers focused on multimodal AI and GUI systems. It is a strong advancement but not essential for those outside the immediate field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e5c8edfe19e5af718945dfc93057b28d1a84dad2",
      "total_authors": 8,
      "authors_found": 7,
      "highest_h_index": 1,
      "average_h_index": 0.14285714285714285,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Shuquan Lian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373720478"
        },
        {
          "name": "Yuhang Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373742863"
        },
        {
          "name": "Jia Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373982578"
        },
        {
          "name": "Yifan Ding",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375855738"
        },
        {
          "name": "Zihan Song",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Bingqi Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373734926"
        },
        {
          "name": "Xiawu Zheng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2354502340"
        },
        {
          "name": "Hui Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374142289"
        }
      ]
    },
    {
      "id": "2507.22028",
      "title": "From Seeing to Experiencing: Scaling Navigation Foundation Models with\n  Reinforcement Learning",
      "authors": [
        "Honglin He",
        "Yukai Ma",
        "Wayne Wu",
        "Bolei Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Navigation foundation models trained on massive webscale data enable agents\nto generalize across diverse environments and embodiments. However, these\nmodels trained solely on offline data, often lack the capacity to reason about\nthe consequences of their actions or adapt through counterfactual\nunderstanding. They thus face significant limitations in the real-world urban\nnavigation where interactive and safe behaviors, such as avoiding obstacles and\nmoving pedestrians, are critical. To tackle these challenges, we introduce the\nSeeing-to-Experiencing framework to scale the capability of navigation\nfoundation models with reinforcement learning. S2E combines the strengths of\npre-training on videos and post-training through RL. It maintains the\ngeneralizability acquired from large-scale real-world videos while enhancing\nits interactivity through RL in simulation environments. Specifically, we\nintroduce two innovations: an Anchor-Guided Distribution Matching strategy,\nwhich stabilizes learning and models diverse motion patterns through\nanchor-based supervision; and a Residual-Attention Module, which obtains\nreactive behaviors from simulation environments without erasing the model's\npretrained knowledge. Moreover, we establish a comprehensive end-to-end\nevaluation benchmark, NavBench-GS, built on photorealistic 3DGS reconstructions\nof real-world scenes that incorporate physical interactions. It can\nsystematically assess the generalizability and safety of navigation foundation\nmodels. Extensive experiments show that S2E mitigates the diminishing returns\noften seen when scaling with offline data alone. We perform a thorough analysis\nof the benefits of Reinforcement Learning compared to Supervised Fine-Tuning in\nthe context of post-training for robot learning. Our findings emphasize the\ncrucial role of integrating interactive online experiences to effectively scale\nfoundation models in Robotics.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22028v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22028v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.47,
      "weak_supervision_score": 0.41,
      "diffusion_reasoning_score": 0.431,
      "distributed_training_score": 0.397,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on reinforcement learning in simulation environments for navigation tasks, but it does not involve human feedback, a reward model trained on human preferences, or any alignment with human-ranked data. Instead, it emphasizes RL based on environmental interactions.",
      "weak_supervision_justification": "The paper uses large-scale web videos and human demonstrations for pre-training, which involves noisy or implicit labels from real-world data, aligning with weak supervision concepts. However, it does not explicitly describe programmatically generating labels, focusing more on imitation learning from videos.",
      "diffusion_reasoning_justification": "The paper does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning. It centers on reinforcement learning and attention mechanisms for navigation, with no components related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces the Seeing-to-Experiencing (S2E) framework to enhance navigation foundation models by combining pre-training on large-scale video data with reinforcement learning (RL) in simulated environments, addressing limitations in interactivity and adaptability for real-world urban navigation. Key methodologies include the Anchor-Guided Distribution Matching strategy for stable learning from videos and the Residual-Attention Module for RL post-training that preserves pre-trained knowledge while adding reactive behaviors; experiments demonstrate that this approach improves performance, generalizability, and safety, outperforming scaling with offline data alone, as evaluated on the new NavBench-GS benchmark.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing pre-training techniques with reinforcement learning innovations like AGDM and Residual-Attention Module to address known limitations in navigation models, offering a notable improvement rather than a completely new paradigm. While it advances state-of-the-art by integrating these elements for better interactivity, it builds on established methods in AI and robotics.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in robotics and computer vision by providing a scalable framework for enhancing navigation models, potentially leading to citations and further developments in subfields like autonomous agents. However, its impact may be confined to specific applications in navigation and simulation, rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution with practical innovations and a new benchmark that advances robot learning, making it important for researchers in AI and robotics to be aware of for improving navigation systems. While not essential for all, it provides insightful methods and findings that could inspire related work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d924ae5d405152ec8822a06ac7359201376b3e85",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 4,
      "average_h_index": 2.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Honglin He",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2223989454"
        },
        {
          "name": "Yukai Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373740757"
        },
        {
          "name": "Wayne Wu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2310774620"
        },
        {
          "name": "Bolei Zhou",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2261182340"
        }
      ]
    },
    {
      "id": "2507.22030",
      "title": "ReXGroundingCT: A 3D Chest CT Dataset for Segmentation of Findings from\n  Free-Text Reports",
      "authors": [
        "Mohammed Baharoon",
        "Luyang Luo",
        "Michael Moritz",
        "Abhinav Kumar",
        "Sung Eun Kim",
        "Xiaoman Zhang",
        "Miao Zhu",
        "Mahmoud Hussain Alabbad",
        "Maha Sbayel Alhazmi",
        "Neel P. Mistry",
        "Kent Ryan Kleinschmidt",
        "Brady Chrisler",
        "Sathvik Suryadevara",
        "Sri Sai Dinesh Jaliparthi",
        "Noah Michael Prudlo",
        "Mark David Marino",
        "Jeremy Palacio",
        "Rithvik Akula",
        "Hong-Yu Zhou",
        "Ibrahim Ethem Hamamci",
        "Scott J. Adams",
        "Hassan Rayhan AlOmaish",
        "Pranav Rajpurkar"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present ReXGroundingCT, the first publicly available dataset to link\nfree-text radiology findings with pixel-level segmentations in 3D chest CT\nscans that is manually annotated. While prior datasets have relied on\nstructured labels or predefined categories, ReXGroundingCT captures the full\nexpressiveness of clinical language represented in free text and grounds it to\nspatially localized 3D segmentation annotations in volumetric imaging. This\naddresses a critical gap in medical AI: the ability to connect complex,\ndescriptive text, such as \"3 mm nodule in the left lower lobe\", to its precise\nanatomical location in three-dimensional space, a capability essential for\ngrounded radiology report generation systems. The dataset comprises 3,142\nnon-contrast chest CT scans paired with standardized radiology reports from the\nCT-RATE dataset. Using a systematic three-stage pipeline, GPT-4 was used to\nextract positive lung and pleural findings, which were then manually segmented\nby expert annotators. A total of 8,028 findings across 16,301 entities were\nannotated, with quality control performed by board-certified radiologists.\nApproximately 79% of findings are focal abnormalities, while 21% are non-focal.\nThe training set includes up to three representative segmentations per finding,\nwhile the validation and test sets contain exhaustive labels for each finding\nentity. ReXGroundingCT establishes a new benchmark for developing and\nevaluating sentence-level grounding and free-text medical segmentation models\nin chest CT. The dataset can be accessed at\nhttps://huggingface.co/datasets/rajpurkarlab/ReXGroundingCT.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22030v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22030v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.306,
      "datasets_score": 0.417,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new dataset, ReXGroundingCT, specifically designed for machine learning and AI applications in medical imaging. It details the dataset creation process, including curation methodologies like report standardization, finding extraction using GPT-4, and manual segmentation by experts. Additionally, it establishes benchmarks for evaluating models, directly aligning with research on creating, analyzing, and benchmarking datasets for AI.",
      "llm_score_status": "completed",
      "summary": "ReXGroundingCT is a pioneering dataset that links free-text radiology findings from chest CT reports to precise 3D spatial segmentations, addressing a key gap in medical AI by enabling the connection between descriptive language and anatomical locations. The dataset was created using a three-stage pipeline involving GPT-4 for extracting findings from 3,142 standardized reports, followed by manual segmentation of 8,028 findings by expert annotators and quality control by radiologists, providing a benchmark for developing grounded radiology report generation models with training sets featuring up to three representative segmentations per finding and exhaustive labels in validation and test sets.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new dataset that manually annotates free-text radiology reports with 3D segmentations, significantly advancing the state-of-the-art in medical AI by addressing the unmet need for linking complex language to precise anatomical locations.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of medical imaging and AI, particularly for developing grounded report generation systems, though its influence may be limited to specialized applications rather than widespread commercial use.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by providing a new, publicly available dataset that advances research in medical AI, making it essential for researchers in computer vision and radiology to be aware of and potentially utilize.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/4d0c1f6310fa9d1aaaa1cedede362fcc10e71113",
      "total_authors": 23,
      "authors_found": 23,
      "highest_h_index": 33,
      "average_h_index": 2.782608695652174,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Mohammed Baharoon",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2253396549"
        },
        {
          "name": "Luyang Luo",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2325715966"
        },
        {
          "name": "Michael Moritz",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372723577"
        },
        {
          "name": "Abhinav Kumar",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2323730973"
        },
        {
          "name": "Sung Eun Kim",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2333508053"
        },
        {
          "name": "Xiaoman Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2317041580"
        },
        {
          "name": "Miao Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374178494"
        },
        {
          "name": "M. H. Alabbad",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/66138862"
        },
        {
          "name": "Maha Sbayel Alhazmi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373723897"
        },
        {
          "name": "Neel P. Mistry",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2311619193"
        },
        {
          "name": "K. Kleinschmidt",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373331745"
        },
        {
          "name": "Brady Chrisler",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2352861275"
        },
        {
          "name": "Sathvik Suryadevara",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2165133960"
        },
        {
          "name": "Sri Sai Dinesh Jaliparthi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373722710"
        },
        {
          "name": "Noah Michael Prudlo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373722508"
        },
        {
          "name": "Mark David Marino",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2335207011"
        },
        {
          "name": "Jeremy Palacio",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373722188"
        },
        {
          "name": "Rithvik Akula",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373721972"
        },
        {
          "name": "Hong-Yu Zhou",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2301261208"
        },
        {
          "name": "Ibrahim Ethem Hamamci",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2111658673"
        },
        {
          "name": "Scott J. Adams",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2366588669"
        },
        {
          "name": "Hassan Alomaish",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/151019212"
        },
        {
          "name": "P. Rajpurkar",
          "h_index": 33,
          "profile_url": "https://www.semanticscholar.org/author/1453104568"
        }
      ]
    },
    {
      "id": "2507.22034",
      "title": "UserBench: An Interactive Gym Environment for User-Centric Agents",
      "authors": [
        "Cheng Qian",
        "Zuxin Liu",
        "Akshara Prabhakar",
        "Zhiwei Liu",
        "Jianguo Zhang",
        "Haolin Chen",
        "Heng Ji",
        "Weiran Yao",
        "Shelby Heinecke",
        "Silvio Savarese",
        "Caiming Xiong",
        "Huan Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large Language Models (LLMs)-based agents have made impressive progress in\nreasoning and tool use, enabling them to solve complex tasks. However, their\nability to proactively collaborate with users, especially when goals are vague,\nevolving, or indirectly expressed, remains underexplored. To address this gap,\nwe introduce UserBench, a user-centric benchmark designed to evaluate agents in\nmulti-turn, preference-driven interactions. UserBench features simulated users\nwho start with underspecified goals and reveal preferences incrementally,\nrequiring agents to proactively clarify intent and make grounded decisions with\ntools. Our evaluation of leading open- and closed-source LLMs reveals a\nsignificant disconnect between task completion and user alignment. For\ninstance, models provide answers that fully align with all user intents only\n20% of the time on average, and even the most advanced models uncover fewer\nthan 30% of all user preferences through active interaction. These results\nhighlight the challenges of building agents that are not just capable task\nexecutors, but true collaborative partners. UserBench offers an interactive\nenvironment to measure and advance this critical capability.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22034v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22034v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.463,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.337,
      "datasets_score": 0.411,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on introducing a benchmark environment for evaluating LLM agents in user interactions, without any mention of training models using human feedback, reward models, or reinforcement learning techniques. It is solely about assessment, not alignment via RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement processes, or treating reasoning paths as entities for correction. It discusses LLM agents in interactive environments but lacks any components related to diffusion-based multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and introduction of UserBench, which includes over 4K+ curated scenarios as a dataset for benchmarking AI agents in user-centric interactions. It details dataset curation methodologies, evaluation through benchmarks, and analysis of model performance, directly aligning with dataset-focused research.",
      "llm_score_status": "completed",
      "summary": "UserBench is an interactive benchmark environment built on the Gymnasium framework to evaluate large language model-based agents in user-centric, multi-turn interactions, where simulated users present underspecified, incrementally evolving, and indirectly expressed preferences. The methodology involves creating over 4,000 scenarios to test agents' abilities to proactively clarify user intents, use tools for grounded decisions, and align with user needs; key findings from benchmarking leading LLMs show that they achieve only 20% full alignment with user intents and uncover less than 30% of preferences, highlighting significant gaps in collaborative capabilities.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel benchmark environment, UserBench, that addresses an underexplored aspect of AI agents by focusing on proactive user collaboration with traits like underspecification, incrementality, and indirectness, significantly advancing evaluation methodologies beyond existing task-oriented assessments.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future AI research and development by emphasizing user alignment in agents, likely leading to improvements in LLMs for real-world applications and being widely cited in fields like AI and machine learning.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a valuable contribution by introducing a new benchmark that reveals critical limitations in current LLMs, making it essential for researchers in AI agents and user interaction to understand and build upon.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/bf3fbc4e0eb57ca4450b41b425a8f9c66ad41157",
      "total_authors": 12,
      "authors_found": 12,
      "highest_h_index": 19,
      "average_h_index": 8.75,
      "notable_authors_count": 8,
      "author_h_indexes": [
        {
          "name": "Cheng Qian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373727425"
        },
        {
          "name": "Zuxin Liu",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2286595352"
        },
        {
          "name": "Akshara Prabhakar",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/116606529"
        },
        {
          "name": "Zhiwei Liu",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2223887365"
        },
        {
          "name": "Jianguo Zhang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2312345256"
        },
        {
          "name": "Haolin Chen",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2319965998"
        },
        {
          "name": "Heng Ji",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375307513"
        },
        {
          "name": "Weiran Yao",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2275161304"
        },
        {
          "name": "Shelby Heinecke",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/71926704"
        },
        {
          "name": "Silvio Savarese",
          "h_index": 19,
          "profile_url": "https://www.semanticscholar.org/author/2238207181"
        },
        {
          "name": "Caiming Xiong",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/2256976968"
        },
        {
          "name": "Huan Wang",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2258793468"
        }
      ]
    },
    {
      "id": "2507.22037",
      "title": "Secure Tug-of-War (SecTOW): Iterative Defense-Attack Training with\n  Reinforcement Learning for Multimodal Model Security",
      "authors": [
        "Muzhi Dai",
        "Shixuan Liu",
        "Zhiyuan Zhao",
        "Junyu Gao",
        "Hao Sun",
        "Xuelong Li"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rapid advancement of multimodal large language models (MLLMs) has led to\nbreakthroughs in various applications, yet their security remains a critical\nchallenge. One pressing issue involves unsafe image-query pairs--jailbreak\ninputs specifically designed to bypass security constraints and elicit\nunintended responses from MLLMs. Compared to general multimodal data, such\nunsafe inputs are relatively sparse, which limits the diversity and richness of\ntraining samples available for developing robust defense models. Meanwhile,\nexisting guardrail-type methods rely on external modules to enforce security\nconstraints but fail to address intrinsic vulnerabilities within MLLMs.\nTraditional supervised fine-tuning (SFT), on the other hand, often over-refuses\nharmless inputs, compromising general performance. Given these challenges, we\npropose Secure Tug-of-War (SecTOW), an innovative iterative defense-attack\ntraining method to enhance the security of MLLMs. SecTOW consists of two\nmodules: a defender and an auxiliary attacker, both trained iteratively using\nreinforcement learning (GRPO). During the iterative process, the attacker\nidentifies security vulnerabilities in the defense model and expands jailbreak\ndata. The expanded data are then used to train the defender, enabling it to\naddress identified security vulnerabilities. We also design reward mechanisms\nused for GRPO to simplify the use of response labels, reducing dependence on\ncomplex generative labels and enabling the efficient use of synthetic data.\nAdditionally, a quality monitoring mechanism is used to mitigate the defender's\nover-refusal of harmless inputs and ensure the diversity of the jailbreak data\ngenerated by the attacker. Experimental results on safety-specific and general\nbenchmarks demonstrate that SecTOW significantly improves security while\npreserving general performance.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22037v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22037v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.446,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.393,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses Reinforcement Learning (RL) with GRPO and designed reward mechanisms based on synthetic data and evaluation metrics, but it does not involve human feedback, human-ranked data, or a separate reward model trained on human preferences. Thus, it does not align with RLHF.",
      "weak_supervision_justification": "The paper employs synthetic data generation for training, such as jailbreak samples created by the attacker module, which reduces reliance on hand-labeled data and aligns with weak supervision's use of programmatically generated labels. However, the primary focus is on RL-based iterative training rather than weak supervision as the core method.",
      "diffusion_reasoning_justification": "The paper describes an iterative RL-based training process for security enhancement, but it does not involve diffusion models, iterative refinement for logical tasks, or treating Chain-of-Thought as a single entity for multi-step correction. There is no component related to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper, titled \"Secure Tug-of-War (SecTOW)\", addresses the security vulnerabilities of multimodal large language models (MLLMs) against jailbreak attacks by proposing an innovative iterative training framework. It employs reinforcement learning (specifically GRPO) with a defender model and an auxiliary attacker that alternately optimize: the attacker generates jailbreak data to expose vulnerabilities, which is then used to train the defender, incorporating reward mechanisms and quality monitoring to enhance security while preserving general performance, as validated through experiments on safety-specific and general benchmarks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining reinforcement learning with adversarial training in an iterative defense-attack framework, offering a clever adaptation of existing techniques to address MLLM security challenges more effectively than traditional methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in AI security subfields by providing a practical method for enhancing MLLM robustness, potentially leading to citations and further developments in defending against jailbreak attacks.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to MLLM security with innovative RL-based training and empirical validation, making it essential for researchers focused on AI safety to be aware of and consider for their work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/bf05e759011b54b38a142c844ef230efb5cc0d89",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 7,
      "average_h_index": 2.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Muzhi Dai",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2360628692"
        },
        {
          "name": "Shixuan Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2349818978"
        },
        {
          "name": "Zhiyuan Zhao",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2303887466"
        },
        {
          "name": "Junyu Gao",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2275029277"
        },
        {
          "name": "Hao Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373972441"
        },
        {
          "name": "Xuelong Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372356770"
        }
      ]
    },
    {
      "id": "2507.22039",
      "title": "Supervised Quantum Image Processing",
      "authors": [
        "Marco Parigi",
        "Mehran Khosrojerdi",
        "Filippo Caruso",
        "Leonardo Banchi"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In the era of big data and artificial intelligence, the increasing volume of\ndata and the demand to solve more and more complex computational challenges are\ntwo driving forces for improving the efficiency of data storage, processing and\nanalysis. Quantum image processing (QIP) is an interdisciplinary field between\nquantum information science and image processing, which has the potential to\nalleviate some of these challenges by leveraging the power of quantum\ncomputing. In this work, we compare and examine the compression properties of\nfour different Quantum Image Representations (QImRs): namely, Tensor Network\nRepresentation (TNR), Flexible Representation of Quantum Image (FRQI), Novel\nEnhanced Quantum Representation NEQR, and Quantum Probability Image Encoding\n(QPIE). Our simulations show that FRQI performs a higher compression of image\ninformation than TNR, NEQR, and QPIE. Furthermore, we investigate the trade-off\nbetween accuracy and memory in binary classification problems, evaluating the\nperformance of quantum kernels based on QImRs compared to the classical linear\nkernel. Our results indicate that quantum kernels provide comparable\nclassification average accuracy but require exponentially fewer resources for\nimage storage.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22039v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22039v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.298,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.35,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22041",
      "title": "Shallow Deep Learning Can Still Excel in Fine-Grained Few-Shot Learning",
      "authors": [
        "Chaofei Qi",
        "Chao Ye",
        "Zhitai Liu",
        "Weiyang Lin",
        "Jianbin Qiu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep learning has witnessed the extensive utilization across a wide spectrum\nof domains, including fine-grained few-shot learning (FGFSL) which heavily\ndepends on deep backbones. Nonetheless, shallower deep backbones such as\nConvNet-4, are not commonly preferred because they're prone to extract a larger\nquantity of non-abstract visual attributes. In this paper, we initially\nre-evaluate the relationship between network depth and the ability to fully\nencode few-shot instances, and delve into whether shallow deep architecture\ncould effectuate comparable or superior performance to mainstream deep\nbackbone. Fueled by the inspiration from vanilla ConvNet-4, we introduce a\nlocation-aware constellation network (LCN-4), equipped with a cutting-edge\nlocation-aware feature clustering module. This module can proficiently encoder\nand integrate spatial feature fusion, feature clustering, and recessive feature\nlocation, thereby significantly minimizing the overall loss. Specifically, we\ninnovatively put forward a general grid position encoding compensation to\neffectively address the issue of positional information missing during the\nfeature extraction process of specific ordinary convolutions. Additionally, we\nfurther propose a general frequency domain location embedding technique to\noffset for the location loss in clustering features. We have carried out\nvalidation procedures on three representative fine-grained few-shot benchmarks.\nRelevant experiments have established that LCN-4 notably outperforms the\nConvNet-4 based State-of-the-Arts and achieves performance that is on par with\nor superior to most ResNet12-based methods, confirming the correctness of our\nconjecture.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22041v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22041v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.397,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is the development of a shallow deep learning architecture (LCN-4) for fine-grained few-shot learning, focusing on feature extraction, clustering, and position encoding to improve performance with limited data. It does not address weak supervision, such as programmatically generating noisy labels or training with imprecise sources, as its emphasis is on architectural enhancements rather than label generation or handling.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22047",
      "title": "The Interspeech 2025 Speech Accessibility Project Challenge",
      "authors": [
        "Xiuwen Zheng",
        "Bornali Phukon",
        "Jonghwan Na",
        "Ed Cutrell",
        "Kyu Han",
        "Mark Hasegawa-Johnson",
        "Pan-Pan Jiang",
        "Aadhrik Kuila",
        "Colin Lea",
        "Bob MacDonald",
        "Gautam Mantena",
        "Venkatesh Ravichandran",
        "Leda Sari",
        "Katrin Tomanek",
        "Chang D. Yoo",
        "Chris Zwilling"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While the last decade has witnessed significant advancements in Automatic\nSpeech Recognition (ASR) systems, performance of these systems for individuals\nwith speech disabilities remains inadequate, partly due to limited public\ntraining data. To bridge this gap, the 2025 Interspeech Speech Accessibility\nProject (SAP) Challenge was launched, utilizing over 400 hours of SAP data\ncollected and transcribed from more than 500 individuals with diverse speech\ndisabilities. Hosted on EvalAI and leveraging the remote evaluation pipeline,\nthe SAP Challenge evaluates submissions based on Word Error Rate and Semantic\nScore. Consequently, 12 out of 22 valid teams outperformed the whisper-large-v2\nbaseline in terms of WER, while 17 teams surpassed the baseline on SemScore.\nNotably, the top team achieved the lowest WER of 8.11\\%, and the highest\nSemScore of 88.44\\% at the same time, setting new benchmarks for future ASR\nsystems in recognizing impaired speech.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22047v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22047v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.295,
      "distributed_training_score": 0.34,
      "datasets_score": 0.381,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22052",
      "title": "Ov3R: Open-Vocabulary Semantic 3D Reconstruction from RGB Videos",
      "authors": [
        "Ziren Gong",
        "Xiaohan Li",
        "Fabio Tosi",
        "Jiawei Han",
        "Stefano Mattoccia",
        "Jianfei Cai",
        "Matteo Poggi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present Ov3R, a novel framework for open-vocabulary semantic 3D\nreconstruction from RGB video streams, designed to advance Spatial AI. The\nsystem features two key components: CLIP3R, a CLIP-informed 3D reconstruction\nmodule that predicts dense point maps from overlapping clips while embedding\nobject-level semantics; and 2D-3D OVS, a 2D-3D open-vocabulary semantic module\nthat lifts 2D features into 3D by learning fused descriptors integrating\nspatial, geometric, and semantic cues. Unlike prior methods, Ov3R incorporates\nCLIP semantics directly into the reconstruction process, enabling globally\nconsistent geometry and fine-grained semantic alignment. Our framework achieves\nstate-of-the-art performance in both dense 3D reconstruction and\nopen-vocabulary 3D segmentation, marking a step forward toward real-time,\nsemantics-aware Spatial AI.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22052v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22052v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.357,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for open-vocabulary semantic 3D reconstruction from RGB videos, focusing on components like CLIP3R and 2D-3D OVS for geometry and semantics. It does not involve diffusion models, iterative refinement for logical tasks, or any multi-step reasoning processes, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22053",
      "title": "Foundation Models for Demand Forecasting via Dual-Strategy Ensembling",
      "authors": [
        "Wei Yang",
        "Defu Cao",
        "Yan Liu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Accurate demand forecasting is critical for supply chain optimization, yet\nremains difficult in practice due to hierarchical complexity, domain shifts,\nand evolving external factors. While recent foundation models offer strong\npotential for time series forecasting, they often suffer from architectural\nrigidity and limited robustness under distributional change. In this paper, we\npropose a unified ensemble framework that enhances the performance of\nfoundation models for sales forecasting in real-world supply chains. Our method\ncombines two complementary strategies: (1) Hierarchical Ensemble (HE), which\npartitions training and inference by semantic levels (e.g., store, category,\ndepartment) to capture localized patterns; and (2) Architectural Ensemble (AE),\nwhich integrates predictions from diverse model backbones to mitigate bias and\nimprove stability. We conduct extensive experiments on the M5 benchmark and\nthree external sales datasets, covering both in-domain and zero-shot\nforecasting. Results show that our approach consistently outperforms strong\nbaselines, improves accuracy across hierarchical levels, and provides a simple\nyet effective mechanism for boosting generalization in complex forecasting\nenvironments.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22053v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22053v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.361,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22057",
      "title": "MetaLab: Few-Shot Game Changer for Image Recognition",
      "authors": [
        "Chaofei Qi",
        "Zhitai Liu",
        "Jianbin Qiu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Difficult few-shot image recognition has significant application prospects,\nyet remaining the substantial technical gaps with the conventional large-scale\nimage recognition. In this paper, we have proposed an efficient original method\nfor few-shot image recognition, called CIELab-Guided Coherent Meta-Learning\n(MetaLab). Structurally, our MetaLab comprises two collaborative neural\nnetworks: LabNet, which can perform domain transformation for the CIELab color\nspace and extract rich grouped features, and coherent LabGNN, which can\nfacilitate mutual learning between lightness graph and color graph. For\nsufficient certification, we have implemented extensive comparative studies on\nfour coarse-grained benchmarks, four fine-grained benchmarks, and four\ncross-domain few-shot benchmarks. Specifically, our method can achieve high\naccuracy, robust performance, and effective generalization capability with\none-shot sample per class. Overall, all experiments have demonstrated that our\nMetaLab can approach 99\\% $\\uparrow\\downarrow$ accuracy, reaching the human\nrecognition ceiling with little visual deviation.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22057v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22057v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.344,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22058",
      "title": "X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image\n  Generative Models Great Again",
      "authors": [
        "Zigang Geng",
        "Yibing Wang",
        "Yeyao Ma",
        "Chen Li",
        "Yongming Rao",
        "Shuyang Gu",
        "Zhao Zhong",
        "Qinglin Lu",
        "Han Hu",
        "Xiaosong Zhang",
        "Linus",
        "Di Wang",
        "Jie Jiang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Numerous efforts have been made to extend the ``next token prediction''\nparadigm to visual contents, aiming to create a unified approach for both image\ngeneration and understanding. Nevertheless, attempts to generate images through\nautoregressive modeling with discrete tokens have been plagued by issues such\nas low visual fidelity, distorted outputs, and failure to adhere to complex\ninstructions when rendering intricate details. These shortcomings are likely\nattributed to cumulative errors during autoregressive inference or information\nloss incurred during the discretization process. Probably due to this\nchallenge, recent research has increasingly shifted toward jointly training\nimage generation with diffusion objectives and language generation with\nautoregressive objectives, moving away from unified modeling approaches. In\nthis work, we demonstrate that reinforcement learning can effectively mitigate\nartifacts and largely enhance the generation quality of a discrete\nautoregressive modeling method, thereby enabling seamless integration of image\nand language generation. Our framework comprises a semantic image tokenizer, a\nunified autoregressive model for both language and images, and an offline\ndiffusion decoder for image generation, termed X-Omni. X-Omni achieves\nstate-of-the-art performance in image generation tasks using a 7B language\nmodel, producing images with high aesthetic quality while exhibiting strong\ncapabilities in following instructions and rendering long texts.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22058v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22058v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.472,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.539,
      "distributed_training_score": 0.383,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with reward models to refine autoregressive image generation, aligning outputs with preferences like aesthetic quality and instruction following, which may involve human-like feedback. However, it does not explicitly mention training the reward model on human-ranked data, a key requirement for RLHF, making it only moderately relevant rather than highly so.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper mentions an offline diffusion decoder for image generation as part of its framework, but it does not adapt diffusion for multi-step logical reasoning or Chain-of-Thought processes. The focus is on improving autoregressive modeling via reinforcement learning, with no components for iterative reasoning tasks, so it does not align with diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces X-Omni, a framework that leverages reinforcement learning to enhance discrete autoregressive image generative models, addressing issues like cumulative errors and low fidelity in image generation. By integrating a semantic image tokenizer, a unified autoregressive model for both language and images, and an offline diffusion decoder, the methodology achieves state-of-the-art performance with a 7B language model, resulting in high-quality images that better follow instructions, render long texts, and seamlessly combine image generation and understanding tasks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying reinforcement learning to mitigate errors in existing discrete autoregressive models for image generation, combining established techniques in a clever way to enhance performance. While not introducing an entirely new paradigm, it advances the field by refining autoregressive methods for better integration with language modeling.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of computer vision and generative AI, as it demonstrates practical enhancements to unified image and language modeling. However, its influence may be limited to specific applications rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to improving image generation techniques, offering insights that are relevant for researchers in multimodal AI. It provides innovative refinements that advance the state-of-the-art, making it important for those tracking developments in generative models.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/594d82ab5ed8ac54c4419e5e10152ac54dcf2f60",
      "total_authors": 13,
      "authors_found": 13,
      "highest_h_index": 7,
      "average_h_index": 1.3076923076923077,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Zigang Geng",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/1780032098"
        },
        {
          "name": "Yibing Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373743525"
        },
        {
          "name": "Yeyao Ma",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2373996819"
        },
        {
          "name": "Chen Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2374045820"
        },
        {
          "name": "Yongming Rao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2275170369"
        },
        {
          "name": "Shuyang Gu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2351410258"
        },
        {
          "name": "Zhao Zhong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375371352"
        },
        {
          "name": "Qinglin Lu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374403780"
        },
        {
          "name": "Han Hu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2351597805"
        },
        {
          "name": "Xiaosong Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2373747040"
        },
        {
          "name": "Linus",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2250529506"
        },
        {
          "name": "Di Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2329467770"
        },
        {
          "name": "Jie Jiang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373760376"
        }
      ]
    },
    {
      "id": "2507.22059",
      "title": "StepAL: Step-aware Active Learning for Cataract Surgical Videos",
      "authors": [
        "Nisarg A. Shah",
        "Bardia Safaei",
        "Shameema Sikder",
        "S. Swaroop Vedula",
        "Vishal M. Patel"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Active learning (AL) can reduce annotation costs in surgical video analysis\nwhile maintaining model performance. However, traditional AL methods, developed\nfor images or short video clips, are suboptimal for surgical step recognition\ndue to inter-step dependencies within long, untrimmed surgical videos. These\nmethods typically select individual frames or clips for labeling, which is\nineffective for surgical videos where annotators require the context of the\nentire video for annotation. To address this, we propose StepAL, an active\nlearning framework designed for full video selection in surgical step\nrecognition. StepAL integrates a step-aware feature representation, which\nleverages pseudo-labels to capture the distribution of predicted steps within\neach video, with an entropy-weighted clustering strategy. This combination\nprioritizes videos that are both uncertain and exhibit diverse step\ncompositions for annotation. Experiments on two cataract surgery datasets\n(Cataract-1k and Cataract-101) demonstrate that StepAL consistently outperforms\nexisting active learning approaches, achieving higher accuracy in step\nrecognition with fewer labeled videos. StepAL offers an effective approach for\nefficient surgical video analysis, reducing the annotation burden in developing\ncomputer-assisted surgical systems.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22059v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22059v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.297,
      "distributed_training_score": 0.304,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22061",
      "title": "MOVE: Motion-Guided Few-Shot Video Object Segmentation",
      "authors": [
        "Kaining Ying",
        "Hengrui Hu",
        "Henghui Ding"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This work addresses motion-guided few-shot video object segmentation (FSVOS),\nwhich aims to segment dynamic objects in videos based on a few annotated\nexamples with the same motion patterns. Existing FSVOS datasets and methods\ntypically focus on object categories, which are static attributes that ignore\nthe rich temporal dynamics in videos, limiting their application in scenarios\nrequiring motion understanding. To fill this gap, we introduce MOVE, a\nlarge-scale dataset specifically designed for motion-guided FSVOS. Based on\nMOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different\nrelated tasks across 2 experimental settings. Our results reveal that current\nmethods struggle to address motion-guided FSVOS, prompting us to analyze the\nassociated challenges and propose a baseline method, Decoupled Motion\nAppearance Network (DMA). Experiments demonstrate that our approach achieves\nsuperior performance in few shot motion understanding, establishing a solid\nfoundation for future research in this direction.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22061v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22061v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.283,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.299,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22062",
      "title": "Meta CLIP 2: A Worldwide Scaling Recipe",
      "authors": [
        "Yung-Sung Chuang",
        "Yang Li",
        "Dong Wang",
        "Ching-Feng Yeh",
        "Kehan Lyu",
        "Ramya Raghavendra",
        "James Glass",
        "Lifei Huang",
        "Jason Weston",
        "Luke Zettlemoyer",
        "Xinlei Chen",
        "Zhuang Liu",
        "Saining Xie",
        "Wen-tau Yih",
        "Shang-Wen Li",
        "Hu Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,\nsupporting from zero-shot classification, retrieval to encoders for multimodal\nlarge language models (MLLMs). Although CLIP is successfully trained on\nbillion-scale image-text pairs from the English world, scaling CLIP's training\nfurther to learning from the worldwide web data is still challenging: (1) no\ncuration method is available to handle data points from non-English world; (2)\nthe English performance from existing multilingual CLIP is worse than its\nEnglish-only counterpart, i.e., \"curse of multilinguality\" that is common in\nLLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch\non worldwide web-scale image-text pairs. To generalize our findings, we conduct\nrigorous ablations with minimal changes that are necessary to address the above\nchallenges and present a recipe enabling mutual benefits from English and\nnon-English world data. In zero-shot ImageNet classification, Meta CLIP 2\nViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,\nand surprisingly sets new state-of-the-art without system-level confounding\nfactors (e.g., translation, bespoke architecture changes) on multilingual\nbenchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with\n64.3% on image-to-text retrieval.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22062v3",
      "pdf_url": "http://arxiv.org/pdf/2507.22062v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.42,
      "datasets_score": 0.393,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on scaling CLIP training to worldwide data through innovations in metadata, curation algorithms, and training frameworks, such as handling increased data sizes and model capacities (e.g., ViT-H/14). While it implies the need for large-scale training that might involve distributed systems to manage billion-scale datasets, it does not explicitly discuss or contribute to distributed training techniques, parallel computing strategies, or multi-node architectures. Thus, distributed training is only indirectly related as a potential underlying requirement, not a core element of the paper's contributions.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22089",
      "title": "Principled Curriculum Learning using Parameter Continuation Methods",
      "authors": [
        "Harsh Nilesh Pathak",
        "Randy Paffenroth"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this work, we propose a parameter continuation method for the optimization\nof neural networks. There is a close connection between parameter continuation,\nhomotopies, and curriculum learning. The methods we propose here are\ntheoretically justified and practically effective for several problems in deep\nneural networks. In particular, we demonstrate better generalization\nperformance than state-of-the-art optimization techniques such as ADAM for\nsupervised and unsupervised learning tasks.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22089v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22089v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.385,
      "datasets_score": 0.27,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22090",
      "title": "Hybrid activation functions for deep neural networks: S3 and S4 -- a\n  novel approach to gradient flow optimization",
      "authors": [
        "Sergii Kavun"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.NE (Neural and Evolutionary Computing)",
        "cs.NI (Networking and Internet Architecture)"
      ],
      "abstract": "Activation functions are critical components in deep neural networks,\ndirectly influencing gradient flow, training stability, and model performance.\nTraditional functions like ReLU suffer from dead neuron problems, while sigmoid\nand tanh exhibit vanishing gradient issues. We introduce two novel hybrid\nactivation functions: S3 (Sigmoid-Softsign) and its improved version S4\n(smoothed S3). S3 combines sigmoid for negative inputs with softsign for\npositive inputs, while S4 employs a smooth transition mechanism controlled by a\nsteepness parameter k. We conducted comprehensive experiments across binary\nclassification, multi-class classification, and regression tasks using three\ndifferent neural network architectures. S4 demonstrated superior performance\ncompared to nine baseline activation functions, achieving 97.4% accuracy on\nMNIST, 96.0% on Iris classification, and 18.7 MSE on Boston Housing regression.\nThe function exhibited faster convergence (-19 for ReLU) and maintained stable\ngradient flow across network depths. Comparative analysis revealed S4's\ngradient range of [0.24, 0.59] compared to ReLU's 18% dead neurons in deep\nnetworks. The S4 activation function addresses key limitations of existing\nfunctions through its hybrid design and smooth transition mechanism. The\ntunable parameter k allows adaptation to different tasks and network depths,\nmaking S4 a versatile choice for deep learning applications. These findings\nsuggest that hybrid activation functions represent a promising direction for\nimproving neural network training dynamics.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22090v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22090v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.332,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22092",
      "title": "Pathology Foundation Models are Scanner Sensitive: Benchmark and\n  Mitigation with Contrastive ScanGen Loss",
      "authors": [
        "Gianluca Carloni",
        "Biagio Brattoli",
        "Seongho Keum",
        "Jongchan Park",
        "Taebum Lee",
        "Chang Ho Ahn",
        "Sergio Pereira"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Computational pathology (CPath) has shown great potential in mining\nactionable insights from Whole Slide Images (WSIs). Deep Learning (DL) has been\nat the center of modern CPath, and while it delivers unprecedented performance,\nit is also known that DL may be affected by irrelevant details, such as those\nintroduced during scanning by different commercially available scanners. This\nmay lead to scanner bias, where the model outputs for the same tissue acquired\nby different scanners may vary. In turn, it hinders the trust of clinicians in\nCPath-based tools and their deployment in real-world clinical practices. Recent\npathology Foundation Models (FMs) promise to provide better domain\ngeneralization capabilities. In this paper, we benchmark FMs using a\nmulti-scanner dataset and show that FMs still suffer from scanner bias.\nFollowing this observation, we propose ScanGen, a contrastive loss function\napplied during task-specific fine-tuning that mitigates scanner bias, thereby\nenhancing the models' robustness to scanner variations. Our approach is applied\nto the Multiple Instance Learning task of Epidermal Growth Factor Receptor\n(EGFR) mutation prediction from H\\&E-stained WSIs in lung cancer. We observe\nthat ScanGen notably enhances the ability to generalize across scanners, while\nretaining or improving the performance of EGFR mutation prediction.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22092v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22092v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.357,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22094",
      "title": "Scaling and Distilling Transformer Models for sEMG",
      "authors": [
        "Nicholas Mehlman",
        "Jean-Christophe Gagnon-Audet",
        "Michael Shvartsman",
        "Kelvin Niu",
        "Alexander H. Miller",
        "Shagun Sodhani"
      ],
      "categories": [
        "eess.AS (Audio and Speech Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Surface electromyography (sEMG) signals offer a promising avenue for\ndeveloping innovative human-computer interfaces by providing insights into\nmuscular activity. However, the limited volume of training data and\ncomputational constraints during deployment have restricted the investigation\nof scaling up the model size for solving sEMG tasks. In this paper, we\ndemonstrate that vanilla transformer models can be effectively scaled up on\nsEMG data and yield improved cross-user performance up to 110M parameters,\nsurpassing the model size regime investigated in other sEMG research (usually\n<10M parameters). We show that >100M-parameter models can be effectively\ndistilled into models 50x smaller with minimal loss of performance (<1.5%\nabsolute). This results in efficient and expressive models suitable for complex\nreal-time sEMG tasks in real-world environments.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22094v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22094v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.422,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper primarily focuses on scaling transformer models for sEMG tasks and applying knowledge distillation for model compression, without any discussion of distributed training, parallel computing, or multi-node machine learning. It does not address algorithms or systems for partitioning data, model architecture, or computation across multiple processors or nodes to accelerate training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22099",
      "title": "Runtime Failure Hunting for Physics Engine Based Software Systems: How\n  Far Can We Go?",
      "authors": [
        "Shuqing Li",
        "Qiang Chen",
        "Xiaoxue Ren",
        "Michael R. Lyu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Physics Engines (PEs) are fundamental software frameworks that simulate\nphysical interactions in applications ranging from entertainment to\nsafety-critical systems. Despite their importance, PEs suffer from physics\nfailures, deviations from expected physical behaviors that can compromise\nsoftware reliability, degrade user experience, and potentially cause critical\nfailures in autonomous vehicles or medical robotics. Current testing approaches\nfor PE-based software are inadequate, typically requiring white-box access and\nfocusing on crash detection rather than semantically complex physics failures.\nThis paper presents the first large-scale empirical study characterizing\nphysics failures in PE-based software. We investigate three research questions\naddressing the manifestations of physics failures, the effectiveness of\ndetection techniques, and developer perceptions of current detection practices.\nOur contributions include: (1) a taxonomy of physics failure manifestations;\n(2) a comprehensive evaluation of detection methods including deep learning,\nprompt-based techniques, and large multimodal models; and (3) actionable\ninsights from developer experiences for improving detection approaches. To\nsupport future research, we release PhysiXFails, code, and other materials at\nhttps://sites.google.com/view/physics-failure-detection.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22099v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22099v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.351,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22100",
      "title": "Trade-offs in Image Generation: How Do Different Dimensions Interact?",
      "authors": [
        "Sicheng Zhang",
        "Binzhu Xie",
        "Zhonghao Yan",
        "Yuli Zhang",
        "Donghao Zhou",
        "Xiaofei Chen",
        "Shi Qiu",
        "Jiaqi Liu",
        "Guoyang Xie",
        "Zhichao Lu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Model performance in text-to-image (T2I) and image-to-image (I2I) generation\noften depends on multiple aspects, including quality, alignment, diversity, and\nrobustness. However, models' complex trade-offs among these dimensions have\nrarely been explored due to (1) the lack of datasets that allow fine-grained\nquantification of these trade-offs, and (2) the use of a single metric for\nmultiple dimensions. To bridge this gap, we introduce TRIG-Bench (Trade-offs in\nImage Generation), which spans 10 dimensions (Realism, Originality, Aesthetics,\nContent, Relation, Style, Knowledge, Ambiguity, Toxicity, and Bias), contains\n40,200 samples, and covers 132 pairwise dimensional subsets. Furthermore, we\ndevelop TRIGScore, a VLM-as-judge metric that automatically adapts to various\ndimensions. Based on TRIG-Bench and TRIGScore, we evaluate 14 models across T2I\nand I2I tasks. In addition, we propose the Relation Recognition System to\ngenerate the Dimension Trade-off Map (DTM) that visualizes the trade-offs among\nmodel-specific capabilities. Our experiments demonstrate that DTM consistently\nprovides a comprehensive understanding of the trade-offs between dimensions for\neach type of generative model. Notably, we show that the model's\ndimension-specific weaknesses can be mitigated through fine-tuning on DTM to\nenhance overall performance. Code is available at:\nhttps://github.com/fesvhtr/TRIG",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22100v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22100v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.329,
      "diffusion_reasoning_score": 0.474,
      "distributed_training_score": 0.393,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluating trade-offs in text-to-image and image-to-image generation using benchmarks and metrics, with models that may include diffusion-based systems. However, it does not adapt the diffusion process for multi-step logical reasoning or treat a 'Chain-of-Thought' as an entity for holistic refinement, as the emphasis is on image quality and dimensional interactions rather than reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22101",
      "title": "AI in Agriculture: A Survey of Deep Learning Techniques for Crops,\n  Fisheries and Livestock",
      "authors": [
        "Umair Nawaz",
        "Muhammad Zaigham Zaheer",
        "Fahad Shahbaz Khan",
        "Hisham Cholakkal",
        "Salman Khan",
        "Rao Muhammad Anwer"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Crops, fisheries and livestock form the backbone of global food production,\nessential to feed the ever-growing global population. However, these sectors\nface considerable challenges, including climate variability, resource\nlimitations, and the need for sustainable management. Addressing these issues\nrequires efficient, accurate, and scalable technological solutions,\nhighlighting the importance of artificial intelligence (AI). This survey\npresents a systematic and thorough review of more than 200 research works\ncovering conventional machine learning approaches, advanced deep learning\ntechniques (e.g., vision transformers), and recent vision-language foundation\nmodels (e.g., CLIP) in the agriculture domain, focusing on diverse tasks such\nas crop disease detection, livestock health management, and aquatic species\nmonitoring. We further cover major implementation challenges such as data\nvariability and experimental aspects: datasets, performance evaluation metrics,\nand geographical focus. We finish the survey by discussing potential open\nresearch directions emphasizing the need for multimodal data integration,\nefficient edge-device deployment, and domain-adaptable AI models for diverse\nfarming environments. Rapid growth of evolving developments in this field can\nbe actively tracked on our project page:\nhttps://github.com/umair1221/AI-in-Agriculture",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22101v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22101v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.331,
      "distributed_training_score": 0.398,
      "datasets_score": 0.419,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper is a survey of AI techniques like conventional ML, CNNs, and transformers for agriculture tasks, with no mention of reinforcement learning, human feedback, or reward models for alignment.",
      "weak_supervision_justification": "The paper discusses ML methods and datasets in agriculture but does not address weak supervision techniques, such as programmatically generating labels from noisy sources.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper reviews and analyzes datasets for agriculture AI applications, including their use in various domains and challenges, though it focuses more on surveying AI techniques rather than primarily creating or benchmarking datasets.",
      "llm_score_status": "completed",
      "summary": "This survey paper provides a comprehensive review of over 200 research works on AI applications in agriculture, covering crops, fisheries, and livestock, by examining conventional machine learning, deep learning techniques such as CNNs and vision transformers, and emerging foundation models for tasks like disease detection, health management, and monitoring. It systematically analyzes methodologies, implementation challenges including data variability, datasets, performance metrics, and geographical focuses, while highlighting future directions such as multimodal data integration and efficient AI deployment for sustainable farming.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by comprehensively combining existing AI techniques across crops, fisheries, and livestock, including recent foundation models, in a way that expands on prior surveys but does not introduce entirely new problems or architectures.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of AI in agriculture due to its broad coverage and identification of research gaps, though its influence may be limited to specialists rather than widespread commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, valuable synthesis of AI advancements in agriculture that is essential for researchers in the field to understand current trends and future directions, making it a strong contribution worth engaging with.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e5893605168c5dba023c586883ef61666146741f",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 33,
      "average_h_index": 13.0,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Umair Nawaz",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2323784572"
        },
        {
          "name": "Muhammad Zaigham Zaheer",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2264967417"
        },
        {
          "name": "F. Khan",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2279959188"
        },
        {
          "name": "Hisham Cholakkal",
          "h_index": 29,
          "profile_url": "https://www.semanticscholar.org/author/2951229"
        },
        {
          "name": "Salman H. Khan",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2274068532"
        },
        {
          "name": "R. Anwer",
          "h_index": 33,
          "profile_url": "https://www.semanticscholar.org/author/3288214"
        }
      ]
    },
    {
      "id": "2507.22136",
      "title": "Color as the Impetus: Transforming Few-Shot Learner",
      "authors": [
        "Chaofei Qi",
        "Zhitai Liu",
        "Jianbin Qiu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Humans possess innate meta-learning capabilities, partly attributable to\ntheir exceptional color perception. In this paper, we pioneer an innovative\nviewpoint on few-shot learning by simulating human color perception mechanisms.\nWe propose the ColorSense Learner, a bio-inspired meta-learning framework that\ncapitalizes on inter-channel feature extraction and interactive learning. By\nstrategically emphasizing distinct color information across different channels,\nour approach effectively filters irrelevant features while capturing\ndiscriminative characteristics. Color information represents the most intuitive\nvisual feature, yet conventional meta-learning methods have predominantly\nneglected this aspect, focusing instead on abstract feature differentiation\nacross categories. Our framework bridges the gap via synergistic color-channel\ninteractions, enabling better intra-class commonality extraction and larger\ninter-class differences. Furthermore, we introduce a meta-distiller based on\nknowledge distillation, ColorSense Distiller, which incorporates prior teacher\nknowledge to augment the student network's meta-learning capacity. We've\nconducted comprehensive coarse/fine-grained and cross-domain experiments on\neleven few-shot benchmarks for validation. Numerous experiments reveal that our\nmethods have extremely strong generalization ability, robustness, and\ntransferability, and effortless handle few-shot classification from the\nperspective of color perception.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22136v2",
      "pdf_url": "http://arxiv.org/pdf/2507.22136v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.351,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22149",
      "title": "When Truthful Representations Flip Under Deceptive Instructions?",
      "authors": [
        "Xianxuan Long",
        "Yao Fu",
        "Runchao Li",
        "Mu Sheng",
        "Haotian Yu",
        "Xiaotian Han",
        "Pan Li"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large language models (LLMs) tend to follow maliciously crafted instructions\nto generate deceptive responses, posing safety challenges. How deceptive\ninstructions alter the internal representations of LLM compared to truthful\nones remains poorly understood beyond output analysis. To bridge this gap, we\ninvestigate when and how these representations ``flip'', such as from truthful\nto deceptive, under deceptive versus truthful/neutral instructions. Analyzing\nthe internal representations of Llama-3.1-8B-Instruct and Gemma-2-9B-Instruct\non a factual verification task, we find the model's instructed True/False\noutput is predictable via linear probes across all conditions based on the\ninternal representation. Further, we use Sparse Autoencoders (SAEs) to show\nthat the Deceptive instructions induce significant representational shifts\ncompared to Truthful/Neutral representations (which are similar), concentrated\nin early-to-mid layers and detectable even on complex datasets. We also\nidentify specific SAE features highly sensitive to deceptive instruction and\nuse targeted visualizations to confirm distinct truthful/deceptive\nrepresentational subspaces. % Our analysis pinpoints layer-wise and\nfeature-level correlates of instructed dishonesty, offering insights for LLM\ndetection and control. Our findings expose feature- and layer-level signatures\nof deception, offering new insights for detecting and mitigating instructed\ndishonesty in LLMs.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22149v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22149v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.317,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on analyzing internal representations of LLMs under deceptive versus truthful instructions using techniques like linear probing and Sparse Autoencoders, primarily for factual verification tasks. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22152",
      "title": "Enhancing efficiency in paediatric brain tumour segmentation using a\n  pathologically diverse single-center clinical dataset",
      "authors": [
        "A. Piffer",
        "J. A. Buchner",
        "A. G. Gennari",
        "P. Grehten",
        "S. Sirin",
        "E. Ross",
        "I. Ezhov",
        "M. Rosier",
        "J. C. Peeken",
        "M. Piraud",
        "B. Menze",
        "A. Guerreiro Stücklin",
        "A. Jakab",
        "F. Kofler"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Background Brain tumours are the most common solid malignancies in children,\nencompassing diverse histological, molecular subtypes and imaging features and\noutcomes. Paediatric brain tumours (PBTs), including high- and low-grade\ngliomas (HGG, LGG), medulloblastomas (MB), ependymomas, and rarer forms, pose\ndiagnostic and therapeutic challenges. Deep learning (DL)-based segmentation\noffers promising tools for tumour delineation, yet its performance across\nheterogeneous PBT subtypes and MRI protocols remains uncertain. Methods A\nretrospective single-centre cohort of 174 paediatric patients with HGG, LGG,\nmedulloblastomas (MB), ependymomas, and other rarer subtypes was used. MRI\nsequences included T1, T1 post-contrast (T1-C), T2, and FLAIR. Manual\nannotations were provided for four tumour subregions: whole tumour (WT),\nT2-hyperintensity (T2H), enhancing tumour (ET), and cystic component (CC). A 3D\nnnU-Net model was trained and tested (121/53 split), with segmentation\nperformance assessed using the Dice similarity coefficient (DSC) and compared\nagainst intra- and inter-rater variability. Results The model achieved robust\nperformance for WT and T2H (mean DSC: 0.85), comparable to human annotator\nvariability (mean DSC: 0.86). ET segmentation was moderately accurate (mean\nDSC: 0.75), while CC performance was poor. Segmentation accuracy varied by\ntumour type, MRI sequence combination, and location. Notably, T1, T1-C, and T2\nalone produced results nearly equivalent to the full protocol. Conclusions DL\nis feasible for PBTs, particularly for T2H and WT. Challenges remain for ET and\nCC segmentation, highlighting the need for further refinement. These findings\nsupport the potential for protocol simplification and automation to enhance\nvolumetric assessment and streamline paediatric neuro-oncology workflows.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22152v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22152v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.291,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.351,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22157",
      "title": "Tiny Noise-Robust Voice Activity Detector for Voice Assistants",
      "authors": [
        "Hamed Jafarzadeh Asl",
        "Mahsa Ghazvini Nejad",
        "Amin Edraki",
        "Masoud Asgharian",
        "Vahid Partovi Nia"
      ],
      "categories": [
        "eess.AS (Audio and Speech Processing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Voice Activity Detection (VAD) in the presence of background noise remains a\nchallenging problem in speech processing. Accurate VAD is essential in\nautomatic speech recognition, voice-to-text, conversational agents, etc, where\nnoise can severely degrade the performance. A modern application includes the\nvoice assistant, specially mounted on Artificial Intelligence of Things (AIoT)\ndevices such as cell phones, smart glasses, earbuds, etc, where the voice\nsignal includes background noise. Therefore, VAD modules must remain\nlight-weight due to their practical on-device limitation. The existing models\noften struggle with low signal-to-noise ratios across diverse acoustic\nenvironments. A simple VAD often detects human voice in a clean environment,\nbut struggles to detect the human voice in noisy conditions. We propose a\nnoise-robust VAD that comprises a light-weight VAD, with data pre-processing\nand post-processing added modules to handle the background noise. This approach\nsignificantly enhances the VAD accuracy in noisy environments and requires\nneither a larger model, nor fine-tuning. Experimental results demonstrate that\nour approach achieves a notable improvement compared to baselines, particularly\nin environments with high background noise interference. This modified VAD\nadditionally improving clean speech detection.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22157v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22157v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.288,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.304,
      "datasets_score": 0.287,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22159",
      "title": "IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian",
      "authors": [
        "Vanessa Rebecca Wiyono",
        "David Anugraha",
        "Ayu Purwarianti",
        "Genta Indra Winata"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Over 200 million people speak Indonesian, yet the language remains\nsignificantly underrepresented in preference-based research for large language\nmodels (LLMs). Most existing multilingual datasets are derived from English\ntranslations, often resulting in content that lacks cultural and linguistic\nauthenticity. To address this gap, we introduce IndoPref, the first fully\nhuman-authored and multi-domain Indonesian preference dataset specifically\ndesigned to evaluate the naturalness and quality of LLM-generated text. All\nannotations are natively written in Indonesian and evaluated using\nKrippendorff's alpha, demonstrating strong inter-annotator agreement.\nAdditionally, we benchmark the dataset across multiple LLMs and assess the\noutput quality of each model.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22159v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22159v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.469,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.311,
      "datasets_score": 0.443,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper introduces a human-authored pairwise preference dataset for Indonesian, which is directly used for aligning language models with human preferences. This dataset supports RLHF by providing human-ranked data for training reward models and fine-tuning LLMs, as it focuses on evaluating and improving model outputs based on naturalness and quality.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation, curation, and benchmarking of a new dataset (IndoPref) for AI applications, including human annotation processes, inter-annotator agreement analysis, and model evaluations. This aligns with research on dataset introduction, methodologies, and benchmarking for machine learning.",
      "llm_score_status": "completed",
      "summary": "The paper introduces IndoPref, the first fully human-authored, multi-domain pairwise preference dataset specifically for Indonesian, aimed at addressing the underrepresentation of the language in LLM preference research. It employs native Indonesian annotations with Krippendorff's alpha to ensure strong inter-annotator agreement and benchmarks multiple LLMs to evaluate their alignment with human preferences, revealing that larger models perform better.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new dataset that is the first of its kind for Indonesian, addressing a significant gap in preference modeling for underrepresented languages and advancing the state-of-the-art in culturally authentic NLP resources.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of multilingual NLP, particularly for Indonesian language models, as it provides a valuable benchmark for improving cultural relevance in LLMs. However, its influence may remain confined to specific research areas rather than broadly affecting the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by introducing a much-needed resource for Indonesian NLP, making it essential for researchers focused on multilingual or underrepresented languages to stay informed. While not universally groundbreaking, its targeted advancements warrant attention from relevant specialists.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3f9531505ff36d7ce363318d7312296f62bdb090",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 39,
      "average_h_index": 12.5,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Vanessa Rebecca Wiyono",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374047185"
        },
        {
          "name": "David Anugraha",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2282529650"
        },
        {
          "name": "Ayu Purwarianti",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2257345523"
        },
        {
          "name": "Genta Indra Winata",
          "h_index": 39,
          "profile_url": "https://www.semanticscholar.org/author/9162688"
        }
      ]
    },
    {
      "id": "2507.22160",
      "title": "Strategic Deflection: Defending LLMs from Logit Manipulation",
      "authors": [
        "Yassine Rachidy",
        "Jihad Rbaiti",
        "Youssef Hmamouche",
        "Faissal Sehbaoui",
        "Amal El Fallah Seghrouchni"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "With the growing adoption of Large Language Models (LLMs) in critical areas,\nensuring their security against jailbreaking attacks is paramount. While\ntraditional defenses primarily rely on refusing malicious prompts, recent\nlogit-level attacks have demonstrated the ability to bypass these safeguards by\ndirectly manipulating the token-selection process during generation. We\nintroduce Strategic Deflection (SDeflection), a defense that redefines the\nLLM's response to such advanced attacks. Instead of outright refusal, the model\nproduces an answer that is semantically adjacent to the user's request yet\nstrips away the harmful intent, thereby neutralizing the attacker's harmful\nintent. Our experiments demonstrate that SDeflection significantly lowers\nAttack Success Rate (ASR) while maintaining model performance on benign\nqueries. This work presents a critical shift in defensive strategies, moving\nfrom simple refusal to strategic content redirection to neutralize advanced\nthreats.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22160v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22160v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.441,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.45,
      "distributed_training_score": 0.348,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions RLHF as an existing post-training alignment method for improving LLM safety, but it does not propose or implement RLHF as part of its main contribution. SDeflection is a new defense mechanism focused on training models for strategic responses, without specifying the use of human feedback or a reward model, making the connection indirect.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes. It focuses solely on defending LLMs against logit manipulation attacks through a new strategy called SDeflection, with no mention of diffusion-based techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22168",
      "title": "Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing\n  Styles",
      "authors": [
        "Kimberly Le Truong",
        "Riccardo Fogliato",
        "Hoda Heidari",
        "Zhiwei Steven Wu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Current benchmarks for evaluating Large Language Models (LLMs) often do not\nexhibit enough writing style diversity, with many adhering primarily to\nstandardized conventions. Such benchmarks do not fully capture the rich variety\nof communication patterns exhibited by humans. Thus, it is possible that LLMs,\nwhich are optimized on these benchmarks, may demonstrate brittle performance\nwhen faced with \"non-standard\" input. In this work, we test this hypothesis by\nrewriting evaluation prompts using persona-based LLM prompting, a low-cost\nmethod to emulate diverse writing styles. Our results show that, even with\nidentical semantic content, variations in writing style and prompt formatting\nsignificantly impact the estimated performance of the LLM under evaluation.\nNotably, we identify distinct writing styles that consistently trigger either\nlow or high performance across a range of models and tasks, irrespective of\nmodel family, size, and recency. Our work offers a scalable approach to augment\nexisting benchmarks, improving the external validity of the assessments they\nprovide for measuring LLM performance across linguistic variations.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22168v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22168v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.447,
      "weak_supervision_score": 0.41,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.356,
      "datasets_score": 0.411,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on evaluating LLMs through persona-based prompt augmentation to assess performance across writing styles, without any involvement in training models using human feedback, reward models, or reinforcement learning.",
      "weak_supervision_justification": "The paper uses programmatic methods (persona-based LLM prompting) to generate varied prompts for evaluation, which somewhat resembles weak supervision by creating noisy or derived data, but it does not involve training models with such labels or focus on weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning adaptations; it centers on prompt rewriting for style diversity in LLM evaluation.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is creating and evaluating augmented benchmark datasets by rewriting prompts to introduce linguistic diversity, directly aligning with research on benchmarking, evaluating, and analyzing datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the limitations of current Large Language Model (LLM) benchmarks by introducing a persona-based prompting method to generate diverse writing styles in evaluation prompts, aiming to better capture real-world linguistic variations. The methodology involves using personas with socio-demographic and psychosocial attributes to rewrite prompts while preserving semantic content, and the key findings reveal that style variations significantly impact LLM performance across tasks, with certain styles consistently leading to high or low results, highlighting the need for more robust benchmarking.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing LLM prompting techniques with persona-based augmentation to address the known issue of benchmark diversity, offering a clever and scalable method rather than a completely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of LLM evaluation, as it provides a practical approach to improving benchmark external validity and revealing model sensitivities to linguistic variations.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a high-quality contribution by identifying critical flaws in current LLM benchmarks and proposing an effective solution, making it essential for researchers focused on AI evaluation and model robustness.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/43bbcae04ff508bbdcfa9995dc179120d3d74f8d",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 3,
      "average_h_index": 0.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Kimberly Le Truong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372638663"
        },
        {
          "name": "Riccardo Fogliato",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374053350"
        },
        {
          "name": "Hoda Heidari",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2256993437"
        },
        {
          "name": "Zhiwei Steven Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374220794"
        }
      ]
    },
    {
      "id": "2507.22186",
      "title": "SourceSplice: Source Selection for Machine Learning Tasks",
      "authors": [
        "Ambarish Singh",
        "Romila Pradhan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.DB (Databases)"
      ],
      "abstract": "Data quality plays a pivotal role in the predictive performance of machine\nlearning (ML) tasks - a challenge amplified by the deluge of data sources\navailable in modern organizations. Prior work in data discovery largely focus\non metadata matching, semantic similarity or identifying tables that should be\njoined to answer a particular query, but do not consider source quality for\nhigh performance of the downstream ML task. This paper addresses the problem of\ndetermining the best subset of data sources that must be combined to construct\nthe underlying training dataset for a given ML task. We propose SourceGrasp and\nSourceSplice, frameworks designed to efficiently select a suitable subset of\nsources that maximizes the utility of the downstream ML model. Both the\nalgorithms rely on the core idea that sources (or their combinations)\ncontribute differently to the task utility, and must be judiciously chosen.\nWhile SourceGrasp utilizes a metaheuristic based on a greediness criterion and\nrandomization, the SourceSplice framework presents a source selection mechanism\ninspired from gene splicing - a core concept used in protein synthesis. We\nempirically evaluate our algorithms on three real-world datasets and synthetic\ndatasets and show that, with significantly fewer subset explorations,\nSourceSplice effectively identifies subsets of data sources leading to high\ntask utility. We also conduct studies reporting the sensitivity of SourceSplice\nto the decision choices under several settings.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22186v2",
      "pdf_url": "http://arxiv.org/pdf/2507.22186v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.421,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.343,
      "datasets_score": 0.4,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is developing algorithms (SourceGrasp and SourceSplice) for selecting optimal subsets of data sources to maximize ML task utility, focusing on data integration and combination for training datasets. It does not involve programmatically generating labels from noisy sources or address weak supervision techniques, such as using high-level rules for label creation. Thus, it lacks direct connection to weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper evaluates its algorithms on real-world and synthetic datasets, including analysis of dataset utility and performance metrics, which touches on dataset evaluation for ML applications. However, the primary focus is on source selection algorithms rather than core activities like creating new datasets, benchmarking methodologies, or in-depth dataset curation and analysis.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of selecting optimal subsets of data sources for machine learning tasks to maximize model utility, given the abundance of available data sources. It introduces two algorithms, SourceGrasp, which uses a greedy randomized adaptive search procedure, and SourceSplice, inspired by gene splicing, to efficiently identify high-utility source combinations based on marginal gains in task performance; empirical evaluations on real-world and synthetic datasets demonstrate that these methods achieve strong results with fewer subset explorations and highlight trade-offs in effectiveness and efficiency.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel problem formalization for source selection to optimize ML task utility and proposes innovative algorithms, including SourceSplice inspired by gene splicing, which represent a significant advancement in data discovery techniques.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like machine learning and databases due to its practical algorithms for efficient data source selection, though its influence may be confined to specific applications rather than broad commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with innovative approaches to a relevant problem in ML data management, making it valuable for researchers and practitioners in AI and databases to stay informed.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ef2da7f44c3b529828fcfe13092e3a16aecf72dd",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 1,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ambarish Singh",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374398369"
        },
        {
          "name": "Romila Pradhan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2329562450"
        }
      ]
    },
    {
      "id": "2507.22187",
      "title": "A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large\n  Language Models",
      "authors": [
        "Adam M. Morgan",
        "Adeen Flinker"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We present an automated pipeline for estimating Verb Frame Frequencies\n(VFFs), the frequency with which a verb appears in particular syntactic frames.\nVFFs provide a powerful window into syntax in both human and machine language\nsystems, but existing tools for calculating them are limited in scale,\naccuracy, or accessibility. We use large language models (LLMs) to generate a\ncorpus of sentences containing 476 English verbs. Next, by instructing an LLM\nto behave like an expert linguist, we had it analyze the syntactic structure of\nthe sentences in this corpus. This pipeline outperforms two widely used\nsyntactic parsers across multiple evaluation datasets. Furthermore, it requires\nfar fewer resources than manual parsing (the gold-standard), thereby enabling\nrapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF\ndatabase with broader verb coverage, finer-grained syntactic distinctions, and\nexplicit estimates of the relative frequencies of structural alternates\ncommonly studied in psycholinguistics. The pipeline is easily customizable and\nextensible to new verbs, syntactic frames, and even other languages. We present\nthis work as a proof of concept for automated frame frequency estimation, and\nrelease all code and data to support future research.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22187v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22187v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.328,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22189",
      "title": "Measuring Time-Series Dataset Similarity using Wasserstein Distance",
      "authors": [
        "Hongjie Chen",
        "Akshay Mehra",
        "Josh Kimball",
        "Ryan A. Rossi"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The emergence of time-series foundation model research elevates the growing\nneed to measure the (dis)similarity of time-series datasets. A time-series\ndataset similarity measure aids research in multiple ways, including model\nselection, finetuning, and visualization. In this paper, we propose a\ndistribution-based method to measure time-series dataset similarity by\nleveraging the Wasserstein distance. We consider a time-series dataset an\nempirical instantiation of an underlying multivariate normal distribution\n(MVN). The similarity between two time-series datasets is thus computed as the\nWasserstein distance between their corresponding MVNs. Comprehensive\nexperiments and visualization show the effectiveness of our approach.\nSpecifically, we show how the Wasserstein distance helps identify similar\ntime-series datasets and facilitates inference performance estimation of\nfoundation models in both out-of-distribution and transfer learning evaluation,\nwith high correlations between our proposed measure and the inference loss\n(>0.60).",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22189v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22189v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.27,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.327,
      "datasets_score": 0.434,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is a method for analyzing and evaluating the similarity of time-series datasets using Wasserstein Distance, which directly aligns with dataset analysis for machine learning and AI applications. It focuses on measuring dataset (dis)similarity to aid in model selection, finetuning, and performance estimation, fitting under the topic's scope of dataset analysis and evaluation.",
      "llm_score_status": "completed",
      "summary": "This paper proposes a method to measure similarity between time-series datasets by modeling them as multivariate normal distributions and computing the Wasserstein distance between these distributions, aiming to aid in model selection, finetuning, and visualization for foundation models. Through comprehensive experiments, the authors demonstrate the effectiveness of their approach, showing high correlations (>0.60) between the proposed similarity measure and inference performance in out-of-distribution and transfer learning scenarios.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing ideas, such as Wasserstein distance and multivariate normal distributions, to address time-series dataset similarity in a new way, offering a notable improvement for applications in foundation models. While not introducing a completely new problem, it advances the state-of-the-art by providing a practical and effective similarity measure.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of time-series machine learning, as it provides tools for estimating model performance and facilitating transfer learning. However, its influence may be limited to specific applications rather than broadly across all AI research.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper represents a strong, valuable contribution for researchers in AI and machine learning working with time-series data, offering practical insights into dataset similarity and model performance estimation. It is essential for those in the subfield but not broadly mandatory for all readers.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d6f040618ac8dae686a1f8623cdbd82aee2ae611",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 5,
      "average_h_index": 1.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Hongjie Chen",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2144146501"
        },
        {
          "name": "Akshay Mehra",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2349901915"
        },
        {
          "name": "Josh Kimball",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2340519663"
        },
        {
          "name": "Ryan A. Rossi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374058908"
        }
      ]
    },
    {
      "id": "2507.22194",
      "title": "Temporally Consistent Unsupervised Segmentation for Mobile Robot\n  Perception",
      "authors": [
        "Christian Ellis",
        "Maggie Wigness",
        "Craig Lennon",
        "Lance Fiondella"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Rapid progress in terrain-aware autonomous ground navigation has been driven\nby advances in supervised semantic segmentation. However, these methods rely on\ncostly data collection and labor-intensive ground truth labeling to train deep\nmodels. Furthermore, autonomous systems are increasingly deployed in\nunrehearsed, unstructured environments where no labeled data exists and\nsemantic categories may be ambiguous or domain-specific. Recent zero-shot\napproaches to unsupervised segmentation have shown promise in such settings but\ntypically operate on individual frames, lacking temporal consistency-a critical\nproperty for robust perception in unstructured environments. To address this\ngap we introduce Frontier-Seg, a method for temporally consistent unsupervised\nsegmentation of terrain from mobile robot video streams. Frontier-Seg clusters\nsuperpixel-level features extracted from foundation model\nbackbones-specifically DINOv2-and enforces temporal consistency across frames\nto identify persistent terrain boundaries or frontiers without human\nsupervision. We evaluate Frontier-Seg on a diverse set of benchmark\ndatasets-including RUGD and RELLIS-3D-demonstrating its ability to perform\nunsupervised segmentation across unstructured off-road environments.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22194v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22194v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.417,
      "diffusion_reasoning_score": 0.336,
      "distributed_training_score": 0.357,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces Frontier-Seg, an unsupervised segmentation method that generates initial pseudo-labels through clustering of features from foundation models, which involves programmatically deriving labels from data without human annotation. This aligns somewhat with weak supervision's concept of using noisy or imprecise sources for label generation, as the pseudo-labels are automatically created and potentially imperfect. However, the method is primarily unsupervised and does not rely on external high-level sources or heuristics, making it only moderately relevant rather than a direct application of weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Frontier-Seg, a method for unsupervised semantic segmentation of terrain in mobile robot video streams, addressing the limitations of supervised approaches by eliminating the need for labeled data and ensuring temporal consistency. It achieves this through a two-phase process: first, extracting and clustering superpixel-level features from foundation models like DINOv2 within temporal windows to assign initial pseudo-labels, and second, refining these labels via feature recomputation and aggregation across frames to produce persistent segmentations; evaluations on datasets such as RUGD and RELLIS-3D demonstrate strong performance in unstructured off-road environments, enabling adaptive terrain perception.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining unsupervised segmentation with temporal consistency enforcement, addressing a key gap in existing methods for mobile robot perception. However, it builds on established techniques like foundation model features and clustering rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like robotics and computer vision for off-road navigation, as it enhances unsupervised segmentation's applicability in real-world scenarios. Nonetheless, its influence may be limited to specific domains rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to terrain perception in autonomous systems, making it essential for researchers in robotics and computer vision working on unsupervised methods. While innovative, it is not groundbreaking enough to be considered must-read material for all in the broader field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7574b2baae3853fbf508e7ebcdb8df7264ffe44e",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 17,
      "average_h_index": 5.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Christian Ellis",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/49348425"
        },
        {
          "name": "Maggie B. Wigness",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2325579800"
        },
        {
          "name": "Craig Lennon",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2302019195"
        },
        {
          "name": "L. Fiondella",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/1923736"
        }
      ]
    },
    {
      "id": "2507.22197",
      "title": "Explainability Through Systematicity: The Hard Systematicity Challenge\n  for Artificial Intelligence",
      "authors": [
        "Matthieu Queloz"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "This paper argues that explainability is only one facet of a broader ideal\nthat shapes our expectations towards artificial intelligence (AI).\nFundamentally, the issue is to what extent AI exhibits systematicity--not\nmerely in being sensitive to how thoughts are composed of recombinable\nconstituents, but in striving towards an integrated body of thought that is\nconsistent, coherent, comprehensive, and parsimoniously principled. This richer\nconception of systematicity has been obscured by the long shadow of the\n\"systematicity challenge\" to connectionism, according to which network\narchitectures are fundamentally at odds with what Fodor and colleagues termed\n\"the systematicity of thought.\" I offer a conceptual framework for thinking\nabout \"the systematicity of thought\" that distinguishes four senses of the\nphrase. I use these distinctions to defuse the perceived tension between\nsystematicity and connectionism and show that the conception of systematicity\nthat historically shaped our sense of what makes thought rational,\nauthoritative, and scientific is more demanding than the Fodorian notion. To\ndetermine whether we have reason to hold AI models to this ideal of\nsystematicity, I then argue, we must look to the rationales for systematization\nand explore to what extent they transfer to AI models. I identify five such\nrationales and apply them to AI. This brings into view the \"hard systematicity\nchallenge.\" However, the demand for systematization itself needs to be\nregulated by the rationales for systematization. This yields a dynamic\nunderstanding of the need to systematize thought, which tells us how systematic\nwe need AI models to be and when.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22197v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22197v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.428,
      "distributed_training_score": 0.29,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a conceptual and philosophical framework for systematicity in AI, emphasizing explainability, consistency, and rationales for systematization, including the \"hard systematicity challenge.\" It discusses historical aspects like the systematicity challenge to connectionism but does not mention diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. As the topic specifically requires elements like diffusion-based iterative refinement for logical tasks, the paper lacks any relevant components.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22208",
      "title": "Quantum-Inspired Audio Unlearning: Towards Privacy-Preserving Voice\n  Biometrics",
      "authors": [
        "Shreyansh Pathak",
        "Sonu Shreshtha",
        "Richa Singh",
        "Mayank Vatsa"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "The widespread adoption of voice-enabled authentication and audio biometric\nsystems have significantly increased privacy vulnerabilities associated with\nsensitive speech data. Compliance with privacy regulations such as GDPR's right\nto be forgotten and India's DPDP Act necessitates targeted and efficient\nerasure of individual-specific voice signatures from already-trained biometric\nmodels. Existing unlearning methods designed for visual data inadequately\nhandle the sequential, temporal, and high-dimensional nature of audio signals,\nleading to ineffective or incomplete speaker and accent erasure. To address\nthis, we introduce QPAudioEraser, a quantum-inspired audio unlearning\nframework. Our our-phase approach involves: (1) weight initialization using\ndestructive interference to nullify target features, (2) superposition-based\nlabel transformations that obscure class identity, (3) an\nuncertainty-maximizing quantum loss function, and (4) entanglement-inspired\nmixing of correlated weights to retain model knowledge. Comprehensive\nevaluations with ResNet18, ViT, and CNN architectures across AudioMNIST, Speech\nCommands, LibriSpeech, and Speech Accent Archive datasets validate\nQPAudioEraser's superior performance. The framework achieves complete erasure\nof target data (0% Forget Accuracy) while incurring minimal impact on model\nutility, with a performance degradation on retained data as low as 0.05%.\nQPAudioEraser consistently surpasses conventional baselines across\nsingle-class, multi-class, sequential, and accent-level erasure scenarios,\nestablishing the proposed approach as a robust privacy-preserving solution.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22208v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22208v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.34,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22219",
      "title": "RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine\n  Translation",
      "authors": [
        "Dongyub Jude Lee",
        "Zhenyi Ye",
        "Pengcheng He"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Preference-learning methods for machine translation (MT)--such as Direct\nPreference Optimization (DPO)--have achieved impressive gains but depend\nheavily on large, carefully curated triplet datasets and often struggle to\ngeneralize beyond their tuning domains. We propose Reinforcement Learning from\nTeacher-Model Refinement (RLfR), a novel framework that removes reliance on\nstatic triplets by leveraging continuous, high-quality feedback from an\nexternal teacher model (GPT-4o). RLfR frames each translation step as a\nmicro-tutorial: the actor generates a hypothesis, the teacher refines it, and\nthe actor is rewarded based on how closely it aligns with the teacher's\nrefinement. Guided by two complementary signals--(i) negative edit distance,\npromoting lexical and structural fidelity, and (ii) COMET score, ensuring\nsemantic adequacy--the actor progressively learns to emulate the teacher,\nmirroring a human learning process through incremental, iterative improvement.\nOn the FLORES-200 benchmark (English to and from German, Spanish, Chinese,\nKorean, and Japanese), RLfR consistently outperforms both MT-SFT and\npreference-based baselines, significantly improving COMET (semantic adequacy)\nand M-ETA (entity preservation) scores.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22219v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22219v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.549,
      "weak_supervision_score": 0.419,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.399,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's RLfR framework uses feedback from a frozen teacher model (e.g., GPT-4o) rather than human-ranked data or a separate reward model trained on human preferences. Since no human feedback is involved, it does not align with the definition of RLHF.",
      "weak_supervision_justification": "The paper employs a teacher model to programmatically generate refinements and rewards, reducing reliance on hand-labeled triplet datasets. This fits weak supervision by using noisy or imprecise sources for labels, though it is more structured and real-time compared to traditional weak supervision methods.",
      "diffusion_reasoning_justification": "The paper describes an iterative generate-refine-reinforce cycle for machine translation, but it does not involve diffusion models, multi-step logical reasoning, or treating a chain-of-thought as a single entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces RL from Teacher-Model Refinement (RLfR), a novel reinforcement learning framework for machine translation that eliminates the need for static triplet datasets by using real-time feedback from a frozen teacher model like GPT-4o. In this approach, an actor model generates initial translations, which the teacher refines, and the actor learns through rewards based on edit distance for lexical fidelity and COMET scores for semantic adequacy, leading to progressive improvements; evaluations on the FLORES-200 benchmark show RLfR outperforms baselines in COMET and M-ETA scores across multiple languages.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework that combines reinforcement learning with real-time teacher model refinement, significantly advancing machine translation by addressing the limitations of static datasets and providing a more dynamic, context-sensitive learning process.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research in machine translation and AI by offering a scalable, data-efficient method that could be applied to other language tasks, reducing reliance on curated datasets.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality contribution with practical innovations in machine translation that are valuable for researchers in AI and computational linguistics, though it may not be essential for those outside the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8a61f300c04a888bdca5b731bd7e9938a0bac21a",
      "total_authors": 3,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Dongyub Jude Lee",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374089407"
        },
        {
          "name": "Zhenyi Ye",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Pengcheng He",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374124674"
        }
      ]
    },
    {
      "id": "2507.22239",
      "title": "Large Language Model-Based Framework for Explainable Cyberattack\n  Detection in Automatic Generation Control Systems",
      "authors": [
        "Muhammad Sharshar",
        "Ahmad Mohammad Saber",
        "Davor Svetinovic",
        "Amr M. Youssef",
        "Deepa Kundur",
        "Ehab F. El-Saadany"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "The increasing digitization of smart grids has improved operational\nefficiency but also introduced new cybersecurity vulnerabilities, such as False\nData Injection Attacks (FDIAs) targeting Automatic Generation Control (AGC)\nsystems. While machine learning (ML) and deep learning (DL) models have shown\npromise in detecting such attacks, their opaque decision-making limits operator\ntrust and real-world applicability. This paper proposes a hybrid framework that\nintegrates lightweight ML-based attack detection with natural language\nexplanations generated by Large Language Models (LLMs). Classifiers such as\nLightGBM achieve up to 95.13% attack detection accuracy with only 0.004 s\ninference latency. Upon detecting a cyberattack, the system invokes LLMs,\nincluding GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o mini, to generate\nhuman-readable explanation of the event. Evaluated on 100 test samples, GPT-4o\nmini with 20-shot prompting achieved 93% accuracy in identifying the attack\ntarget, a mean absolute error of 0.075 pu in estimating attack magnitude, and\n2.19 seconds mean absolute error (MAE) in estimating attack onset. These\nresults demonstrate that the proposed framework effectively balances real-time\ndetection with interpretable, high-fidelity explanations, addressing a critical\nneed for actionable AI in smart grid cybersecurity.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22239v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22239v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.375,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using pre-trained LLMs like GPT models with prompting for generating explanations of cyberattacks, but it does not involve reinforcement learning, human feedback, or training a reward model based on human-ranked data. There is no element of fine-tuning or aligning models with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes LLMs for straightforward natural language explanations of ML-based detections, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning. There is no mention of treating a chain-of-thought as a holistically corrected entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22250",
      "title": "Using Scaling Laws for Data Source Utility Estimation in Domain-Specific\n  Pre-Training",
      "authors": [
        "Oleksiy Ostapenko",
        "Charles Guille-Escuret",
        "Luke Kumar",
        "Max Tian",
        "Denis Kocetkov",
        "Gopeshh Subbaraj",
        "Raymond Li",
        "Joel Lamy-Poirier",
        "Sebastien Paquet",
        "Torsten Scholak"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce a framework for optimizing domain-specific dataset construction\nin foundation model training. Specifically, we seek a cost-efficient way to\nestimate the quality of data sources (e.g. synthetically generated or filtered\nweb data, etc.) in order to make optimal decisions about resource allocation\nfor data sourcing from these sources for the stage two pre-training phase, aka\nannealing, with the goal of specializing a generalist pre-trained model to\nspecific domains. Our approach extends the usual point estimate approaches, aka\nmicro-annealing, to estimating scaling laws by performing multiple annealing\nruns of varying compute spent on data curation and training. This addresses a\nkey limitation in prior work, where reliance on point estimates for data\nscaling decisions can be misleading due to the lack of rank invariance across\ncompute scales -- a phenomenon we confirm in our experiments. By systematically\nanalyzing performance gains relative to acquisition costs, we find that scaling\ncurves can be estimated for different data sources. Such scaling laws can\ninform cost effective resource allocation across different data acquisition\nmethods (e.g. synthetic data), data sources (e.g. user or web data) and\navailable compute resources. We validate our approach through experiments on a\npre-trained model with 7 billion parameters. We adapt it to: a domain\nwell-represented in the pre-training data -- the medical domain, and a domain\nunderrepresented in the pretraining corpora -- the math domain. We show that\none can efficiently estimate the scaling behaviors of a data source by running\nmultiple annealing runs, which can lead to different conclusions, had one used\npoint estimates using the usual micro-annealing technique instead. This enables\ndata-driven decision-making for selecting and optimizing data sources.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22250v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22250v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.443,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.437,
      "datasets_score": 0.396,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on estimating data source utility using scaling laws for domain-specific pre-training, with no mention of reinforcement learning, human feedback, reward models, or aligning models with preferences.",
      "weak_supervision_justification": "The paper discusses methods like model-based filtering and synthetic data generation, which involve noisy or programmatically derived data similar to weak supervision, but its main contribution is on scaling laws for utility estimation, not on training models with weakly supervised labels.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes; it centers on data sourcing and scaling laws for pre-training.",
      "distributed_training_justification": "The paper mentions compute resources for annealing runs but does not address algorithms, systems, or techniques for distributed training, parallel computing, or multi-node setups.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22255",
      "title": "Agent-centric learning: from external reward maximization to internal\n  knowledge curation",
      "authors": [
        "Hanqi Zhou",
        "Fryderyk Mantiuk",
        "David G. Nagy",
        "Charley M. Wu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.SC (Symbolic Computation)"
      ],
      "abstract": "The pursuit of general intelligence has traditionally centered on external\nobjectives: an agent's control over its environments or mastery of specific\ntasks. This external focus, however, can produce specialized agents that lack\nadaptability. We propose representational empowerment, a new perspective\ntowards a truly agent-centric learning paradigm by moving the locus of control\ninward. This objective measures an agent's ability to controllably maintain and\ndiversify its own knowledge structures. We posit that the capacity -- to shape\none's own understanding -- is an element for achieving better ``preparedness''\ndistinct from direct environmental influence. Focusing on internal\nrepresentations as the main substrate for computing empowerment offers a new\nlens through which to design adaptable intelligent systems.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22255v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22255v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.454,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.335,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on agent-centric learning and representational empowerment, emphasizing internal knowledge structures and adaptability without external influences like human feedback. RLHF specifically involves training a reward model using human-ranked data to align AI with human preferences, which is not mentioned or utilized in the paper.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22264",
      "title": "SmartCLIP: Modular Vision-language Alignment with Identification\n  Guarantees",
      "authors": [
        "Shaoan Xie",
        "Lingjing Kong",
        "Yujia Zheng",
        "Yu Yao",
        "Zeyu Tang",
        "Eric P. Xing",
        "Guangyi Chen",
        "Kun Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Contrastive Language-Image Pre-training (CLIP)~\\citep{radford2021learning}\nhas emerged as a pivotal model in computer vision and multimodal learning,\nachieving state-of-the-art performance at aligning visual and textual\nrepresentations through contrastive learning. However, CLIP struggles with\npotential information misalignment in many image-text datasets and suffers from\nentangled representation. On the one hand, short captions for a single image in\ndatasets like MSCOCO may describe disjoint regions in the image, leaving the\nmodel uncertain about which visual features to retain or disregard. On the\nother hand, directly aligning long captions with images can lead to the\nretention of entangled details, preventing the model from learning\ndisentangled, atomic concepts -- ultimately limiting its generalization on\ncertain downstream tasks involving short prompts.\n  In this paper, we establish theoretical conditions that enable flexible\nalignment between textual and visual representations across varying levels of\ngranularity. Specifically, our framework ensures that a model can not only\n\\emph{preserve} cross-modal semantic information in its entirety but also\n\\emph{disentangle} visual representations to capture fine-grained textual\nconcepts. Building on this foundation, we introduce \\ours, a novel approach\nthat identifies and aligns the most relevant visual and textual representations\nin a modular manner. Superior performance across various tasks demonstrates its\ncapability to handle information misalignment and supports our identification\ntheory. The code is available at https://github.com/Mid-Push/SmartCLIP.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22264v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22264v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.404,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.361,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on improving vision-language alignment in CLIP through contrastive learning and modular representations, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses issues in CLIP related to representation alignment and introduces SmartCLIP, but it does not involve diffusion models, iterative refinement for logical reasoning, or multi-step chain-of-thought processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22267",
      "title": "Promoting Online Safety by Simulating Unsafe Conversations with LLMs",
      "authors": [
        "Owen Hoffman",
        "Kangze Peng",
        "Zehua You",
        "Sajid Kamal",
        "Sukrit Venkatagiri"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generative AI, including large language models (LLMs) have the potential --\nand already are being used -- to increase the speed, scale, and types of unsafe\nconversations online. LLMs lower the barrier for entry for bad actors to create\nunsafe conversations in particular because of their ability to generate\npersuasive and human-like text. In our current work, we explore ways to promote\nonline safety by teaching people about unsafe conversations that can occur\nonline with and without LLMs. We build on prior work that shows that LLMs can\nsuccessfully simulate scam conversations. We also leverage research in the\nlearning sciences that shows that providing feedback on one's hypothetical\nactions can promote learning. In particular, we focus on simulating scam\nconversations using LLMs. Our work incorporates two LLMs that converse with\neach other to simulate realistic, unsafe conversations that people may\nencounter online between a scammer LLM and a target LLM but users of our system\nare asked provide feedback to the target LLM.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22267v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22267v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.478,
      "weak_supervision_score": 0.396,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.306,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes a system where users provide feedback to a target LLM in simulated unsafe conversations to promote online safety and learning. While human feedback is involved, it is used for educational purposes rather than for training a reward model or fine-tuning an AI model via reinforcement learning, which is the core of RLHF. Thus, it is only tangentially related through the general concept of human feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22268",
      "title": "Multi-modal Relational Item Representation Learning for Inferring\n  Substitutable and Complementary Items",
      "authors": [
        "Junting Wang",
        "Chenghuan Guo",
        "Jiao Yang",
        "Yanhui Guo",
        "Yan Gao",
        "Hari Sundaram"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce a novel self-supervised multi-modal relational item\nrepresentation learning framework designed to infer substitutable and\ncomplementary items. Existing approaches primarily focus on modeling item-item\nassociations deduced from user behaviors using graph neural networks (GNNs) or\nleveraging item content information. However, these methods often overlook\ncritical challenges, such as noisy user behavior data and data sparsity due to\nthe long-tailed distribution of these behaviors. In this paper, we propose\nMMSC, a self-supervised multi-modal relational item representation learning\nframework to address these challenges. Specifically, MMSC consists of three\nmain components: (1) a multi-modal item representation learning module that\nleverages a multi-modal foundational model and learns from item metadata, (2) a\nself-supervised behavior-based representation learning module that denoises and\nlearns from user behavior data, and (3) a hierarchical representation\naggregation mechanism that integrates item representations at both the semantic\nand task levels. Additionally, we leverage LLMs to generate augmented training\ndata, further enhancing the denoising process during training. We conduct\nextensive experiments on five real-world datasets, showing that MMSC\noutperforms existing baselines by 26.1% for substitutable recommendation and\n39.2% for complementary recommendation. In addition, we empirically show that\nMMSC is effective in modeling cold-start items.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22268v2",
      "pdf_url": "http://arxiv.org/pdf/2507.22268v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.319,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22274",
      "title": "HOG-CNN: Integrating Histogram of Oriented Gradients with Convolutional\n  Neural Networks for Retinal Image Classification",
      "authors": [
        "Faisal Ahmed"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The analysis of fundus images is critical for the early detection and\ndiagnosis of retinal diseases such as Diabetic Retinopathy (DR), Glaucoma, and\nAge-related Macular Degeneration (AMD). Traditional diagnostic workflows,\nhowever, often depend on manual interpretation and are both time- and\nresource-intensive. To address these limitations, we propose an automated and\ninterpretable clinical decision support framework based on a hybrid feature\nextraction model called HOG-CNN. Our key contribution lies in the integration\nof handcrafted Histogram of Oriented Gradients (HOG) features with deep\nconvolutional neural network (CNN) representations. This fusion enables our\nmodel to capture both local texture patterns and high-level semantic features\nfrom retinal fundus images. We evaluated our model on three public benchmark\ndatasets: APTOS 2019 (for binary and multiclass DR classification), ORIGA (for\nGlaucoma detection), and IC-AMD (for AMD diagnosis); HOG-CNN demonstrates\nconsistently high performance. It achieves 98.5\\% accuracy and 99.2 AUC for\nbinary DR classification, and 94.2 AUC for five-class DR classification. On the\nIC-AMD dataset, it attains 92.8\\% accuracy, 94.8\\% precision, and 94.5 AUC,\noutperforming several state-of-the-art models. For Glaucoma detection on ORIGA,\nour model achieves 83.9\\% accuracy and 87.2 AUC, showing competitive\nperformance despite dataset limitations. We show, through comprehensive\nappendix studies, the complementary strength of combining HOG and CNN features.\nThe model's lightweight and interpretable design makes it particularly suitable\nfor deployment in resource-constrained clinical environments. These results\nposition HOG-CNN as a robust and scalable tool for automated retinal disease\nscreening.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22274v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22274v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.332,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22281",
      "title": "CoEx -- Co-evolving World-model and Exploration",
      "authors": [
        "Minsoo Kim",
        "Seung-won Hwang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Planning in modern LLM agents relies on the utilization of LLM as an internal\nworld model, acquired during pretraining. However, existing agent designs fail\nto effectively assimilate new observations into dynamic updates of the world\nmodel. This reliance on the LLM's static internal world model is progressively\nprone to misalignment with the underlying true state of the world, leading to\nthe generation of divergent and erroneous plans. We introduce a hierarchical\nagent architecture, CoEx, in which hierarchical state abstraction allows LLM\nplanning to co-evolve with a dynamically updated model of the world. CoEx plans\nand interacts with the world by using LLM reasoning to orchestrate dynamic\nplans consisting of subgoals, and its learning mechanism continuously\nincorporates these subgoal experiences into a persistent world model in the\nform of a neurosymbolic belief state, comprising textual inferences and\ncode-based symbolic memory. We evaluate our agent across a diverse set of agent\nscenarios involving rich environments and complex tasks including ALFWorld,\nPDDL, and Jericho. Our experiments show that CoEx outperforms existing agent\nparadigms in planning and exploration.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22281v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22281v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.419,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.443,
      "distributed_training_score": 0.314,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a hierarchical agent architecture for dynamic world model updates and subgoal planning in LLM agents, without any mention of human feedback, reward models, or fine-tuning based on human preferences. It relies on autonomous learning from interactions, not RLHF mechanisms.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes LLM-based planning and iterative refinement of world models through subgoal experiences, but it does not adapt diffusion models or their iterative processes for multi-step logical reasoning. There is no reference to treating Chain-of-Thought as a holistically correctable entity via diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22286",
      "title": "Meaning-infused grammar: Gradient Acceptability Shapes the Geometric\n  Representations of Constructions in LLMs",
      "authors": [
        "Supantho Rakshit",
        "Adele Goldberg"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The usage-based constructionist (UCx) approach posits that language comprises\na network of learned form-meaning pairings (constructions) whose use is largely\ndetermined by their meanings or functions, requiring them to be graded and\nprobabilistic. This study investigates whether the internal representations in\nLarge Language Models (LLMs) reflect the proposed function-infused gradience.\nWe analyze the neural representations of the English dative constructions\n(Double Object and Prepositional Object) in Pythia-$1.4$B, using a dataset of\n$5000$ sentence pairs systematically varied for human-rated preference\nstrength. A macro-level geometric analysis finds that the separability between\nconstruction representations, as measured by Energy Distance or Jensen-Shannon\nDivergence, is systematically modulated by gradient preference strength. More\nprototypical exemplars of each construction occupy more distinct regions in the\nactivation space of LLMs. These results provide strong evidence that LLMs learn\nrich, meaning-infused, graded representations of constructions and offer\nsupport for geometric measures of basic constructionist principles in LLMs.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22286v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22286v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.468,
      "distributed_training_score": 0.324,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines the geometric representations of linguistic constructions in LLMs, focusing on gradient acceptability and usage-based approaches, without any mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It deals solely with representational geometry for language patterns, not with adapting diffusion for complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22291",
      "title": "AlphaEarth Foundations: An embedding field model for accurate and\n  efficient global mapping from sparse label data",
      "authors": [
        "Christopher F. Brown",
        "Michal R. Kazmierski",
        "Valerie J. Pasquarella",
        "William J. Rucklidge",
        "Masha Samsikova",
        "Chenhui Zhang",
        "Evan Shelhamer",
        "Estefania Lahera",
        "Olivia Wiles",
        "Simon Ilyushchenko",
        "Noel Gorelick",
        "Lihui Lydia Zhang",
        "Sophia Alj",
        "Emily Schechter",
        "Sean Askay",
        "Oliver Guinan",
        "Rebecca Moore",
        "Alexis Boukouvalas",
        "Pushmeet Kohli"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Unprecedented volumes of Earth observation data are continually collected\naround the world, but high-quality labels remain scarce given the effort\nrequired to make physical measurements and observations. This has led to\nconsiderable investment in bespoke modeling efforts translating sparse labels\ninto maps. Here we introduce AlphaEarth Foundations, an embedding field model\nyielding a highly general, geospatial representation that assimilates spatial,\ntemporal, and measurement contexts across multiple sources, enabling accurate\nand efficient production of maps and monitoring systems from local to global\nscales. The embeddings generated by AlphaEarth Foundations are the only to\nconsistently outperform all previous featurization approaches tested on a\ndiverse set of mapping evaluations without re-training. We will release a\ndataset of global, annual, analysis-ready embedding field layers from 2017\nthrough 2024.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22291v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22291v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.426,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.386,
      "datasets_score": 0.401,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on a model that uses sparse label data to generate embeddings for mapping, which aligns with weak supervision's emphasis on noisy or imprecise labels rather than perfect ones. However, it does not explicitly describe techniques for programmatically generating labels, making it relevant but not central to the topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper announces the release of a new dataset of global, annual embedding field layers from 2017 to 2024, directly contributing to dataset creation for machine learning applications in geospatial analysis. This fits the topic's focus on introducing and curating datasets for AI.",
      "llm_score_status": "completed",
      "summary": "The paper introduces AlphaEarth Foundations, an innovative embedding field model designed to create a universal geospatial representation by integrating spatial, temporal, and measurement contexts from sparse label data in Earth observation. Its core objectives are to enable accurate and efficient global mapping and monitoring, with the methodology outperforming all previous featurization approaches across diverse evaluations without retraining, and the authors plan to release a dataset of global embedding layers from 2017 to 2024.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel embedding field model that advances the state-of-the-art by providing a highly general geospatial representation capable of assimilating multiple contexts and outperforming existing methods without retraining, addressing the challenge of sparse labels in Earth observation data.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of applications in environmental monitoring, disaster response, and policy-making by offering a scalable solution for global mapping from sparse data, with the planned dataset release likely to facilitate further research and commercial adoption.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to computer vision and machine learning for geospatial applications, with its innovative model and demonstrated superior performance making it essential for researchers in the field to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c51b90a204368446c0d0d80d78426090fb147914",
      "total_authors": 19,
      "authors_found": 19,
      "highest_h_index": 111,
      "average_h_index": 7.052631578947368,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Christopher F. Brown",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375252688"
        },
        {
          "name": "Michal R. Kazmierski",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2174642462"
        },
        {
          "name": "Valerie J. Pasquarella",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/3365752"
        },
        {
          "name": "W. J. Rucklidge",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2252123766"
        },
        {
          "name": "Masha Samsikova",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2347349081"
        },
        {
          "name": "Chenhui Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374072370"
        },
        {
          "name": "Evan Shelhamer",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2293641948"
        },
        {
          "name": "Estefania Lahera",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374048945"
        },
        {
          "name": "Olivia Wiles",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2293642892"
        },
        {
          "name": "S. Ilyushchenko",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/103555414"
        },
        {
          "name": "Noel Gorelick",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374047540"
        },
        {
          "name": "Lihui Lydia Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374077050"
        },
        {
          "name": "Sophia Alj",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374056635"
        },
        {
          "name": "Emily Schechter",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374051013"
        },
        {
          "name": "Sean Askay",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374051017"
        },
        {
          "name": "Oliver Guinan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2169650607"
        },
        {
          "name": "Rebecca Moore",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374192656"
        },
        {
          "name": "Alexis Boukouvalas",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2270751411"
        },
        {
          "name": "Pushmeet Kohli",
          "h_index": 111,
          "profile_url": "https://www.semanticscholar.org/author/143967473"
        }
      ]
    },
    {
      "id": "2507.22951",
      "title": "Unifying Post-hoc Explanations of Knowledge Graph Completions",
      "authors": [
        "Alessandro Lonardi",
        "Samy Badreddine",
        "Tarek R. Besold",
        "Pablo Sanchez Martin"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Post-hoc explainability for Knowledge Graph Completion (KGC) lacks\nformalization and consistent evaluations, hindering reproducibility and\ncross-study comparisons. This paper argues for a unified approach to post-hoc\nexplainability in KGC. First, we propose a general framework to characterize\npost-hoc explanations via multi-objective optimization, balancing their\neffectiveness and conciseness. This unifies existing post-hoc explainability\nalgorithms in KGC and the explanations they produce. Next, we suggest and\nempirically support improved evaluation protocols using popular metrics like\nMean Reciprocal Rank and Hits@$k$. Finally, we stress the importance of\ninterpretability as the ability of explanations to address queries meaningful\nto end-users. By unifying methods and refining evaluation standards, this work\naims to make research in KGC explainability more reproducible and impactful.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22951v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22951v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.301,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on unifying post-hoc explanations for Knowledge Graph Completion (KGC) through multi-objective optimization and improved evaluation protocols. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning or Chain-of-Thought generation.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2507.22952",
      "title": "Automated Label Placement on Maps via Large Language Models",
      "authors": [
        "Harry Shomer",
        "Jiejun Xu"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Label placement is a critical aspect of map design, serving as a form of\nspatial annotation that directly impacts clarity and interpretability. Despite\nits importance, label placement remains largely manual and difficult to scale,\nas existing automated systems struggle to integrate cartographic conventions,\nadapt to context, or interpret labeling instructions. In this work, we\nintroduce a new paradigm for automatic label placement (ALP) that formulates\nthe task as a data editing problem and leverages large language models (LLMs)\nfor context-aware spatial annotation. To support this direction, we curate\nMAPLE, the first known benchmarking dataset for evaluating ALP on real-world\nmaps, encompassing diverse landmark types and label placement annotations from\nopen-source data. Our method retrieves labeling guidelines relevant to each\nlandmark type leveraging retrieval-augmented generation (RAG), integrates them\ninto prompts, and employs instruction-tuned LLMs to generate ideal label\ncoordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overall\nperformance and generalization across different types of landmarks. This\nincludes both zero-shot and instruction-tuned performance. Our results\ndemonstrate that LLMs, when guided by structured prompts and domain-specific\nretrieval, can learn to perform accurate spatial edits, aligning the generated\noutputs with expert cartographic standards. Overall, our work presents a\nscalable framework for AI-assisted map finishing and demonstrates the potential\nof foundation models in structured data editing tasks. The code and data can be\nfound at https://github.com/HarryShomer/MAPLE.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22952v2",
      "pdf_url": "http://arxiv.org/pdf/2507.22952v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.453,
      "weak_supervision_score": 0.457,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.381,
      "datasets_score": 0.401,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on using LLMs with RAG and instruction tuning for label placement, but it does not involve training a reward model on human-ranked data or using reinforcement learning to align with human preferences. There is no mention of RLHF techniques.",
      "weak_supervision_justification": "The paper curates a dataset from open-source data like OpenStreetMap, which could involve noisy sources, but it primarily emphasizes dataset creation for benchmarking rather than programmatically generating labels for model training. Weak supervision is not a core method used.",
      "diffusion_reasoning_justification": "The paper employs LLMs with RAG for reasoning and label generation, but it does not use diffusion models or iterative refinement processes for multi-step logical reasoning. There is no component involving diffusion-based techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes creating, curating, and benchmarking the MAPLE dataset for automatic label placement, which directly aligns with research on dataset introduction, evaluation, and analysis for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper presents a novel approach to automated label placement (ALP) on maps by leveraging large language models (LLMs) to address the limitations of existing methods, formulating the task as a context-aware data editing problem. The authors introduce the MAPLE dataset for benchmarking, utilize retrieval-augmented generation (RAG) to incorporate relevant cartographic guidelines into LLM prompts, and evaluate four open-source LLMs through experiments that demonstrate accurate label generation aligning with expert standards, highlighting the potential for scalable AI-assisted map design.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new paradigm by applying LLMs and RAG to automated label placement, significantly advancing the state-of-the-art in cartography by integrating context-aware reasoning into a traditionally rule-based task. This represents a fresh technique that combines AI advancements with a specific real-world problem, marking it as more than just an incremental improvement.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like human-computer interaction and computer vision for enhancing automated mapping tools, given its introduction of a new dataset and methodology. However, its influence may remain confined to niche applications in cartography rather than broadly transforming unrelated areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a high-quality contribution with innovative use of LLMs for a practical problem, including a new dataset, making it valuable for researchers in AI and geospatial fields. While not groundbreaking across all domains, its insights and resources warrant attention for those interested in advancing automated design tasks.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/fceace0f2037f79387138be9ca7d81318993d9d6",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Harry Shomer",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374152653"
        },
        {
          "name": "Jiejun Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374367160"
        }
      ]
    },
    {
      "id": "2507.22953",
      "title": "CADS: A Comprehensive Anatomical Dataset and Segmentation for Whole-Body\n  Anatomy in Computed Tomography",
      "authors": [
        "Murong Xu",
        "Tamaz Amiranashvili",
        "Fernando Navarro",
        "Maksym Fritsak",
        "Ibrahim Ethem Hamamci",
        "Suprosanna Shit",
        "Bastian Wittmann",
        "Sezgin Er",
        "Sebastian M. Christ",
        "Ezequiel de la Rosa",
        "Julian Deseoe",
        "Robert Graf",
        "Hendrik Möller",
        "Anjany Sekuboyina",
        "Jan C. Peeken",
        "Sven Becker",
        "Giulia Baldini",
        "Johannes Haubold",
        "Felix Nensa",
        "René Hosch",
        "Nikhil Mirajkar",
        "Saad Khalid",
        "Stefan Zachow",
        "Marc-André Weber",
        "Georg Langs",
        "Jakob Wasserthal",
        "Mehmet Kemal Ozdemir",
        "Andrey Fedorov",
        "Ron Kikinis",
        "Stephanie Tanadini-Lang",
        "Jan S. Kirschke",
        "Stephanie E. Combs",
        "Bjoern Menze"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate delineation of anatomical structures in volumetric CT scans is\ncrucial for diagnosis and treatment planning. While AI has advanced automated\nsegmentation, current approaches typically target individual structures,\ncreating a fragmented landscape of incompatible models with varying performance\nand disparate evaluation protocols. Foundational segmentation models address\nthese limitations by providing a holistic anatomical view through a single\nmodel. Yet, robust clinical deployment demands comprehensive training data,\nwhich is lacking in existing whole-body approaches, both in terms of data\nheterogeneity and, more importantly, anatomical coverage. In this work, rather\nthan pursuing incremental optimizations in model architecture, we present CADS,\nan open-source framework that prioritizes the systematic integration,\nstandardization, and labeling of heterogeneous data sources for whole-body CT\nsegmentation. At its core is a large-scale dataset of 22,022 CT volumes with\ncomplete annotations for 167 anatomical structures, representing a significant\nadvancement in both scale and coverage, with 18 times more scans than existing\ncollections and 60% more distinct anatomical targets. Building on this diverse\ndataset, we develop the CADS-model using established architectures for\naccessible and automated full-body CT segmentation. Through comprehensive\nevaluation across 18 public datasets and an independent real-world hospital\ncohort, we demonstrate advantages over SoTA approaches. Notably, thorough\ntesting of the model's performance in segmentation tasks from radiation\noncology validates its direct utility for clinical interventions. By making our\nlarge-scale dataset, our segmentation models, and our clinical software tool\npublicly available, we aim to advance robust AI solutions in radiology and make\ncomprehensive anatomical analysis accessible to clinicians and researchers\nalike.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22953v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22953v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.272,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.359,
      "datasets_score": 0.432,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of CADS, a new large-scale dataset comprising 22,022 CT volumes with annotations for 167 anatomical structures, which directly aligns with research on creating and curating datasets for AI applications. It details methodologies for dataset integration, standardization, and labeling from heterogeneous sources, including pseudo-labeling and iterative refinement, and evaluates the dataset's impact through model performance and external benchmarks. This fits the topic's focus on new dataset introduction, curation methodologies, and benchmark evaluation in machine learning.",
      "llm_score_status": "completed",
      "summary": "The paper introduces CADS, an open-source framework that creates a large-scale dataset of 22,022 CT volumes annotated for 167 anatomical structures to advance whole-body CT segmentation, emphasizing systematic data integration and standardization through techniques like pseudo-labeling and self-training. The authors develop a segmentation model using established architectures, which outperforms state-of-the-art methods in evaluations across 18 public datasets and a real-world hospital cohort, demonstrating its clinical utility in areas like radiation oncology, and make the dataset, model, and tools publicly available to facilitate broader adoption.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a significantly larger and more diverse dataset for whole-body CT segmentation through clever integration of existing techniques like pseudo-labeling and self-training, rather than introducing a new problem or architecture. While it advances the field by addressing data limitations, its core contribution lies in scaling and combining established methods rather than pioneering entirely novel concepts.",
      "impact_score": "High",
      "impact_justification": "The work's comprehensive dataset and robust model are likely to influence a wide range of future research in medical imaging and commercial applications in radiology, given its open-source nature and demonstrated real-world utility in clinical settings like radiation oncology. By providing accessible tools and extensive annotations, it has the potential to standardize and accelerate advancements in AI-driven anatomical analysis across diverse subfields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a high-quality and valuable contribution to medical image segmentation due to its extensive dataset and practical model enhancements, making it essential for researchers and clinicians in AI and radiology. While not groundbreaking in architecture, its focus on data diversity and clinical applicability ensures it is a significant resource worth engaging with.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c1b2257238bcdb51753d39e6f884da8edefe9253",
      "total_authors": 33,
      "authors_found": 33,
      "highest_h_index": 63,
      "average_h_index": 7.818181818181818,
      "notable_authors_count": 11,
      "author_h_indexes": [
        {
          "name": "Murong Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374179819"
        },
        {
          "name": "Tamaz Amiranashvili",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/114604076"
        },
        {
          "name": "Fernando Navarro",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2279965325"
        },
        {
          "name": "Maksym Fritsak",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2138659876"
        },
        {
          "name": "Ibrahim Ethem Hamamci",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2111658673"
        },
        {
          "name": "Suprosanna Shit",
          "h_index": 21,
          "profile_url": "https://www.semanticscholar.org/author/1561461499"
        },
        {
          "name": "Bastian Wittmann",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2159547167"
        },
        {
          "name": "Sezgin Er",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2161427250"
        },
        {
          "name": "S. Christ",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2071207415"
        },
        {
          "name": "E. D. L. Rosa",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/49840386"
        },
        {
          "name": "Julian Deseoe",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374149215"
        },
        {
          "name": "Robert Graf",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2243008382"
        },
        {
          "name": "H. Moller",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2232600396"
        },
        {
          "name": "A. Sekuboyina",
          "h_index": 26,
          "profile_url": "https://www.semanticscholar.org/author/9938382"
        },
        {
          "name": "J. Peeken",
          "h_index": 23,
          "profile_url": "https://www.semanticscholar.org/author/4892738"
        },
        {
          "name": "Sven Becker",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374150746"
        },
        {
          "name": "Giulia Baldini",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2180548281"
        },
        {
          "name": "Johannes Haubold",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2251812358"
        },
        {
          "name": "Felix Nensa",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2368440293"
        },
        {
          "name": "R. Hosch",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/1791199007"
        },
        {
          "name": "Nikhil Mirajkar",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2293598288"
        },
        {
          "name": "Saad Khalid",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374150266"
        },
        {
          "name": "Stefan Zachow",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2288176808"
        },
        {
          "name": "Marc-Andr'e Weber",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2322876969"
        },
        {
          "name": "Georg Langs",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2249029742"
        },
        {
          "name": "Jakob Wasserthal",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2303844695"
        },
        {
          "name": "M. K. Ozdemir",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2268493641"
        },
        {
          "name": "Andrey Fedorov",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2249228581"
        },
        {
          "name": "R. Kikinis",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2272048358"
        },
        {
          "name": "S. Tanadini-Lang",
          "h_index": 33,
          "profile_url": "https://www.semanticscholar.org/author/1401391763"
        },
        {
          "name": "J. Kirschke",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2305003260"
        },
        {
          "name": "S. E. Combs",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2253692393"
        },
        {
          "name": "Bjoern H Menze",
          "h_index": 63,
          "profile_url": "https://www.semanticscholar.org/author/143893221"
        }
      ]
    },
    {
      "id": "2507.22958",
      "title": "CHECK-MAT: Checking Hand-Written Mathematical Answers for the Russian\n  Unified State Exam",
      "authors": [
        "Ruslan Khrulev"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "This paper introduces a novel benchmark, EGE-Math Solutions Assessment\nBenchmark, for evaluating Vision-Language Models (VLMs) on their ability to\nassess hand-written mathematical solutions. Unlike existing benchmarks that\nfocus on problem solving, our approach centres on understanding student\nsolutions, identifying mistakes, and assigning grades according to fixed\ncriteria. We compile 122 scanned solutions from the Russian Unified State Exam\n(EGE) together with official expert grades, and evaluate seven modern VLMs from\nGoogle, OpenAI, Arcee AI, and Alibaba Cloud in three inference modes. The\nresults reveal current limitations in mathematical reasoning and human-rubric\nalignment, opening new research avenues in AI-assisted assessment. You can find\ncode in https://github.com/Karifannaa/Auto-check-EGE-math",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2507.22958v1",
      "pdf_url": "http://arxiv.org/pdf/2507.22958v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.3,
      "datasets_score": 0.423,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a benchmark for evaluating Vision-Language Models (VLMs) on assessing handwritten mathematical solutions, but it does not mention, involve, or adapt diffusion-based models or iterative refinement processes for reasoning tasks. There is no component related to treating Chain-of-Thought as a single entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and introduction of a new benchmark and dataset (EGE-Math Solutions Assessment Benchmark) comprising 122 scanned handwritten solutions from the Russian Unified State Exam, along with expert grades. It involves dataset curation, benchmarking, and evaluation of VLMs, directly aligning with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces the EGE-Math Solutions Assessment Benchmark, a novel dataset comprising 122 scanned handwritten mathematical solutions from the Russian Unified State Exam, along with official expert grades, to evaluate Vision-Language Models (VLMs) on their ability to assess student solutions, identify errors, and assign grades based on predefined criteria. The methodology involves testing seven state-of-the-art VLMs from various companies in three inference modes, revealing significant limitations in their mathematical reasoning and alignment with human rubrics, thereby identifying new research opportunities in AI-assisted educational assessment.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark focused on assessing handwritten mathematical solutions and emulating expert grading, which advances the state-of-the-art by shifting from problem-solving to solution-evaluation tasks in AI. This addresses a critical gap in existing benchmarks, making it a significant innovation in Vision-Language Models and educational technology.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfields of AI in education and VLM evaluation, as it provides a practical case study for improving automated grading systems. However, its specialized focus on a national exam limits its broader influence across all AI applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution by highlighting key limitations in current VLMs for real-world educational assessment, making it valuable for researchers in AI and educational technology. While not essential for all audiences, it provides important insights that warrant attention from those interested in advancing AI-assisted grading.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1f4eae724e0f3398ec951538bda3dd80a1a86d44",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ruslan Khrulev",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374149515"
        }
      ]
    },
    {
      "id": "2508.00904",
      "title": "Forecasting LLM Inference Performance via Hardware-Agnostic Analytical\n  Modeling",
      "authors": [
        "Rajeev Patwari",
        "Ashish Sirasao",
        "Devleena Das"
      ],
      "categories": [
        "cs.PF (Performance)",
        "cs.AI (Artificial Intelligence)",
        "cs.AR (Hardware Architecture)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large language models (LLMs) have been increasingly deployed as local agents\non personal devices with CPUs, NPUs and integrated GPUs. However, forecasting\ninference performance on devices with such heterogeneity remains challenging\ndue to the dynamic compute and memory demands. Existing approaches rely on GPU\nbenchmarking or machine learning-based latency predictors, which are often\nhardware-specific and lack generalizability. To this end, we introduce LIFE, a\nlightweight and modular analytical framework that is comprised of modular\nanalytical model of operators, configurable to characterize LLM inference\nworkloads in a hardware and dataset-agnostic manner. LIFE characterizes the\ninfluence of software and model optimizations, such as quantization, KV cache\ncompression, LoRA adapters, chunked prefill, different attentions, and operator\nfusion, on performance metrics such as time-to-first-token (TTFT),\ntime-per-output-token (TPOT) and tokens-per-second (TPS). LIFE enables\nperformance forecasting using only hardware specifications, such as TOPS and\nmemory bandwidth, without requiring extensive dataset benchmarking. We validate\nLIFE's forecasting with inference on AMD Ryzen CPUs, NPUs, iGPUs and NVIDIA\nV100 GPUs, with Llama2-7B variants, demonstrating the utility of LIFE in\nforecasting LLM performance through lens of system efficiency to enable\nefficient LLM deployment across different hardware platforms.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2508.00904v1",
      "pdf_url": "http://arxiv.org/pdf/2508.00904v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.451,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.483,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on forecasting LLM inference performance using analytical modeling and hardware specifications, with no mention of human feedback, reward models, or reinforcement learning techniques for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses LLM inference performance and optimizations, but does not involve diffusion models, iterative refinement for logical reasoning, or multi-step Chain-of-Thought processes.",
      "distributed_training_justification": "The paper is centered on inference performance forecasting on heterogeneous hardware, not on distributed training, parallel computing for model training, or strategies for accelerating training across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.00912",
      "title": "Predictive Auditing of Hidden Tokens in LLM APIs via Reasoning Length\n  Estimation",
      "authors": [
        "Ziyao Wang",
        "Guoheng Sun",
        "Yexiao He",
        "Zheyu Shen",
        "Bowei Tian",
        "Ang Li"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Commercial LLM services often conceal internal reasoning traces while still\ncharging users for every generated token, including those from hidden\nintermediate steps, raising concerns of token inflation and potential\noverbilling. This gap underscores the urgent need for reliable token auditing,\nyet achieving it is far from straightforward: cryptographic verification (e.g.,\nhash-based signature) offers little assurance when providers control the entire\nexecution pipeline, while user-side prediction struggles with the inherent\nvariance of reasoning LLMs, where token usage fluctuates across domains and\nprompt styles. To bridge this gap, we present PALACE (Predictive Auditing of\nLLM APIs via Reasoning Token Count Estimation), a user-side framework that\nestimates hidden reasoning token counts from prompt-answer pairs without access\nto internal traces. PALACE introduces a GRPO-augmented adaptation module with a\nlightweight domain router, enabling dynamic calibration across diverse\nreasoning tasks and mitigating variance in token usage patterns. Experiments on\nmath, coding, medical, and general reasoning benchmarks show that PALACE\nachieves low relative error and strong prediction accuracy, supporting both\nfine-grained cost auditing and inflation detection. Taken together, PALACE\nrepresents an important first step toward standardized predictive auditing,\noffering a practical path to greater transparency, accountability, and user\ntrust.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2508.00912v1",
      "pdf_url": "http://arxiv.org/pdf/2508.00912v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.357,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions GRPO (Group Relative Policy Optimization) as part of its adaptation module, which involves policy optimization techniques from reinforcement learning. However, it does not involve human feedback, a reward model trained on human-ranked data, or alignment with human preferences, making it only indirectly related to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on estimating hidden token counts in LLM APIs using predictive methods, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.00913",
      "title": "TESPEC: Temporally-Enhanced Self-Supervised Pretraining for Event\n  Cameras",
      "authors": [
        "Mohammad Mohammadi",
        "Ziyi Wu",
        "Igor Gilitschenski"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Long-term temporal information is crucial for event-based perception tasks,\nas raw events only encode pixel brightness changes. Recent works show that when\ntrained from scratch, recurrent models achieve better results than feedforward\nmodels in these tasks. However, when leveraging self-supervised pre-trained\nweights, feedforward models can outperform their recurrent counterparts.\nCurrent self-supervised learning (SSL) methods for event-based pre-training\nlargely mimic RGB image-based approaches. They pre-train feedforward models on\nraw events within a short time interval, ignoring the temporal information of\nevents. In this work, we introduce TESPEC, a self-supervised pre-training\nframework tailored for learning spatio-temporal information. TESPEC is\nwell-suited for recurrent models, as it is the first framework to leverage long\nevent sequences during pre-training. TESPEC employs the masked image modeling\nparadigm with a new reconstruction target. We design a novel method to\naccumulate events into pseudo grayscale videos containing high-level semantic\ninformation about the underlying scene, which is robust to sensor noise and\nreduces motion blur. Reconstructing this target thus requires the model to\nreason about long-term history of events. Extensive experiments demonstrate our\nstate-of-the-art results in downstream tasks, including object detection,\nsemantic segmentation, and monocular depth estimation. Project webpage:\nhttps://mhdmohammadi.github.io/TESPEC_webpage.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2508.00913v1",
      "pdf_url": "http://arxiv.org/pdf/2508.00913v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.343,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.00914",
      "title": "Knowledge Editing for Multi-Hop Question Answering Using Semantic\n  Analysis",
      "authors": [
        "Dominic Simon",
        "Rickard Ewetz"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large Language Models (LLMs) require lightweight avenues of updating stored\ninformation that has fallen out of date. Knowledge Editing (KE) approaches have\nbeen successful in updating model knowledge for simple factual queries but\nstruggle with handling tasks that require compositional reasoning such as\nmulti-hop question answering (MQA). We observe that existing knowledge editors\nleverage decompositional techniques that result in illogical reasoning\nprocesses. In this paper, we propose a knowledge editor for MQA based on\nsemantic analysis called CHECK. Our framework is based on insights from an\nanalogy between compilers and reasoning using LLMs. Similar to how source code\nis first compiled before being executed, we propose to semantically analyze\nreasoning chains before executing the chains to answer questions. Reasoning\nchains with semantic errors are revised to ensure consistency through logic\noptimization and re-prompting the LLM model at a higher temperature. We\nevaluate the effectiveness of CHECK against five state-of-the-art frameworks on\nfour datasets and achieve an average 22.8% improved MQA accuracy.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2508.00914v1",
      "pdf_url": "http://arxiv.org/pdf/2508.00914v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.312,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a knowledge editing framework called CHECK, which uses semantic analysis and type checking to revise reasoning chains in LLMs for multi-hop question answering. It draws an analogy to compilers for semantic consistency checks, but it does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity over multiple steps. There is no mention of diffusion-based techniques, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.00917",
      "title": "A Survey on Deep Multi-Task Learning in Connected Autonomous Vehicles",
      "authors": [
        "Jiayuan Wang",
        "Farhad Pourpanah",
        "Q. M. Jonathan Wu",
        "Ning Zhang"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Connected autonomous vehicles (CAVs) must simultaneously perform multiple\ntasks, such as object detection, semantic segmentation, depth estimation,\ntrajectory prediction, motion prediction, and behaviour prediction, to ensure\nsafe and reliable navigation in complex environments. Vehicle-to-everything\n(V2X) communication enables cooperative driving among CAVs, thereby mitigating\nthe limitations of individual sensors, reducing occlusions, and improving\nperception over long distances. Traditionally, these tasks are addressed using\ndistinct models, which leads to high deployment costs, increased computational\noverhead, and challenges in achieving real-time performance. Multi-task\nlearning (MTL) has recently emerged as a promising solution that enables the\njoint learning of multiple tasks within a single unified model. This offers\nimproved efficiency and resource utilization. To the best of our knowledge,\nthis survey is the first comprehensive review focused on MTL in the context of\nCAVs. We begin with an overview of CAVs and MTL to provide foundational\nbackground. We then explore the application of MTL across key functional\nmodules, including perception, prediction, planning, control, and multi-agent\ncollaboration. Finally, we discuss the strengths and limitations of existing\nmethods, identify key research gaps, and provide directions for future research\naimed at advancing MTL methodologies for CAV systems.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2508.00917v1",
      "pdf_url": "http://arxiv.org/pdf/2508.00917v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.431,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper is a survey on deep multi-task learning (MTL) for connected autonomous vehicles (CAVs), focusing on applications in perception, prediction, planning, control, and multi-agent collaboration. It discusses the efficiency of MTL through shared models but does not address distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes. Therefore, it lacks any direct or indirect connection to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.03722",
      "title": "Multimodal Video Emotion Recognition with Reliable Reasoning Priors",
      "authors": [
        "Zhepeng Wang",
        "Yingjian Zhu",
        "Guanghao Dong",
        "Hongzhu Yi",
        "Feng Chen",
        "Xinming Wang",
        "Jun Xie"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study investigates the integration of trustworthy prior reasoning\nknowledge from MLLMs into multimodal emotion recognition. We employ Gemini to\ngenerate fine-grained, modality-separable reasoning traces, which are injected\nas priors during the fusion stage to enrich cross-modal interactions. To\nmitigate the pronounced class-imbalance in multimodal emotion recognition, we\nintroduce Balanced Dual-Contrastive Learning, a loss formulation that jointly\nbalances inter-class and intra-class distributions. Applied to the MER2024\nbenchmark, our prior-enhanced framework yields substantial performance gains,\ndemonstrating that the reliability of MLLM-derived reasoning can be\nsynergistically combined with the domain adaptability of lightweight fusion\nnetworks for robust, scalable emotion recognition.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2508.03722v1",
      "pdf_url": "http://arxiv.org/pdf/2508.03722v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.491,
      "distributed_training_score": 0.349,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses using MLLMs like Gemini for generating reasoning traces and chain-of-thought paradigms in multimodal emotion recognition, but it does not involve diffusion models or any iterative refinement process for reasoning. There is no mention of adapting diffusion techniques for multi-step logical tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.03723",
      "title": "Technical specification of a framework for the collection of clinical\n  images and data",
      "authors": [
        "Alistair Mackenzie",
        "Mark Halling-Brown",
        "Ruben van Engen",
        "Carlijn Roozemond",
        "Lucy Warren",
        "Dominic Ward",
        "Nadia Smith"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this report a framework for the collection of clinical images and data for\nuse when training and validating artificial intelligence (AI) tools is\ndescribed. The report contains not only information about the collection of the\nimages and clinical data, but the ethics and information governance processes\nto consider ensuring the data is collected safely, and the infrastructure and\nagreements required to allow for the sharing of data with other groups.\n  A key characteristic of the main collection framework described here is that\nit can enable automated and ongoing collection of datasets to ensure that the\ndata is up-to-date and representative of current practice. This is important in\nthe context of training and validating AI tools as it is vital that datasets\nhave a mix of older cases with long term follow-up such that the clinical\noutcome is as accurate as possible, and current data. Validations run on old\ndata will provide findings and conclusions relative to the status of the\nimaging units when that data was generated. It is important that a validation\ndataset can assess the AI tools with data that it would see if deployed and\nactive now.\n  Other types of collection frameworks, which do not follow a fully automated\napproach, are also described. Whilst the fully automated method is recommended\nfor large scale, long-term image collection, there may be reasons to start data\ncollection using semi-automated methods and indications of how to do that are\nprovided.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2508.03723v1",
      "pdf_url": "http://arxiv.org/pdf/2508.03723v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.254,
      "distributed_training_score": 0.305,
      "datasets_score": 0.422,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the description of a framework for collecting and curating clinical images and data specifically for AI training and validation. This directly aligns with the topic of datasets, as it involves creating new datasets through automated and ongoing collection methods, addressing curation methodologies (e.g., ethics, governance, and infrastructure), and emphasizing the need for up-to-date, representative datasets to ensure accurate AI evaluation. While it does not focus on benchmarking or analysis per se, its emphasis on dataset introduction and curation makes it highly relevant to research on datasets for machine learning and AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper outlines a technical framework for the automated and ongoing collection of clinical images and data specifically for training and validating AI tools, emphasizing the integration of ethical considerations, information governance, and data sharing protocols to ensure safe and representative datasets. It describes methodologies for automated collection to maintain up-to-date data, including a mix of historical cases with long-term follow-up and current data for accurate AI validation, while also providing alternatives for semi-automated approaches suitable for initial or smaller-scale implementations.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of automated data collection techniques with ethical and governance processes for clinical AI applications, offering a notable improvement on existing methods rather than introducing a entirely new problem or technique. While it addresses a known challenge in data collection, its integration of ongoing automation for representativeness adds practical value without groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of clinical image processing and AI validation due to its practical guidelines for data collection and ethics. However, its influence may be limited to specific applications in healthcare AI rather than broadly transforming the field.",
      "recommendation_score": "Can Skip",
      "recommendation_justification": "The paper provides useful practical insights for those involved in clinical data collection for AI, but its incremental contributions make it non-essential for most readers outside of niche areas. It is more of a technical specification than a transformative work, suitable only for targeted audiences.",
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.03724",
      "title": "From Waveforms to Pixels: A Survey on Audio-Visual Segmentation",
      "authors": [
        "Jia Li",
        "Yapeng Tian"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Audio-Visual Segmentation (AVS) aims to identify and segment sound-producing\nobjects in videos by leveraging both visual and audio modalities. It has\nemerged as a significant research area in multimodal perception, enabling\nfine-grained object-level understanding. In this survey, we present a\ncomprehensive overview of the AVS field, covering its problem formulation,\nbenchmark datasets, evaluation metrics, and the progression of methodologies.\nWe analyze a wide range of approaches, including architectures for unimodal and\nmultimodal encoding, key strategies for audio-visual fusion, and various\ndecoder designs. Furthermore, we examine major training paradigms, from fully\nsupervised learning to weakly supervised and training-free methods. Notably, we\nprovide an extensive comparison of AVS methods across standard benchmarks,\nhighlighting the impact of different architectural choices, fusion strategies,\nand training paradigms on performance. Finally, we outline the current\nchallenges, such as limited temporal modeling, modality bias toward vision,\nlack of robustness in complex environments, and high computational demands, and\npropose promising future directions, including improving temporal reasoning and\nmultimodal fusion, leveraging foundation models for better generalization and\nfew-shot learning, reducing reliance on labeled data through selfand weakly\nsupervised learning, and incorporating higher-level reasoning for more\nintelligent AVS systems.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2508.03724v1",
      "pdf_url": "http://arxiv.org/pdf/2508.03724v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.321,
      "distributed_training_score": 0.289,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.05654",
      "title": "Comparison of Information Retrieval Techniques Applied to IT Support\n  Tickets",
      "authors": [
        "Leonardo Santiago Benitez Pereira",
        "Robinson Pizzio",
        "Samir Bonho"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Institutions dependent on IT services and resources acknowledge the crucial\nsignificance of an IT help desk system, that act as a centralized hub\nconnecting IT staff and users for service requests. Employing various Machine\nLearning models, these IT help desk systems allow access to corrective actions\nused in the past, but each model has different performance when applied to\ndifferent datasets. This work compares eleven Information Retrieval techniques\nin a dataset of IT support tickets, with the goal of implementing a software\nthat facilitates the work of Information Technology support analysts. The best\nresults were obtained with the Sentence-BERT technique, in its multi-language\nvariation distilluse-base-multilingual-cased-v1, where 78.7% of the\nrecommendations made by the model were considered relevant. TF-IDF (69.0%),\nWord2vec (68.7%) and LDA (66.3%) techniques also had consistent results.\nFurthermore, the used datasets and essential parts of coding have been\npublished and made open source. It also demonstrated the practicality of a\nsupport ticket recovery system by implementing a minimal viable prototype, and\ndescribed in detail the implementation of the system. Finally, this work\nproposed a novel metric for comparing the techniques, whose aim is to closely\nreflect the perception of the IT analysts about the retrieval quality.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2508.05654v1",
      "pdf_url": "http://arxiv.org/pdf/2508.05654v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.32,
      "datasets_score": 0.412,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper uses a proprietary dataset of IT support tickets for evaluating Information Retrieval techniques, which involves benchmarking and analysis in the context of machine learning applications. It also makes the dataset open source, contributing to dataset sharing. However, the primary focus is on comparing IR techniques rather than creating, curating, or deeply analyzing datasets, so it is not the main contribution.",
      "llm_score_status": "completed",
      "summary": "This paper compares eleven information retrieval techniques on a dataset of IT support tickets from Skaylink to identify the most effective method for recommending similar past tickets to assist IT analysts. The methodology involves applying models such as Sentence-BERT, which achieved the highest performance at 78.7% relevant recommendations, alongside TF-IDF (69.0%), Word2vec (68.7%), and LDA (66.3%), while proposing a novel metric that better reflects analysts' perceptions, making the dataset and code open source, and demonstrating a prototype for practical implementation.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by comparing a larger number of IR techniques than previous works and proposing a new metric tailored to IT analysts' perceptions, though it builds on existing methods rather than introducing a entirely new problem or architecture. This clever combination and expansion enhance the application of known techniques in a specific context without revolutionizing the field.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of information retrieval for IT support systems, given the open-sourcing of datasets and code, which could facilitate further research and practical applications. However, its influence may remain limited to niche areas rather than broadly affecting commercial or academic fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with practical insights, a new metric, and open resources that could benefit researchers and practitioners in IT support and information retrieval. It is valuable for those in the field but not essential for a general audience.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/999b58c5a189e3934f011d03cb8a78d27c7b4412",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 3,
      "average_h_index": 2.3333333333333335,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Leonardo Santiago Benitez Pereira",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2150191778"
        },
        {
          "name": "Robinson Pizzio",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2098952781"
        },
        {
          "name": "Samir Bonho",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/49784684"
        }
      ]
    },
    {
      "id": "2508.06511",
      "title": "DiTalker: A Unified DiT-based Framework for High-Quality and Speaking\n  Styles Controllable Portrait Animation",
      "authors": [
        "He Feng",
        "Yongjia Ma",
        "Donglin Di",
        "Lei Fan",
        "Tonghua Su",
        "Xiangqian Wu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Portrait animation aims to synthesize talking videos from a static reference\nface, conditioned on audio and style frame cues (e.g., emotion and head poses),\nwhile ensuring precise lip synchronization and faithful reproduction of\nspeaking styles. Existing diffusion-based portrait animation methods primarily\nfocus on lip synchronization or static emotion transformation, often\noverlooking dynamic styles such as head movements. Moreover, most of these\nmethods rely on a dual U-Net architecture, which preserves identity consistency\nbut incurs additional computational overhead. To this end, we propose DiTalker,\na unified DiT-based framework for speaking style-controllable portrait\nanimation. We design a Style-Emotion Encoding Module that employs two separate\nbranches: a style branch extracting identity-specific style information (e.g.,\nhead poses and movements), and an emotion branch extracting identity-agnostic\nemotion features. We further introduce an Audio-Style Fusion Module that\ndecouples audio and speaking styles via two parallel cross-attention layers,\nusing these features to guide the animation process. To enhance the quality of\nresults, we adopt and modify two optimization constraints: one to improve lip\nsynchronization and the other to preserve fine-grained identity and background\ndetails. Extensive experiments demonstrate the superiority of DiTalker in terms\nof lip synchronization and speaking style controllability. Project Page:\nhttps://thenameishope.github.io/DiTalker/",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2508.06511v1",
      "pdf_url": "http://arxiv.org/pdf/2508.06511v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.289,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.324,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using Diffusion Transformers (DiT) for portrait animation, specifically for generating talking face videos with lip synchronization and style control. It employs diffusion models for iterative refinement in image/video synthesis, but this is limited to generative tasks and does not involve adapting the process for complex logical tasks, such as treating a Chain-of-Thought as a single entity for holistic correction. There is no evidence of multi-step logical reasoning components in the framework.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.08269",
      "title": "emg2tendon: From sEMG Signals to Tendon Control in Musculoskeletal Hands",
      "authors": [
        "Sagar Verma"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Tendon-driven robotic hands offer unparalleled dexterity for manipulation\ntasks, but learning control policies for such systems presents unique\nchallenges. Unlike joint-actuated robotic hands, tendon-driven systems lack a\ndirect one-to-one mapping between motion capture (mocap) data and tendon\ncontrols, making the learning process complex and expensive. Additionally,\nvisual tracking methods for real-world applications are prone to occlusions and\ninaccuracies, further complicating joint tracking. Wrist-wearable surface\nelectromyography (sEMG) sensors present an inexpensive, robust alternative to\ncapture hand motion. However, mapping sEMG signals to tendon control remains a\nsignificant challenge despite the availability of EMG-to-pose data sets and\nregression-based models in the existing literature.\n  We introduce the first large-scale EMG-to-Tendon Control dataset for robotic\nhands, extending the emg2pose dataset, which includes recordings from 193\nsubjects, spanning 370 hours and 29 stages with diverse gestures. This dataset\nincorporates tendon control signals derived using the MyoSuite MyoHand model,\naddressing limitations such as invalid poses in prior methods. We provide three\nbaseline regression models to demonstrate emg2tendon utility and propose a\nnovel diffusion-based regression model for predicting tendon control from sEMG\nrecordings. This dataset and modeling framework marks a significant step\nforward for tendon-driven dexterous robotic manipulation, laying the groundwork\nfor scalable and accurate tendon control in robotic hands.\nhttps://emg2tendon.github.io/",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2508.08269v1",
      "pdf_url": "http://arxiv.org/pdf/2508.08269v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.341,
      "datasets_score": 0.397,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.09142",
      "title": "Bayesian-Driven Graph Reasoning for Active Radio Map Construction",
      "authors": [
        "Wenlihan Lu",
        "Shijian Gao",
        "Miaowen Wen",
        "Yuxuan Liang",
        "Chan-Byoung Chae",
        "H. Vincent Poor"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "With the emergence of the low-altitude economy, radio maps have become\nessential for ensuring reliable wireless connectivity to aerial platforms.\nAutonomous aerial agents are commonly deployed for data collection using\nwaypoint-based navigation; however, their limited battery capacity\nsignificantly constrains coverage and efficiency. To address this, we propose\nan uncertainty-aware radio map (URAM) reconstruction framework that explicitly\nleverages graph-based reasoning tailored for waypoint navigation. Our approach\nintegrates two key deep learning components: (1) a Bayesian neural network that\nestimates spatial uncertainty in real time, and (2) an attention-based\nreinforcement learning policy that performs global reasoning over a\nprobabilistic roadmap, using uncertainty estimates to plan informative and\nenergy-efficient trajectories. This graph-based reasoning enables intelligent,\nnon-myopic trajectory planning, guiding agents toward the most informative\nregions while satisfying safety constraints. Experimental results show that\nURAM improves reconstruction accuracy by up to 34% over existing baselines.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2508.09142v1",
      "pdf_url": "http://arxiv.org/pdf/2508.09142v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.337,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves Bayesian neural networks and reinforcement learning for graph-based trajectory planning in radio map construction, focusing on uncertainty estimation and efficient navigation. It does not incorporate diffusion models, iterative refinement processes for logical tasks, or any adaptation of diffusion to handle Chain-of-Thought reasoning. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.09992",
      "title": "OpenFPL: An open-source forecasting method rivaling state-of-the-art\n  Fantasy Premier League services",
      "authors": [
        "Daniel Groos"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Fantasy Premier League engages the football community in selecting the\nPremier League players who will perform best from gameweek to gameweek. Access\nto accurate performance forecasts gives participants an edge over competitors\nby guiding expectations about player outcomes and reducing uncertainty in squad\nselection. However, high-accuracy forecasts are currently limited to commercial\nservices whose inner workings are undisclosed and that rely on proprietary\ndata. This paper aims to democratize access to highly accurate forecasts of\nplayer performance by presenting OpenFPL, an open-source Fantasy Premier League\nforecasting method developed exclusively from public data. Comprising\nposition-specific ensemble models optimized on Fantasy Premier League and\nUnderstat data from four previous seasons (2020-21 to 2023-24), OpenFPL\nachieves accuracy comparable to a leading commercial service when tested\nprospectively on data from the 2024-25 season. OpenFPL also surpasses the\ncommercial benchmark for high-return players ($>$ 2 points), which are most\ninfluential for rank gains. These findings hold across one-, two-, and\nthree-gameweek forecast horizons, supporting long-term planning of transfers\nand strategies while also informing final-day decisions.",
      "published_date": "2025-07-29",
      "arxiv_url": "http://arxiv.org/abs/2508.09992v1",
      "pdf_url": "http://arxiv.org/pdf/2508.09992v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.309,
      "distributed_training_score": 0.331,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    }
  ],
  "total_papers": 216,
  "date": "2025-07-29"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Relevance Filter State Management
        let currentRelevanceFilters = {
            highlyRelevant: true,
            moderatelyRelevant: true,
            tangentiallyRelevant: true,
            notRelevant: true
        };
        
        let pendingRelevanceFilters = { ...currentRelevanceFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function calculateRelevanceScore(paper) {
            let score = 0;
            
            // Only consider topics that are currently selected/enabled in the topic filter
            const topicsToConsider = [];
            if (currentTopicFilters.rlhf) topicsToConsider.push('rlhf_relevance');
            if (currentTopicFilters.weakSupervision) topicsToConsider.push('weak_supervision_relevance');
            if (currentTopicFilters.diffusionReasoning) topicsToConsider.push('diffusion_reasoning_relevance');
            if (currentTopicFilters.distributedTraining) topicsToConsider.push('distributed_training_relevance');
            if (currentTopicFilters.datasets) topicsToConsider.push('datasets_relevance');
            
            // If no topics are selected, return 0
            if (topicsToConsider.length === 0) return 0;
            
            // Weighted scoring system
            const relevanceWeights = {
                'Highly Relevant': 4,
                'Moderately Relevant': 3,
                'Tangentially Relevant': 2,
                'Not Relevant': 1
            };
            
            // Sum up scores for selected topics only
            for (let topicField of topicsToConsider) {
                const relevance = paper[topicField];
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = relevance === "not_validated" ? "Not Relevant" : relevance;
                score += relevanceWeights[normalizedRelevance] || 1; // Default to 1 if unknown
            }
            
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'relevance_high':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(b) - calculateRelevanceScore(a));
                    break;
                case 'relevance_low':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(a) - calculateRelevanceScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'relevance_high': 'Relevance (Highest to Lowest)',
                'relevance_low': 'Relevance (Lowest to Highest)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
            updateNoveltyButtonText();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
            updateImpactButtonText();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for relevance checkboxes to sync between mobile and desktop
            document.getElementById('mobile-relevance-highly').addEventListener('change', () => {
                document.getElementById('desktop-relevance-highly').checked = 
                    document.getElementById('mobile-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-moderately').addEventListener('change', () => {
                document.getElementById('desktop-relevance-moderately').checked = 
                    document.getElementById('mobile-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('desktop-relevance-tangentially').checked = 
                    document.getElementById('mobile-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-not').addEventListener('change', () => {
                document.getElementById('desktop-relevance-not').checked = 
                    document.getElementById('mobile-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-highly').addEventListener('change', () => {
                document.getElementById('mobile-relevance-highly').checked = 
                    document.getElementById('desktop-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-moderately').addEventListener('change', () => {
                document.getElementById('mobile-relevance-moderately').checked = 
                    document.getElementById('desktop-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('mobile-relevance-tangentially').checked = 
                    document.getElementById('desktop-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-not').addEventListener('change', () => {
                document.getElementById('mobile-relevance-not').checked = 
                    document.getElementById('desktop-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function resetPendingScoringFilters() {
            pendingScoringFilters = { ...currentScoringFilters };
            syncPendingScoringUI();
            updateScoringButtonText();
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function resetPendingRecommendationFilters() {
            pendingRecommendationFilters = { ...currentRecommendationFilters };
            syncPendingRecommendationUI();
            updateRecommendationButtonText();
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
            updateTopicButtonText();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // RELEVANCE FILTER FUNCTIONS  
        // ============================================================================
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = pendingRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = pendingRelevanceFilters.notRelevant;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = currentRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = currentRelevanceFilters.notRelevant;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            // Read the current checkbox states from the UI (like H-Index filter does)
            const highlyRelevantChecked = document.getElementById('mobile-relevance-highly').checked;
            const moderatelyRelevantChecked = document.getElementById('mobile-relevance-moderately').checked;
            const tangentiallyRelevantChecked = document.getElementById('mobile-relevance-tangentially').checked;
            const notRelevantChecked = document.getElementById('mobile-relevance-not').checked;
            
            const selectedOptions = [];
            if (highlyRelevantChecked) selectedOptions.push("Highly Relevant");
            if (moderatelyRelevantChecked) selectedOptions.push("Moderately Relevant");
            if (tangentiallyRelevantChecked) selectedOptions.push("Tangentially Relevant");
            if (notRelevantChecked) selectedOptions.push("Not Relevant");
            
            const selectionText = selectedOptions.length === 4 ? "All Selected" : 
                                selectedOptions.length === 0 ? "None Selected" : 
                                `${selectedOptions.length} Selected`;
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            // Get values from mobile (primary source)
            const mobileHighly = document.getElementById('mobile-relevance-highly');
            const mobileModerately = document.getElementById('mobile-relevance-moderately');
            const mobileTangentially = document.getElementById('mobile-relevance-tangentially');
            const mobileNot = document.getElementById('mobile-relevance-not');
            
            // Update pending filters from mobile if available, otherwise from desktop
            pendingRelevanceFilters.highlyRelevant = mobileHighly ? mobileHighly.checked : document.getElementById('desktop-relevance-highly').checked;
            pendingRelevanceFilters.moderatelyRelevant = mobileModerately ? mobileModerately.checked : document.getElementById('desktop-relevance-moderately').checked;
            pendingRelevanceFilters.tangentiallyRelevant = mobileTangentially ? mobileTangentially.checked : document.getElementById('desktop-relevance-tangentially').checked;
            pendingRelevanceFilters.notRelevant = mobileNot ? mobileNot.checked : document.getElementById('desktop-relevance-not').checked;
        }
        
        function resetPendingRelevanceFilters() {
            pendingRelevanceFilters = { ...currentRelevanceFilters };
            syncPendingRelevanceUI();
            updateRelevanceButtonText();
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_highly', currentRelevanceFilters.highlyRelevant.toString());
            params.set('relevance_moderately', currentRelevanceFilters.moderatelyRelevant.toString());
            params.set('relevance_tangentially', currentRelevanceFilters.tangentiallyRelevant.toString());
            params.set('relevance_not', currentRelevanceFilters.notRelevant.toString());
            window.history.replaceState({}, '', `${window.location.pathname}?${params.toString()}`);
        }
        
        function loadRelevanceFiltersFromURL() {
            const highlyRelevant = getUrlParameter('relevance_highly');
            const moderatelyRelevant = getUrlParameter('relevance_moderately');
            const tangentiallyRelevant = getUrlParameter('relevance_tangentially');
            const notRelevant = getUrlParameter('relevance_not');
            
            if (highlyRelevant !== null) {
                currentRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
                pendingRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
            }
            if (moderatelyRelevant !== null) {
                currentRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
                pendingRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
            }
            if (tangentiallyRelevant !== null) {
                currentRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
                pendingRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
            }
            if (notRelevant !== null) {
                currentRelevanceFilters.notRelevant = notRelevant === 'true';
                pendingRelevanceFilters.notRelevant = notRelevant === 'true';
            }
            
            syncRelevanceUI();
        }
        
        function passesRelevanceFilter(paper) {
            // Get selected topics
            const selectedTopics = [];
            if (currentTopicFilters.rlhf) selectedTopics.push('rlhf');
            if (currentTopicFilters.weakSupervision) selectedTopics.push('weak_supervision');
            if (currentTopicFilters.diffusionReasoning) selectedTopics.push('diffusion_reasoning');
            if (currentTopicFilters.distributedTraining) selectedTopics.push('distributed_training');
            if (currentTopicFilters.datasets) selectedTopics.push('datasets');
            
            // If no topics selected, skip relevance filtering
            if (selectedTopics.length === 0) return true;
            
            // Get selected relevance levels
            const selectedRelevanceLevels = [];
            if (currentRelevanceFilters.highlyRelevant) selectedRelevanceLevels.push('Highly Relevant');
            if (currentRelevanceFilters.moderatelyRelevant) selectedRelevanceLevels.push('Moderately Relevant');
            if (currentRelevanceFilters.tangentiallyRelevant) selectedRelevanceLevels.push('Tangentially Relevant');
            if (currentRelevanceFilters.notRelevant) selectedRelevanceLevels.push('Not Relevant');
            
            // If no relevance levels selected, show 0 papers
            if (selectedRelevanceLevels.length === 0) return false;
            
            // Check each selected topic
            for (let topic of selectedTopics) {
                const relevanceField = `${topic}_relevance`;
                const paperRelevance = paper[relevanceField];
                
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = paperRelevance === "not_validated" ? "Not Relevant" : paperRelevance;
                
                // If this topic's relevance matches any selected relevance level, paper passes
                if (selectedRelevanceLevels.includes(normalizedRelevance)) {
                    return true; // At least one topic matches
                }
            }
            
            // No selected topics had matching relevance levels
            return false;
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
            
            // Update Relevance
            updateDropdownDisabledState('mobile-relevance-btn', 'mobile-relevance-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-relevance-btn', 'desktop-relevance-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            // Check if there are no papers to display
            if (filteredSortedPapers.length === 0) {
                showNoPapersMessage();
                hidePaginationSections();
                return;
            }
            
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            showPaginationSections();
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        function showNoPapersMessage() {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const noPapersHTML = '<div class="flex items-center justify-center min-h-screen"><h2 class="font-heading text-2xl text-neutral-600">No papers to show</h2></div>';
            
            if (mobileContainer) {
                mobileContainer.innerHTML = noPapersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = noPapersHTML;
            }
        }

        function hidePaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = 'none';
                }
            });
        }

        function showPaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = '';
                }
            });
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            // Find the similarity container in the currently visible layout
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            // Find the justification container in the currently visible layout
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            // Find the normalized similarity container in the currently visible layout
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // QUICK FILTER FUNCTIONS
        // ============================================================================


        function applyQuickFilter(filterType) {
            // Build URL parameters based on filter type
            const url = new URL(window.location.href);
            
            // Clear all existing parameters
            url.search = '';
            
            // Set sort to recommendation best first for all filters
            url.searchParams.set('sort', 'recommend_best');
            
            switch(filterType) {
                case 'must-read':
                    // Recommendation: Only Must Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'true');
                    url.searchParams.set('recommendation_should', 'false');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'should-read':
                    // Recommendation: Only Should Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'false');
                    url.searchParams.set('recommendation_should', 'true');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'rlhf':
                    // Topic: Only RLHF + Relevance: Exclude Not Relevant
                    url.searchParams.set('topic_rlhf', 'true');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'weak-supervision':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'true');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'diffusion-reasoning':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'true');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'distributed-training':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'true');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'datasets':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'true');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'reset':
                    // Clear all parameters, which will reset everything to defaults
                    break;
                    
                default:
                    console.warn('Unknown quick filter type:', filterType);
                    return;
            }
            
            // Navigate to new URL, which will trigger existing URL parsing logic
            window.location.href = url.toString();
        }



        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Load Relevance filters from URL
            loadRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-relevance-dropdown'), button: document.getElementById('mobile-relevance-btn') },
                { dropdown: document.getElementById('desktop-relevance-dropdown'), button: document.getElementById('desktop-relevance-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('relevance')) {
                            resetPendingRelevanceFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        } else if (dropdown.id.includes('recommendation')) {
                            resetPendingRecommendationFilters();
                        } else if (dropdown.id.includes('scoring')) {
                            resetPendingScoringFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
