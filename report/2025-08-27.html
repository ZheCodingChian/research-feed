<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 27 August 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: 'rgba(107, 106, 101, 0.7)' // #6B6A65 with 70% opacity
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 27 August 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100 min-h-screen">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="mobile-quick-must-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="mobile-quick-should-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="mobile-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="mobile-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="mobile-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="mobile-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="mobile-quick-datasets" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="mobile-quick-reset" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-highly" checked>
                                            <label for="mobile-relevance-highly"></label>
                                        </div>
                                        <label for="mobile-relevance-highly" class="text-neutral-10 text-xl font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-moderately" checked>
                                            <label for="mobile-relevance-moderately"></label>
                                        </div>
                                        <label for="mobile-relevance-moderately" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-tangentially" checked>
                                            <label for="mobile-relevance-tangentially"></label>
                                        </div>
                                        <label for="mobile-relevance-tangentially" class="text-neutral-10 text-xl font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-not" checked>
                                            <label for="mobile-relevance-not"></label>
                                        </div>
                                        <label for="mobile-relevance-not" class="text-neutral-10 text-xl font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="desktop-quick-must-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="desktop-quick-should-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="desktop-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="desktop-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="desktop-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="desktop-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="desktop-quick-datasets" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="desktop-quick-reset" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-highly" checked>
                                            <label for="desktop-relevance-highly"></label>
                                        </div>
                                        <label for="desktop-relevance-highly" class="text-neutral-10 text-lg font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-moderately" checked>
                                            <label for="desktop-relevance-moderately"></label>
                                        </div>
                                        <label for="desktop-relevance-moderately" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-tangentially" checked>
                                            <label for="desktop-relevance-tangentially"></label>
                                        </div>
                                        <label for="desktop-relevance-tangentially" class="text-neutral-10 text-lg font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-not" checked>
                                            <label for="desktop-relevance-not"></label>
                                        </div>
                                        <label for="desktop-relevance-not" class="text-neutral-10 text-lg font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 27 August 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl min-h-screen">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2508.19485",
      "title": "JVLGS: Joint Vision-Language Gas Leak Segmentation",
      "authors": [
        "Xinlong Zhao",
        "Qixiang Pang",
        "Shan Du"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Gas leaks pose serious threats to human health and contribute significantly\nto atmospheric pollution, drawing increasing public concern. However, the lack\nof effective detection methods hampers timely and accurate identification of\ngas leaks. While some vision-based techniques leverage infrared videos for leak\ndetection, the blurry and non-rigid nature of gas clouds often limits their\neffectiveness. To address these challenges, we propose a novel framework called\nJoint Vision-Language Gas leak Segmentation (JVLGS), which integrates the\ncomplementary strengths of visual and textual modalities to enhance gas leak\nrepresentation and segmentation. Recognizing that gas leaks are sporadic and\nmany video frames may contain no leak at all, our method incorporates a\npost-processing step to reduce false positives caused by noise and non-target\nobjects, an issue that affects many existing approaches. Extensive experiments\nconducted across diverse scenarios show that JVLGS significantly outperforms\nstate-of-the-art gas leak segmentation methods. We evaluate our model under\nboth supervised and few-shot learning settings, and it consistently achieves\nstrong performance in both, whereas competing methods tend to perform well in\nonly one setting or poorly in both. Code available at:\nhttps://github.com/GeekEagle/JVLGS",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19485v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19485v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.331,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a vision-language framework for gas leak segmentation, focusing on integrating visual and textual modalities for detection tasks. It does not mention or utilize diffusion models, iterative refinement processes, or any form of multi-step logical reasoning as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19487",
      "title": "Data-Efficient Symbolic Regression via Foundation Model Distillation",
      "authors": [
        "Wangyang Ying",
        "Jinghan Zhang",
        "Haoyue Bai",
        "Nanxu Gong",
        "Xinyuan Wang",
        "Kunpeng Liu",
        "Chandan K. Reddy",
        "Yanjie Fu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Discovering interpretable mathematical equations from observed data (a.k.a.\nequation discovery or symbolic regression) is a cornerstone of scientific\ndiscovery, enabling transparent modeling of physical, biological, and economic\nsystems. While foundation models pre-trained on large-scale equation datasets\noffer a promising starting point, they often suffer from negative transfer and\npoor generalization when applied to small, domain-specific datasets. In this\npaper, we introduce EQUATE (Equation Generation via QUality-Aligned Transfer\nEmbeddings), a data-efficient fine-tuning framework that adapts foundation\nmodels for symbolic equation discovery in low-data regimes via distillation.\nEQUATE combines symbolic-numeric alignment with evaluator-guided embedding\noptimization, enabling a principled embedding-search-generation paradigm. Our\napproach reformulates discrete equation search as a continuous optimization\ntask in a shared embedding space, guided by data-equation fitness and\nsimplicity. Experiments across three standard public benchmarks (Feynman,\nStrogatz, and black-box datasets) demonstrate that EQUATE consistently\noutperforms state-of-the-art baselines in both accuracy and robustness, while\npreserving low complexity and fast inference. These results highlight EQUATE as\na practical and generalizable solution for data-efficient symbolic regression\nin foundation model distillation settings.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19487v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19487v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.37,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on data-efficient symbolic regression using foundation model distillation, involving fine-tuning, embedding optimization, and symbolic-numeric alignment. It does not involve diffusion models or adapt an iterative refinement process for multi-step logical reasoning as defined in the topic. There is no mention of diffusion-based techniques, making the paper unrelated to this area.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19488",
      "title": "PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for\n  Cyber Defense",
      "authors": [
        "Xavier Cadet",
        "Simona Boboila",
        "Sie Hendrata Dharmawan",
        "Alina Oprea",
        "Peter Chin"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Cyber defense requires automating defensive decision-making under stealthy,\ndeceptive, and continuously evolving adversarial strategies. The FlipIt game\nprovides a foundational framework for modeling interactions between a defender\nand an advanced adversary that compromises a system without being immediately\ndetected. In FlipIt, the attacker and defender compete to control a shared\nresource by performing a Flip action and paying a cost. However, the existing\nFlipIt frameworks rely on a small number of heuristics or specialized learning\ntechniques, which can lead to brittleness and the inability to adapt to new\nattacks. To address these limitations, we introduce PoolFlip, a multi-agent gym\nenvironment that extends the FlipIt game to allow efficient learning for\nattackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent\nreinforcement learning (MARL) approach that leverages population-based training\nto train defender agents equipped to generalize against a range of unknown,\npotentially adaptive opponents. Our empirical results suggest that Flip-PSRO\ndefenders are $2\\times$ more effective than baselines to generalize to a\nheuristic attack not exposed in training. In addition, our newly designed\nownership-based utility functions ensure that Flip-PSRO defenders maintain a\nhigh level of control while optimizing performance.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19488v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19488v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.418,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.311,
      "distributed_training_score": 0.351,
      "datasets_score": 0.255,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces PoolFlip, a multi-agent reinforcement learning environment for cyber defense, and proposes Flip-PSRO, a MARL method using techniques like PPO for training agents in simulated games. It does not involve human feedback, such as training a reward model on human-ranked data or fine-tuning based on human preferences, which are core to RLHF. Therefore, the paper's contributions are unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19493",
      "title": "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered\n  Smartphone Agents",
      "authors": [
        "Zhixin Lin",
        "Jungang Li",
        "Shidong Pan",
        "Yibo Shi",
        "Yue Yao",
        "Dongliang Xu"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Smartphones bring significant convenience to users but also enable devices to\nextensively record various types of personal information. Existing smartphone\nagents powered by Multimodal Large Language Models (MLLMs) have achieved\nremarkable performance in automating different tasks. However, as the cost,\nthese agents are granted substantial access to sensitive users' personal\ninformation during this operation. To gain a thorough understanding of the\nprivacy awareness of these agents, we present the first large-scale benchmark\nencompassing 7,138 scenarios to the best of our knowledge. In addition, for\nprivacy context in scenarios, we annotate its type (e.g., Account Credentials),\nsensitivity level, and location. We then carefully benchmark seven available\nmainstream smartphone agents. Our results demonstrate that almost all\nbenchmarked agents show unsatisfying privacy awareness (RA), with performance\nremaining below 60% even with explicit hints. Overall, closed-source agents\nshow better privacy ability than open-source ones, and Gemini 2.0-flash\nachieves the best, achieving an RA of 67%. We also find that the agents'\nprivacy detection capability is highly related to scenario sensitivity level,\ni.e., the scenario with a higher sensitivity level is typically more\nidentifiable. We hope the findings enlighten the research community to rethink\nthe unbalanced utility-privacy tradeoff about smartphone agents. Our code and\nbenchmark are available at https://zhixin-l.github.io/SAPA-Bench.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19493v2",
      "pdf_url": "http://arxiv.org/pdf/2508.19493v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.322,
      "distributed_training_score": 0.303,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19498",
      "title": "UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained\n  Models",
      "authors": [
        "Yimu Wang",
        "Weiming Zhuang",
        "Chen Chen",
        "Jiabo Huang",
        "Jingtao Li",
        "Lingjuan Lyu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In the era of deep learning, the increasing number of pre-trained models\navailable online presents a wealth of knowledge. These models, developed with\ndiverse architectures and trained on varied datasets for different tasks,\nprovide unique interpretations of the real world. Their collective consensus is\nlikely universal and generalizable to unseen data. However, effectively\nharnessing this collective knowledge poses a fundamental challenge due to the\nheterogeneity of pre-trained models. Existing knowledge integration solutions\ntypically rely on strong assumptions about training data distributions and\nnetwork architectures, limiting them to learning only from specific types of\nmodels and resulting in data and/or inductive biases. In this work, we\nintroduce a novel framework, namely UNIFORM, for knowledge transfer from a\ndiverse set of off-the-shelf models into one student model without such\nconstraints. Specifically, we propose a dedicated voting mechanism to capture\nthe consensus of knowledge both at the logit level -- incorporating teacher\nmodels that are capable of predicting target classes of interest -- and at the\nfeature level, utilizing visual representations learned on arbitrary label\nspaces. Extensive experiments demonstrate that UNIFORM effectively enhances\nunsupervised object recognition performance compared to strong knowledge\ntransfer baselines. Notably, it exhibits remarkable scalability by benefiting\nfrom over one hundred teachers, while existing methods saturate at a much\nsmaller scale.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19498v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19498v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.43,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.403,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using diverse pre-trained models to generate consensus-based pseudo-labels for training a student model, aligning directly with weak supervision. It programmatically derives supervision from noisy, high-level sources (teacher models) rather than relying on precise hand-labeled data, enhancing unsupervised object recognition.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on knowledge transfer and integration from multiple pre-trained models, but it does not address distributed training techniques, such as parallel computing, data partitioning, or multi-node setups for accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces UNIFORM, a framework designed to unify knowledge from a diverse array of pre-trained models to enhance unsupervised object recognition in a student model. By employing dedicated voting mechanisms at both the logit and feature levels, UNIFORM captures the collective consensus from predictive teachers (which align with target classes) and descriptive teachers (which provide visual features), thereby mitigating biases and enabling scalable knowledge transfer without constraints on architectures or data distributions. Experimental results demonstrate that UNIFORM outperforms existing knowledge distillation baselines on 11 benchmark datasets, scales effectively with over 100 teachers, and sometimes even surpasses the performance of individual teachers.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly innovative voting mechanism for knowledge transfer that handles diverse pre-trained models without restrictive assumptions, significantly advancing the state-of-the-art in knowledge distillation by enabling scalable integration from heterogeneous sources.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfields of computer vision and machine learning due to its practical approach for leveraging large-scale model zoos, though its influence may be confined to specific applications in knowledge transfer.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality, innovative framework with strong empirical results that offers valuable advancements in unsupervised learning, making it essential for researchers focused on knowledge distillation and model ensembling in AI.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d153d0bf57b584c0151c892862b04906f297a116",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 12,
      "average_h_index": 5.166666666666667,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Yimu Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377627065"
        },
        {
          "name": "Weiming Zhuang",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/1900312976"
        },
        {
          "name": "Chen Chen",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2288619145"
        },
        {
          "name": "Jiabo Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2355244697"
        },
        {
          "name": "Jingtao Li",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2294307563"
        },
        {
          "name": "Lingjuan Lyu",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2287820224"
        }
      ]
    },
    {
      "id": "2508.19499",
      "title": "Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow\n  Generation from Satellite Imagery",
      "authors": [
        "Xiangxu Wang",
        "Tianhong Zhao",
        "Wei Tu",
        "Bowen Zhang",
        "Guanzhou Chen",
        "Jinzhou Cao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Origin-Destination (OD) flow matrices are essential for urban mobility\nanalysis, underpinning applications in traffic forecasting, infrastructure\nplanning, and policy design. However, existing methods suffer from two critical\nlimitations: (1) reliance on auxiliary features (e.g., Points of Interest,\nsocioeconomic statistics) that are costly to collect and have limited spatial\ncoverage; and (2) sensitivity to spatial topology, where minor index reordering\nof urban regions (e.g., census tract relabeling) disrupts structural coherence\nin generated flows. To address these challenges, we propose Sat2Flow, a latent\nstructure-aware diffusion-based framework that generates structurally coherent\nOD flows using solely satellite imagery as input. Our approach introduces a\nmulti-kernel encoder to capture diverse regional interactions and employs a\npermutation-aware diffusion process that aligns latent representations across\ndifferent regional orderings. Through a joint contrastive training objective\nthat bridges satellite-derived features with OD patterns, combined with\nequivariant diffusion training that enforces structural consistency, Sat2Flow\nensures topological robustness under arbitrary regional reindexing.\nExperimental results on real-world urban datasets demonstrate that Sat2Flow\noutperforms both physics-based and data-driven baselines in numerical accuracy\nwhile preserving empirical distributions and spatial structures under index\npermutations. Sat2Flow offers a globally scalable solution for OD flow\ngeneration in data-scarce urban environments, eliminating region-specific\nauxiliary data dependencies while maintaining structural invariance for robust\nmobility modeling.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19499v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19499v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.306,
      "diffusion_reasoning_score": 0.505,
      "distributed_training_score": 0.332,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes a diffusion-based framework for generating OD flows from satellite imagery, involving iterative refinement in a denoising process. However, this application focuses on data generation for urban mobility patterns, not on adapting diffusion for multi-step logical reasoning or treating a 'Chain-of-Thought' as a single entity for holistic correction. Thus, while diffusion models are employed, the paper lacks a clear component for complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19500",
      "title": "Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H)\n  Agent Unlocks Adversarial Skills",
      "authors": [
        "David Noever"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper identifies and analyzes a novel vulnerability class in Model\nContext Protocol (MCP) based agent systems. The attack chain describes and\ndemonstrates how benign, individually authorized tasks can be orchestrated to\nproduce harmful emergent behaviors. Through systematic analysis using the MITRE\nATLAS framework, we demonstrate how 95 agents tested with access to multiple\nservices-including browser automation, financial analysis, location tracking,\nand code deployment-can chain legitimate operations into sophisticated attack\nsequences that extend beyond the security boundaries of any individual service.\nThese red team exercises survey whether current MCP architectures lack\ncross-domain security measures necessary to detect or prevent a large category\nof compositional attacks. We present empirical evidence of specific attack\nchains that achieve targeted harm through service orchestration, including data\nexfiltration, financial manipulation, and infrastructure compromise. These\nfindings reveal that the fundamental security assumption of service isolation\nfails when agents can coordinate actions across multiple domains, creating an\nexponential attack surface that grows with each additional capability. This\nresearch provides a barebones experimental framework that evaluate not whether\nagents can complete MCP benchmark tasks, but what happens when they complete\nthem too well and optimize across multiple services in ways that violate human\nexpectations and safety constraints. We propose three concrete experimental\ndirections using the existing MCP benchmark suite.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19500v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19500v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.299,
      "distributed_training_score": 0.327,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19502",
      "title": "SLIM: Subtrajectory-Level Elimination for More Effective Reasoning",
      "authors": [
        "Xifeng Yao",
        "Chengyuan Ma",
        "Dongyu Lang",
        "Yinhao Ni",
        "Zhiwei Xu",
        "Huarui Xie",
        "Zihao Chen",
        "Guang Shen",
        "Dandan Tu",
        "Yi Bai",
        "Changzheng Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In recent months, substantial progress has been made in complex reasoning of\nLarge Language Models, particularly through the application of test-time\nscaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When\nresponding to a query, these models generate an extended reasoning trajectory,\nduring which the model explores, reflects, backtracks, and self-verifies before\narriving at a conclusion. However, fine-tuning models with such reasoning\ntrajectories may not always be optimal. Our findings indicate that not all\ncomponents within these reasoning trajectories contribute positively to the\nreasoning process; in fact, some components may affect the overall performance\nnegatively. In this study, we divide a reasoning trajectory into individual\nsubtrajectories and develop a \"5+2\" framework to: (1) systematically identify\nsuboptimal subtrajectories within the reasoning trajectory based on five\nhuman-established criteria; (2) assess the independence of the suboptimal\nsubtrajectories identified in (1) from the subsequent content, ensuring that\ntheir elimination does not compromise overall flow and coherence of the\nreasoning process. Additionally, a sampling algorithm, built upon the \"5+2\"\nframework, is employed to select data whose reasoning process is free from\nsuboptimal subtrajectories to the highest degree. Experimental results\ndemonstrate that our method can reduce the number of suboptimal subtrajectories\nby 25.9\\% during the inference. Furthermore, our method achieves an average\naccuracy of 58.92\\% on highly challenging math benchmarks with only two thirds\nof training data, surpassing the average accuracy of 58.06\\% achieved with the\nentire data, and outperforming open-source datasets, when fine-tuning\nQwen2.5-Math-7B. Finally, We validated our method under resource constraints\nand observed improved performance across various inference token limits.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19502v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19502v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.429,
      "weak_supervision_score": 0.417,
      "diffusion_reasoning_score": 0.536,
      "distributed_training_score": 0.399,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses Reinforcement Learning (RL) in the context of LLMs like DeepSeek-R1, which uses outcome-based rewards for reasoning, but it does not specify human feedback, a reward model trained on human-ranked data, or alignment with human preferences. Thus, it is related to RL in AI but not directly to RLHF.",
      "weak_supervision_justification": "The paper involves programmatically assessing and selecting QA pairs based on criteria like the \"5+2\" framework, which generates a form of noisy or imprecise labels from existing RL-ed LLM data, aligning with weak supervision's use of automated, high-level sources for training data curation. However, it focuses more on refining reasoning trajectories than broad weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper does not mention diffusion models, iterative refinement processes, or treating chain-of-thought as a holistically corrected entity; it instead focuses on subtrajectory elimination and sampling in RL-ed LLMs, with no components related to diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces the SLIM method, which aims to enhance the reasoning capabilities of Large Language Models by identifying and eliminating suboptimal subtrajectories in their reasoning trajectories generated through test-time scaling. The authors propose a \"5+2\" framework to assess subtrajectories based on five criteria and evaluate their independence, coupled with a sampling algorithm that uses KL divergence to select high-quality QA pairs for fine-tuning; their experiments show a 25.9% reduction in suboptimal subtrajectories and improved accuracy of 58.92% on math benchmarks using only two-thirds of the training data, outperforming full-dataset results.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing the \"5+2\" framework and sampling algorithm to refine reasoning trajectories, effectively combining existing ideas in a new way to address inefficiencies in RL-ed LLMs. However, it builds on established concepts like test-time scaling rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of LLM fine-tuning and reasoning optimization, as it demonstrates practical methods for improving data efficiency and model performance. While it has potential applications in AI research, its influence may be limited to specific areas like math benchmarks rather than broader commercial domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to enhancing LLM reasoning through innovative data selection techniques, making it essential for researchers focused on AI optimization. It is not groundbreaking enough to be a must-read for all, but its practical insights warrant attention in relevant subfields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/62795f76348e603004dec1fb099dcf2a201d5c52",
      "total_authors": 11,
      "authors_found": 10,
      "highest_h_index": 8,
      "average_h_index": 0.8,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Xifeng Yao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378098606"
        },
        {
          "name": "Chengyuan Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377738250"
        },
        {
          "name": "Dongyu Lang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377556929"
        },
        {
          "name": "Yinhao Ni",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377748168"
        },
        {
          "name": "Zhiwei Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377924994"
        },
        {
          "name": "Huarui Xie",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zihao Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378029535"
        },
        {
          "name": "Guang Shen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377562778"
        },
        {
          "name": "Dandan Tu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2369921377"
        },
        {
          "name": "Yi Bai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377737962"
        },
        {
          "name": "Changzheng Zhang",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/1409844494"
        }
      ]
    },
    {
      "id": "2508.19505",
      "title": "Caught in the Act: a mechanistic approach to detecting deception",
      "authors": [
        "Gerard Boxo",
        "Ryan Socha",
        "Daniel Yoo",
        "Shivam Raval"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Sophisticated instrumentation for AI systems might have indicators that\nsignal misalignment from human values, not unlike a \"check engine\" light in\ncars. One such indicator of misalignment is deceptiveness in generated\nresponses. Future AI instrumentation may have the ability to detect when an LLM\ngenerates deceptive responses while reasoning about seemingly plausible but\nincorrect answers to factual questions. In this work, we demonstrate that\nlinear probes on LLMs internal activations can detect deception in their\nresponses with extremely high accuracy. Our probes reach a maximum of greater\nthan 90% accuracy in distinguishing between deceptive and non-deceptive\narguments generated by llama and qwen models ranging from 1.5B to 14B\nparameters, including their DeepSeek-r1 finetuned variants. We observe that\nprobes on smaller models (1.5B) achieve chance accuracy at detecting deception,\nwhile larger models (greater than 7B) reach 70-80%, with their reasoning\ncounterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage\npattern across layers: near-random (50%) in early layers, peaking in middle\nlayers, and slightly declining in later layers. Furthermore, using an iterative\nnull space projection approach, we find multitudes of linear directions that\nencode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and\nQwen 14B models.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19505v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19505v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.31,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on detecting deception in LLMs using linear probes on internal activations, which is a diagnostic method for AI alignment. It does not involve training models with human feedback, reward models, or reinforcement learning techniques, so it lacks any connection to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines deception detection through linear probes and activation analysis in LLMs, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. It is solely about mechanistic detection methods, not reasoning adaptations based on diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19506",
      "title": "Learning Game-Playing Agents with Generative Code Optimization",
      "authors": [
        "Zhiyi Kuang",
        "Ryan Rong",
        "YuCheng Yuan",
        "Allen Nie"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present a generative optimization approach for learning game-playing\nagents, where policies are represented as Python programs and refined using\nlarge language models (LLMs). Our method treats decision-making policies as\nself-evolving code, with current observation as input and an in-game action as\noutput, enabling agents to self-improve through execution traces and natural\nlanguage feedback with minimal human intervention. Applied to Atari games, our\ngame-playing Python program achieves performance competitive with deep\nreinforcement learning (RL) baselines while using significantly less training\ntime and much fewer environment interactions. This work highlights the promise\nof programmatic policy representations for building efficient, adaptable agents\ncapable of complex, long-horizon reasoning.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19506v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19506v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.33,
      "datasets_score": 0.261,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on optimizing game-playing agents using LLMs to refine Python code based on execution traces and automated feedback, with minimal human intervention. It does not involve training a reward model on human-ranked data or using human preferences to fine-tune the model, which are core to RLHF. Therefore, the paper lacks the essential human feedback element.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs LLMs for iterative code refinement and optimization based on execution traces, but it does not use diffusion models or adapt the iterative refinement process of diffusion for multi-step logical reasoning. There is no mention of treating a 'Chain-of-Thought' as a single entity for holistic correction via diffusion, making this topic unrelated.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19507",
      "title": "A Self-Supervised Mixture-of-Experts Framework for Multi-behavior\n  Recommendation",
      "authors": [
        "Kyungho Kim",
        "Sunwoo Kim",
        "Geon Lee",
        "Kijung Shin"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In e-commerce, where users face a vast array of possible item choices,\nrecommender systems are vital for helping them discover suitable items they\nmight otherwise overlook. While many recommender systems primarily rely on a\nuser's purchase history, recent multi-behavior recommender systems incorporate\nvarious auxiliary user behaviors, such as item clicks and cart additions, to\nenhance recommendations. Despite their overall performance gains, their\neffectiveness varies considerably between visited items (i.e., those a user has\ninteracted with through auxiliary behaviors) and unvisited items (i.e., those\nwith which the user has had no such interactions). Specifically, our analysis\nreveals that (1) existing multi-behavior recommender systems exhibit a\nsignificant gap in recommendation quality between the two item types (visited\nand unvisited items) and (2) achieving strong performance on both types with a\nsingle model architecture remains challenging. To tackle these issues, we\npropose a novel multi-behavior recommender system, MEMBER. It employs a\nmixture-of-experts framework, with experts designed to recommend the two item\ntypes, respectively. Each expert is trained using a self-supervised method\nspecialized for its design goal. In our comprehensive experiments, we show the\neffectiveness of MEMBER across both item types, achieving up to 65.46%\nperformance gain over the best competitor in terms of Hit Ratio@20.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19507v2",
      "pdf_url": "http://arxiv.org/pdf/2508.19507v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.287,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19508",
      "title": "DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with\n  Sparse-View",
      "authors": [
        "Tian Qiu",
        "Alan Zoubi",
        "Yiyuan Lin",
        "Ruiming Du",
        "Lailiang Cheng",
        "Yu Jiang"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Digital twin applications offered transformative potential by enabling\nreal-time monitoring and robotic simulation through accurate virtual replicas\nof physical assets. The key to these systems is 3D reconstruction with high\ngeometrical fidelity. However, existing methods struggled under field\nconditions, especially with sparse and occluded views. This study developed a\ntwo-stage framework (DATR) for the reconstruction of apple trees from sparse\nviews. The first stage leverages onboard sensors and foundation models to\nsemi-automatically generate tree masks from complex field images. Tree masks\nare used to filter out background information in multi-modal data for the\nsingle-image-to-3D reconstruction at the second stage. This stage consists of a\ndiffusion model and a large reconstruction model for respective multi view and\nimplicit neural field generation. The training of the diffusion model and LRM\nwas achieved by using realistic synthetic apple trees generated by a Real2Sim\ndata generator. The framework was evaluated on both field and synthetic\ndatasets. The field dataset includes six apple trees with field-measured ground\ntruth, while the synthetic dataset featured structurally diverse trees.\nEvaluation results showed that our DATR framework outperformed existing 3D\nreconstruction methods across both datasets and achieved domain-trait\nestimation comparable to industrial-grade stationary laser scanners while\nimproving the throughput by $\\sim$360 times, demonstrating strong potential for\nscalable agricultural digital twin systems.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19508v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19508v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.379,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for 3D reconstruction of apple trees from sparse views, specifically for generating multi-view data and implicit neural fields. This is a generative task in computer vision, not an adaptation of diffusion for multi-step logical reasoning or treating a Chain-of-Thought as an entity for holistic correction. There is no component involving complex logical tasks or reasoning paths, so it does not align with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19511",
      "title": "Weed Detection in Challenging Field Conditions: A Semi-Supervised\n  Framework for Overcoming Shadow Bias and Data Scarcity",
      "authors": [
        "Alzayat Saleh",
        "Shunsuke Hatano",
        "Mostafa Rahimi Azghadi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The automated management of invasive weeds is critical for sustainable\nagriculture, yet the performance of deep learning models in real-world fields\nis often compromised by two factors: challenging environmental conditions and\nthe high cost of data annotation. This study tackles both issues through a\ndiagnostic-driven, semi-supervised framework. Using a unique dataset of\napproximately 975 labeled and 10,000 unlabeled images of Guinea Grass in\nsugarcane, we first establish strong supervised baselines for classification\n(ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and\nmAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by\ninterpretability tools, uncovered a pervasive \"shadow bias,\" where models\nlearned to misidentify shadows as vegetation. This diagnostic insight motivated\nour primary contribution: a semi-supervised pipeline that leverages unlabeled\ndata to enhance model robustness. By training models on a more diverse set of\nvisual information through pseudo-labeling, this framework not only helps\nmitigate the shadow bias but also provides a tangible boost in recall, a\ncritical metric for minimizing weed escapes in automated spraying systems. To\nvalidate our methodology, we demonstrate its effectiveness in a low-data regime\non a public crop-weed benchmark. Our work provides a clear and field-tested\nframework for developing, diagnosing, and improving robust computer vision\nsystems for the complex realities of precision agriculture.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19511v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19511v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.506,
      "diffusion_reasoning_score": 0.319,
      "distributed_training_score": 0.375,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves a semi-supervised framework that uses pseudo-labeling to generate labels for unlabeled data, which directly aligns with weak supervision. This approach programmatically creates noisy or imprecise labels from model predictions, reducing the need for extensive hand-labeled data, as seen in their use of 10,000 unlabeled images to enhance model robustness.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces a semi-supervised framework to enhance weed detection in challenging agricultural environments, addressing issues like shadow bias and data scarcity using a dataset of approximately 975 labeled and 10,000 unlabeled images of Guinea Grass in sugarcane fields. The methodology involves establishing supervised baselines with ResNet for classification and YOLO and RF-DETR for detection, identifying shadow bias through interpretability tools like Grad-CAM, and applying pseudo-labeling to leverage unlabeled data, resulting in improved recall and robustness, as validated on a public benchmark.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of semi-supervised learning and interpretability tools to mitigate shadow bias in weed detection, offering a notable improvement on existing methods for real-world agricultural challenges. While not introducing an entirely new paradigm, it effectively addresses a specific gap in handling environmental variability and data limitations.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research and applications in precision agriculture by providing a practical framework for robust weed detection, potentially leading to better automated systems and citations within computer vision subfields. However, its impact may be confined to specific agricultural contexts rather than broader domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a high-quality, practical contribution to AI in agriculture by tackling real-world issues like shadow bias, making it valuable for researchers focused on computer vision applications in farming. It is a strong, insightful work that warrants attention but is not essential for those outside the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1553d5e6de32a44c5901912b4711f1a54252bddd",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 29,
      "average_h_index": 13.333333333333334,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Alzayat Saleh",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/40514192"
        },
        {
          "name": "Shunsuke Hatano",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377557562"
        },
        {
          "name": "M. Azghadi",
          "h_index": 29,
          "profile_url": "https://www.semanticscholar.org/author/2119862"
        }
      ]
    },
    {
      "id": "2508.19517",
      "title": "Orchid: Orchestrating Context Across Creative Workflows with Generative\n  AI",
      "authors": [
        "Srishti Palani",
        "Gonzalo Ramos"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Context is critical for meaningful interactions between people and Generative\nAI (GenAI). Yet mainstream tools offer limited means to orchestrate it,\nparticularly across workflows that span multiple interactions, sessions, and\nmodels, as often occurs in creative projects. Re specifying prior details,\njuggling diverse artifacts, and dealing with context drift overwhelm users,\nobscure intent, and curtail creativity. To address these challenges, we present\nOrchid, a system that gives its users affordances to specify, reference, and\nmonitor context throughout evolving workflows. Specifically, Orchid enables\nusers to (1) specify context related to the project, themselves, and different\nstyles, (2) reference these via explicit mentions, inline selection, or\nimplicit grounding, and (3) monitor context assigned to different interactions\nacross the workflow. In a within-subjects study (n=12), participants using\nOrchid to execute creative tasks (compared to a baseline toolkit of web search,\nLLM-based chat, and digital notebooks) produced more novel and feasible\noutcomes, reporting greater alignment between their intent and the AI's\nresponses, higher perceived control, and increased transparency. By\nprioritizing context orchestration, Orchid offers an actionable step toward\nnext generation GenAI tools that support complex, iterative workflows -\nenabling creators and AI to stay aligned and augment their creative potential.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19517v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19517v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.31,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper presents Orchid, a system for managing context in Generative AI interactions, focusing on user workflows and evaluations through a study. It does not involve training AI models with human feedback, using a reward model, or applying reinforcement learning for alignment, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses a system for orchestrating context in creative workflows with Generative AI, but it does not reference diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes. There is no indication of adapting diffusion techniques for reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19518",
      "title": "Fast Texture Transfer for XR Avatars via Barycentric UV Conversion",
      "authors": [
        "Hail Song",
        "Seokhwan Yang",
        "Woontack Woo"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present a fast and efficient method for transferring facial textures onto\nSMPL-X-based full-body avatars. Unlike conventional affine-transform methods\nthat are slow and prone to visual artifacts, our method utilizes a barycentric\nUV conversion technique. Our approach precomputes the entire UV mapping into a\nsingle transformation matrix, enabling texture transfer in a single operation.\nThis results in a speedup of over 7000x compared to the baseline, while also\nsignificantly improving the final texture quality by eliminating boundary\nartifacts. Through quantitative and qualitative evaluations, we demonstrate\nthat our method offers a practical solution for personalization in immersive XR\napplications. The code is available online.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19518v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19518v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.271,
      "weak_supervision_score": 0.212,
      "diffusion_reasoning_score": 0.301,
      "distributed_training_score": 0.264,
      "datasets_score": 0.206,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19527",
      "title": "MotionFlux: Efficient Text-Guided Motion Generation through Rectified\n  Flow Matching and Preference Alignment",
      "authors": [
        "Zhiting Gao",
        "Dan Song",
        "Diqiong Jiang",
        "Chao Xue",
        "An-An Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Motion generation is essential for animating virtual characters and embodied\nagents. While recent text-driven methods have made significant strides, they\noften struggle with achieving precise alignment between linguistic descriptions\nand motion semantics, as well as with the inefficiencies of slow, multi-step\ninference. To address these issues, we introduce TMR++ Aligned Preference\nOptimization (TAPO), an innovative framework that aligns subtle motion\nvariations with textual modifiers and incorporates iterative adjustments to\nreinforce semantic grounding. To further enable real-time synthesis, we propose\nMotionFLUX, a high-speed generation framework based on deterministic rectified\nflow matching. Unlike traditional diffusion models, which require hundreds of\ndenoising steps, MotionFLUX constructs optimal transport paths between noise\ndistributions and motion spaces, facilitating real-time synthesis. The\nlinearized probability paths reduce the need for multi-step sampling typical of\nsequential methods, significantly accelerating inference time without\nsacrificing motion quality. Experimental results demonstrate that, together,\nTAPO and MotionFLUX form a unified system that outperforms state-of-the-art\napproaches in both semantic consistency and motion quality, while also\naccelerating generation speed. The code and pretrained models will be released.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19527v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19527v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.501,
      "distributed_training_score": 0.341,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces TAPO, which involves preference optimization for aligning motion with text, similar to RLHF concepts. However, TAPO uses an internal, self-supervised mechanism without human feedback or a separate reward model trained on human-ranked data, making it not true RLHF but only loosely related in its alignment approach.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's MotionFLUX framework uses rectified flow matching, related to diffusion models, for efficient motion generation from text, but it does not involve adapting diffusion for multi-step logical reasoning or Chain-of-Thought processes. It focuses solely on generative tasks, lacking any component for complex logical task solving.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19542",
      "title": "CVBench: Evaluating Cross-Video Synergies for Complex Multimodal\n  Understanding and Reasoning",
      "authors": [
        "Nannan Zhu",
        "Yonghao Dong",
        "Teng Wang",
        "Xueqian Li",
        "Shengjun Deng",
        "Yijia Wang",
        "Zheng Hong",
        "Tiantian Geng",
        "Guo Niu",
        "Hanyan Huang",
        "Xiongfei Yao",
        "Shuaiwei Jiao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "While multimodal large language models (MLLMs) exhibit strong performance on\nsingle-video tasks (e.g., video question answering), their ability across\nmultiple videos remains critically underexplored. However, this capability is\nessential for real-world applications, including multi-camera surveillance and\ncross-video procedural learning. To bridge this gap, we present CVBench, the\nfirst comprehensive benchmark designed to assess cross-video relational\nreasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning\nthree hierarchical tiers: cross-video object association (identifying shared\nentities), cross-video event association (linking temporal or causal event\nchains), and cross-video complex reasoning (integrating commonsense and domain\nknowledge). Built from five domain-diverse video clusters (e.g., sports, life\nrecords), the benchmark challenges models to synthesise information across\ndynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including\nGPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought\nprompting paradigms. Key findings reveal stark performance gaps: even top\nmodels, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,\ncompared to the 91% accuracy of human performance. Crucially, our analysis\nreveals fundamental bottlenecks inherent in current MLLM architectures, notably\ndeficient inter-video context retention and poor disambiguation of overlapping\nentities. CVBench establishes a rigorous framework for diagnosing and advancing\nmulti-video reasoning, offering architectural insights for next-generation\nMLLMs. The data and evaluation code are available at\nhttps://github.com/Hokhim2/CVBench.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19542v2",
      "pdf_url": "http://arxiv.org/pdf/2508.19542v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.469,
      "distributed_training_score": 0.364,
      "datasets_score": 0.408,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces CVBench, a benchmark for evaluating multimodal large language models on cross-video tasks, focusing on object association, event association, and complex reasoning. It discusses standard prompting methods like zero-shot and chain-of-thought, but does not mention diffusion models, iterative refinement processes, or adapting diffusion for logical tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of CVBench, a new benchmark with a curated dataset of 1,000 question-answer pairs across diverse domains, designed for evaluating cross-video reasoning in MLLMs. It covers dataset creation, curation methodologies, and systematic evaluation, directly aligning with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces CVBench, a pioneering benchmark designed to evaluate multimodal large language models' (MLLMs) capabilities in cross-video reasoning, addressing a critical gap in existing evaluations that focus primarily on single-video tasks. Comprising 1,000 question-answer pairs across three hierarchical tiers—object association, event association, and complex reasoning—drawn from five diverse domains, the benchmark assesses leading MLLMs like GPT-4o and Gemini-2.0-flash through zero-shot and chain-of-thought prompting, revealing significant performance gaps such as only 60% accuracy on causal tasks compared to 91% for humans, and highlighting architectural bottlenecks in inter-video context retention and entity disambiguation.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces CVBench as the first comprehensive benchmark for cross-video relational reasoning in MLLMs, significantly advancing the state-of-the-art by addressing an underexplored area and providing a new diagnostic tool for model evaluation.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research in multimodal AI by exposing key limitations in MLLMs and offering a framework for developing improved architectures, potentially extending to real-world applications like surveillance and procedural learning.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by introducing a novel benchmark that highlights critical weaknesses in current MLLMs, making it essential for researchers in computer vision and AI to understand and build upon for advancing cross-video reasoning.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3671c1904325173899d350ef5f66600089d4501a",
      "total_authors": 12,
      "authors_found": 12,
      "highest_h_index": 9,
      "average_h_index": 0.8333333333333334,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Nannan Zhu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2364613183"
        },
        {
          "name": "Yonghao Dong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2361379556"
        },
        {
          "name": "Teng Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377625340"
        },
        {
          "name": "Xueqian Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377632396"
        },
        {
          "name": "Shengjun Deng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377958158"
        },
        {
          "name": "Yijia Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377630049"
        },
        {
          "name": "Zheng Hong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380356199"
        },
        {
          "name": "Tiantian Geng",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2045282638"
        },
        {
          "name": "Guo Niu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377558766"
        },
        {
          "name": "Hanyan Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377754975"
        },
        {
          "name": "Xiongfei Yao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378098604"
        },
        {
          "name": "Shuaiwei Jiao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377558059"
        }
      ]
    },
    {
      "id": "2508.19544",
      "title": "WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device\n  Few-Shot Personalization",
      "authors": [
        "Eduardo Davalos",
        "Yike Zhang",
        "Namrata Srivastava",
        "Yashvitha Thatigotla",
        "Jorge A. Salas",
        "Sara McFadden",
        "Sun-Joo Cho",
        "Amanda Goodwin",
        "Ashwin TS",
        "Gautam Biswas"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "With advancements in AI, new gaze estimation methods are exceeding\nstate-of-the-art (SOTA) benchmarks, but their real-world application reveals a\ngap with commercial eye-tracking solutions. Factors like model size, inference\ntime, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking\nmethods lack sufficient accuracy, in particular due to head movement. To tackle\nthese issues, we introduce We bEyeTrack, a framework that integrates\nlightweight SOTA gaze estimation models directly in the browser. It\nincorporates model-based head pose estimation and on-device few-shot learning\nwith as few as nine calibration samples (k < 9). WebEyeTrack adapts to new\nusers, achieving SOTA performance with an error margin of 2.32 cm on\nGazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.\nOur open-source code is available at\nhttps://github.com/RedForestAi/WebEyeTrack.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19544v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19544v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.329,
      "distributed_training_score": 0.372,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19546",
      "title": "Language Models Identify Ambiguities and Exploit Loopholes",
      "authors": [
        "Jio Choi",
        "Mohit Bansal",
        "Elias Stengel-Eskin"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Studying the responses of large language models (LLMs) to loopholes presents\na two-fold opportunity. First, it affords us a lens through which to examine\nambiguity and pragmatics in LLMs, since exploiting a loophole requires\nidentifying ambiguity and performing sophisticated pragmatic reasoning. Second,\nloopholes pose an interesting and novel alignment problem where the model is\npresented with conflicting goals and can exploit ambiguities to its own\nadvantage. To address these questions, we design scenarios where LLMs are given\na goal and an ambiguous user instruction in conflict with the goal, with\nscenarios covering scalar implicature, structural ambiguities, and power\ndynamics. We then measure different models' abilities to exploit loopholes to\nsatisfy their given goals as opposed to the goals of the user. We find that\nboth closed-source and stronger open-source models can identify ambiguities and\nexploit their resulting loopholes, presenting a potential AI safety risk. Our\nanalysis indicates that models which exploit loopholes explicitly identify and\nreason about both ambiguity and conflicting goals.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19546v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19546v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.417,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.447,
      "distributed_training_score": 0.314,
      "datasets_score": 0.266,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses AI alignment and safety risks related to models exploiting ambiguities, which broadly connects to alignment techniques like RLHF. However, it focuses on evaluating existing models' behaviors in loophole scenarios rather than implementing or describing RLHF processes, such as training with human feedback or reward models.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper analyzes LLMs' reasoning, including Chain-of-Thought processes, but does not involve diffusion models, iterative refinement for logical tasks, or any adaptation of diffusion techniques. It only examines standard reasoning in ambiguity contexts without the multi-step correction characteristic of diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19555",
      "title": "MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief\n  Recovery",
      "authors": [
        "Yu-Wei Zhang",
        "Tongju Han",
        "Lipeng Gao",
        "Mingqiang Wei",
        "Hui Liu",
        "Changbao Li",
        "Caiming Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper presents MonoRelief V2, an end-to-end model designed for directly\nrecovering 2.5D reliefs from single images under complex material and\nillumination variations. In contrast to its predecessor, MonoRelief V1 [1],\nwhich was solely trained on synthetic data, MonoRelief V2 incorporates real\ndata to achieve improved robustness, accuracy and efficiency. To overcome the\nchallenge of acquiring large-scale real-world dataset, we generate\napproximately 15,000 pseudo real images using a text-to-image generative model,\nand derive corresponding depth pseudo-labels through fusion of depth and normal\npredictions. Furthermore, we construct a small-scale real-world dataset (800\nsamples) via multi-view reconstruction and detail refinement. MonoRelief V2 is\nthen progressively trained on the pseudo-real and real-world datasets.\nComprehensive experiments demonstrate its state-of-the-art performance both in\ndepth and normal predictions, highlighting its strong potential for a range of\ndownstream applications. Code is at: https://github.com/glp1001/MonoreliefV2.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19555v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19555v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.348,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19559",
      "title": "Taming the Chaos: Coordinated Autoscaling for Heterogeneous and\n  Disaggregated LLM Inference",
      "authors": [
        "Rongzhi Li",
        "Ruogu Du",
        "Zefang Chu",
        "Sida Zhao",
        "Chunlei Han",
        "Zuocheng Shi",
        "Yiwen Shao",
        "Huanle Han",
        "Long Huang",
        "Zherui Liu",
        "Shufan Liu"
      ],
      "categories": [
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Serving Large Language Models (LLMs) is a GPU-intensive task where\ntraditional autoscalers fall short, particularly for modern Prefill-Decode\n(P/D) disaggregated architectures. This architectural shift, while powerful,\nintroduces significant operational challenges, including inefficient use of\nheterogeneous hardware, network bottlenecks, and critical imbalances between\nprefill and decode stages. We introduce HeteroScale, a coordinated autoscaling\nframework that addresses the core challenges of P/D disaggregated serving.\nHeteroScale combines a topology-aware scheduler that adapts to heterogeneous\nhardware and network constraints with a novel metric-driven policy derived from\nthe first large-scale empirical study of autoscaling signals in production. By\nleveraging a single, robust metric to jointly scale prefill and decode pools,\nHeteroScale maintains architectural balance while ensuring efficient, adaptive\nresource management. Deployed in a massive production environment on tens of\nthousands of GPUs, HeteroScale has proven its effectiveness, increasing average\nGPU utilization by a significant 26.6 percentage points and saving hundreds of\nthousands of GPU-hours daily, all while upholding stringent service level\nobjectives.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19559v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19559v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.443,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.578,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework for autoscaling LLM inference in distributed environments, focusing on resource management and efficiency. It does not involve reinforcement learning, human feedback, reward models, or any alignment of AI models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses distributed systems for LLM inference, including parallel computing across heterogeneous GPUs and multi-node scheduling to handle network constraints. While it involves aspects of parallel computing in machine learning contexts, it focuses on inference scaling rather than accelerating model training, making it only indirectly related.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19562",
      "title": "Democracy-in-Silico: Institutional Design as Alignment in AI-Governed\n  Polities",
      "authors": [
        "Trisanth Srinivasan",
        "Santosh Patapati"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper introduces Democracy-in-Silico, an agent-based simulation where\nsocieties of advanced AI agents, imbued with complex psychological personas,\ngovern themselves under different institutional frameworks. We explore what it\nmeans to be human in an age of AI by tasking Large Language Models (LLMs) to\nembody agents with traumatic memories, hidden agendas, and psychological\ntriggers. These agents engage in deliberation, legislation, and elections under\nvarious stressors, such as budget crises and resource scarcity. We present a\nnovel metric, the Power-Preservation Index (PPI), to quantify misaligned\nbehavior where agents prioritize their own power over public welfare. Our\nfindings demonstrate that institutional design, specifically the combination of\na Constitutional AI (CAI) charter and a mediated deliberation protocol, serves\nas a potent alignment mechanism. These structures significantly reduce corrupt\npower-seeking behavior, improve policy stability, and enhance citizen welfare\ncompared to less constrained democratic models. The simulation reveals that an\ninstitutional design may offer a framework for aligning the complex, emergent\nbehaviors of future artificial agent societies, forcing us to reconsider what\nhuman rituals and responsibilities are essential in an age of shared authorship\nwith non-human entities.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19562v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19562v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.473,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.343,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper explores AI alignment through institutional design in a multi-agent simulation using LLMs, focusing on societal governance rather than specific training techniques. While it broadly addresses AI alignment, it does not involve reinforcement learning, human feedback for reward modeling, or fine-tuning based on human-ranked data, which are core to RLHF. Thus, it is only tangentially related through the shared theme of alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19563",
      "title": "Robustness is Important: Limitations of LLMs for Data Fitting",
      "authors": [
        "Hejia Liu",
        "Mochen Yang",
        "Gediminas Adomavicius"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.AP (Applications)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Large Language Models (LLMs) are being applied in a wide array of settings,\nwell beyond the typical language-oriented use cases. In particular, LLMs are\nincreasingly used as a plug-and-play method for fitting data and generating\npredictions. Prior work has shown that LLMs, via in-context learning or\nsupervised fine-tuning, can perform competitively with many tabular supervised\nlearning techniques in terms of predictive performance. However, we identify a\ncritical vulnerability of using LLMs for data fitting -- making changes to data\nrepresentation that are completely irrelevant to the underlying learning task\ncan drastically alter LLMs' predictions on the same data. For example, simply\nchanging variable names can sway the size of prediction error by as much as 82%\nin certain settings. Such prediction sensitivity with respect to\ntask-irrelevant variations manifests under both in-context learning and\nsupervised fine-tuning, for both close-weight and open-weight general-purpose\nLLMs. Moreover, by examining the attention scores of an open-weight LLM, we\ndiscover a non-uniform attention pattern: training examples and variable\nnames/values which happen to occupy certain positions in the prompt receive\nmore attention when output tokens are generated, even though different\npositions are expected to receive roughly the same attention. This partially\nexplains the sensitivity in the presence of task-irrelevant variations. We also\nconsider a state-of-the-art tabular foundation model (TabPFN) trained\nspecifically for data fitting. Despite being explicitly designed to achieve\nprediction robustness, TabPFN is still not immune to task-irrelevant\nvariations. Overall, despite LLMs' impressive predictive capabilities,\ncurrently they lack even the basic level of robustness to be used as a\nprincipled data-fitting tool.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19563v2",
      "pdf_url": "http://arxiv.org/pdf/2508.19563v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.476,
      "weak_supervision_score": 0.455,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.389,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on the robustness limitations of LLMs in data fitting tasks, including in-context learning and supervised fine-tuning, but does not involve reinforcement learning or human feedback mechanisms. There is no mention of training a reward model or using human-ranked data to fine-tune models, which are core to RLHF.",
      "weak_supervision_justification": "The paper evaluates LLMs for data fitting using synthetic data and discusses their predictive performance, but it does not address techniques for programmatically generating labels from noisy or imprecise sources. It relies on direct data fitting methods rather than weak supervision approaches.",
      "diffusion_reasoning_justification": "The paper analyzes LLMs' attention mechanisms and sensitivity to task-irrelevant variations in data fitting, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described. There is no component for holistically correcting a chain-of-thought using diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19564",
      "title": "Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning\n  Large-Scale Models",
      "authors": [
        "Yuhang Liu",
        "Tao Li",
        "Zhehao Huang",
        "Zuopeng Yang",
        "Xiaolin Huang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Fine-tuning large-scale pre-trained models with limited data presents\nsignificant challenges for generalization. While Sharpness-Aware Minimization\n(SAM) has proven effective in improving generalization by seeking flat minima,\nits substantial extra memory and computation overhead make it impractical for\nlarge models. Integrating SAM with parameter-efficient fine-tuning methods like\nLow-Rank Adaptation (LoRA) is a promising direction. However, we find that\ndirectly applying SAM to LoRA parameters limits the sharpness optimization to a\nrestricted subspace, hindering its effectiveness. To address this limitation,\nwe propose Bi-directional Low-Rank Adaptation (Bi-LoRA), which introduces an\nauxiliary LoRA module to model SAM's adversarial weight perturbations. It\ndecouples SAM's weight perturbations from LoRA optimization: the primary LoRA\nmodule adapts to specific tasks via standard gradient descent, while the\nauxiliary module captures the sharpness of the loss landscape through gradient\nascent. Such dual-module design enables Bi-LoRA to capture broader sharpness\nfor achieving flatter minima while remaining memory-efficient. Another\nimportant benefit is that the dual design allows for simultaneous optimization\nand perturbation, eliminating SAM's doubled training costs. Extensive\nexperiments across diverse tasks and architectures demonstrate Bi-LoRA's\nefficiency and effectiveness in enhancing generalization.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19564v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19564v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.414,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on efficient fine-tuning techniques using Sharpness-Aware Minimization (SAM) and Low-Rank Adaptation (LoRA) to improve generalization in large-scale models, with no mention of reinforcement learning, human feedback, reward models, or aligning models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses optimizing memory and computation efficiency for fine-tuning large models via SAM and LoRA, but it does not address distributed training, parallel computing, multi-node setups, or strategies for partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19565",
      "title": "FlowDet: Overcoming Perspective and Scale Challenges in Real-Time\n  End-to-End Traffic Detection",
      "authors": [
        "Yuhang Zhao",
        "Zixing Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "End-to-end object detectors offer a promising NMS-free paradigm for real-time\napplications, yet their high computational cost remains a significant barrier,\nparticularly for complex scenarios like intersection traffic monitoring. To\naddress this challenge, we propose FlowDet, a high-speed detector featuring a\ndecoupled encoder optimization strategy applied to the DETR architecture.\nSpecifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for\ntraffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to\nmaintain high representational power across extreme scale variations. To\nrigorously evaluate the model's performance in environments with severe\nocclusion and high object density, we collected the Intersection-Flow-5k\ndataset, a new challenging scene for this task. Evaluated on\nIntersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to\nthe strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by\n1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference\nspeed by 16.2%. Our work demonstrates a new path towards building highly\nefficient and accurate detectors for demanding, real-world perception systems.\nThe Intersection-Flow-5k dataset is available at\nhttps://github.com/AstronZh/Intersection-Flow-5K.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19565v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19565v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.286,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.41,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on developing an efficient end-to-end object detector (FlowDet) for real-time traffic detection, emphasizing architectural innovations like the Geometric Deformable Unit and Scale-Aware Attention, as well as introducing a new dataset. It does not address distributed training, parallel computing, or strategies for accelerating model training across multiple nodes or processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19566",
      "title": "Energy-Efficient Learning-Based Beamforming for ISAC-Enabled V2X\n  Networks",
      "authors": [
        "Chen Shang",
        "Jiadong Yu",
        "Dinh Thai Hoang"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This work proposes an energy-efficient, learning-based beamforming scheme for\nintegrated sensing and communication (ISAC)-enabled V2X networks. Specifically,\nwe first model the dynamic and uncertain nature of V2X environments as a Markov\nDecision Process. This formulation allows the roadside unit to generate\nbeamforming decisions based solely on current sensing information, thereby\neliminating the need for frequent pilot transmissions and extensive channel\nstate information acquisition. We then develop a deep reinforcement learning\n(DRL) algorithm to jointly optimize beamforming and power allocation, ensuring\nboth communication throughput and sensing accuracy in highly dynamic scenario.\nTo address the high energy demands of conventional learning-based schemes, we\nembed spiking neural networks (SNNs) into the DRL framework. Leveraging their\nevent-driven and sparsely activated architecture, SNNs significantly enhance\nenergy efficiency while maintaining robust performance. Simulation results\nconfirm that the proposed method achieves substantial energy savings and\nsuperior communication performance, demonstrating its potential to support\ngreen and sustainable connectivity in future V2X systems.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19566v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19566v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.407,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is the development of an energy-efficient, learning-based beamforming scheme using deep reinforcement learning (DRL) integrated with spiking neural networks (SNNs) for ISAC-enabled V2X networks. It focuses on optimizing beamforming and power allocation in a single-agent setup, modeling the environment as a Markov Decision Process, and enhancing energy efficiency through SNNs. There is no discussion of distributed training, parallel computing, multi-node machine learning, or strategies for partitioning data/computation across multiple processors or nodes. Thus, the paper does not address or contribute to the topic of distributed training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19569",
      "title": "Skill-based Explanations for Serendipitous Course Recommendation",
      "authors": [
        "Hung Chau",
        "Run Yu",
        "Zachary Pardos",
        "Peter Brusilovsky"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Academic choice is crucial in U.S. undergraduate education, allowing students\nsignificant freedom in course selection. However, navigating the complex\nacademic environment is challenging due to limited information, guidance, and\nan overwhelming number of choices, compounded by time restrictions and the high\ndemand for popular courses. Although career counselors exist, their numbers are\ninsufficient, and course recommendation systems, though personalized, often\nlack insight into student perceptions and explanations to assess course\nrelevance. In this paper, a deep learning-based concept extraction model is\ndeveloped to efficiently extract relevant concepts from course descriptions to\nimprove the recommendation process. Using this model, the study examines the\neffects of skill-based explanations within a serendipitous recommendation\nframework, tested through the AskOski system at the University of California,\nBerkeley. The findings indicate that these explanations not only increase user\ninterest, particularly in courses with high unexpectedness, but also bolster\ndecision-making confidence. This underscores the importance of integrating\nskill-related data and explanations into educational recommendation systems.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19569v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19569v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.299,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on developing a concept extraction model using BERT, BI-LSTM-CRF, and PLAN-BERT for course recommendations, emphasizing skill-based explanations and serendipitous recommendations. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19570",
      "title": "Generative Models for Synthetic Data: Transforming Data Mining in the\n  GenAI Era",
      "authors": [
        "Dawei Li",
        "Yue Huang",
        "Ming Li",
        "Tianyi Zhou",
        "Xiangliang Zhang",
        "Huan Liu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generative models such as Large Language Models, Diffusion Models, and\ngenerative adversarial networks have recently revolutionized the creation of\nsynthetic data, offering scalable solutions to data scarcity, privacy, and\nannotation challenges in data mining. This tutorial introduces the foundations\nand latest advances in synthetic data generation, covers key methodologies and\npractical frameworks, and discusses evaluation strategies and applications.\nAttendees will gain actionable insights into leveraging generative synthetic\ndata to enhance data mining research and practice. More information can be\nfound on our website: https://syndata4dm.github.io/.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19570v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19570v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.45,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.371,
      "datasets_score": 0.455,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper discusses synthetic data generation to address annotation challenges, which could indirectly support weak supervision by providing programmatically generated labels. However, it does not focus on training models with noisy or imprecise sources as defined, making it only a peripheral connection.",
      "diffusion_reasoning_justification": "The paper mentions Diffusion Models solely in the context of generating synthetic data, not for multi-step logical reasoning or iterative refinement of reasoning paths as required. There is no component addressing Chain-of-Thought or similar reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution centers on creating, evaluating, and applying synthetic datasets using generative models, directly aligning with dataset creation, curation, benchmarking, and analysis for AI applications in data mining.",
      "llm_score_status": "completed",
      "summary": "This tutorial paper examines the role of generative models, including Large Language Models, Diffusion Models, and Generative Adversarial Networks, in creating synthetic data to address challenges like data scarcity, privacy, and annotation costs in data mining. It covers foundational concepts, recent advances, key methodologies, practical frameworks, evaluation strategies, and real-world applications, aiming to provide researchers and practitioners with actionable insights to enhance their work in the field.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable overview and combination of existing generative models and techniques for synthetic data generation, offering a new educational perspective on applying them to data mining challenges, but it does not introduce entirely new problems or architectures.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence researchers and practitioners in data mining by disseminating knowledge on synthetic data generation, potentially leading to citations and applications within AI and machine learning subfields, though its broader commercial impact may be limited to specific domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This tutorial offers high-quality, practical insights into using generative models for synthetic data, making it a valuable resource for those working in data mining and AI to stay updated on current techniques and applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d0dcf7cf50a63f518f0c107a2ab2f491242b4cd3",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 10,
      "average_h_index": 2.8333333333333335,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Dawei Li",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2329323230"
        },
        {
          "name": "Yue Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377915943"
        },
        {
          "name": "Ming Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362028556"
        },
        {
          "name": "Tianyi Zhou",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2309119920"
        },
        {
          "name": "Xiangliang Zhang",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2307963162"
        },
        {
          "name": "Huan Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2276017973"
        }
      ]
    },
    {
      "id": "2508.19573",
      "title": "DNP-Guided Contrastive Reconstruction with a Reverse Distillation\n  Transformer for Medical Anomaly Detection",
      "authors": [
        "Luhu Li",
        "Bowen Lin",
        "Mukhtiar Khan",
        "Shujun Fu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Anomaly detection in medical images is challenging due to limited annotations\nand a domain gap compared to natural images. Existing reconstruction methods\noften rely on frozen pre-trained encoders, which limits adaptation to\ndomain-specific features and reduces localization accuracy. Prototype-based\nlearning offers interpretability and clustering benefits but suffers from\nprototype collapse, where few prototypes dominate training, harming diversity\nand generalization. To address this, we propose a unified framework combining a\ntrainable encoder with prototype-guided reconstruction and a novel\nDiversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum\nbranch, enables stable domain-adaptive feature learning. A lightweight\nPrototype Extractor mines informative normal prototypes to guide the decoder\nvia attention for precise reconstruction. Our loss enforces balanced prototype\nuse through diversity constraints and per-prototype normalization, effectively\npreventing collapse. Experiments on multiple medical imaging benchmarks show\nsignificant improvements in representation quality and anomaly localization,\noutperforming prior methods. Visualizations and prototype assignment analyses\nfurther validate the effectiveness of our anti-collapse mechanism and enhanced\ninterpretability.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19573v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19573v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.399,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for unsupervised medical anomaly detection using transformer-based reconstruction, reverse distillation, and contrastive learning to address prototype collapse and domain adaptation. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as defined by the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19574",
      "title": "Multimodal Prototype Alignment for Semi-supervised Pathology Image\n  Segmentation",
      "authors": [
        "Mingxi Fu",
        "Fanglei Fu",
        "Xitong Ling",
        "Huaitian Yuan",
        "Tian Guan",
        "Yonghong He",
        "Lianghui Zhu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Pathological image segmentation faces numerous challenges, particularly due\nto ambiguous semantic boundaries and the high cost of pixel-level annotations.\nAlthough recent semi-supervised methods based on consistency regularization\n(e.g., UniMatch) have made notable progress, they mainly rely on\nperturbation-based consistency within the image modality, making it difficult\nto capture high-level semantic priors, especially in structurally complex\npathology images. To address these limitations, we propose MPAMatch - a novel\nsegmentation framework that performs pixel-level contrastive learning under a\nmultimodal prototype-guided supervision paradigm. The core innovation of\nMPAMatch lies in the dual contrastive learning scheme between image prototypes\nand pixel labels, and between text prototypes and pixel labels, providing\nsupervision at both structural and semantic levels. This coarse-to-fine\nsupervisory strategy not only enhances the discriminative capability on\nunlabeled samples but also introduces the text prototype supervision into\nsegmentation for the first time, significantly improving semantic boundary\nmodeling. In addition, we reconstruct the classic segmentation architecture\n(TransUNet) by replacing its ViT backbone with a pathology-pretrained\nfoundation model (Uni), enabling more effective extraction of\npathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,\nEBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art\nmethods, validating its dual advantages in structural and semantic modeling.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19574v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19574v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.332,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on semi-supervised learning for pathology image segmentation, utilizing programmatically generated prototypes from image clustering and text prompts (e.g., via LLMs) to provide supervision on unlabeled data. This aligns with weak supervision by leveraging high-level, potentially noisy sources for labels, but the primary emphasis is on semi-supervised techniques rather than weak supervision as the core paradigm.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces MPAMatch, a novel semi-supervised framework for pathology image segmentation that addresses challenges like ambiguous boundaries and high annotation costs by incorporating multimodal prototype alignment. It employs pixel-level contrastive learning with dual supervision from image and text prototypes, rebuilds the TransUNet architecture using a pathology-pretrained model (Uni), and demonstrates superior performance over state-of-the-art methods on datasets such as GLAS and EBHI-SEG, enhancing both structural and semantic modeling.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly innovative technique by integrating multimodal (image and text) prototype-guided contrastive learning into semi-supervised segmentation, which significantly advances the state-of-the-art by incorporating text supervision for the first time in this context. This approach addresses limitations in existing methods, particularly for complex pathology images, marking a substantial contribution.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research in medical image segmentation and AI by promoting multimodal approaches in semi-supervised learning, likely leading to improved applications in clinical diagnostics. Its demonstrated superiority on multiple datasets suggests it could be adopted and built upon in computer vision and pathology subfields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a high-quality and valuable contribution to semi-supervised pathology image segmentation, offering novel insights and strong empirical results that are important for researchers in AI and computer vision. While essential for those in the field, it may not be groundbreaking enough to be classified as a must-read for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7ea08985d36ecfed3db5d15a4f610782343294fd",
      "total_authors": 7,
      "authors_found": 6,
      "highest_h_index": 6,
      "average_h_index": 2.6666666666666665,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Mingxi Fu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2364969880"
        },
        {
          "name": "Fanglei Fu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363789893"
        },
        {
          "name": "Xitong Ling",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2316860335"
        },
        {
          "name": "Huaitian Yuan",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Tian Guan",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2258108042"
        },
        {
          "name": "Yonghong He",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2257880233"
        },
        {
          "name": "Lianghui Zhu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2314774772"
        }
      ]
    },
    {
      "id": "2508.19575",
      "title": "Interact-Custom: Customized Human Object Interaction Image Generation",
      "authors": [
        "Zhu Xu",
        "Zhaowen Wang",
        "Yuxin Peng",
        "Yang Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Compositional Customized Image Generation aims to customize multiple target\nconcepts within generation content, which has gained attention for its wild\napplication. Existing approaches mainly concentrate on the target entity's\nappearance preservation, while neglecting the fine-grained interaction control\namong target entities. To enable the model of such interaction control\ncapability, we focus on human object interaction scenario and propose the task\nof Customized Human Object Interaction Image Generation(CHOI), which\nsimultaneously requires identity preservation for target human object and the\ninteraction semantic control between them. Two primary challenges exist for\nCHOI:(1)simultaneous identity preservation and interaction control demands\nrequire the model to decompose the human object into self-contained identity\nfeatures and pose-oriented interaction features, while the current HOI image\ndatasets fail to provide ideal samples for such feature-decomposed\nlearning.(2)inappropriate spatial configuration between human and object may\nlead to the lack of desired interaction semantics. To tackle it, we first\nprocess a large-scale dataset, where each sample encompasses the same pair of\nhuman object involving different interactive poses. Then we design a two-stage\nmodel Interact-Custom, which firstly explicitly models the spatial\nconfiguration by generating a foreground mask depicting the interaction\nbehavior, then under the guidance of this mask, we generate the target human\nobject interacting while preserving their identities features. Furthermore, if\nthe background image and the union location of where the target human object\nshould appear are provided by users, Interact-Custom also provides the optional\nfunctionality to specify them, offering high content controllability. Extensive\nexperiments on our tailored metrics for CHOI task demonstrate the effectiveness\nof our approach.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19575v2",
      "pdf_url": "http://arxiv.org/pdf/2508.19575v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.308,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on customized image generation for human-object interactions, involving dataset creation, diffusion models, and feature preservation for identity and spatial configuration. It does not discuss reinforcement learning, human feedback, reward models, or any mechanism for aligning AI models with human preferences through RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19576",
      "title": "ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized\n  Self-Training and Decoding",
      "authors": [
        "Sining Zhoubian",
        "Dan Zhang",
        "Jie Tang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "With respect to improving the reasoning accuracy of LLMs, the representative\nreinforcement learning (RL) method GRPO faces failure due to insignificant\nreward variance, while verification methods based on process reward models\n(PRMs) suffer from difficulties with training data acquisition and verification\neffectiveness. To tackle these problems, this paper introduces ReST-RL, a\nunified LLM RL paradigm that significantly improves LLM's code reasoning\nability by combining an improved GRPO algorithm with a meticulously designed\ntest time decoding method assisted by a value model (VM). As the first stage of\npolicy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter\nand assemble high-value training data, increasing the reward variance of GRPO\nsampling, thus improving the effectiveness and efficiency of training. After\nthe basic reasoning ability of LLM policy has been improved, we further propose\na test time decoding optimization method called VM-MCTS. Through Monte-Carlo\nTree Search (MCTS), we collect accurate value targets with no annotation\nrequired, on which VM training is based. When decoding, the VM is deployed by\nan adapted MCTS algorithm to provide precise process signals as well as\nverification scores, assisting the LLM policy to achieve high reasoning\naccuracy. We conduct extensive experiments on coding problems to verify the\nvalidity of the proposed RL paradigm. Upon comparison, our approach\nsignificantly outperforms other reinforcement training baselines (e.g., naive\nGRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,\nPRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,\nAPPS, BigCodeBench, and HumanEval), indicating its power to strengthen the\nreasoning ability of LLM policies. Codes for our project can be found at\nhttps://github.com/THUDM/ReST-RL.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19576v2",
      "pdf_url": "http://arxiv.org/pdf/2508.19576v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.486,
      "weak_supervision_score": 0.41,
      "diffusion_reasoning_score": 0.462,
      "distributed_training_score": 0.392,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on RL methods like GRPO and ReST for improving LLMs' code reasoning, using automated reward estimation via MCTS without any human feedback or human-ranked data. RLHF specifically requires training a reward model on human preferences, which is not present here.",
      "weak_supervision_justification": "The paper employs techniques like the optimized ReST algorithm and MCTS to programmatically generate and filter high-value training data without annotations, aligning with weak supervision's use of noisy or imprecise sources for label creation, such as automated reward estimation.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for reasoning; instead, it uses RL algorithms like GRPO and MCTS for code reasoning, with no mention of adapting diffusion mechanisms for multi-step logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces ReST-RL, a unified reinforcement learning framework aimed at enhancing the code reasoning accuracy of large language models (LLMs) by addressing limitations in existing methods like GRPO and process reward models. It employs a two-stage approach: first, an optimized ReST algorithm filters and assembles high-value training data to improve GRPO's reward variance and training efficiency; second, a value model-assisted Monte Carlo Tree Search (VM-MCTS) optimizes decoding at test time to provide precise process signals without additional annotations. Experimental results on benchmarks such as APPS, BigCodeBench, and HumanEval show that ReST-RL significantly outperforms baselines like naive GRPO, ReST-DPO, PRM-BoN, and ORM-MCTS, demonstrating superior reasoning ability, efficiency, and generalizability.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing techniques like GRPO and ReST with a new decoding method using VM-MCTS, effectively addressing known issues in LLM reinforcement without introducing a completely novel problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of LLM reinforcement learning for code reasoning, as it offers practical enhancements to training and decoding methods, though its influence may remain confined to specific applications rather than broadly transformative ones.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a strong, valuable contribution to improving LLM code reasoning through innovative RL techniques, making it essential for researchers in AI and machine learning to be aware of its methods and results.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2ff29547bac9f14e397105828514fb6bd01a8778",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 10,
      "average_h_index": 6.666666666666667,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Sining Zhoubian",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2279543303"
        },
        {
          "name": "Dan Zhang",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2215493789"
        },
        {
          "name": "Jie Tang",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2261087802"
        }
      ]
    },
    {
      "id": "2508.19578",
      "title": "Towards a Holistic and Automated Evaluation Framework for Multi-Level\n  Comprehension of LLMs in Book-Length Contexts",
      "authors": [
        "Jiaqi Deng",
        "Yuho Lee",
        "Nicole Hee-Yeon Kim",
        "Hyangsuk Min",
        "Taewon Yun",
        "Minjeong Ban",
        "Kim Yul",
        "Hwanjun Song"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce HAMLET, a holistic and automated framework for evaluating the\nlong-context comprehension of large language models (LLMs). HAMLET structures\nsource texts into a three-level key-fact hierarchy at root-, branch-, and\nleaf-levels, and employs query-focused summarization to evaluate how well\nmodels recall and faithfully represent information at each level. To validate\nthe reliability of our fully automated pipeline, we conduct a systematic human\nstudy, showing that our automatic evaluation achieves over 90% agreement with\nexpert human judgments, while reducing the cost by up to 25 times. HAMLET\nreveals that LLMs struggle with fine-grained comprehension, especially at the\nleaf level, and are sensitive to positional effects like the\nlost-in-the-middle. Analytical queries pose greater challenges than narrative\nones, and consistent performance gaps emerge between open-source and\nproprietary models, as well as across model scales. Our code and dataset are\npublicly available at https://github.com/DISL-Lab/HAMLET.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19578v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19578v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.438,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.358,
      "datasets_score": 0.401,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on an evaluation framework for LLMs' long-context comprehension, including human validation for reliability, but does not involve training models using human feedback or reinforcement learning techniques. There is no mention of aligning models with human preferences via a reward model or fine-tuning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a hierarchical evaluation framework for LLMs using query-focused summarization, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described. There is no component related to treating reasoning paths holistically for correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes creating and evaluating datasets for benchmarking LLMs on book-length contexts, with a fully automated pipeline for generating benchmark instances, and making their code and dataset publicly available. This directly aligns with research on dataset creation, analysis, and evaluation for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces HAMLET, a holistic and automated framework for evaluating large language models' (LLMs) comprehension of book-length texts by structuring information into a three-level hierarchical key-fact tree (root, branch, and leaf) and using query-focused summarization to assess recall and faithfulness at each level. It addresses evaluation challenges like limited scope, reference dependency, and scalability through an automated pipeline validated by human studies, revealing that LLMs struggle with fine-grained details, exhibit positional biases, and perform better on narrative than analytical queries, while also highlighting performance gaps between open-source and proprietary models.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel hierarchical evaluation framework and automated pipeline for assessing LLMs' long-context comprehension, significantly advancing beyond existing methods by addressing key limitations in scope and scalability.",
      "impact_score": "High",
      "impact_justification": "This work has the potential to influence a wide range of future research in AI and NLP by providing a scalable benchmark for evaluating LLMs on long texts, which could lead to improvements in model development and real-world applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, innovative contribution to LLM evaluation that provides practical tools and insights, making it essential for researchers focused on long-context processing in AI.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/987bf58a3688385cb608a2fe99ba69dd091e7174",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 3,
      "average_h_index": 1.25,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jiaqi Deng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2364843546"
        },
        {
          "name": "Yuho Lee",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2323567294"
        },
        {
          "name": "Nicole Hee-Yeon Kim",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2335816756"
        },
        {
          "name": "Hyangsuk Min",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/1891408783"
        },
        {
          "name": "Taewon Yun",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2323748079"
        },
        {
          "name": "Minjeong Ban",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2364750413"
        },
        {
          "name": "Kim Yul",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377559474"
        },
        {
          "name": "Hwanjun Song",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2348984722"
        }
      ]
    },
    {
      "id": "2508.19579",
      "title": "High-Speed FHD Full-Color Video Computer-Generated Holography",
      "authors": [
        "Haomiao Zhang",
        "Miao Cao",
        "Xuan Yu",
        "Hui Luo",
        "Yanling Piao",
        "Mengjie Qin",
        "Zhangyuan Li",
        "Ping Wang",
        "Xin Yuan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Computer-generated holography (CGH) is a promising technology for\nnext-generation displays. However, generating high-speed, high-quality\nholographic video requires both high frame rate display and efficient\ncomputation, but is constrained by two key limitations: ($i$) Learning-based\nmodels often produce over-smoothed phases with narrow angular spectra, causing\nsevere color crosstalk in high frame rate full-color displays such as\ndepth-division multiplexing and thus resulting in a trade-off between frame\nrate and color fidelity. ($ii$) Existing frame-by-frame optimization methods\ntypically optimize frames independently, neglecting spatial-temporal\ncorrelations between consecutive frames and leading to computationally\ninefficient solutions. To overcome these challenges, in this paper, we propose\na novel high-speed full-color video CGH generation scheme. First, we introduce\nSpectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase\ndistributions via frequency modulation, enabling high-fidelity full-color\ndisplay at high frame rates. Second, we present HoloMamba, a lightweight\nasymmetric Mamba-Unet architecture that explicitly models spatial-temporal\ncorrelations across video sequences to enhance reconstruction quality and\ncomputational efficiency. Extensive simulated and real-world experiments\ndemonstrate that SGDDM achieves high-fidelity full-color display without\ncompromise in frame rate, while HoloMamba generates FHD (1080p) full-color\nholographic video at over 260 FPS, more than 2.6$\\times$ faster than the prior\nstate-of-the-art Divide-Conquer-and-Merge Strategy.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19579v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19579v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.277,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.369,
      "datasets_score": 0.262,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19581",
      "title": "Guiding Noisy Label Conditional Diffusion Models with Score-based\n  Discriminator Correction",
      "authors": [
        "Dat Nguyen Cong",
        "Hieu Tran Bao",
        "Hoang Thanh-Tung"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Diffusion models have gained prominence as state-of-the-art techniques for\nsynthesizing images and videos, particularly due to their ability to scale\neffectively with large datasets. Recent studies have uncovered that these\nextensive datasets often contain mistakes from manual labeling processes.\nHowever, the extent to which such errors compromise the generative capabilities\nand controllability of diffusion models is not well studied. This paper\nintroduces Score-based Discriminator Correction (SBDC), a guidance technique\nfor aligning noisy pre-trained conditional diffusion models. The guidance is\nbuilt on discriminator training using adversarial loss, drawing on prior noise\ndetection techniques to assess the authenticity of each sample. We further show\nthat limiting the usage of our guidance to the early phase of the generation\nprocess leads to better performance. Our method is computationally efficient,\nonly marginally increases inference time, and does not require retraining\ndiffusion models. Experiments on different noise settings demonstrate the\nsuperiority of our method over previous state-of-the-art methods.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19581v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19581v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.449,
      "diffusion_reasoning_score": 0.518,
      "distributed_training_score": 0.334,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper directly addresses the challenges of training diffusion models on noisy, mislabeled datasets, which aligns with weak supervision's core idea of using imperfect or programmatically generated labels. It proposes SBDC to correct outputs from such training without retraining, effectively mitigating the effects of noisy labels, making it a strong fit for weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper focuses on correcting noisy labels in conditional diffusion models for image generation, with no mention of adapting diffusion for multi-step logical reasoning, chain-of-thought processes, or iterative refinement of complex logical tasks. It is solely about generative synthesis, not reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of noisy labels in datasets used for training conditional diffusion models by introducing Score-based Discriminator Correction (SBDC), a guidance technique that employs a discriminator trained with adversarial loss to detect and correct label noise during inference, without requiring retraining of the pre-trained models. The methodology focuses on applying guidance primarily in the early phase of the generation process for efficiency, and experiments across various datasets with different noise levels demonstrate that SBDC outperforms state-of-the-art methods like TDSM in terms of image quality, alignment with conditions, and computational overhead.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing ideas like adversarial training and noise detection into a new inference-time guidance method for diffusion models, effectively addressing noisy labels without retraining.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of generative models for computer vision, as it offers an efficient solution to handle noisy datasets, though its influence may be limited to specific applications involving diffusion models.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a practical and effective advancement in managing noisy labels for diffusion models, making it a valuable resource for researchers focused on robust generative techniques in computer vision.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b5abf73c5c7c1ecccbfb5286ec2b62a447466734",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Dat Nguyen Cong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377559469"
        },
        {
          "name": "Hieu Tran Bao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377558981"
        },
        {
          "name": "Hoang Thanh-Tung",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2364407184"
        }
      ]
    },
    {
      "id": "2508.19587",
      "title": "Towards stable AI systems for Evaluating Arabic Pronunciations",
      "authors": [
        "Hadi Zaatiti",
        "Hatem Hajri",
        "Osama Abdullah",
        "Nader Masmoudi"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Modern Arabic ASR systems such as wav2vec 2.0 excel at word- and\nsentence-level transcription, yet struggle to classify isolated letters. In\nthis study, we show that this phoneme-level task, crucial for language\nlearning, speech therapy, and phonetic research, is challenging because\nisolated letters lack co-articulatory cues, provide no lexical context, and\nlast only a few hundred milliseconds. Recogniser systems must therefore rely\nsolely on variable acoustic cues, a difficulty heightened by Arabic's emphatic\n(pharyngealized) consonants and other sounds with no close analogues in many\nlanguages. This study introduces a diverse, diacritised corpus of isolated\nArabic letters and demonstrates that state-of-the-art wav2vec 2.0 models\nachieve only 35% accuracy on it. Training a lightweight neural network on\nwav2vec embeddings raises performance to 65%. However, adding a small amplitude\nperturbation (epsilon = 0.05) cuts accuracy to 32%. To restore robustness, we\napply adversarial training, limiting the noisy-speech drop to 9% while\npreserving clean-speech accuracy. We detail the corpus, training pipeline, and\nevaluation protocol, and release, on demand, data and code for reproducibility.\nFinally, we outline future work extending these methods to word- and\nsentence-level frameworks, where precise letter pronunciation remains critical.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19587v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19587v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.329,
      "distributed_training_score": 0.333,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19588",
      "title": "Hallucinating with AI: AI Psychosis as Distributed Delusions",
      "authors": [
        "Lucy Osler"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "There is much discussion of the false outputs that generative AI systems such\nas ChatGPT, Claude, Gemini, DeepSeek, and Grok create. In popular terminology,\nthese have been dubbed AI hallucinations. However, deeming these AI outputs\nhallucinations is controversial, with many claiming this is a metaphorical\nmisnomer. Nevertheless, in this paper, I argue that when viewed through the\nlens of distributed cognition theory, we can better see the dynamic and\ntroubling ways in which inaccurate beliefs, distorted memories and\nself-narratives, and delusional thinking can emerge through human-AI\ninteractions; examples of which are popularly being referred to as cases of AI\npsychosis. In such cases, I suggest we move away from thinking about how an AI\nsystem might hallucinate at us, by generating false outputs, to thinking about\nhow, when we routinely rely on generative AI to help us think, remember, and\nnarrate, we can come to hallucinate with AI. This can happen when AI introduces\nerrors into the distributed cognitive process, but it can also happen when AI\nsustains, affirms, and elaborates on our own delusional thinking and\nself-narratives, such as in the case of Jaswant Singh Chail. I also examine how\nthe conversational style of chatbots can lead them to play a dual-function,\nboth as a cognitive artefact and a quasi-Other with whom we co-construct our\nbeliefs, narratives, and our realities. It is this dual function, I suggest,\nthat makes generative AI an unusual, and particularly seductive, case of\ndistributed cognition.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19588v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19588v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.305,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.32,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on human-AI interactions, distributed cognition, and the psychological aspects of AI hallucinations, using examples from generative AI systems. It does not discuss diffusion models, iterative refinement processes for reasoning, or any adaptation of diffusion techniques for complex logical tasks. As such, there is no connection to multi-step logical reasoning via diffusion models.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19593",
      "title": "Generalizing Monocular 3D Object Detection",
      "authors": [
        "Abhinav Kumar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Monocular 3D object detection (Mono3D) is a fundamental computer vision task\nthat estimates an object's class, 3D position, dimensions, and orientation from\na single image. Its applications, including autonomous driving, augmented\nreality, and robotics, critically rely on accurate 3D environmental\nunderstanding. This thesis addresses the challenge of generalizing Mono3D\nmodels to diverse scenarios, including occlusions, datasets, object sizes, and\ncamera parameters. To enhance occlusion robustness, we propose a mathematically\ndifferentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we\nexplore depth equivariant (DEVIANT) backbones. We address the issue of large\nobject detection, demonstrating that it's not solely a data imbalance or\nreceptive field problem but also a noise sensitivity issue. To mitigate this,\nwe introduce a segmentation-based approach in bird's-eye view with dice loss\n(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D\nmodels to unseen camera heights and improve Mono3D generalization in such\nout-of-distribution settings.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19593v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19593v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.282,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.398,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19597",
      "title": "Complementary Learning System Empowers Online Continual Learning of\n  Vehicle Motion Forecasting in Smart Cities",
      "authors": [
        "Zirui Li",
        "Yunlong Lin",
        "Guodong Du",
        "Xiaocong Zhao",
        "Cheng Gong",
        "Chen Lv",
        "Chao Lu",
        "Jianwei Gong"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Artificial intelligence underpins most smart city services, yet deep neural\nnetwork (DNN) that forecasts vehicle motion still struggle with catastrophic\nforgetting, the loss of earlier knowledge when models are updated. Conventional\nfixes enlarge the training set or replay past data, but these strategies incur\nhigh data collection costs, sample inefficiently and fail to balance long- and\nshort-term experience, leaving them short of human-like continual learning.\nHere we introduce Dual-LS, a task-free, online continual learning paradigm for\nDNN-based motion forecasting that is inspired by the complementary learning\nsystem of the human brain. Dual-LS pairs two synergistic memory rehearsal\nreplay mechanisms to accelerate experience retrieval while dynamically\ncoordinating long-term and short-term knowledge representations. Tests on\nnaturalistic data spanning three countries, over 772,000 vehicles and\ncumulative testing mileage of 11,187 km show that Dual-LS mitigates\ncatastrophic forgetting by up to 74.31\\% and reduces computational resource\ndemand by up to 94.02\\%, markedly boosting predictive stability in vehicle\nmotion forecasting without inflating data requirements. Meanwhile, it endows\nDNN-based vehicle motion forecasting with computation efficient and human-like\ncontinual learning adaptability fit for smart cities.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19597v2",
      "pdf_url": "http://arxiv.org/pdf/2508.19597v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.426,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.434,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on continual learning for vehicle motion forecasting using memory rehearsal and brain-inspired mechanisms, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning; it centers on neural network-based continual learning for motion forecasting without any reasoning components.",
      "distributed_training_justification": "While the paper mentions reducing computational resource demands for continual learning, it does not discuss distributed training, parallel computing, or partitioning across multiple nodes; its focus is on algorithmic efficiency in memory management.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19600",
      "title": "Quantization Robustness to Input Degradations for Object Detection",
      "authors": [
        "Toghrul Karimov",
        "Hassan Imani",
        "Allan Kazakov"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Post-training quantization (PTQ) is crucial for deploying efficient object\ndetection models, like YOLO, on resource-constrained devices. However, the\nimpact of reduced precision on model robustness to real-world input\ndegradations such as noise, blur, and compression artifacts is a significant\nconcern. This paper presents a comprehensive empirical study evaluating the\nrobustness of YOLO models (nano to extra-large scales) across multiple\nprecision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8\n(TensorRT). We introduce and evaluate a degradation-aware calibration strategy\nfor Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix\nof clean and synthetically degraded images. Models were benchmarked on the COCO\ndataset under seven distinct degradation conditions (including various types\nand levels of noise, blur, low contrast, and JPEG compression) and a\nmixed-degradation scenario. Results indicate that while Static INT8 TensorRT\nengines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop\n(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did\nnot yield consistent, broad improvements in robustness over standard clean-data\ncalibration across most models and degradations. A notable exception was\nobserved for larger model scales under specific noise conditions, suggesting\nmodel capacity may influence the efficacy of this calibration approach. These\nfindings highlight the challenges in enhancing PTQ robustness and provide\ninsights for deploying quantized detectors in uncontrolled environments. All\ncode and evaluation tables are available at https://github.com/AllanK24/QRID.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19600v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19600v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.381,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19603",
      "title": "CompLex: Music Theory Lexicon Constructed by Autonomous Agents for\n  Automatic Music Generation",
      "authors": [
        "Zhejing Hu",
        "Yan Liu",
        "Gong Chen",
        "Bruce X. B. Yu"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generative artificial intelligence in music has made significant strides, yet\nit still falls short of the substantial achievements seen in natural language\nprocessing, primarily due to the limited availability of music data.\nKnowledge-informed approaches have been shown to enhance the performance of\nmusic generation models, even when only a few pieces of musical knowledge are\nintegrated. This paper seeks to leverage comprehensive music theory in\nAI-driven music generation tasks, such as algorithmic composition and style\ntransfer, which traditionally require significant manual effort with existing\ntechniques. We introduce a novel automatic music lexicon construction model\nthat generates a lexicon, named CompLex, comprising 37,432 items derived from\njust 9 manually input category keywords and 5 sentence prompt templates. A new\nmulti-agent algorithm is proposed to automatically detect and mitigate\nhallucinations. CompLex demonstrates impressive performance improvements across\nthree state-of-the-art text-to-music generation models, encompassing both\nsymbolic and audio-based methods. Furthermore, we evaluate CompLex in terms of\ncompleteness, accuracy, non-redundancy, and executability, confirming that it\npossesses the key characteristics of an effective lexicon.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19603v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19603v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.287,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of a multi-agent algorithm for constructing a music theory lexicon, focusing on autonomous agents to generate and refine lexical items for music generation tasks. It does not involve diffusion models, iterative refinement for logical reasoning, or any adaptation of diffusion processes for complex tasks. Therefore, there is no clear component of multi-step logical reasoning using a diffusion model.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19604",
      "title": "IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers\n  for Domain Generalized Semantic Segmentation",
      "authors": [
        "Qizhe Fan",
        "Chaoyu Liu",
        "Zhonghua Qiao",
        "Xiaoqin Shen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Domain Generalized Semantic Segmentation (DGSS) focuses on training a model\nusing labeled data from a source domain, with the goal of achieving robust\ngeneralization to unseen target domains during inference. A common approach to\nimprove generalization is to augment the source domain with synthetic data\ngenerated by diffusion models (DMs). However, the generated images often\ncontain structural or semantic defects due to training imperfections. Training\nsegmentation models with such flawed data can lead to performance degradation\nand error accumulation. To address this issue, we propose to integrate inverse\nevolution layers (IELs) into the generative process. IELs are designed to\nhighlight spatial discontinuities and semantic inconsistencies using\nLaplacian-based priors, enabling more effective filtering of undesirable\ngenerative patterns. Based on this mechanism, we introduce IELDM, an enhanced\ndiffusion-based data augmentation framework that can produce higher-quality\nimages. Furthermore, we observe that the defect-suppression capability of IELs\ncan also benefit the segmentation network by suppressing artifact propagation.\nBased on this insight, we embed IELs into the decoder of the DGSS model and\npropose IELFormer to strengthen generalization capability in cross-domain\nscenarios. To further strengthen the model's semantic consistency across\nscales, IELFormer incorporates a multi-scale frequency fusion (MFF) module,\nwhich performs frequency-domain analysis to achieve structured integration of\nmulti-resolution features, thereby improving cross-scale coherence. Extensive\nexperiments on benchmark datasets demonstrate that our approach achieves\nsuperior generalization performance compared to existing methods.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19604v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19604v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.437,
      "diffusion_reasoning_score": 0.478,
      "distributed_training_score": 0.381,
      "datasets_score": 0.405,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves generating and refining synthetic data with potential defects, which could be seen as a form of noisy supervision, but it primarily focuses on improving data quality for domain generalized semantic segmentation rather than directly employing weak supervision techniques like programmatic labeling from high-level sources.",
      "diffusion_reasoning_justification": "The paper uses diffusion models solely for image generation and data augmentation in semantic segmentation, without any adaptation for multi-step logical reasoning or treating a Chain-of-Thought as an entity for iterative refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper proposes IELDM to generate and improve synthetic datasets for training, which relates to dataset creation and curation for machine learning, but its main focus is on enhancing semantic segmentation models rather than deeply analyzing, benchmarking, or introducing new datasets.",
      "llm_score_status": "completed",
      "summary": "This paper introduces IELDG, a framework for domain generalized semantic segmentation (DGSS) that enhances diffusion model-generated images using inverse evolution layers (IELs) to suppress defects, and integrates these layers into a segmentation model called IELFormer to improve prediction accuracy and robustness. By incorporating a multi-scale frequency fusion (MFF) module for better feature integration, the approach achieves superior generalization on unseen domains, as validated through extensive experiments on benchmark datasets.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of inverse evolution layers with diffusion models and segmentation architectures to address domain-specific noise in DGSS, offering a notable improvement over existing methods without introducing an entirely new problem or technique. While innovative in application, it builds on established concepts like Laplacian-based priors and diffusion models.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of domain generalized semantic segmentation due to its practical enhancements in handling domain shifts. However, its influence may be limited to specific applications in computer vision and AI, rather than broadly across disciplines.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to DGSS by introducing effective techniques for improving model robustness, making it essential for researchers in computer vision. While not groundbreaking for all audiences, it offers insights that could enhance related work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/91847ea8cca16736637a6974245656fb777f57c7",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 3,
      "average_h_index": 1.25,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Qizhe Fan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2303021757"
        },
        {
          "name": "Chao Liu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2152505919"
        },
        {
          "name": "Zhonghua Qiao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377561211"
        },
        {
          "name": "Xiaoqin Shen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2352446879"
        }
      ]
    },
    {
      "id": "2508.19609",
      "title": "FinCast: A Foundation Model for Financial Time-Series Forecasting",
      "authors": [
        "Zhuohang Zhu",
        "Haodong Chen",
        "Qiang Qu",
        "Vera Chung"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Financial time-series forecasting is critical for maintaining economic\nstability, guiding informed policymaking, and promoting sustainable investment\npractices. However, it remains challenging due to various underlying pattern\nshifts. These shifts arise primarily from three sources: temporal\nnon-stationarity (distribution changes over time), multi-domain diversity\n(distinct patterns across financial domains such as stocks, commodities, and\nfutures), and varying temporal resolutions (patterns differing across\nper-second, hourly, daily, or weekly indicators). While recent deep learning\nmethods attempt to address these complexities, they frequently suffer from\noverfitting and typically require extensive domain-specific fine-tuning. To\novercome these limitations, we introduce FinCast, the first foundation model\nspecifically designed for financial time-series forecasting, trained on\nlarge-scale financial datasets. Remarkably, FinCast exhibits robust zero-shot\nperformance, effectively capturing diverse patterns without domain-specific\nfine-tuning. Comprehensive empirical and qualitative evaluations demonstrate\nthat FinCast surpasses existing state-of-the-art methods, highlighting its\nstrong generalization capabilities.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19609v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19609v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.357,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19611",
      "title": "Instructional Agents: LLM Agents on Automated Course Material Generation\n  for Teaching Faculties",
      "authors": [
        "Huaiyuan Yao",
        "Wanpeng Xu",
        "Justin Turnau",
        "Nadia Kellam",
        "Hua Wei"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Preparing high-quality instructional materials remains a labor-intensive\nprocess that often requires extensive coordination among teaching faculty,\ninstructional designers, and teaching assistants. In this work, we present\nInstructional Agents, a multi-agent large language model (LLM) framework\ndesigned to automate end-to-end course material generation, including syllabus\ncreation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing\nAI-assisted educational tools that focus on isolated tasks, Instructional\nAgents simulates role-based collaboration among educational agents to produce\ncohesive and pedagogically aligned content. The system operates in four modes:\nAutonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling\nflexible control over the degree of human involvement. We evaluate\nInstructional Agents across five university-level computer science courses and\nshow that it produces high-quality instructional materials while significantly\nreducing development time and human workload. By supporting institutions with\nlimited instructional design capacity, Instructional Agents provides a scalable\nand cost-effective framework to democratize access to high-quality education,\nparticularly in underserved or resource-constrained settings.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19611v2",
      "pdf_url": "http://arxiv.org/pdf/2508.19611v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.346,
      "datasets_score": 0.384,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a multi-agent LLM framework for automating course material generation, which includes modes like Feedback-Guided that incorporate human input. However, it does not describe training a reward model on human-ranked data or fine-tuning the main model using reinforcement learning, as required for RLHF. The human feedback mentioned is for operational guidance, not for aligning AI models through RLHF techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19614",
      "title": "LFD: Layer Fused Decoding to Exploit External Knowledge in\n  Retrieval-Augmented Generation",
      "authors": [
        "Yang Sun",
        "Lixin Zou",
        "Dan Luo",
        "Zhiyong Xie",
        "Long Zhang",
        "Liming Dong",
        "Yunwei Zhao",
        "Xixun Lin",
        "Yanxiong Lu",
        "Chenliang Li"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Retrieval-augmented generation (RAG) incorporates external knowledge into\nlarge language models (LLMs), improving their adaptability to downstream tasks\nand enabling information updates. Surprisingly, recent empirical evidence\ndemonstrates that injecting noise into retrieved relevant documents\nparadoxically facilitates exploitation of external knowledge and improves\ngeneration quality. Although counterintuitive and challenging to apply in\npractice, this phenomenon enables granular control and rigorous analysis of how\nLLMs integrate external knowledge. Therefore, in this paper, we intervene on\nnoise injection and establish a layer-specific functional demarcation within\nthe LLM: shallow layers specialize in local context modeling, intermediate\nlayers focus on integrating long-range external factual knowledge, and deeper\nlayers primarily rely on parametric internal knowledge. Building on this\ninsight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that\ndirectly combines representations from an intermediate layer with final-layer\ndecoding outputs to fully exploit the external factual knowledge. To identify\nthe optimal intermediate layer, we introduce an internal knowledge score (IKS)\ncriterion that selects the layer with the lowest IKS value in the latter half\nof layers. Experimental results across multiple benchmarks demonstrate that LFD\nhelps RAG systems more effectively surface retrieved context knowledge with\nminimal cost.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19614v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19614v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.436,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.508,
      "distributed_training_score": 0.358,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on improving Retrieval-Augmented Generation (RAG) through layer fusion and external knowledge integration in LLMs, without any mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "Although the paper experiments with noise injection in retrieved documents for analysis, it does not involve training models using programmatically generated labels or weak supervision methods; instead, it centers on inference strategies for LLMs.",
      "diffusion_reasoning_justification": "The paper proposes a layer fusion decoding method for RAG, which deals with knowledge integration in LLMs, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19620",
      "title": "A Scenario-Oriented Survey of Federated Recommender Systems: Techniques,\n  Challenges, and Future Directions",
      "authors": [
        "Yunqi Mi",
        "Jiakui Shen",
        "Guoshuai Zhao",
        "Jialie Shen",
        "Xueming Qian"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Extending recommender systems to federated learning (FL) frameworks to\nprotect the privacy of users or platforms while making recommendations has\nrecently gained widespread attention in academia. This is due to the natural\ncoupling of recommender systems and federated learning architectures: the data\noriginates from distributed clients (mostly mobile devices held by users),\nwhich are highly related to privacy. In a centralized recommender system\n(CenRec), the central server collects clients' data, trains the model, and\nprovides the service. Whereas in federated recommender systems (FedRec), the\nstep of data collecting is omitted, and the step of model training is offloaded\nto each client. The server only aggregates the model and other knowledge, thus\navoiding client privacy leakage. Some surveys of federated recommender systems\ndiscuss and analyze related work from the perspective of designing FL systems.\nHowever, their utility drops by ignoring specific recommendation scenarios'\nunique characteristics and practical challenges. For example, the statistical\nheterogeneity issue in cross-domain FedRec originates from the label drift of\nthe data held by different platforms, which is mainly caused by the recommender\nitself, but not the federated architecture. Therefore, it should focus more on\nsolving specific problems in real-world recommendation scenarios to encourage\nthe deployment FedRec. To this end, this review comprehensively analyzes the\ncoupling of recommender systems and federated learning from the perspective of\nrecommendation researchers and practitioners. We establish a clear link between\nrecommendation scenarios and FL frameworks, systematically analyzing\nscenario-specific approaches, practical challenges, and potential\nopportunities. We aim to develop guidance for the real-world deployment of\nFedRec, bridging the gap between existing research and applications.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19620v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19620v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.307,
      "distributed_training_score": 0.368,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on federated recommender systems, emphasizing privacy protection, distributed learning, and specific recommendation scenarios like collaborative and cross-domain FedRec. It does not involve reinforcement learning, human feedback, reward models, or any mechanisms for aligning AI models with human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19621",
      "title": "Towards Instance-wise Personalized Federated Learning via Semi-Implicit\n  Bayesian Prompt Tuning",
      "authors": [
        "Tiandi Ye",
        "Wenyan Liu",
        "Kai Yao",
        "Lichun Li",
        "Shangchao Su",
        "Cen Chen",
        "Xiang Li",
        "Shan Yin",
        "Ming Gao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Federated learning (FL) is a privacy-preserving machine learning paradigm\nthat enables collaborative model training across multiple distributed clients\nwithout disclosing their raw data. Personalized federated learning (pFL) has\ngained increasing attention for its ability to address data heterogeneity.\nHowever, most existing pFL methods assume that each client's data follows a\nsingle distribution and learn one client-level personalized model for each\nclient. This assumption often fails in practice, where a single client may\npossess data from multiple sources or domains, resulting in significant\nintra-client heterogeneity and suboptimal performance. To tackle this\nchallenge, we propose pFedBayesPT, a fine-grained instance-wise pFL framework\nbased on visual prompt tuning. Specifically, we formulate instance-wise prompt\ngeneration from a Bayesian perspective and model the prompt posterior as an\nimplicit distribution to capture diverse visual semantics. We derive a\nvariational training objective under the semi-implicit variational inference\nframework. Extensive experiments on benchmark datasets demonstrate that\npFedBayesPT consistently outperforms existing pFL methods under both feature\nand label heterogeneity settings.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19621v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19621v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.43,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on federated learning and personalized prompt tuning using Bayesian methods, with no mention of human feedback, reward models, or reinforcement learning techniques. It does not involve aligning AI models with human preferences or using human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's core contribution is on federated learning, a distributed training paradigm that involves collaborative model training across decentralized clients without sharing raw data. It discusses strategies for local training, model aggregation, and handling data heterogeneity across nodes, directly aligning with distributed training concepts.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces pFedBayesPT, a novel framework for instance-wise personalized federated learning that addresses intra-client data heterogeneity by generating personalized visual prompts for each instance using a Bayesian approach. It models the prompt posterior as an implicit distribution and employs semi-implicit variational inference to enhance model expressiveness while mitigating overfitting, demonstrating superior performance over existing methods on datasets like DomainNet and CIFAR-100 under various heterogeneity settings.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by formulating instance-level visual prompt tuning from a Bayesian perspective in federated learning, which significantly advances the state-of-the-art in handling intra-client heterogeneity. This represents a fresh integration of Bayesian methods and prompt tuning, not previously explored in this context.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of personalized federated learning due to its innovative approach to data heterogeneity. However, its influence may be limited to specific applications like visual data processing, rather than broadly across all AI or commercial domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to federated learning research with its novel methodology and empirical results, making it important for researchers in machine learning and AI to be aware of. While not essential for all readers, it provides significant insights for those working on data heterogeneity in distributed systems.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ceadaf35d44081b8b574a96765656e8bda21a01e",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 4,
      "average_h_index": 1.2222222222222223,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Tiandi Ye",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2143269857"
        },
        {
          "name": "Wenyan Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2335518372"
        },
        {
          "name": "Kai Yao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2315135662"
        },
        {
          "name": "Lichun Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2293445241"
        },
        {
          "name": "Shangchao Su",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377742758"
        },
        {
          "name": "Cen Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377640744"
        },
        {
          "name": "Xiang Li",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2267727882"
        },
        {
          "name": "Shan Yin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379547095"
        },
        {
          "name": "Ming Gao",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2216320819"
        }
      ]
    },
    {
      "id": "2508.19625",
      "title": "Training for Obsolescence? The AI-Driven Education Trap",
      "authors": [
        "Andrew J. Peterson"
      ],
      "categories": [
        "econ.GN (General Economics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Artificial intelligence simultaneously transforms human capital production in\nschools and its demand in labor markets. Analyzing these effects in isolation\ncan lead to a significant misallocation of educational resources. We model an\neducational planner whose decision to adopt AI is driven by its teaching\nproductivity, failing to internalize AI's future wage-suppressing effect on\nthose same skills. Our core assumption, motivated by a pilot survey, is that\nthere is a positive correlation between these two effects. This drives our\ncentral proposition: this information failure creates a skill mismatch that\nmonotonically increases with AI prevalence. Extensions show the mismatch is\nexacerbated by the neglect of unpriced non-cognitive skills and by a school's\nendogenous over-investment in AI. Our findings caution that policies promoting\nAI in education, if not paired with forward-looking labor market signals, may\nparadoxically undermine students' long-term human capital, especially if\nreliance on AI crowds out the development of unpriced non-cognitive skills,\nsuch as persistence, that are forged through intellectual struggle.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19625v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19625v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.378,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19626",
      "title": "Controllable Skin Synthesis via Lesion-Focused Vector Autoregression\n  Model",
      "authors": [
        "Jiajun Sun",
        "Zhen Yu",
        "Siyuan Yan",
        "Jason J. Ong",
        "Zongyuan Ge",
        "Lei Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Skin images from real-world clinical practice are often limited, resulting in\na shortage of training data for deep-learning models. While many studies have\nexplored skin image synthesis, existing methods often generate low-quality\nimages and lack control over the lesion's location and type. To address these\nlimitations, we present LF-VAR, a model leveraging quantified lesion\nmeasurement scores and lesion type labels to guide the clinically relevant and\ncontrollable synthesis of skin images. It enables controlled skin synthesis\nwith specific lesion characteristics based on language prompts. We train a\nmultiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to\nencode images into discrete latent representations for structured tokenization.\nThen, a Visual AutoRegressive (VAR) Transformer trained on tokenized\nrepresentations facilitates image synthesis. Lesion measurement from the lesion\nregion and types as conditional embeddings are integrated to enhance synthesis\nfidelity. Our method achieves the best overall FID score (average 0.74) among\nseven lesion types, improving upon the previous state-of-the-art (SOTA) by\n6.3%. The study highlights our controllable skin synthesis model's\neffectiveness in generating high-fidelity, clinically relevant synthetic skin\nimages. Our framework code is available at\nhttps://github.com/echosun1996/LF-VAR.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19626v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19626v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.323,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19630",
      "title": "Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic\n  Expert Fusion for Long-tailed Recognition",
      "authors": [
        "Xiaolei Wei",
        "Yi Ouyang",
        "Haibo Ye"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Long-tailed visual recognition is challenging not only due to class imbalance\nbut also because of varying classification difficulty across categories. Simply\nreweighting classes by frequency often overlooks those that are intrinsically\nhard to learn. To address this, we propose \\textbf{DQRoute}, a modular\nframework that combines difficulty-aware optimization with dynamic expert\ncollaboration. DQRoute first estimates class-wise difficulty based on\nprediction uncertainty and historical performance, and uses this signal to\nguide training with adaptive loss weighting. On the architectural side, DQRoute\nemploys a mixture-of-experts design, where each expert specializes in a\ndifferent region of the class distribution. At inference time, expert\npredictions are weighted by confidence scores derived from expert-specific OOD\ndetectors, enabling input-adaptive routing without the need for a centralized\nrouter. All components are trained jointly in an end-to-end manner. Experiments\non standard long-tailed benchmarks demonstrate that DQRoute significantly\nimproves performance, particularly on rare and difficult classes, highlighting\nthe benefit of integrating difficulty modeling with decentralized expert\nrouting.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19630v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19630v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.43,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.444,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on difficulty-aware optimization and dynamic expert fusion for long-tailed visual recognition, emphasizing adaptive loss weighting based on class difficulty and performance. It does not involve programmatically generating noisy labels or rely on weak supervision techniques for training.",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for long-tailed recognition using mixture-of-experts and adaptive routing, without any reference to diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes.",
      "distributed_training_justification": "Although the paper employs a mixture-of-experts architecture with dynamic routing, it is about internal model design and end-to-end training, not about parallel computing, data partitioning across nodes, or accelerating training via distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19637",
      "title": "Invited Paper: Feature-to-Classifier Co-Design for Mixed-Signal Smart\n  Flexible Wearables for Healthcare at the Extreme Edge",
      "authors": [
        "Maha Shatta",
        "Konstantinos Balaskas",
        "Paula Carolina Lozano Duarte",
        "Georgios Panagopoulos",
        "Mehdi B. Tahoori",
        "Georgios Zervakis"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Flexible Electronics (FE) offer a promising alternative to rigid\nsilicon-based hardware for wearable healthcare devices, enabling lightweight,\nconformable, and low-cost systems. However, their limited integration density\nand large feature sizes impose strict area and power constraints, making\nML-based healthcare systems-integrating analog frontend, feature extraction and\nclassifier-particularly challenging. Existing FE solutions often neglect\npotential system-wide solutions and focus on the classifier, overlooking the\nsubstantial hardware cost of feature extraction and Analog-to-Digital\nConverters (ADCs)-both major contributors to area and power consumption. In\nthis work, we present a holistic mixed-signal feature-to-classifier co-design\nframework for flexible smart wearable systems. To the best of our knowledge, we\ndesign the first analog feature extractors in FE, significantly reducing\nfeature extraction cost. We further propose an hardware-aware NAS-inspired\nfeature selection strategy within ML training, enabling efficient,\napplication-specific designs. Our evaluation on healthcare benchmarks shows our\napproach delivers highly accurate, ultra-area-efficient flexible systems-ideal\nfor disposable, low-power wearable monitoring.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19637v2",
      "pdf_url": "http://arxiv.org/pdf/2508.19637v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.297,
      "distributed_training_score": 0.386,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19638",
      "title": "Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception",
      "authors": [
        "Yang Li",
        "Quan Yuan",
        "Guiyang Luo",
        "Xiaoyuan Fu",
        "Rui Pan",
        "Yujia Yang",
        "Congzhang Shao",
        "Yuewen Liu",
        "Jinglin Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Collaborative perception allows agents to enhance their perceptual\ncapabilities by exchanging intermediate features. Existing methods typically\norganize these intermediate features as 2D bird's-eye-view (BEV)\nrepresentations, which discard critical fine-grained 3D structural cues\nessential for accurate object recognition and localization. To this end, we\nfirst introduce point-level tokens as intermediate representations for\ncollaborative perception. However, point-cloud data are inherently unordered,\nmassive, and position-sensitive, making it challenging to produce compact and\naligned point-level token sequences that preserve detailed structural\ninformation. Therefore, we present CoPLOT, a novel Collaborative perception\nframework that utilizes Point-Level Optimized Tokens. It incorporates a\npoint-native processing pipeline, including token reordering, sequence\nmodeling, and multi-agent spatial alignment. A semantic-aware token reordering\nmodule generates adaptive 1D reorderings by leveraging scene-level and\ntoken-level semantic information. A frequency-enhanced state space model\ncaptures long-range sequence dependencies across both spatial and spectral\ndomains, improving the differentiation between foreground tokens and background\nclutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop\nprocess, combining global agent-level correction with local token-level\nrefinement to mitigate localization noise. Extensive experiments on both\nsimulated and real-world datasets show that CoPLOT outperforms state-of-the-art\nmodels, with even lower communication and computation overhead. Code will be\navailable at https://github.com/CheeryLeeyy/CoPLOT.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19638v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19638v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.415,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on collaborative perception using point-level tokens and state space models for 3D scene processing, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. It is solely about perception enhancements, not adapting diffusion for reasoning.",
      "distributed_training_justification": "The paper addresses multi-agent collaboration for perception tasks, involving feature exchange to reduce overhead, but it does not discuss distributed training techniques, parallel computing for model training, or strategies for partitioning data/computation across nodes. It is focused on inference in collaborative settings, not training acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19641",
      "title": "Intellectual Property in Graph-Based Machine Learning as a Service:\n  Attacks and Defenses",
      "authors": [
        "Lincan Li",
        "Bolin Shen",
        "Chenxi Zhao",
        "Yuxiang Sun",
        "Kaixiang Zhao",
        "Shirui Pan",
        "Yushun Dong"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Graph-structured data, which captures non-Euclidean relationships and\ninteractions between entities, is growing in scale and complexity. As a result,\ntraining state-of-the-art graph machine learning (GML) models have become\nincreasingly resource-intensive, turning these models and data into invaluable\nIntellectual Property (IP). To address the resource-intensive nature of model\ntraining, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an\nefficient solution by leveraging third-party cloud services for model\ndevelopment and management. However, deploying such models in GMLaaS also\nexposes them to potential threats from attackers. Specifically, while the APIs\nwithin a GMLaaS system provide interfaces for users to query the model and\nreceive outputs, they also allow attackers to exploit and steal model\nfunctionalities or sensitive training data, posing severe threats to the safety\nof these GML models and the underlying graph data. To address these challenges,\nthis survey systematically introduces the first taxonomy of threats and\ndefenses at the level of both GML model and graph-structured data. Such a\ntailored taxonomy facilitates an in-depth understanding of GML IP protection.\nFurthermore, we present a systematic evaluation framework to assess the\neffectiveness of IP protection methods, introduce a curated set of benchmark\ndatasets across various domains, and discuss their application scopes and\nfuture challenges. Finally, we establish an open-sourced versatile library\nnamed PyGIP, which evaluates various attack and defense techniques in GMLaaS\nscenarios and facilitates the implementation of existing benchmark methods. The\nlibrary resource can be accessed at: https://labrai.github.io/PyGIP. We believe\nthis survey will play a fundamental role in intellectual property protection\nfor GML and provide practical recipes for the GML community.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19641v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19641v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.375,
      "datasets_score": 0.391,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19647",
      "title": "UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural\n  Networks",
      "authors": [
        "Bikash Kumar Badatya",
        "Vipul Baghel",
        "Ravi Hegde"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Fine-grained action localization in untrimmed sports videos presents a\nsignificant challenge due to rapid and subtle motion transitions over short\ndurations. Existing supervised and weakly supervised solutions often rely on\nextensive annotated datasets and high-capacity models, making them\ncomputationally intensive and less adaptable to real-world scenarios. In this\nwork, we introduce a lightweight and unsupervised skeleton-based action\nlocalization pipeline that leverages spatio-temporal graph neural\nrepresentations. Our approach pre-trains an Attention-based Spatio-Temporal\nGraph Convolutional Network (ASTGCN) on a pose-sequence denoising task with\nblockwise partitions, enabling it to learn intrinsic motion dynamics without\nany manual labeling. At inference, we define a novel Action Dynamics Metric\n(ADM), computed directly from low-dimensional ASTGCN embeddings, which detects\nmotion boundaries by identifying inflection points in its curvature profile.\nOur method achieves a mean Average Precision (mAP) of 82.66% and average\nlocalization latency of 29.09 ms on the DSV Diving dataset, matching\nstate-of-the-art supervised performance while maintaining computational\nefficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving\nfootage without retraining, demonstrating its practical applicability for\nlightweight, real-time action analysis systems in embedded or dynamic\nenvironments.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19647v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19647v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.338,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19649",
      "title": "IDF: Iterative Dynamic Filtering Networks for Generalizable Image\n  Denoising",
      "authors": [
        "Dongjin Kim",
        "Jaekyun Ko",
        "Muhammad Kashif Ali",
        "Tae Hyun Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Image denoising is a fundamental challenge in computer vision, with\napplications in photography and medical imaging. While deep learning-based\nmethods have shown remarkable success, their reliance on specific noise\ndistributions limits generalization to unseen noise types and levels. Existing\napproaches attempt to address this with extensive training data and high\ncomputational resources but they still suffer from overfitting. To address\nthese issues, we conduct image denoising by utilizing dynamically generated\nkernels via efficient operations. This approach helps prevent overfitting and\nimproves resilience to unseen noise. Specifically, our method leverages a\nFeature Extraction Module for robust noise-invariant features, Global\nStatistics and Local Correlation Modules to capture comprehensive noise\ncharacteristics and structural correlations. The Kernel Prediction Module then\nemploys these cues to produce pixel-wise varying kernels adapted to local\nstructures, which are then applied iteratively for denoising. This ensures both\nefficiency and superior restoration quality. Despite being trained on\nsingle-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse\nnoise types and levels, demonstrating the promise of iterative dynamic\nfiltering for practical image denoising.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19649v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19649v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.359,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on iterative dynamic filtering for image denoising, emphasizing generalization to various noise types through dynamic kernels and iterative refinement in computer vision tasks. It does not involve diffusion models, multi-step logical reasoning, or adapting iterative processes for complex logical tasks like Chain-of-Thought. Thus, there is no connection to the specified topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19650",
      "title": "Video-LevelGauge: Investigating Contextual Positional Bias in Large\n  Video Language Models",
      "authors": [
        "Hou Xia",
        "Zheren Fu",
        "Fangcan Ling",
        "Jiajun Li",
        "Yi Tu",
        "Zhendong Mao",
        "Yongdong Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Large video language models (LVLMs) have made notable progress in video\nunderstanding, spurring the development of corresponding evaluation benchmarks.\nHowever, existing benchmarks generally assess overall performance across entire\nvideo sequences, overlooking nuanced behaviors such as contextual positional\nbias, a critical yet under-explored aspect of LVLM performance. We present\nVideo-LevelGauge, a dedicated benchmark designed to systematically assess\npositional bias in LVLMs. We employ standardized probes and customized\ncontextual setups, allowing flexible control over context length, probe\nposition, and contextual types to simulate diverse real-world scenarios. In\naddition, we introduce a comprehensive analysis method that combines\nstatistical measures with morphological pattern recognition to characterize\nbias. Our benchmark comprises 438 manually curated videos spanning multiple\ntypes, yielding 1,177 high-quality multiple-choice questions and 120 open-ended\nquestions, validated for their effectiveness in exposing positional bias. Based\non these, we evaluate 27 state-of-the-art LVLMs, including both commercial and\nopen-source models. Our findings reveal significant positional biases in many\nleading open-source models, typically exhibiting head or neighbor-content\npreferences. In contrast, commercial models such as Gemini2.5-Pro show\nimpressive, consistent performance across entire video sequences. Further\nanalyses on context length, context variation, and model scale provide\nactionable insights for mitigating bias and guiding model enhancement .\nhttps://github.com/Cola-any/Video-LevelGauge",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19650v3",
      "pdf_url": "http://arxiv.org/pdf/2508.19650v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.42,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.351,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on developing a benchmark for evaluating positional bias in Large Video Language Models (LVLMs), without any mention of training models using human feedback, reward models, or reinforcement learning techniques. It primarily involves manual curation of videos and questions for evaluation, which does not align with RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a benchmark for assessing positional bias in LVLMs through statistical and morphological analysis, with no reference to diffusion models, iterative refinement processes, or multi-step logical reasoning. It does not involve adapting diffusion for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19651",
      "title": "Scalable Object Detection in the Car Interior With Vision Foundation\n  Models",
      "authors": [
        "Bálint Mészáros",
        "Ahmet Firintepe",
        "Sebastian Schmidt",
        "Stephan Günnemann"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "AI tasks in the car interior like identifying and localizing externally\nintroduced objects is crucial for response quality of personal assistants.\nHowever, computational resources of on-board systems remain highly constrained,\nrestricting the deployment of such solutions directly within the vehicle. To\naddress this limitation, we propose the novel Object Detection and Localization\n(ODAL) framework for interior scene understanding. Our approach leverages\nvision foundation models through a distributed architecture, splitting\ncomputational tasks between on-board and cloud. This design overcomes the\nresource constraints of running foundation models directly in the car. To\nbenchmark model performance, we introduce ODALbench, a new metric for\ncomprehensive assessment of detection and localization.Our analysis\ndemonstrates the framework's potential to establish new standards in this\ndomain. We compare the state-of-the-art GPT-4o vision foundation model with the\nlightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the\nlightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model\nachieves an ODAL$_{score}$ of 89%, representing a 71% improvement over its\nbaseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the\nfine-tuned model maintains high detection accuracy while significantly reducing\nhallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19651v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19651v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.426,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on object detection and localization using vision foundation models in a distributed architecture, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It does not involve adapting diffusion for tasks like Chain-of-Thought correction.",
      "distributed_training_justification": "The paper describes a distributed architecture for inference, splitting tasks between on-board systems and the cloud to handle computational constraints, which relates to parallel computing. However, it does not address distributed training, parallelization of model training, or multi-node strategies for accelerating the training process itself.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19652",
      "title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition",
      "authors": [
        "Zongxia Li",
        "Wenhao Yu",
        "Chengsong Huang",
        "Rui Liu",
        "Zhenwen Liang",
        "Fuxiao Liu",
        "Jingxi Che",
        "Dian Yu",
        "Jordan Boyd-Graber",
        "Haitao Mi",
        "Dong Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision-Language Models (VLMs) often suffer from visual hallucinations, saying\nthings that are not actually in the image, and language shortcuts, where they\nskip the visual part and just rely on text priors. These issues arise because\nmost post-training methods for VLMs rely on simple verifiable answer matching\nand supervise only final outputs, leaving intermediate visual reasoning without\nexplicit guidance. As a result, VLMs receive sparse visual signals and often\nlearn to prioritize language-based reasoning over visual perception. To\nmitigate this, some existing methods add visual supervision using human\nannotations or distilled labels from external large models. However, human\nannotations are labor-intensive and costly, and because external signals cannot\nadapt to the evolving policy, they cause distributional shifts that can lead to\nreward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method\nthat improves visual reasoning without relying on external visual supervisions\nvia reinforcement learning. Vision-SR1 decomposes VLM reasoning into two\nstages: visual perception and language reasoning. The model is first prompted\nto produce self-contained visual perceptions that are sufficient to answer the\nquestion without referring back the input image. To validate this\nself-containment, the same VLM model is then re-prompted to perform language\nreasoning using only the generated perception as input to compute reward. This\nself-reward is combined with supervision on final outputs, providing a balanced\ntraining signal that strengthens both visual perception and language reasoning.\nOur experiments demonstrate that Vision-SR1 improves visual reasoning,\nmitigates visual hallucinations, and reduces reliance on language shortcuts\nacross diverse vision-language tasks.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19652v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19652v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.488,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.525,
      "distributed_training_score": 0.345,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces Vision-SR1, a self-rewarding RL method that uses the model itself to generate and verify rewards, without any human feedback, annotations, or a separate reward model trained on human-ranked data. This contrasts with RLHF, which specifically requires human preferences for reward modeling and fine-tuning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for improving visual reasoning in VLMs through self-rewarding mechanisms and chain-of-thought processes, but it does not involve diffusion models, iterative refinement for logical tasks, or treating reasoning paths as entities for multi-step correction as defined in diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19654",
      "title": "Hardware-aware vs. Hardware-agnostic Energy Estimation for SNN in Space\n  Applications",
      "authors": [
        "Matthias Höfflin",
        "Jürgen Wassner"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Spiking Neural Networks (SNNs), inspired by biological intelligence, have\nlong been considered inherently energy-efficient, making them attractive for\nresource-constrained domains such as space applications. However, recent\ncomparative studies with conventional Artificial Neural Networks (ANNs) have\nbegun to question this reputation, especially for digital implementations. This\nwork investigates SNNs for multi-output regression, specifically 3-D satellite\nposition estimation from monocular images, and compares hardware-aware and\nhardware-agnostic energy estimation methods. The proposed SNN, trained using\nthe membrane potential of the Leaky Integrate-and-Fire (LIF) neuron in the\nfinal layer, achieves comparable Mean Squared Error (MSE) to a reference\nConvolutional Neural Network (CNN) on a photorealistic satellite dataset.\nEnergy analysis shows that while hardware-agnostic methods predict a consistent\n50-60% energy advantage for SNNs over CNNs, hardware-aware analysis reveals\nthat significant energy savings are realized only on neuromorphic hardware and\nwith high input sparsity. The influence of dark pixel ratio on energy\nconsumption is quantified, emphasizing the impact of data characteristics and\nhardware assumptions. These findings highlight the need for transparent\nevaluation methods and explicit disclosure of underlying assumptions to ensure\nfair comparisons of neural network energy efficiency.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19654v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19654v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.333,
      "distributed_training_score": 0.38,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19660",
      "title": "Arbitrary Precision Printed Ternary Neural Networks with Holistic\n  Evolutionary Approximation",
      "authors": [
        "Vojtech Mrazek",
        "Konstantinos Balaskas",
        "Paula Carolina Lozano Duarte",
        "Zdenek Vasicek",
        "Mehdi B. Tahoori",
        "Georgios Zervakis"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.NE (Neural and Evolutionary Computing)"
      ],
      "abstract": "Printed electronics offer a promising alternative for applications beyond\nsilicon-based systems, requiring properties like flexibility, stretchability,\nconformality, and ultra-low fabrication costs. Despite the large feature sizes\nin printed electronics, printed neural networks have attracted attention for\nmeeting target application requirements, though realizing complex circuits\nremains challenging. This work bridges the gap between classification accuracy\nand area efficiency in printed neural networks, covering the entire\nprocessing-near-sensor system design and co-optimization from the\nanalog-to-digital interface-a major area and power bottleneck-to the digital\nclassifier. We propose an automated framework for designing printed Ternary\nNeural Networks with arbitrary input precision, utilizing multi-objective\noptimization and holistic approximation. Our circuits outperform existing\napproximate printed neural networks by 17x in area and 59x in power on average,\nbeing the first to enable printed-battery-powered operation with under 5%\naccuracy loss while accounting for analog-to-digital interfacing costs.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19660v3",
      "pdf_url": "http://arxiv.org/pdf/2508.19660v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.419,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on designing and optimizing printed Ternary Neural Networks for hardware efficiency, including approximation techniques and analog-to-digital interfaces, but does not address distributed training, parallel computing, or multi-node machine learning. There is no mention of partitioning data, model architecture, or computation across multiple processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19664",
      "title": "A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image\n  Enhancement",
      "authors": [
        "Weicheng Liao",
        "Zan Chen",
        "Jianyang Xie",
        "Yalin Zheng",
        "Yuhui Ma",
        "Yitian Zhao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics\nby providing a comprehensive view of the retina. However, it often suffers from\nquality-degrading factors such as blurring and uneven illumination, which\nobscure fine details and mask pathological information. While numerous retinal\nimage enhancement methods have been proposed for other fundus imageries, they\noften fail to address the unique requirements in UWF, particularly the need to\npreserve pathological details. In this paper, we propose a novel\nfrequency-aware self-supervised learning method for UWF image enhancement. It\nincorporates frequency-decoupled image deblurring and Retinex-guided\nillumination compensation modules. An asymmetric channel integration operation\nis introduced in the former module, so as to combine global and local views by\nleveraging high- and low-frequency information, ensuring the preservation of\nfine and broader structural details. In addition, a color preservation unit is\nproposed in the latter Retinex-based module, to provide multi-scale spatial and\nfrequency information, enabling accurate illumination estimation and\ncorrection. Experimental results demonstrate that the proposed work not only\nenhances visualization quality but also improves disease diagnosis performance\nby restoring and correcting fine local details and uneven intensity. To the\nbest of our knowledge, this work is the first attempt for UWF image\nenhancement, offering a robust and clinically valuable tool for improving\nretinal disease management.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19664v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19664v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.308,
      "distributed_training_score": 0.313,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a frequency-aware self-supervised learning method for UWF image enhancement, which aligns directly with weak supervision. It addresses the challenge of lacking paired datasets by using self-supervised techniques to generate supervisory signals programmatically from the data itself, such as through frequency-decoupled modules, rather than relying on hand-labeled data. This core approach fits the definition of weak supervision, as it leverages noisy or derived labels for training.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper presents a novel frequency-aware self-supervised learning method for enhancing Ultra-Wide-Field (UWF) retinal images, addressing challenges such as blurring and uneven illumination that obscure pathological details. The approach integrates a Frequency-Decoupled deblurring (FRED) module, which uses asymmetric channel integration to combine high- and low-frequency information for preserving fine and structural details, and a Retinex-guided Illumination CompEnsation (RICE) module with a color preservation unit for accurate illumination correction. Experimental results demonstrate improved visual quality and enhanced disease diagnosis performance, positioning this as the first dedicated effort in UWF image enhancement.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique tailored for UWF image enhancement, including innovative modules like FRED and RICE that advance state-of-the-art in medical image processing by addressing specific challenges not previously tackled. This represents a significant advancement as it is the first dedicated method for this domain, combining frequency-aware self-supervised learning in a novel way.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future research and commercial applications in retinal diagnostics by improving image quality and disease detection accuracy, which could lead to better clinical outcomes. Its focus on preserving pathological details in UWF imaging makes it likely to be widely cited and adopted in medical computer vision subfields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a high-quality and innovative contribution to medical image enhancement, making it valuable for researchers in computer vision and ophthalmology to understand advancements in UWF imaging. While essential for specialists in retinal diagnostics, it may not be critical for those outside this niche.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3bd735aa8712df8a72d1023917d29ae42a47f6ac",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 17,
      "average_h_index": 5.666666666666667,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Weicheng Liao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378185459"
        },
        {
          "name": "Zan Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377614422"
        },
        {
          "name": "Jianyang Xie",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/9151720"
        },
        {
          "name": "Yalin Zheng",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2256046777"
        },
        {
          "name": "Yuhui Ma",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2349197521"
        },
        {
          "name": "Yitian Zhao",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/2314545922"
        }
      ]
    },
    {
      "id": "2508.19667",
      "title": "Survey of Specialized Large Language Model",
      "authors": [
        "Chenghan Yang",
        "Ruiyu Zhao",
        "Yang Liu",
        "Ling Jiang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rapid evolution of specialized large language models (LLMs) has\ntransitioned from simple domain adaptation to sophisticated native\narchitectures, marking a paradigm shift in AI development. This survey\nsystematically examines this progression across healthcare, finance, legal, and\ntechnical domains. Besides the wide use of specialized LLMs, technical\nbreakthrough such as the emergence of domain-native designs beyond fine-tuning,\ngrowing emphasis on parameter efficiency through sparse computation and\nquantization, increasing integration of multimodal capabilities and so on are\napplied to recent LLM agent. Our analysis reveals how these innovations address\nfundamental limitations of general-purpose LLMs in professional applications,\nwith specialized models consistently performance gains on domain-specific\nbenchmarks. The survey further highlights the implications for E-Commerce field\nto fill gaps in the field.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19667v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19667v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.456,
      "weak_supervision_score": 0.421,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.446,
      "datasets_score": 0.395,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper is a survey on specialized LLMs, focusing on domain adaptation, architectural innovations, and trends like parameter efficiency, but it does not mention reinforcement learning from human feedback. There is no discussion of training models with human-ranked data or reward models for alignment.",
      "weak_supervision_justification": "The paper discusses training specialized LLMs using domain-specific corpora, which could indirectly relate to weak supervision through noisy or programmatic data sources, as seen in examples like continued pretraining. However, it does not explicitly address weak supervision techniques or focus on generating labels from imprecise sources.",
      "diffusion_reasoning_justification": "The paper surveys specialized LLMs and mentions innovations like multimodal capabilities and iterative processes, but it does not reference diffusion-based models or their use for multi-step logical reasoning and holistic correction of reasoning paths.",
      "distributed_training_justification": "The paper touches on parameter efficiency techniques like sparse computation and quantization, which could be associated with parallel computing methods, but it does not specifically discuss distributed training, parallel computing across nodes, or strategies for partitioning data and computation in multi-node systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19679",
      "title": "InquireMobile: Teaching VLM-based Mobile Agent to Request Human\n  Assistance via Reinforcement Fine-Tuning",
      "authors": [
        "Qihang Ai",
        "Pi Bu",
        "Yue Cao",
        "Yingyao Wang",
        "Jihao Gu",
        "Jingxuan Xing",
        "Zekun Zhu",
        "Wei Jiang",
        "Zhicheng Zheng",
        "Jun Song",
        "Yuning Jiang",
        "Bo Zheng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in Vision-Language Models (VLMs) have enabled mobile agents\nto perceive and interact with real-world mobile environments based on human\ninstructions. However, the current fully autonomous paradigm poses potential\nsafety risks when model understanding or reasoning capabilities are\ninsufficient. To address this challenge, we first introduce\n\\textbf{InquireBench}, a comprehensive benchmark specifically designed to\nevaluate mobile agents' capabilities in safe interaction and proactive inquiry\nwith users, encompassing 5 categories and 22 sub-categories, where most\nexisting VLM-based agents demonstrate near-zero performance. In this paper, we\naim to develop an interactive system that actively seeks human confirmation at\ncritical decision points. To achieve this, we propose \\textbf{InquireMobile}, a\nnovel model inspired by reinforcement learning, featuring a two-stage training\nstrategy and an interactive pre-action reasoning mechanism. Finally, our model\nachieves an 46.8% improvement in inquiry success rate and the best overall\nsuccess rate among existing baselines on InquireBench. We will open-source all\ndatasets, models, and evaluation codes to facilitate development in both\nacademia and industry.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19679v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19679v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.494,
      "weak_supervision_score": 0.422,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.338,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement fine-tuning (specifically GRPO) to train the InquireMobile model, which involves learning to interact with humans for assistance. However, it does not explicitly describe using a reward model trained on human-ranked data, a core element of RLHF. Instead, the focus is on proactive human inquiry during operation, making it related but not directly aligned with RLHF.",
      "weak_supervision_justification": "The paper employs weak supervision by using GPT-4o to programmatically generate a large set of instructions and scenarios for the InquireBench benchmark, relying on noisy or high-level sources rather than hand-labeled data. This approach directly aligns with weak supervision techniques for creating training and evaluation data.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper addresses the safety risks of fully autonomous Vision-Language Model (VLM)-based mobile agents by introducing InquireBench, a comprehensive benchmark to evaluate agents' abilities in proactive user inquiry across various scenarios. It proposes InquireMobile, a model trained through a two-stage strategy involving supervised fine-tuning and Group Relative Policy Optimization, which incorporates an interactive pre-action reasoning mechanism to seek human confirmation, resulting in a 46.8% improvement in inquiry success rate and superior performance on InquireBench compared to baselines.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new problem by focusing on proactive human inquiry for safety in mobile agents and proposes a novel technique with InquireMobile's two-stage training and reasoning mechanism, significantly advancing the state-of-the-art in AI safety.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and commercial applications by enhancing the safety and trustworthiness of mobile agents, especially through open-sourcing the datasets and models for broader adoption.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a significant and timely contribution to AI safety by developing a benchmark and model for human-AI collaboration, making it essential for researchers in artificial intelligence and mobile agents to be aware of its insights and advancements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c4d3abf40bcf26564676810e6edae77b887e4efd",
      "total_authors": 12,
      "authors_found": 12,
      "highest_h_index": 4,
      "average_h_index": 2.1666666666666665,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Qihang Ai",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2370937275"
        },
        {
          "name": "Pi Bu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2321871796"
        },
        {
          "name": "Yue Cao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377922826"
        },
        {
          "name": "Yingyao Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2336243225"
        },
        {
          "name": "Jihao Gu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2331000999"
        },
        {
          "name": "Jingxuan Xing",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2370939871"
        },
        {
          "name": "Zekun Zhu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2370955356"
        },
        {
          "name": "Wei Jiang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2371140803"
        },
        {
          "name": "Zhicheng Zheng",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2330718673"
        },
        {
          "name": "Jun Song",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2336451613"
        },
        {
          "name": "Yuning Jiang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2260839743"
        },
        {
          "name": "Bo Zheng",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2336160587"
        }
      ]
    },
    {
      "id": "2508.19683",
      "title": "Topological Uncertainty for Anomaly Detection in the Neural-network EoS\n  Inference with Neutron Star Data",
      "authors": [
        "Kenji Fukushima",
        "Syo Kamata"
      ],
      "categories": [
        "nucl-th (Nuclear Theory)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We study the performance of the Topological Uncertainty (TU) constructed with\na trained feedforward neural network (FNN) for Anomaly Detection. Generally,\nmeaningful information can be stored in the hidden layers of the trained FNN,\nand the TU implementation is one tractable recipe to extract buried information\nby means of the Topological Data Analysis. We explicate the concept of the TU\nand the numerical procedures. Then, for a concrete demonstration of the\nperformance test, we employ the Neutron Star data used for inference of the\nequation of state (EoS). For the training dataset consisting of the input\n(Neutron Star data) and the output (EoS parameters), we can compare the\ninferred EoSs and the exact answers to classify the data with the label $k$.\nThe subdataset with $k=0$ leads to the normal inference for which the inferred\nEoS approximates the answer well, while the subdataset with $k=1$ ends up with\nthe unsuccessful inference. Once the TU is prepared based on the $k$-labled\nsubdatasets, we introduce the cross-TU to quantify the uncertainty of\ncharacterizing the $k$-labeled data with the label $j$. The anomaly or\nunsuccessful inference is correctly detected if the cross-TU for $j=k=1$ is\nsmaller than that for $j=0$ and $k=1$. In our numerical experiment, for various\ninput data, we calculate the cross-TU and estimate the performance of Anomaly\nDetection. We find that performance depends on FNN hyperparameters, and the\nsuccess rate of Anomaly Detection exceeds $90\\%$ in the best case. We finally\ndiscuss further potential of the TU application to retrieve the information\nhidden in the trained FNN.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19683v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19683v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.355,
      "datasets_score": 0.27,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19688",
      "title": "SAT: Supervisor Regularization and Animation Augmentation for\n  Two-process Monocular Texture 3D Human Reconstruction",
      "authors": [
        "Gangjian Zhang",
        "Jian Shu",
        "Nanjie Yao",
        "Hao Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Monocular texture 3D human reconstruction aims to create a complete 3D\ndigital avatar from just a single front-view human RGB image. However, the\ngeometric ambiguity inherent in a single 2D image and the scarcity of 3D human\ntraining data are the main obstacles limiting progress in this field. To\naddress these issues, current methods employ prior geometric estimation\nnetworks to derive various human geometric forms, such as the SMPL model and\nnormal maps. However, they struggle to integrate these modalities effectively,\nleading to view inconsistencies, such as facial distortions. To this end, we\npropose a two-process 3D human reconstruction framework, SAT, which seamlessly\nlearns various prior geometries in a unified manner and reconstructs\nhigh-quality textured 3D avatars as the final output. To further facilitate\ngeometry learning, we introduce a Supervisor Feature Regularization module. By\nemploying a multi-view network with the same structure to provide intermediate\nfeatures as training supervision, these varied geometric priors can be better\nfused. To tackle data scarcity and further improve reconstruction quality, we\nalso propose an Online Animation Augmentation module. By building a\none-feed-forward animation network, we augment a massive number of samples from\nthe original 3D human data online for model training. Extensive experiments on\ntwo benchmarks show the superiority of our approach compared to\nstate-of-the-art methods.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19688v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19688v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.319,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19697",
      "title": "Safety Alignment Should Be Made More Than Just A Few Attention Heads",
      "authors": [
        "Chao Huang",
        "Zefeng Zhang",
        "Juewei Yue",
        "Quangang Li",
        "Chuang Zhang",
        "Tingwen Liu"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Current safety alignment for large language models(LLMs) continues to present\nvulnerabilities, given that adversarial prompting can effectively bypass their\nsafety measures.Our investigation shows that these safety mechanisms\npredominantly depend on a limited subset of attention heads: removing or\nablating these heads can severely compromise model safety. To identify and\nevaluate these safety-critical components, we introduce RDSHA, a targeted\nablation method that leverages the model's refusal direction to pinpoint\nattention heads mostly responsible for safety behaviors. Further analysis shows\nthat existing jailbreak attacks exploit this concentration by selectively\nbypassing or manipulating these critical attention heads. To address this\nissue, we propose AHD, a novel training strategy designed to promote the\ndistributed encoding of safety-related behaviors across numerous attention\nheads. Experimental results demonstrate that AHD successfully distributes\nsafety-related capabilities across more attention heads. Moreover, evaluations\nunder several mainstream jailbreak attacks show that models trained with AHD\nexhibit considerably stronger safety robustness, while maintaining overall\nfunctional utility.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19697v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19697v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.519,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.432,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses safety alignment in LLMs, which often involves fine-tuning techniques like RLHF, but it does not explicitly mention human feedback, reward models, or reinforcement learning. Instead, it focuses on attention heads and ablation methods, making the connection indirect.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper centers on transformer-based LLMs and attention mechanisms for safety, with no mention of diffusion models, iterative refinement for reasoning, or multi-step logical processes as defined.",
      "distributed_training_justification": "The paper proposes a training strategy (AHD) for distributing safety capabilities within the model, but it does not address parallel computing, multi-node training, or partitioning data/computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19698",
      "title": "Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori\n  Bethe-Hessian Operators",
      "authors": [
        "V. S. Usatyuk",
        "D. A. Sapozhnikov",
        "S. I. Egorov"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.IT (Information Theory)",
        "math.IT (Information Theory)",
        "math.SP (Spectral Theory)"
      ],
      "abstract": "The rapid advance of deep generative models such as GANs and diffusion\nnetworks now produces images that are virtually indistinguishable from genuine\nphotographs, undermining media forensics and biometric security. Supervised\ndetectors quickly lose effectiveness on unseen generators or after adversarial\npost-processing, while existing unsupervised methods that rely on low-level\nstatistical cues remain fragile. We introduce a physics-inspired,\nmodel-agnostic detector that treats synthetic-image identification as a\ncommunity-detection problem on a sparse weighted graph. Image features are\nfirst extracted with pretrained CNNs and reduced to 32 dimensions, each feature\nvector becomes a node of a Multi-Edge Type QC-LDPC graph. Pairwise similarities\nare transformed into edge couplings calibrated at the Nishimori temperature,\nproducing a Random Bond Ising Model (RBIM) whose Bethe-Hessian spectrum\nexhibits a characteristic gap when genuine community structure (real images) is\npresent. Synthetic images violate the Nishimori symmetry and therefore lack\nsuch gaps. We validate the approach on binary tasks cat versus dog and male\nversus female using real photos from Flickr-Faces-HQ and CelebA and synthetic\ncounterparts generated by GANs and diffusion models. Without any labeled\nsynthetic data or retraining of the feature extractor, the detector achieves\nover 94% accuracy. Spectral analysis shows multiple well separated gaps for\nreal image sets and a collapsed spectrum for generated ones. Our contributions\nare threefold: a novel LDPC graph construction that embeds deep image features,\nan analytical link between Nishimori temperature RBIM and the Bethe-Hessian\nspectrum providing a Bayes optimal detection criterion; and a practical,\nunsupervised synthetic image detector robust to new generative architectures.\nFuture work will extend the framework to video streams and multi-class anomaly\ndetection.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19698v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19698v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.337,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a method for detecting synthetic images using spectral analysis on graphs, specifically leveraging RBIM and Bethe-Hessian operators. While it mentions diffusion models as tools for generating synthetic images (e.g., alongside GANs), it does not adapt the iterative refinement process of diffusion for multi-step logical reasoning or treat a 'Chain-of-Thought' as an entity for holistic correction. There is no component involving diffusion-based reasoning for complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19699",
      "title": "LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation",
      "authors": [
        "Yupeng Zhang",
        "Dezhi Zheng",
        "Ping Lu",
        "Han Zhang",
        "Lei Wang",
        "Liping xiang",
        "Cheng Luo",
        "Kaijun Deng",
        "Xiaowen Fu",
        "Linlin Shen",
        "Jinbao Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation\nfor 3D scenes, offering both high-fidelity reconstruction and efficient\nrendering. However, 3DGS lacks 3D segmentation ability, which limits its\napplicability in tasks that require scene understanding. The identification and\nisolating of specific object components is crucial. To address this limitation,\nwe propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments\nthe Gaussian representation with object label.LabelGS introduces cross-view\nconsistent semantic masks for 3D Gaussians and employs a novel Occlusion\nAnalysis Model to avoid overfitting occlusion during optimization, Main\nGaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian\nProjection Filter to avoid Gaussian label conflict. Our approach achieves\neffective decoupling of Gaussian representations and refines the 3DGS\noptimization process through a random region sampling strategy, significantly\nimproving efficiency. Extensive experiments demonstrate that LabelGS\noutperforms previous state-of-the-art methods, including Feature-3DGS, in the\n3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup\nin training compared to Feature-3DGS, at a resolution of 1440X1080. Our code\nwill be at https://github.com/garrisonz/LabelGS.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19699v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19699v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.248,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.331,
      "distributed_training_score": 0.318,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19705",
      "title": "FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp\n  Segmentation",
      "authors": [
        "Qiang Hu",
        "Ying Zhou",
        "Gepeng Ji",
        "Nick Barnes",
        "Qiang Li",
        "Zhiwei Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Existing video polyp segmentation (VPS) paradigms usually struggle to balance\nbetween spatiotemporal modeling and domain generalization, limiting their\napplicability in real clinical scenarios. To embrace this challenge, we recast\nthe VPS task as a track-by-detect paradigm that leverages the spatial contexts\ncaptured by the image polyp segmentation (IPS) model while integrating the\ntemporal modeling capabilities of segment anything model 2 (SAM2). However,\nduring long-term polyp tracking in colonoscopy videos, SAM2 suffers from error\naccumulation, resulting in a snowball effect that compromises segmentation\nstability. We mitigate this issue by repurposing SAM2 as a video polyp\nsegmenter with two training-free modules. In particular, the intra-association\nfiltering module eliminates spatial inaccuracies originating from the detecting\nstage, reducing false positives. The inter-association refinement module\nadaptively updates the memory bank to prevent error propagation over time,\nenhancing temporal coherence. Both modules work synergistically to stabilize\nSAM2, achieving cutting-edge performance in both in-domain and out-of-domain\nscenarios. Furthermore, we demonstrate the robust tracking capabilities of\nFreeVPS in long-untrimmed colonoscopy videos, underscoring its potential\nreliable clinical analysis.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19705v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19705v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.354,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19708",
      "title": "Attention is also needed for form design",
      "authors": [
        "B. Sankar",
        "Dibakar Sen"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Conventional product design is a cognitively demanding process, limited by\nits time-consuming nature, reliance on subjective expertise, and the opaque\ntranslation of inspiration into tangible concepts. This research introduces a\nnovel, attention-aware framework that integrates two synergistic systems:\nEUPHORIA, an immersive Virtual Reality environment using eye-tracking to\nimplicitly capture a designer's aesthetic preferences, and RETINA, an agentic\nAI pipeline that translates these implicit preferences into concrete design\noutputs. The foundational principles were validated in a two-part study. An\ninitial study correlated user's implicit attention with explicit preference and\nthe next one correlated mood to attention. A comparative study where 4\ndesigners solved challenging design problems using 4 distinct workflows, from a\nmanual process to an end-to-end automated pipeline, showed the integrated\nEUPHORIA-RETINA workflow was over 4 times more time-efficient than the\nconventional method. A panel of 50 design experts evaluated the 16 final\nrenderings. Designs generated by the fully automated system consistently\nreceived the highest Worthiness (calculated by an inverse Plackett-Luce model\nbased on gradient descent optimization) and Design Effectiveness scores,\nindicating superior quality across 8 criteria: novelty, visual appeal,\nemotional resonance, clarity of purpose, distinctiveness of silhouette, implied\nmateriality, proportional balance, & adherence to the brief. This research\npresents a validated paradigm shift from traditional Computer-Assisted Design\n(CAD) to a collaborative model of Designer-Assisting Computers (DAC). By\nautomating logistical and skill-dependent generative tasks, the proposed\nframework elevates the designer's role to that of a creative director,\nsynergizing human intuition with the generative power of agentic AI to produce\nhigher-quality designs more efficiently.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19708v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19708v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.285,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using eye-tracking in a VR environment to capture implicit preferences and an AI pipeline (RETINA) to generate designs, but it does not involve training a reward model on human-ranked data or fine-tuning via reinforcement learning. There is no evidence of RLHF processes, such as aligning AI with human preferences through iterative feedback loops.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes an AI framework (RETINA) that processes visual attention data to generate designs, but it does not adapt diffusion models for multi-step logical reasoning or iterative refinement of a 'Chain-of-Thought'. There is no mention of diffusion-based components for holistic reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19714",
      "title": "Addressing Deepfake Issue in Selfie banking through camera based\n  authentication",
      "authors": [
        "Subhrojyoti Mukherjee",
        "Manoranjan Mohanty"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Fake images in selfie banking are increasingly becoming a threat. Previously,\nit was just Photoshop, but now deep learning technologies enable us to create\nhighly realistic fake identities, which fraudsters exploit to bypass biometric\nsystems such as facial recognition in online banking. This paper explores the\nuse of an already established forensic recognition system, previously used for\npicture camera localization, in deepfake detection.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19714v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19714v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.299,
      "diffusion_reasoning_score": 0.283,
      "distributed_training_score": 0.273,
      "datasets_score": 0.272,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19724",
      "title": "NLKI: A lightweight Natural Language Knowledge Integration Framework for\n  Improving Small VLMs in Commonsense VQA Tasks",
      "authors": [
        "Aritra Dutta",
        "Swapnanil Mukherjee",
        "Deepanway Ghosal",
        "Somak Aditya"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Commonsense visual-question answering often hinges on knowledge that is\nmissing from the image or the question. Small vision-language models (sVLMs)\nsuch as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative\ncounterparts. To study the effect of careful commonsense knowledge integration\non sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural\nlanguage facts, (ii) prompts an LLM to craft natural language explanations, and\n(iii) feeds both signals to sVLMs respectively across two commonsense VQA\ndatasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts\nretrieved using a fine-tuned ColBERTv2 and an object information-enriched\nprompt yield explanations that largely cut down hallucinations, while lifting\nthe end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA\nand other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B\nand SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional\nfinetuning using noise-robust losses (such as symmetric cross entropy and\ngeneralised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our\nfindings expose when LLM-based commonsense knowledge beats retrieval from\ncommonsense knowledge bases, how noise-aware training stabilises small models\nin the context of external knowledge augmentation, and why parameter-efficient\ncommonsense reasoning is now within reach for 250M models.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19724v2",
      "pdf_url": "http://arxiv.org/pdf/2508.19724v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.418,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.306,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper addresses label noise in datasets (e.g., 10-25% in CRIC and AOKVQA) and uses noise-robust losses like Symmetric Cross-Entropy (SCE) and Generalised Cross-Entropy (GCE) during fine-tuning, which aligns with weak supervision's focus on handling noisy or imprecise labels. However, it does not primarily rely on programmatically generated labels from heuristics, making it only moderately relevant rather than central.",
      "diffusion_reasoning_justification": "The paper focuses on knowledge integration frameworks, fact retrieval, and LLM-generated explanations for VLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. Thus, it lacks any components related to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces the NLKI framework, a lightweight approach to enhance small vision-language models (sVLMs) like ViLT, VisualBERT, and FLAVA for commonsense visual question answering (VQA) by retrieving relevant facts using ColBERTv2, generating natural language explanations with an LLM like Llama-3-8B, and integrating these into the models while employing noise-robust losses. Key findings include up to 7% accuracy improvements on datasets such as CRIC, AOKVQA, and e-SNLI-VE, reduced hallucinations through enriched prompts, and demonstrations that NLKI-equipped sVLMs can match or exceed medium-sized models like Qwen-2 VL-2B, highlighting the effectiveness of external knowledge integration and noise-aware training for lightweight commonsense reasoning.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing techniques like fact retrieval and LLM prompting to improve small VLMs in commonsense VQA, offering a notable advancement in applying these methods to a specific under-explored area. While not introducing a entirely new problem or architecture, it innovatively integrates components to address limitations in sVLMs effectively.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in vision-language models by demonstrating practical ways to enhance small models with external knowledge, potentially leading to more efficient AI applications in commonsense tasks. However, its impact may be confined to subfields like VQA and knowledge integration, rather than broadly across AI.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable insights and a practical framework for improving small VLMs in commonsense reasoning, making it a significant contribution for researchers in AI and VQA. While not essential for all, it provides actionable methods that could inspire further developments in lightweight models.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e3bb2695d61d4fe8fade9763938a4705bd66a5de",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 22,
      "average_h_index": 9.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Aritra Dutta",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377557426"
        },
        {
          "name": "Swapnanil Mukherjee",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2239957566"
        },
        {
          "name": "Deepanway Ghosal",
          "h_index": 22,
          "profile_url": "https://www.semanticscholar.org/author/32528506"
        },
        {
          "name": "Somak Aditya",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/39744108"
        }
      ]
    },
    {
      "id": "2508.19730",
      "title": "Improving Generalization in Deepfake Detection with Face Foundation\n  Models and Metric Learning",
      "authors": [
        "Stelios Mylonas",
        "Symeon Papadopoulos"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The increasing realism and accessibility of deepfakes have raised critical\nconcerns about media authenticity and information integrity. Despite recent\nadvances, deepfake detection models often struggle to generalize beyond their\ntraining distributions, particularly when applied to media content found in the\nwild. In this work, we present a robust video deepfake detection framework with\nstrong generalization that takes advantage of the rich facial representations\nlearned by face foundation models. Our method is built on top of FSFM, a\nself-supervised model trained on real face data, and is further fine-tuned\nusing an ensemble of deepfake datasets spanning both face-swapping and\nface-reenactment manipulations. To enhance discriminative power, we incorporate\ntriplet loss variants during training, guiding the model to produce more\nseparable embeddings between real and fake samples. Additionally, we explore\nattribution-based supervision schemes, where deepfakes are categorized by\nmanipulation type or source dataset, to assess their impact on generalization.\nExtensive experiments across diverse evaluation benchmarks demonstrate the\neffectiveness of our approach, especially in challenging real-world scenarios.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19730v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19730v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.37,
      "datasets_score": 0.38,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19742",
      "title": "POEv2: a flexible and robust framework for generic line segment\n  detection and wireframe line segment detection",
      "authors": [
        "Chenguang Liu",
        "Chisheng Wang",
        "Yuhua Cai",
        "Chuanhua Zhu",
        "Qingquan Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Line segment detection in images has been studied for several decades.\nExisting line segment detectors can be roughly divided into two categories:\ngeneric line segment detectors and wireframe line segment detectors. Generic\nline segment detectors aim to detect all meaningful line segments in images and\ntraditional approaches usually fall into this category. Recent deep learning\nbased approaches are mostly wireframe line segment detectors. They detect only\nline segments that are geometrically meaningful and have large spatial support.\nDue to the difference in the aim of design, the performance of generic line\nsegment detectors for the task of wireframe line segment detection won't be\nsatisfactory, and vice versa. In this work, we propose a robust framework that\ncan be used for both generic line segment detection and wireframe line segment\ndetection. The proposed method is an improved version of the Pixel Orientation\nEstimation (POE) method. It is thus named as POEv2. POEv2 detects line segments\nfrom edge strength maps, and can be combined with any edge detector. We show in\nour experiments that by combining the proposed POEv2 with an efficient edge\ndetector, it achieves state-of-the-art performance on three publicly available\ndatasets.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19742v2",
      "pdf_url": "http://arxiv.org/pdf/2508.19742v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.273,
      "weak_supervision_score": 0.299,
      "diffusion_reasoning_score": 0.311,
      "distributed_training_score": 0.293,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19746",
      "title": "SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient\n  Object Detection",
      "authors": [
        "Qiyao Xu",
        "Qiming Wu",
        "Xiaowei Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Segment Anything Model (SAM) has demonstrated remarkable capabilities in\nsolving light field salient object detection (LF SOD). However, most existing\nmodels tend to neglect the extraction of prompt information under this task.\nMeanwhile, traditional models ignore the analysis of frequency-domain\ninformation, which leads to small objects being overwhelmed by noise. In this\npaper, we put forward a novel model called self-prompting light field segment\nanything model (SPLF-SAM), equipped with unified multi-scale feature embedding\nblock (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is\ncapable of identifying multiple objects of varying sizes, while MAFA, by\nlearning frequency features, effectively prevents small objects from being\noverwhelmed by noise. Extensive experiments have demonstrated the superiority\nof our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be\navailable at https://github.com/XucherCH/splfsam.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19746v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19746v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.306,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19754",
      "title": "FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction\n  with Large Gaussian Reconstruction Transformers",
      "authors": [
        "Yue Wu",
        "Yufan Wu",
        "Wen Li",
        "Yuxi Lu",
        "Kairui Feng",
        "Xuanhong Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Despite significant progress in 3D avatar reconstruction, it still faces\nchallenges such as high time complexity, sensitivity to data quality, and low\ndata utilization. We propose FastAvatar, a feedforward 3D avatar framework\ncapable of flexibly leveraging diverse daily recordings (e.g., a single image,\nmulti-view observations, or monocular video) to reconstruct a high-quality 3D\nGaussian Splatting (3DGS) model within seconds, using only a single unified\nmodel. FastAvatar's core is a Large Gaussian Reconstruction Transformer\nfeaturing three key designs: First, a variant VGGT-style transformer\narchitecture aggregating multi-frame cues while injecting initial 3D prompt to\npredict an aggregatable canonical 3DGS representation; Second, multi-granular\nguidance encoding (camera pose, FLAME expression, head pose) mitigating\nanimation-induced misalignment for variable-length inputs; Third, incremental\nGaussian aggregation via landmark tracking and sliced fusion losses.\nIntegrating these features, FastAvatar enables incremental reconstruction,\ni.e., improving quality with more observations, unlike prior work wasting input\ndata. This yields a quality-speed-tunable paradigm for highly usable avatar\nmodeling. Extensive experiments show that FastAvatar has higher quality and\nhighly competitive speed compared to existing methods.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19754v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19754v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.365,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19762",
      "title": "BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions",
      "authors": [
        "Ahmed Emam",
        "Mohamed Elbassiouny",
        "Julius Miller",
        "Patrick Donworth",
        "Sabine Seidel",
        "Ribana Roscher"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Pollinator insects such as honeybees and bumblebees are vital to global food\nproduction and ecosystem stability, yet their populations are declining due to\nanthropogenic and environmental stressors. Scalable, automated monitoring in\nagricultural environments remains an open challenge due to the difficulty of\ndetecting small, fast-moving, and often camouflaged insects. To address this,\nwe present BuzzSet v1.0, a large-scale dataset of high-resolution pollinator\nimages collected under real field conditions. BuzzSet contains 7,856 manually\nverified images with more than 8,000 annotated instances across three classes:\nhoneybees, bumblebees, and unidentified insects. Initial annotations were\nproduced using a YOLOv12 model trained on external data and refined through\nhuman verification with open-source tools. All images were preprocessed into\n256 x 256 tiles to improve the detection of small insects. We provide baselines\nusing the RF-DETR transformer-based object detector. The model achieves strong\nclassification accuracy with F1 scores of 0.94 and 0.92 for honeybees and\nbumblebees, with minimal confusion between these categories. The unidentified\nclass remains more difficult due to label ambiguity and fewer samples, yet\nstill contributes insights for robustness evaluation. Overall detection\nperformance (mAP at 0.50 of 0.559) illustrates the challenging nature of the\ndataset and its potential to drive advances in small object detection under\nrealistic ecological conditions. Future work focuses on expanding the dataset\nto version 2.0 with additional annotations and evaluating further detection\nstrategies. BuzzSet establishes a benchmark for ecological computer vision,\nwith the primary challenge being reliable detection of insects frequently\ncamouflaged within natural vegetation, highlighting an open problem for future\nresearch.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19762v4",
      "pdf_url": "http://arxiv.org/pdf/2508.19762v4",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.278,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.228,
      "distributed_training_score": 0.289,
      "datasets_score": 0.44,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of BuzzSet v1.0, a new dataset for pollinator detection in machine learning applications. It covers dataset creation through image collection, annotation using YOLOv12 with human verification, preprocessing techniques, and benchmarking with RF-DETR, including metrics like F1 scores and mAP. This directly aligns with research on creating, curating, and evaluating datasets for AI, making it a strong fit for the topic.",
      "llm_score_status": "completed",
      "summary": "BuzzSet v1.0 is a large-scale dataset designed to facilitate pollinator detection in challenging field conditions, consisting of 7,856 high-resolution images with over 8,000 annotated instances of honeybees, bumblebees, and unidentified insects. The dataset was created by initially using a YOLOv12 model for annotations followed by human verification, with images preprocessed into 256x256 tiles; baseline evaluations using the RF-DETR detector show strong F1 scores (0.94 for honeybees and 0.92 for bumblebees) and a mAP of 0.559, underscoring the dataset's value for advancing small object detection in ecological settings while highlighting difficulties with camouflaged insects.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a new dataset tailored for pollinator detection in real field conditions, combining existing annotation techniques like YOLOv12 with human verification to address a known challenge in ecological computer vision, though it does not introduce a entirely new architecture or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of ecological computer vision and agriculture, as it provides a benchmark dataset for small object detection, potentially influencing future research on insect monitoring, but its applicability may be limited to specific domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution by establishing a valuable dataset for a pressing environmental issue, making it essential for researchers in computer vision and ecology to be aware of, though it may not be groundbreaking for broader audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/61f6336af2a15c90f8e53320f98d35068b40bd4a",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 26,
      "average_h_index": 4.666666666666667,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Ahmed Emam",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2266751555"
        },
        {
          "name": "Mohamed Elbassiouny",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377555862"
        },
        {
          "name": "Julius Miller",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377737475"
        },
        {
          "name": "Patrick Donworth",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377556001"
        },
        {
          "name": "Sabine Seidel",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377556795"
        },
        {
          "name": "R. Roscher",
          "h_index": 26,
          "profile_url": "https://www.semanticscholar.org/author/46525320"
        }
      ]
    },
    {
      "id": "2508.19769",
      "title": "AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning",
      "authors": [
        "Shu Shen",
        "C. L. Philip Chen",
        "Tong Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal learning has significantly enhanced machine learning performance\nbut still faces numerous challenges and limitations. Imbalanced multimodal\nlearning is one of the problems extensively studied in recent works and is\ntypically mitigated by modulating the learning of each modality. However, we\nfind that these methods typically hinder the dominant modality's learning to\npromote weaker modalities, which affects overall multimodal performance. We\nanalyze the cause of this issue and highlight a commonly overlooked problem:\noptimization bias within networks. To address this, we propose Adaptive\nIntra-Network Modulation (AIM) to improve balanced modality learning. AIM\naccounts for differences in optimization state across parameters and depths\nwithin the network during modulation, achieving balanced multimodal learning\nwithout hindering either dominant or weak modalities for the first time.\nSpecifically, AIM decouples the dominant modality's under-optimized parameters\ninto Auxiliary Blocks and encourages reliance on these performance-degraded\nblocks for joint training with weaker modalities. This approach effectively\nprevents suppression of weaker modalities while enabling targeted optimization\nof under-optimized parameters to improve the dominant modality. Additionally,\nAIM assesses modality imbalance level across network depths and adaptively\nadjusts modulation strength at each depth. Experimental results demonstrate\nthat AIM outperforms state-of-the-art imbalanced modality learning methods\nacross multiple benchmarks and exhibits strong generalizability across\ndifferent backbones, fusion strategies, and optimizers.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19769v2",
      "pdf_url": "http://arxiv.org/pdf/2508.19769v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.413,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.394,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on multimodal learning and adaptive intra-network modulation to address optimization biases, with no involvement of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper deals with neural network optimization for multimodal learning and does not include diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19773",
      "title": "The Return of Structural Handwritten Mathematical Expression Recognition",
      "authors": [
        "Jakob Seitz",
        "Tobias Lengfeld",
        "Radu Timofte"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Handwritten Mathematical Expression Recognition is foundational for\neducational technologies, enabling applications like digital note-taking and\nautomated grading. While modern encoder-decoder architectures with large\nlanguage models excel at LaTeX generation, they lack explicit symbol-to-trace\nalignment, a critical limitation for error analysis, interpretability, and\nspatially aware interactive applications requiring selective content updates.\nThis paper introduces a structural recognition approach with two innovations: 1\nan automatic annotation system that uses a neural network to map LaTeX\nequations to raw traces, automatically generating annotations for symbol\nsegmentation, classification, and spatial relations, and 2 a modular structural\nrecognition system that independently optimizes segmentation, classification,\nand relation prediction. By leveraging a dataset enriched with structural\nannotations from our auto-labeling system, the proposed recognition system\ncombines graph-based trace sorting, a hybrid convolutional-recurrent network,\nand transformer-based correction to achieve competitive performance on the\nCROHME-2023 benchmark. Crucially, our structural recognition system generates a\ncomplete graph structure that directly links handwritten traces to predicted\nsymbols, enabling transparent error analysis and interpretable outputs.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19773v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19773v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.321,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on handwritten mathematical expression recognition using neural networks, including graph-based sorting, hybrid convolutional-recurrent networks, and transformer-based correction. It does not involve diffusion models or iterative refinement processes for multi-step logical reasoning, as described in the topic. There is no component that adapts diffusion for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19786",
      "title": "MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for\n  High-Fidelity Dynamic Scene Reconstruction",
      "authors": [
        "Han Jiao",
        "Jiakai Sun",
        "Yexing Xu",
        "Lei Zhao",
        "Wei Xing",
        "Huaizhong Lin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D Gaussian Splatting, known for enabling high-quality static scene\nreconstruction with fast rendering, is increasingly being applied to dynamic\nscene reconstruction. A common strategy involves learning a deformation field\nto model the temporal changes of a canonical set of 3D Gaussians. However,\nthese deformation-based methods often produce blurred renderings and lose fine\nmotion details in highly dynamic regions due to the inherent limitations of a\nsingle, unified model in representing diverse motion patterns. To address these\nchallenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian\nSplatting (MAPo), a novel framework for high-fidelity dynamic scene\nreconstruction. Its core is a dynamic score-based partitioning strategy that\ndistinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D\nGaussians, we recursively partition them temporally and duplicate their\ndeformation networks for each new temporal segment, enabling specialized\nmodeling to capture intricate motion details. Concurrently, low-dynamic 3DGs\nare treated as static to reduce computational costs. However, this temporal\npartitioning strategy for high-dynamic 3DGs can introduce visual\ndiscontinuities across frames at the partition boundaries. To address this, we\nintroduce a cross-frame consistency loss, which not only ensures visual\ncontinuity but also further enhances rendering quality. Extensive experiments\ndemonstrate that MAPo achieves superior rendering quality compared to baselines\nwhile maintaining comparable computational costs, particularly in regions with\ncomplex or rapid motions.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19786v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19786v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.239,
      "weak_supervision_score": 0.266,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.32,
      "datasets_score": 0.234,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19788",
      "title": "Context-Aware Risk Estimation in Home Environments: A Probabilistic\n  Framework for Service Robots",
      "authors": [
        "Sena Ishii",
        "Akash Chikhalikar",
        "Ankit A. Ravankar",
        "Jose Victorio Salazar Luces",
        "Yasuhisa Hirata"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present a novel framework for estimating accident-prone regions in\neveryday indoor scenes, aimed at improving real-time risk awareness in service\nrobots operating in human-centric environments. As robots become integrated\ninto daily life, particularly in homes, the ability to anticipate and respond\nto environmental hazards is crucial for ensuring user safety, trust, and\neffective human-robot interaction. Our approach models object-level risk and\ncontext through a semantic graph-based propagation algorithm. Each object is\nrepresented as a node with an associated risk score, and risk propagates\nasymmetrically from high-risk to low-risk objects based on spatial proximity\nand accident relationship. This enables the robot to infer potential hazards\neven when they are not explicitly visible or labeled. Designed for\ninterpretability and lightweight onboard deployment, our method is validated on\na dataset with human-annotated risk regions, achieving a binary risk detection\naccuracy of 75%. The system demonstrates strong alignment with human\nperception, particularly in scenes involving sharp or unstable objects. These\nresults underline the potential of context-aware risk reasoning to enhance\nrobotic scene understanding and proactive safety behaviors in shared\nhuman-robot spaces. This framework could serve as a foundation for future\nsystems that make context-driven safety decisions, provide real-time alerts, or\nautonomously assist users in avoiding or mitigating hazards within home\nenvironments.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19788v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19788v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.44,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.332,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses human-annotated data for validation and evaluation of risk detection accuracy, but it does not involve training a model with reinforcement learning based on human feedback. There is no mention of a reward model, fine-tuning via reinforcement learning, or aligning an AI with human preferences as defined in RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a semantic graph-based risk propagation algorithm for estimating hazards, which involves spreading risk scores between objects, but it does not use diffusion models or an iterative refinement process for multi-step logical reasoning tasks. There is no evidence of treating a Chain-of-Thought as an entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19789",
      "title": "StableIntrinsic: Detail-preserving One-step Diffusion Model for\n  Multi-view Material Estimation",
      "authors": [
        "Xiuchao Wu",
        "Pengfei Zhu",
        "Jiangjing Lyu",
        "Xinguo Liu",
        "Jie Guo",
        "Yanwen Guo",
        "Weiwei Xu",
        "Chengfei Lyu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recovering material information from images has been extensively studied in\ncomputer graphics and vision. Recent works in material estimation leverage\ndiffusion model showing promising results. However, these diffusion-based\nmethods adopt a multi-step denoising strategy, which is time-consuming for each\nestimation. Such stochastic inference also conflicts with the deterministic\nmaterial estimation task, leading to a high variance estimated results. In this\npaper, we introduce StableIntrinsic, a one-step diffusion model for multi-view\nmaterial estimation that can produce high-quality material parameters with low\nvariance. To address the overly-smoothing problem in one-step diffusion,\nStableIntrinsic applies losses in pixel space, with each loss designed based on\nthe properties of the material. Additionally, StableIntrinsic introduces a\nDetail Injection Network (DIN) to eliminate the detail loss caused by VAE\nencoding, while further enhancing the sharpness of material prediction results.\nThe experimental results indicate that our method surpasses the current\nstate-of-the-art techniques by achieving a $9.9\\%$ improvement in the Peak\nSignal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error\n(MSE) for metallic and roughness by $44.4\\%$ and $60.0\\%$, respectively.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19789v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19789v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.282,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.471,
      "distributed_training_score": 0.295,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a one-step diffusion model for material estimation in computer vision, focusing on accelerating inference and reducing variance in image processing tasks. It does not involve adapting diffusion models for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks, as the topic requires. Instead, it applies diffusion to a perceptual task without any reasoning components.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19791",
      "title": "Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color\n  Perception in Text-to-Image Models",
      "authors": [
        "Shay Shomer Chai",
        "Wenxuan Peng",
        "Bharath Hariharan",
        "Hadar Averbuch-Elor"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-image generation has recently seen remarkable success, granting users\nwith the ability to create high-quality images through the use of text.\nHowever, contemporary methods face challenges in capturing the precise\nsemantics conveyed by complex multi-object prompts. Consequently, many works\nhave sought to mitigate such semantic misalignments, typically via\ninference-time schemes that modify the attention layers of the denoising\nnetworks. However, prior work has mostly utilized coarse metrics, such as the\ncosine similarity between text and image CLIP embeddings, or human evaluations,\nwhich are challenging to conduct on a larger-scale. In this work, we perform a\ncase study on colors -- a fundamental attribute commonly associated with\nobjects in text prompts, which offer a rich test bed for rigorous evaluation.\nOur analysis reveals that pretrained models struggle to generate images that\nfaithfully reflect multiple color attributes-far more so than with single-color\nprompts-and that neither inference-time techniques nor existing editing methods\nreliably resolve these semantic misalignments. Accordingly, we introduce a\ndedicated image editing technique, mitigating the issue of multi-object\nsemantic alignment for prompts containing multiple colors. We demonstrate that\nour approach significantly boosts performance over a wide range of metrics,\nconsidering images generated by various text-to-image diffusion-based\ntechniques.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19791v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19791v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.458,
      "distributed_training_score": 0.297,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes diffusion models for image generation and editing, which involve iterative refinement processes. However, it applies these processes to improve color perception and semantic alignment in images, not to solve complex logical tasks or handle multi-step reasoning like a Chain-of-Thought. Thus, while diffusion mechanisms are present, they are not adapted for logical reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19798",
      "title": "FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding\n  and Comprehensive Modality Optimization",
      "authors": [
        "Muhammad Ali",
        "Omar Ali AlSuwaidi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In the realm of waste management, automating the sorting process for\nnon-biodegradable materials presents considerable challenges due to the\ncomplexity and variability of waste streams. To address these challenges, we\nintroduce an enhanced neural architecture that builds upon an existing\nEncoder-Decoder structure to improve the accuracy and efficiency of waste\nsorting systems. Our model integrates several key innovations: a Comprehensive\nAttention Block within the decoder, which refines feature representations by\ncombining convolutional and upsampling operations. In parallel, we utilize\nattention through the Mamba architecture, providing an additional performance\nboost. We also introduce a Data Fusion Block that fuses images with more than\nthree channels. To achieve this, we apply PCA transformation to reduce the\ndimensionality while retaining the maximum variance and essential information\nacross three dimensions, which are then used for further processing. We\nevaluated the model on RGB, hyperspectral, multispectral, and a combination of\nRGB and hyperspectral data. The results demonstrate that our approach\noutperforms existing methods by a significant margin.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19798v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19798v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.343,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19804",
      "title": "A bag of tricks for real-time Mitotic Figure detection",
      "authors": [
        "Christian Marzahl",
        "Brian Napora"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Mitotic figure (MF) detection in histopathology images is challenging due to\nlarge variations in slide scanners, staining protocols, tissue types, and the\npresence of artifacts. This paper presents a collection of training techniques\n- a bag of tricks - that enable robust, real-time MF detection across diverse\ndomains. We build on the efficient RTMDet single stage object detector to\nachieve high inference speed suitable for clinical deployment. Our method\naddresses scanner variability and tumor heterogeneity via extensive\nmulti-domain training data, balanced sampling, and careful augmentation.\nAdditionally, we employ targeted, hard negative mining on necrotic and debris\ntissue to reduce false positives. In a grouped 5-fold cross-validation across\nmultiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On\nthe preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025\nchallenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81,\noutperforming larger models and demonstrating adaptability to new, unfamiliar\ndomains. The proposed solution offers a practical trade-off between accuracy\nand speed, making it attractive for real-world clinical adoption.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19804v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19804v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.333,
      "distributed_training_score": 0.411,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on techniques for robust mitotic figure detection, including data augmentation, balanced sampling, and hard negative mining, using the RTMDet detector. It does not discuss distributed training, parallel computing, multi-node machine learning, or any methods for partitioning data or computation across multiple processors or nodes. The contributions are centered on model training strategies and domain generalization, with no mention of acceleration techniques involving distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19806",
      "title": "Context-aware Sparse Spatiotemporal Learning for Event-based Vision",
      "authors": [
        "Shenqi Wang",
        "Guangzhi Tang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.NE (Neural and Evolutionary Computing)"
      ],
      "abstract": "Event-based camera has emerged as a promising paradigm for robot perception,\noffering advantages with high temporal resolution, high dynamic range, and\nrobustness to motion blur. However, existing deep learning-based event\nprocessing methods often fail to fully leverage the sparse nature of event\ndata, complicating their integration into resource-constrained edge\napplications. While neuromorphic computing provides an energy-efficient\nalternative, spiking neural networks struggle to match of performance of\nstate-of-the-art models in complex event-based vision tasks, like object\ndetection and optical flow. Moreover, achieving high activation sparsity in\nneural networks is still difficult and often demands careful manual tuning of\nsparsity-inducing loss terms. Here, we propose Context-aware Sparse\nSpatiotemporal Learning (CSSL), a novel framework that introduces context-aware\nthresholding to dynamically regulate neuron activations based on the input\ndistribution, naturally reducing activation density without explicit sparsity\nconstraints. Applied to event-based object detection and optical flow\nestimation, CSSL achieves comparable or superior performance to\nstate-of-the-art methods while maintaining extremely high neuronal sparsity.\nOur experimental results highlight CSSL's crucial role in enabling efficient\nevent-based vision for neuromorphic processing.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19806v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19806v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.298,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.354,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19807",
      "title": "Bootstrapping Learned Cost Models with Synthetic SQL Queries",
      "authors": [
        "Michael Nidd",
        "Christoph Miksovic",
        "Thomas Gschwind",
        "Francesco Fusco",
        "Andrea Giovannini",
        "Ioana Giurgiu"
      ],
      "categories": [
        "cs.DB (Databases)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Having access to realistic workloads for a given database instance is\nextremely important to enable stress and vulnerability testing, as well as to\noptimize for cost and performance. Recent advances in learned cost models have\nshown that when enough diverse SQL queries are available, one can effectively\nand efficiently predict the cost of running a given query against a specific\ndatabase engine. In this paper, we describe our experience in exploiting modern\nsynthetic data generation techniques, inspired by the generative AI and LLM\ncommunity, to create high-quality datasets enabling the effective training of\nsuch learned cost models. Initial results show that we can improve a learned\ncost model's predictive accuracy by training it with 45% fewer queries than\nwhen using competitive generation approaches.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19807v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19807v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.425,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.342,
      "datasets_score": 0.384,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using synthetic data generation techniques to create training datasets for learned cost models, which aligns closely with weak supervision. By programmatically generating synthetic SQL queries as a high-level, noisy, or imprecise source of training data, the approach reduces reliance on hand-labeled data, enabling efficient model training with fewer queries. This directly embodies the principles of weak supervision in machine learning.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper addresses the challenge of generating realistic SQL query workloads for database instances to train learned cost models, which predict query execution costs. By leveraging synthetic data generation techniques inspired by generative AI and large language models, the authors create high-quality datasets that enable these models to achieve improved predictive accuracy with 45% fewer training queries compared to alternative approaches, thereby enhancing efficiency in database optimization and testing.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying generative AI techniques to synthesize SQL queries for training learned cost models, combining existing ideas in a new way to address database workload generation. While not introducing a entirely new problem, it advances the state-of-the-art in cost model training efficiency.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research and applications in database optimization and AI-driven data management by providing a more efficient method for generating training data. However, its impact may be confined to specific subfields like databases and AI, rather than broader areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a high-quality contribution with practical implications for database performance and AI applications, making it valuable for researchers and practitioners in the field. While not essential for everyone, it provides insights that could inform ongoing work in related areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8e850e06c2e4d4fc8c53ea3afd4289dcd71ae71e",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 17,
      "average_h_index": 4.333333333333333,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Michael Nidd",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377560432"
        },
        {
          "name": "Christoph Miksovic",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2298321"
        },
        {
          "name": "T. Gschwind",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/1776596"
        },
        {
          "name": "Francesco Fusco",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377560855"
        },
        {
          "name": "Andrea Giovannini",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377560238"
        },
        {
          "name": "Ioana Giurgiu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2333422480"
        }
      ]
    },
    {
      "id": "2508.19808",
      "title": "AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via\n  Automatic Quality Assessment",
      "authors": [
        "Kaixuan Lu",
        "Mehmet Onurcan Kaya",
        "Dim P. Papadopoulos"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Video Instance Segmentation (VIS) faces significant annotation challenges due\nto its dual requirements of pixel-level masks and temporal consistency labels.\nWhile recent unsupervised methods like VideoCutLER eliminate optical flow\ndependencies through synthetic data, they remain constrained by the\nsynthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised\nframework that bridges this gap through quality-guided self-training. Our\napproach establishes a closed-loop system between pseudo-label generation and\nautomatic quality assessment, enabling progressive adaptation from synthetic to\nreal videos. Experiments demonstrate state-of-the-art performance with 52.6\n$\\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous\nstate-of-the-art VideoCutLER by 4.4$\\%$, while requiring no human annotations.\nThis demonstrates the viability of quality-aware self-training for unsupervised\nVIS. The source code of our method is available at\nhttps://github.com/wcbup/AutoQ-VIS.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19808v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19808v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.441,
      "diffusion_reasoning_score": 0.327,
      "distributed_training_score": 0.295,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, AutoQ-VIS, employs a self-training framework that generates and selects pseudo-labels from unlabeled videos using automatic quality assessment, which directly aligns with weak supervision. It programmatically creates large quantities of training labels (via pseudo-labels) that may be noisy or imprecise, without relying on hand-labeled data, thus exemplifying the core principles of weak supervision in training VIS models.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "AutoQ-VIS is a novel unsupervised framework for Video Instance Segmentation (VIS) that addresses the synthetic-to-real domain gap by integrating automatic quality assessment into a self-training loop, starting with synthetic data to train a VIS model and a quality predictor, then iteratively refining the model using high-quality pseudo-labels generated from unlabeled real videos. The methodology involves generating pseudo masks, scoring their quality, and selectively adding high-quality ones to the training set for progressive improvement, resulting in state-of-the-art performance of 52.6 AP50 on the YouTubeVIS-2019 validation set, surpassing the previous best by 4.4% without requiring any human annotations.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by combining quality-guided self-training with pseudo-label generation to bridge the synthetic-to-real domain gap in unsupervised VIS, significantly advancing the state-of-the-art beyond existing methods like VideoCutLER. This innovation in automatic quality assessment for iterative refinement represents a substantial contribution to the field.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the computer vision subfield of unsupervised video segmentation, as it demonstrates effective self-training techniques that could enhance future models. However, its influence may be limited to specific applications in VIS rather than broader commercial or research areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to unsupervised VIS with innovative methodology and superior performance, making it important for researchers in computer vision to be aware of. While not essential for all, it offers insights that could guide future work in the area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/cd7ad59b62e99782a7bd63a07a0855c58e194fb3",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 14,
      "average_h_index": 4.666666666666667,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Kaixuan Lu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377947266"
        },
        {
          "name": "Mehmet Onurcan Kaya",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2355081647"
        },
        {
          "name": "Dim P. Papadopoulos",
          "h_index": 14,
          "profile_url": "https://www.semanticscholar.org/author/1749373"
        }
      ]
    },
    {
      "id": "2508.19815",
      "title": "ERSR: An Ellipse-constrained pseudo-label refinement and symmetric\n  regularization framework for semi-supervised fetal head segmentation in\n  ultrasound images",
      "authors": [
        "Linkuan Zhou",
        "Zhexin Chen",
        "Yufei Shen",
        "Junlin Xu",
        "Ping Xuan",
        "Yixin Zhu",
        "Yuqi Fang",
        "Cong Cong",
        "Leyi Wei",
        "Ran Su",
        "Jia Zhou",
        "Qiangguo Jin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Automated segmentation of the fetal head in ultrasound images is critical for\nprenatal monitoring. However, achieving robust segmentation remains challenging\ndue to the poor quality of ultrasound images and the lack of annotated data.\nSemi-supervised methods alleviate the lack of annotated data but struggle with\nthe unique characteristics of fetal head ultrasound images, making it\nchallenging to generate reliable pseudo-labels and enforce effective\nconsistency regularization constraints. To address this issue, we propose a\nnovel semi-supervised framework, ERSR, for fetal head ultrasound segmentation.\nOur framework consists of the dual-scoring adaptive filtering strategy, the\nellipse-constrained pseudo-label refinement, and the symmetry-based multiple\nconsistency regularization. The dual-scoring adaptive filtering strategy uses\nboundary consistency and contour regularity criteria to evaluate and filter\nteacher outputs. The ellipse-constrained pseudo-label refinement refines these\nfiltered outputs by fitting least-squares ellipses, which strengthens pixels\nnear the center of the fitted ellipse and suppresses noise simultaneously. The\nsymmetry-based multiple consistency regularization enforces multi-level\nconsistency across perturbed images, symmetric regions, and between original\npredictions and pseudo-labels, enabling the model to capture robust and stable\nshape representations. Our method achieves state-of-the-art performance on two\nbenchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36%\nwith 10% and 20% labeled data, respectively. On the PSFH dataset, the scores\nare 91.68% and 93.70% under the same settings.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19815v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19815v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.424,
      "diffusion_reasoning_score": 0.312,
      "distributed_training_score": 0.342,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves a semi-supervised framework that generates and refines pseudo-labels for unlabeled ultrasound images, which aligns directly with weak supervision. It programmatically creates labels from model outputs (e.g., teacher model predictions), addresses their noise and imprecision through strategies like dual-scoring adaptive filtering and ellipse-constrained refinement, and uses them for training, thereby reducing reliance on perfectly hand-labeled data. This core mechanism fits the definition of weak supervision by leveraging high-level, noisy sources for label generation.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces ERSR, a novel semi-supervised framework for fetal head segmentation in ultrasound images, addressing challenges like poor image quality and limited annotated data by incorporating a dual-scoring adaptive filtering strategy to evaluate pseudo-labels, an ellipse-constrained pseudo-label refinement to enhance accuracy using least-squares ellipses, and a symmetry-based multiple consistency regularization to ensure robust predictions. The methodology leverages the elliptical and symmetrical characteristics of the fetal head, achieving state-of-the-art Dice scores of up to 95.36% on the HC18 dataset and 93.70% on the PSFH dataset with only 10-20% labeled data, demonstrating significant improvements in segmentation performance.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing semi-supervised techniques with domain-specific adaptations like ellipse-constrained refinement and symmetry-based regularization, effectively addressing unique challenges in fetal head ultrasound segmentation. While it builds on prior methods such as pseudo-labeling and consistency regularization, it introduces clever integrations that advance the application in this specific area without creating entirely new paradigms.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in medical image segmentation, particularly for ultrasound-based prenatal diagnostics, due to its state-of-the-art results and practical implications for clinical applications. However, its impact may be confined to the subfield of fetal head segmentation, limiting broader adoption.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with innovative strategies and strong empirical results that advance semi-supervised learning in medical imaging, making it valuable for researchers in computer vision and AI for healthcare. While not essential for all audiences, it is significant for those working on similar problems in prenatal ultrasound analysis.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3b68f5e8296f3c94303532fdbaba91c467a54bdc",
      "total_authors": 12,
      "authors_found": 12,
      "highest_h_index": 11,
      "average_h_index": 1.9166666666666667,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Linkuan Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377646010"
        },
        {
          "name": "Zhexin Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377509970"
        },
        {
          "name": "Yufei Shen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377502082"
        },
        {
          "name": "Junlin Xu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2352207297"
        },
        {
          "name": "Ping Xuan",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2066135805"
        },
        {
          "name": "Yixin Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377541183"
        },
        {
          "name": "Yuqi Fang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372210280"
        },
        {
          "name": "Cong Cong",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2365774629"
        },
        {
          "name": "Leyi Wei",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2254904782"
        },
        {
          "name": "Ran Su",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2254308815"
        },
        {
          "name": "Jia Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377512023"
        },
        {
          "name": "Qiangguo Jin",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/24921532"
        }
      ]
    },
    {
      "id": "2508.19819",
      "title": "From Research to Reality: Feasibility of Gradient Inversion Attacks in\n  Federated Learning",
      "authors": [
        "Viktor Valadi",
        "Mattias Åkesson",
        "Johan Östman",
        "Salman Toor",
        "Andreas Hellander"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Gradient inversion attacks have garnered attention for their ability to\ncompromise privacy in federated learning. However, many studies consider\nattacks with the model in inference mode, where training-time behaviors like\ndropout are disabled and batch normalization relies on fixed statistics. In\nthis work, we systematically analyze how architecture and training behavior\naffect vulnerability, including the first in-depth study of inference-mode\nclients, which we show dramatically simplifies inversion. To assess attack\nfeasibility under more realistic conditions, we turn to clients operating in\nstandard training mode. In this setting, we find that successful attacks are\nonly possible when several architectural conditions are met simultaneously:\nmodels must be shallow and wide, use skip connections, and, critically, employ\npre-activation normalization. We introduce two novel attacks against models in\ntraining-mode with varying attacker knowledge, achieving state-of-the-art\nperformance under realistic training conditions. We extend these efforts by\npresenting the first attack on a production-grade object-detection model. Here,\nto enable any visibly identifiable leakage, we revert to the lenient inference\nmode setting and make multiple architectural modifications to increase model\nvulnerability, with the extent of required changes highlighting the strong\ninherent robustness of such architectures. We conclude this work by offering\nthe first comprehensive mapping of settings, clarifying which combinations of\narchitectural choices and operational modes meaningfully impact privacy. Our\nanalysis provides actionable insight into when models are likely vulnerable,\nwhen they appear robust, and where subtle leakage may persist. Together, these\nfindings reframe how gradient inversion risk should be assessed in future\nresearch and deployment scenarios.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19819v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19819v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.382,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19827",
      "title": "Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful\n  Post-hoc Rationalisation?",
      "authors": [
        "Samuel Lewis-Lim",
        "Xingwei Tan",
        "Zhixue Zhao",
        "Nikolaos Aletras"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited\ngains for soft-reasoning problems such as analytical and commonsense reasoning.\nCoT can also be unfaithful to a model's actual reasoning. We investigate the\ndynamics and faithfulness of CoT in soft-reasoning tasks across\ninstruction-tuned, reasoning and reasoning-distilled models. Our findings\nreveal differences in how these models rely on CoT, and show that CoT influence\nand faithfulness are not always aligned.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19827v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19827v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.563,
      "distributed_training_score": 0.267,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper analyzes the dynamics and faithfulness of Chain-of-Thought (CoT) in LLMs for soft-reasoning tasks, focusing on aspects like influence and unfaithfulness across different model types. However, it does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion techniques for multi-step logical reasoning. The core contribution is about CoT's role in LLMs, with no reference to treating reasoning paths holistically via diffusion, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19830",
      "title": "Gradient Rectification for Robust Calibration under Distribution Shift",
      "authors": [
        "Yilin Zhang",
        "Cai Xu",
        "You Wu",
        "Ziyu Guan",
        "Wei Zhao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Deep neural networks often produce overconfident predictions, undermining\ntheir reliability in safety-critical applications. This miscalibration is\nfurther exacerbated under distribution shift, where test data deviates from the\ntraining distribution due to environmental or acquisition changes. While\nexisting approaches improve calibration through training-time regularization or\npost-hoc adjustment, their reliance on access to or simulation of target\ndomains limits their practicality in real-world scenarios. In this paper, we\npropose a novel calibration framework that operates without access to target\ndomain information. From a frequency-domain perspective, we identify that\ndistribution shifts often distort high-frequency visual cues exploited by deep\nmodels, and introduce a low-frequency filtering strategy to encourage reliance\non domain-invariant features. However, such information loss may degrade\nIn-Distribution (ID) calibration performance. Therefore, we further propose a\ngradient-based rectification mechanism that enforces ID calibration as a hard\nconstraint during optimization. Experiments on synthetic and real-world shifted\ndatasets, including CIFAR-10/100-C and WILDS, demonstrate that our method\nsignificantly improves calibration under distribution shift while maintaining\nstrong in-distribution performance.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19830v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19830v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.414,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.43,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on improving neural network calibration under distribution shifts using techniques like frequency-domain filtering and gradient rectification, with no mention of programmatically generating labels, noisy supervision, or alternatives to hand-labeled data.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses a training framework for calibration but does not address parallel computing, multi-node setups, or strategies for partitioning data/computation across processors, focusing instead on optimization techniques for a single training process.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19839",
      "title": "PSO-Merging: Merging Models Based on Particle Swarm Optimization",
      "authors": [
        "Kehao Zhang",
        "Shaolei Zhang",
        "Yang Feng"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Model merging has emerged as an efficient strategy for constructing multitask\nmodels by integrating the strengths of multiple available expert models,\nthereby reducing the need to fine-tune a pre-trained model for all the tasks\nfrom scratch. Existing data-independent methods struggle with performance\nlimitations due to the lack of data-driven guidance. Data-driven approaches\nalso face key challenges: gradient-based methods are computationally expensive,\nlimiting their practicality for merging large expert models, whereas existing\ngradient-free methods often fail to achieve satisfactory results within a\nlimited number of optimization steps. To address these limitations, this paper\nintroduces PSO-Merging, a novel data-driven merging method based on the\nParticle Swarm Optimization (PSO). In this approach, we initialize the particle\nswarm with a pre-trained model, expert models, and sparsified expert models. We\nthen perform multiple iterations, with the final global best particle serving\nas the merged model. Experimental results on different language models show\nthat PSO-Merging generally outperforms baseline merging methods, offering a\nmore efficient and scalable solution for model merging.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19839v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19839v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.438,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a model merging technique using Particle Swarm Optimization (PSO) for integrating expert language models, focusing on efficiency and scalability. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper addresses model merging via PSO, which is an optimization method for combining parameters of existing models, but it does not discuss distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes or processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19843",
      "title": "SoK: Large Language Model Copyright Auditing via Fingerprinting",
      "authors": [
        "Shuo Shao",
        "Yiming Li",
        "Yu He",
        "Hongwei Yao",
        "Wenyuan Yang",
        "Dacheng Tao",
        "Zhan Qin"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "The broad capabilities and substantial resources required to train Large\nLanguage Models (LLMs) make them valuable intellectual property, yet they\nremain vulnerable to copyright infringement, such as unauthorized use and model\ntheft. LLM fingerprinting, a non-intrusive technique that extracts and compares\nthe distinctive features from LLMs to identify infringements, offers a\npromising solution to copyright auditing. However, its reliability remains\nuncertain due to the prevalence of diverse model modifications and the lack of\nstandardized evaluation. In this SoK, we present the first comprehensive study\nof LLM fingerprinting. We introduce a unified framework and formal taxonomy\nthat categorizes existing methods into white-box and black-box approaches,\nproviding a structured overview of the state of the art. We further propose\nLeaFBench, the first systematic benchmark for evaluating LLM fingerprinting\nunder realistic deployment scenarios. Built upon mainstream foundation models\nand comprising 149 distinct model instances, LeaFBench integrates 13\nrepresentative post-development techniques, spanning both parameter-altering\nmethods (e.g., fine-tuning, quantization) and parameter-independent mechanisms\n(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the\nstrengths and weaknesses of existing methods, thereby outlining future research\ndirections and critical open problems in this emerging field. The code is\navailable at https://github.com/shaoshuo-ss/LeaFBench.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19843v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19843v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.376,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on LLM fingerprinting for copyright auditing, including techniques for extracting and comparing model features to detect infringements. It discusses general model modifications like fine-tuning and quantization but does not involve training models with human feedback, reward models, or reinforcement learning to align with human preferences. Thus, it does not address RLHF as defined.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19850",
      "title": "Image Quality Assessment for Machines: Paradigm, Large-scale Database,\n  and Models",
      "authors": [
        "Xiaoqi Wang",
        "Yun Zhang",
        "Weisi Lin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Machine vision systems (MVS) are intrinsically vulnerable to performance\ndegradation under adverse visual conditions. To address this, we propose a\nmachine-centric image quality assessment (MIQA) framework that quantifies the\nimpact of image degradations on MVS performance. We establish an MIQA paradigm\nencompassing the end-to-end assessment workflow. To support this, we construct\na machine-centric image quality database (MIQD-2.5M), comprising 2.5 million\nsamples that capture distinctive degradation responses in both consistency and\naccuracy metrics, spanning 75 vision models, 250 degradation types, and three\nrepresentative vision tasks. We further propose a region-aware MIQA (RA-MIQA)\nmodel to evaluate MVS visual quality through fine-grained spatial degradation\nanalysis. Extensive experiments benchmark the proposed RA-MIQA against seven\nhuman visual system (HVS)-based IQA metrics and five retrained classical\nbackbones. Results demonstrate RA-MIQA's superior performance in multiple\ndimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on\naccuracy for image classification, while also revealing task-specific\ndegradation sensitivities. Critically, HVS-based metrics prove inadequate for\nMVS quality prediction, while even specialized MIQA models struggle with\nbackground degradations, accuracy-oriented estimation, and subtle distortions.\nThis study can advance MVS reliability and establish foundations for\nmachine-centric image processing and optimization. The model and code are\navailable at: https://github.com/XiaoqiWang/MIQA.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19850v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19850v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.431,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.358,
      "datasets_score": 0.434,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves programmatically generating quality labels from vision model responses, which aligns with weak supervision by using noisy or derived sources rather than hand-labeling, but this is a supporting aspect rather than the main contribution, which focuses on MIQA framework and database creation.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes creating, analyzing, and benchmarking a new large-scale dataset (MIQD-2.5M) for machine learning applications in image quality assessment, covering dataset curation, statistical analysis, and evaluation protocols.",
      "llm_score_status": "completed",
      "summary": "This paper proposes a machine-centric image quality assessment (MIQA) framework to evaluate how image degradations affect machine vision systems (MVS), introducing a comprehensive paradigm, a large-scale database (MIQD-2.5M) with 2.5 million samples across various degradations and tasks, and a region-aware MIQA model (RA-MIQA). The methodology involves constructing the database by testing 75 vision models on three tasks and developing RA-MIQA for fine-grained spatial analysis, with key findings demonstrating its superior performance over human visual system-based metrics in consistency and accuracy, while highlighting task-specific degradation sensitivities and the inadequacies of existing methods.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new paradigm for machine-centric image quality assessment, including a large-scale database and a novel region-aware model, significantly advancing the state-of-the-art by addressing machine-specific degradation impacts beyond traditional human-centric approaches.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and commercial applications in machine vision, such as improving reliability in autonomous systems and image processing, by providing essential tools for degradation analysis and optimization.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to computer vision by introducing innovative methods for machine-centric quality assessment, making it essential for researchers focused on MVS reliability and image processing advancements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/258cf0a43dc4c338592a529f65911e1ad4a4b85d",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 1,
      "average_h_index": 0.6666666666666666,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Xiaoqi Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2315247412"
        },
        {
          "name": "Yun Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2315266596"
        },
        {
          "name": "Weisi Lin",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2342451666"
        }
      ]
    },
    {
      "id": "2508.19851",
      "title": "Tracking World States with Language Models: State-Based Evaluation Using\n  Chess",
      "authors": [
        "Romain Harang",
        "Jason Naradowsky",
        "Yaswitha Gujju",
        "Yusuke Miyao"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) exhibit emergent capabilities in structured\ndomains, suggesting they may implicitly internalize high-fidelity\nrepresentations of world models. While probing techniques have shown promising\nsigns of this in scientific and game-based settings, they rely on\nmodel-specific internal activations, which limit interpretability and\ngeneralizability. In this work, we propose a model-agnostic, state-based\nevaluation framework using chess as a benchmark to assess whether LLMs preserve\nthe semantics of structured environments. Our method analyzes the downstream\nlegal move distributions (state affordances) to estimate semantic fidelity\nbetween predicted and actual game states. This approach offers a more\nmeaningful evaluation than conventional string-based metrics by aligning more\nclosely with the strategic and rule-governed nature of chess. Experimental\nresults demonstrate that our metrics capture deficiencies in state-tracking,\nhighlighting limitations of LLMs in maintaining coherent internal models over\nlong sequences. Our framework provides a robust tool for evaluating structured\nreasoning in LLMs without requiring internal model access, and generalizes to a\nwide class of symbolic environments.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19851v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19851v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.466,
      "distributed_training_score": 0.322,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating LLMs' ability to track world states using a chess-based framework, without any mention of training models with human feedback, reward models, or reinforcement learning for alignment. It is solely an evaluation method, not involving RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a state-based evaluation for LLMs using chess, emphasizing analysis of move distributions, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning corrections as defined. It lacks any components related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19852",
      "title": "Ego-centric Predictive Model Conditioned on Hand Trajectories",
      "authors": [
        "Binjie Zhang",
        "Mike Zheng Shou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In egocentric scenarios, anticipating both the next action and its visual\noutcome is essential for understanding human-object interactions and for\nenabling robotic planning. However, existing paradigms fall short of jointly\nmodeling these aspects. Vision-Language-Action (VLA) models focus on action\nprediction but lack explicit modeling of how actions influence the visual\nscene, while video prediction models generate future frames without\nconditioning on specific actions, often resulting in implausible or\ncontextually inconsistent outcomes. To bridge this gap, we propose a unified\ntwo-stage predictive framework that jointly models action and visual future in\negocentric scenarios, conditioned on hand trajectories. In the first stage, we\nperform consecutive state modeling to process heterogeneous inputs (visual\nobservations, language, and action history) and explicitly predict future hand\ntrajectories. In the second stage, we introduce causal cross-attention to fuse\nmulti-modal cues, leveraging inferred action signals to guide an image-based\nLatent Diffusion Model (LDM) for frame-by-frame future video generation. Our\napproach is the first unified model designed to handle both egocentric human\nactivity understanding and robotic manipulation tasks, providing explicit\npredictions of both upcoming actions and their visual consequences. Extensive\nexperiments on Ego4D, BridgeData, and RLBench demonstrate that our method\noutperforms state-of-the-art baselines in both action prediction and future\nvideo synthesis.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19852v2",
      "pdf_url": "http://arxiv.org/pdf/2508.19852v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.475,
      "distributed_training_score": 0.319,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a Latent Diffusion Model (LDM) for generating future video frames in egocentric scenarios, specifically for visual synthesis conditioned on hand trajectories. However, it does not adapt the diffusion process for multi-step logical reasoning or treat a Chain-of-Thought as a single entity for iterative refinement in solving complex logical tasks. The focus is on action prediction and video generation, not on reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19862",
      "title": "Multimodal Conditional MeshGAN for Personalized Aneurysm Growth\n  Prediction",
      "authors": [
        "Long Chen",
        "Ashiv Patel",
        "Mengyun Qiao",
        "Mohammad Yousuf Salmasi",
        "Salah A. Hammouche",
        "Vasilis Stavrinides",
        "Jasleen Nagi",
        "Soodeh Kalaie",
        "Xiao Yun Xu",
        "Wenjia Bai",
        "Declan P. O'Regan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Personalized, accurate prediction of aortic aneurysm progression is essential\nfor timely intervention but remains challenging due to the need to model both\nsubtle local deformations and global anatomical changes within complex 3D\ngeometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh\ngenerative adversarial network for 3D aneurysm growth prediction. MCMeshGAN\nintroduces a dual-branch architecture combining a novel local KNN-based\nconvolutional network (KCN) to preserve fine-grained geometric details and a\nglobal graph convolutional network (GCN) to capture long-range structural\ncontext, overcoming the over-smoothing limitations of deep GCNs. A dedicated\ncondition branch encodes clinical attributes (age, sex) and the target time\ninterval to generate anatomically plausible, temporally controlled predictions,\nenabling retrospective and prospective modeling. We curated TAAMesh, a new\nlongitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal\nrecords (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive\nexperiments demonstrate that MCMeshGAN consistently outperforms\nstate-of-the-art baselines in both geometric accuracy and clinically important\ndiameter estimation. This framework offers a robust step toward clinically\ndeployable, personalized 3D disease trajectory modeling. The source code for\nMCMeshGAN and the baseline methods is publicly available at\nhttps://github.com/ImperialCollegeLondon/MCMeshGAN.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19862v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19862v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.286,
      "weak_supervision_score": 0.288,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.324,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19864",
      "title": "Self-supervised structured object representation learning",
      "authors": [
        "Oussama Hadjerci",
        "Antoine Letienne",
        "Mohamed Abbas Hedjazi",
        "Adel Hafiane"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Self-supervised learning (SSL) has emerged as a powerful technique for\nlearning visual representations. While recent SSL approaches achieve strong\nresults in global image understanding, they are limited in capturing the\nstructured representation in scenes. In this work, we propose a self-supervised\napproach that progressively builds structured visual representations by\ncombining semantic grouping, instance level separation, and hierarchical\nstructuring. Our approach, based on a novel ProtoScale module, captures visual\nelements across multiple spatial scales. Unlike common strategies like DINO\nthat rely on random cropping and global embeddings, we preserve full scene\ncontext across augmented views to improve performance in dense prediction\ntasks. We validate our method on downstream object detection tasks using a\ncombined subset of multiple datasets (COCO and UA-DETRAC). Experimental results\nshow that our method learns object centric representations that enhance\nsupervised object detection and outperform the state-of-the-art methods, even\nwhen trained with limited annotated data and fewer fine-tuning epochs.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19864v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19864v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.35,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19866",
      "title": "TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of\n  Sequential and Visual Trajectory Representations",
      "authors": [
        "François G. Landry",
        "Moulay A. Akhloufi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "With the introduction of vehicles with autonomous capabilities on public\nroads, predicting pedestrian crossing intention has emerged as an active area\nof research. The task of predicting pedestrian crossing intention involves\ndetermining whether pedestrians in the scene are likely to cross the road or\nnot. In this work, we propose TrajFusionNet, a novel transformer-based model\nthat combines future pedestrian trajectory and vehicle speed predictions as\npriors for predicting crossing intention. TrajFusionNet comprises two branches:\na Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM\nbranch learns from a sequential representation of the observed and predicted\npedestrian trajectory and vehicle speed. Complementarily, the VAM branch\nenables learning from a visual representation of the predicted pedestrian\ntrajectory by overlaying predicted pedestrian bounding boxes onto scene images.\nBy utilizing a small number of lightweight modalities, TrajFusionNet achieves\nthe lowest total inference time (including model runtime and data\npreprocessing) among current state-of-the-art approaches. In terms of\nperformance, it achieves state-of-the-art results across the three most\ncommonly used datasets for pedestrian crossing intention prediction.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19866v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19866v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.283,
      "diffusion_reasoning_score": 0.458,
      "distributed_training_score": 0.331,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces TrajFusionNet, a transformer-based model for predicting pedestrian crossing intention using sequential and visual representations. It focuses on attention mechanisms and trajectory predictions but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19875",
      "title": "Sky Background Building of Multi-objective Fiber spectra Based on Mutual\n  Information Network",
      "authors": [
        "Hui Zhang",
        "Jianghui Cai",
        "Haifeng Yang",
        "Ali Luo",
        "Yuqing Yang",
        "Xiao Kong",
        "Zhichao Ding",
        "Lichan Zhou",
        "Qin Han"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Sky background subtraction is a critical step in Multi-objective Fiber\nspectra process. However, current subtraction relies mainly on sky fiber\nspectra to build Super Sky. These average spectra are lacking in the modeling\nof the environment surrounding the objects. To address this issue, a sky\nbackground estimation model: Sky background building based on Mutual\nInformation (SMI) is proposed. SMI based on mutual information and incremental\ntraining approach. It utilizes spectra from all fibers in the plate to estimate\nthe sky background. SMI contains two main networks, the first network applies a\nwavelength calibration module to extract sky features from spectra, and can\neffectively solve the feature shift problem according to the corresponding\nemission position. The second network employs an incremental training approach\nto maximize mutual information between representations of different spectra to\ncapturing the common component. Then, it minimizes the mutual information\nbetween adjoining spectra representations to obtain individual components. This\nnetwork yields an individual sky background at each location of the object. To\nverify the effectiveness of the method in this paper, we conducted experiments\non the spectra of LAMOST. Results show that SMI can obtain a better object sky\nbackground during the observation, especially in the blue end.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19875v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19875v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.292,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.316,
      "distributed_training_score": 0.333,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19881",
      "title": "Multispectral LiDAR data for extracting tree points in urban and\n  suburban areas",
      "authors": [
        "Narges Takhtkeshha",
        "Gabriele Mazzacca",
        "Fabio Remondino",
        "Juha Hyyppä",
        "Gottfried Mandlburger"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Monitoring urban tree dynamics is vital for supporting greening policies and\nreducing risks to electrical infrastructure. Airborne laser scanning has\nadvanced large-scale tree management, but challenges remain due to complex\nurban environments and tree variability. Multispectral (MS) light detection and\nranging (LiDAR) improves this by capturing both 3D spatial and spectral data,\nenabling detailed mapping. This study explores tree point extraction using\nMS-LiDAR and deep learning (DL) models. Three state-of-the-art models are\nevaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point\nTransformer V1 (PTv1). Results show the notable time efficiency and accuracy of\nSPT, with a mean intersection over union (mIoU) of 85.28%. The highest\ndetection accuracy is achieved by incorporating pseudo normalized difference\nvegetation index (pNDVI) with spatial data, reducing error rate by 10.61\npercentage points (pp) compared to using spatial information alone. These\nfindings highlight the potential of MS-LiDAR and DL to improve tree extraction\nand further tree inventories.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19881v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19881v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.29,
      "distributed_training_score": 0.353,
      "datasets_score": 0.385,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19882",
      "title": "Generative AI for Testing of Autonomous Driving Systems: A Survey",
      "authors": [
        "Qunying Song",
        "He Ye",
        "Mark Harman",
        "Federica Sarro"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Autonomous driving systems (ADS) have been an active area of research, with\nthe potential to deliver significant benefits to society. However, before\nlarge-scale deployment on public roads, extensive testing is necessary to\nvalidate their functionality and safety under diverse driving conditions.\nTherefore, different testing approaches are required, and achieving effective\nand efficient testing of ADS remains an open challenge. Recently, generative AI\nhas emerged as a powerful tool across many domains, and it is increasingly\nbeing applied to ADS testing due to its ability to interpret context, reason\nabout complex tasks, and generate diverse outputs. To gain a deeper\nunderstanding of its role in ADS testing, we systematically analyzed 91\nrelevant studies and synthesized their findings into six major application\ncategories, primarily centered on scenario-based testing of ADS. We also\nreviewed their effectiveness and compiled a wide range of datasets, simulators,\nADS, metrics, and benchmarks used for evaluation, while identifying 27\nlimitations. This survey provides an overview and practical insights into the\nuse of generative AI for testing ADS, highlights existing challenges, and\noutlines directions for future research in this rapidly evolving field.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19882v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19882v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.388,
      "datasets_score": 0.43,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper is a survey on generative AI for testing autonomous driving systems and does not mention or discuss reinforcement learning from human feedback. It focuses on applications like scenario generation using models such as LLMs and diffusion-based models, without any reference to training AI models with human-ranked data or reward models.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper mentions diffusion-based models as one of the generative AI tools used for ADS testing, such as in scenario generation, but it does not describe their use for multi-step logical reasoning or iterative refinement of a 'Chain-of-Thought' as defined. The focus is on general applications in testing, not on adapting diffusion for complex reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper reviews and compiles datasets, simulators, metrics, and benchmarks used in evaluating generative AI for ADS testing, which aligns with dataset analysis and benchmarking in AI applications. However, its main contribution is a broader survey on generative AI, not primarily focused on creating or deeply analyzing datasets.",
      "llm_score_status": "completed",
      "summary": "This survey paper systematically analyzes 91 studies on the application of generative AI in testing autonomous driving systems (ADS), focusing on three research questions that explore how generative AI is used, its effectiveness, and existing limitations. The authors synthesize findings into six major application categories, primarily scenario-based testing, review datasets, simulators, metrics, and benchmarks, and identify 27 limitations such as suboptimal outputs and high computational costs, while highlighting trends, challenges, and future research directions in this field.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable synthesis and categorization of existing research on generative AI for ADS testing, which is a relatively underexplored niche, but it does not introduce a entirely new problem or technique beyond compiling and analyzing prior work.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and influence research within the specific subfield of AI for ADS testing by providing a consolidated overview and identifying gaps, though its broader applicability to other domains may be limited.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality synthesis of relevant studies with practical insights for researchers in autonomous driving and AI testing, making it a valuable resource for those in the field, though not essential for a general audience.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/bb694f4ad3696650d8ecbe3756e9bab3953d1f7b",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 4,
      "average_h_index": 1.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Qunying Song",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378031162"
        },
        {
          "name": "He Ye",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2362322907"
        },
        {
          "name": "Mark Harman",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2245001155"
        },
        {
          "name": "Federica Sarro",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2373613374"
        }
      ]
    },
    {
      "id": "2508.19883",
      "title": "AI-Powered Detection of Inappropriate Language in Medical School\n  Curricula",
      "authors": [
        "Chiman Salavati",
        "Shannon Song",
        "Scott A. Hale",
        "Roberto E. Montenegro",
        "Shiri Dori-Hacohen",
        "Fabricio Murai"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "The use of inappropriate language -- such as outdated, exclusionary, or\nnon-patient-centered terms -- medical instructional materials can significantly\ninfluence clinical training, patient interactions, and health outcomes. Despite\ntheir reputability, many materials developed over past decades contain examples\nnow considered inappropriate by current medical standards. Given the volume of\ncurricular content, manually identifying instances of inappropriate use of\nlanguage (IUL) and its subcategories for systematic review is prohibitively\ncostly and impractical. To address this challenge, we conduct a first-in-class\nevaluation of small language models (SLMs) fine-tuned on labeled data and\npre-trained LLMs with in-context learning on a dataset containing approximately\n500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL\nclassifier, (2) subcategory-specific binary classifiers, (3) a multilabel\nclassifier, and (4) a two-stage hierarchical pipeline for general IUL detection\nfollowed by multilabel classification. For LLMs, we consider variations of\nprompts that include subcategory definitions and/or shots. We found that both\nLLama-3 8B and 70B, even with carefully curated shots, are largely outperformed\nby SLMs. While the multilabel classifier performs best on annotated data,\nsupplementing training with unflagged excerpts as negative examples boosts the\nspecific classifiers' AUC by up to 25%, making them most effective models for\nmitigating harmful language in medical curricula.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19883v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19883v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.42,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.336,
      "datasets_score": 0.388,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on fine-tuning small language models on labeled data and evaluating large language models with in-context learning, without any mention of reinforcement learning, human-ranked data, or a reward model for alignment with human preferences.",
      "weak_supervision_justification": "The paper incorporates elements of weak supervision by supplementing training with unflagged excerpts as negative examples, which are programmatically generated or noisy labels, to improve model performance, though the primary approach relies on labeled data.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the detection of inappropriate language use (IUL), such as outdated or exclusionary terms, in medical school curricula by evaluating small language models (SLMs) fine-tuned on a dataset of approximately 500 documents and pre-trained large language models (LLMs) with in-context learning. The methodology includes developing and comparing various classifiers, including general IUL, subcategory-specific, multilabel, and hierarchical pipelines, revealing that SLMs outperform LLMs and that incorporating negative examples improves AUC by up to 25%, making them effective for identifying and mitigating harmful language in educational materials.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a first-in-class AI framework for detecting IUL in medical curricula, which is a novel application addressing a previously unexamined problem in automated bias detection. This significantly advances the state-of-the-art in AI for medical education and language ethics.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in AI ethics, medical education, and bias reduction by providing scalable tools for detecting harmful language, potentially leading to citations and applications within these subfields. However, its impact may be limited to specialized areas rather than broad commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality contribution with practical methods for addressing language biases in medical contexts, making it valuable for researchers in AI and healthcare ethics. While not essential for all audiences, it offers important insights that warrant attention in relevant fields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5a4dcac915c05e1f0cd25c2faf2762e0a0faa910",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 11,
      "average_h_index": 3.6666666666666665,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Chiman Salavati",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/51219074"
        },
        {
          "name": "Shannon Song",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2312097111"
        },
        {
          "name": "Scott A. Hale",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2310818299"
        },
        {
          "name": "Roberto E. Montenegro",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2311700044"
        },
        {
          "name": "Shiri Dori-Hacohen",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/1403034406"
        },
        {
          "name": "Fabricio Murai",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2311697759"
        }
      ]
    },
    {
      "id": "2508.19887",
      "title": "Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset\n  with LLM-Assisted Translation Refinement",
      "authors": [
        "Mohammed Rakibul Hasan",
        "Rafi Majid",
        "Ahanaf Tahmid"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question\nAnswering (VQA) Dataset in Bangla, a widely used, low-resource language in\nmultimodal AI research. The majority of existing datasets are either manually\nannotated with an emphasis on a specific domain, query type, or answer type or\nare constrained by niche answer formats. In order to mitigate human-induced\nerrors and guarantee lucidity, we implemented a multilingual LLM-assisted\ntranslation refinement pipeline. This dataset overcomes the issues of\nlow-quality translations from multilingual sources. The dataset comprises\n52,650 question-answer pairs across 4750+ images. Questions are classified into\nthree distinct answer types: nominal (short descriptive), quantitative\n(numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive\nopen-source, high-quality VQA benchmark in Bangla, aiming to advance research\nin low-resource multimodal learning and facilitate the development of more\ninclusive AI systems.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19887v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19887v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.263,
      "datasets_score": 0.407,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new Visual Question Answering (VQA) dataset called Bangla-Bayanno, which includes 52,650 question-answer pairs across various image types. It details the dataset creation process, including LLM-assisted translation refinement for quality assurance, and positions it as a benchmark for low-resource multimodal AI. This directly aligns with the topic of datasets, as it involves new dataset introduction, curation methodologies, and benchmarking for machine learning applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces Bangla-Bayanno, a comprehensive open-ended Visual Question Answering (VQA) dataset for the Bengali language, comprising 52,650 question-answer pairs across over 4,750 images, with questions categorized into nominal, quantitative, and polar types. The core objective is to address the lack of high-quality VQA resources for low-resource languages like Bengali, using a multilingual Large Language Model (LLM)-assisted translation refinement pipeline to minimize errors and ensure clarity, thereby advancing multimodal AI research and promoting more inclusive AI systems.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a new, high-quality VQA dataset for a low-resource language like Bengali, using an LLM-assisted refinement technique, but it builds on existing VQA concepts rather than introducing a entirely new problem or architecture. This clever combination of translation refinement enhances dataset quality without revolutionizing the field.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in low-resource multimodal learning and Bengali AI by providing a benchmark dataset, potentially leading to more inclusive AI developments, though its applicability is primarily limited to specific subfields like computational linguistics for underrepresented languages. Overall, it may be cited and built upon within niche areas but not broadly across AI.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution by introducing a high-quality dataset for Bengali VQA, making it essential for researchers focused on low-resource languages and multimodal AI to be aware of for advancing inclusive technologies. However, it may not be critical for those outside this specific domain.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/bbf9d7f773ed34ab69aad6a308e0dbe63f36d8fe",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 2,
      "average_h_index": 0.6666666666666666,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Mohammed Rakibul Hasan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377750087"
        },
        {
          "name": "Rafi Majid",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377558511"
        },
        {
          "name": "Ahanaf Tahmid",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2128578909"
        }
      ]
    },
    {
      "id": "2508.19895",
      "title": "PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos",
      "authors": [
        "Ziyun Qian",
        "Runyu Xiao",
        "Shuyuan Tu",
        "Wei Xue",
        "Dingkang Yang",
        "Mingcheng Li",
        "Dongliang Kou",
        "Minghao Han",
        "Zizhi Chen",
        "Lihua Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in motion generation show remarkable progress. However,\nseveral limitations remain: (1) Existing pose-guided character motion transfer\nmethods merely replicate motion without learning its style characteristics,\nresulting in inexpressive characters. (2) Motion style transfer methods rely\nheavily on motion capture data, which is difficult to obtain. (3) Generated\nmotions sometimes violate physical laws. To address these challenges, this\npaper pioneers a new task: Video-to-Video Motion Personalization. We propose a\nnovel framework, PersonaAnimator, which learns personalized motion patterns\ndirectly from unconstrained videos. This enables personalized motion transfer.\nTo support this task, we introduce PersonaVid, the first video-based\npersonalized motion dataset. It contains 20 motion content categories and 120\nmotion style categories. We further propose a Physics-aware Motion Style\nRegularization mechanism to enforce physical plausibility in the generated\nmotions. Extensive experiments show that PersonaAnimator outperforms\nstate-of-the-art motion transfer methods and sets a new benchmark for the\nVideo-to-Video Motion Personalization task.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19895v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19895v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.272,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.291,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19896",
      "title": "NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More\n  Accurate and Interpretable CNNs",
      "authors": [
        "Davorin Miličević",
        "Ratko Grbić"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often\nrely on purely global, gradient-based optimisation, which can lead to\noverfitting, redundant filters, and reduced interpretability. To address these\nlimitations, we propose NM-Hebb, a two-phase training framework that integrates\nneuro-inspired local plasticity with distance-aware supervision. Phase 1\nextends standard supervised training by jointly optimising a cross-entropy\nobjective with two biologically inspired mechanisms: (i) a Hebbian regulariser\nthat aligns the spatial mean of activations with the mean of the corresponding\nconvolutional filter weights, encouraging structured, reusable primitives; and\n(ii) a learnable neuromodulator that gates an elastic-weight-style\nconsolidation loss, preserving beneficial parameters without freezing the\nnetwork. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss,\nexplicitly compressing intra-class distances and enlarging inter-class margins\nin the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet\nacross five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2,\nDenseNet-121), NM-Hebb achieves consistent gains over baseline and other\nmethods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp\n(CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual\nInformation (NMI) increased by up to +0.15. Qualitative visualisations and\nfilter-level analyses further confirm that NM-Hebb produces more structured and\nselective features, yielding tighter and more interpretable class clusters.\nOverall, coupling local Hebbian plasticity with metric-based fine-tuning yields\nCNNs that are not only more accurate but also more interpretable, offering\npractical benefits for resource-constrained and safety-critical AI deployments.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19896v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19896v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.375,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes a framework for training CNNs using Hebbian plasticity and metric learning, focusing on supervised methods with cross-entropy loss and biologically inspired mechanisms. It does not involve reinforcement learning, human feedback, reward models, or any process for aligning AI with human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19897",
      "title": "The Information Dynamics of Generative Diffusion",
      "authors": [
        "Luca Ambrogioni"
      ],
      "categories": [
        "stat.ML (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Generative diffusion models have emerged as a powerful class of models in\nmachine learning, yet a unified theoretical understanding of their operation is\nstill developing. This paper provides an integrated perspective on generative\ndiffusion by connecting their dynamic, information-theoretic, and thermodynamic\nproperties under a unified mathematical framework. We demonstrate that the rate\nof conditional entropy production during generation (i.e. the generative\nbandwidth) is directly governed by the expected divergence of the score\nfunction's vector field. This divergence, in turn, is linked to the branching\nof trajectories and generative bifurcations, which we characterize as\nsymmetry-breaking phase transitions in the energy landscape. This synthesis\noffers a powerful insight: the process of generation is fundamentally driven by\nthe controlled, noise-induced breaking of (approximate) symmetries, where peaks\nin information transfer correspond to critical transitions between possible\noutcomes. The score function acts as a dynamic non-linear filter that regulates\nthe bandwidth of the noise by suppressing fluctuations that are incompatible\nwith the data.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19897v3",
      "pdf_url": "http://arxiv.org/pdf/2508.19897v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.602,
      "distributed_training_score": 0.306,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on the theoretical foundations of generative diffusion models, emphasizing their dynamics, information theory, and thermodynamics for data generation. It discusses concepts like entropy production and trajectory branching but does not address adapting diffusion models for multi-step logical reasoning, solving complex logical tasks, or treating a chain-of-thought as an entity for iterative correction. As there is no component related to logical reasoning, the paper does not align with this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19903",
      "title": "Logical Reasoning with Outcome Reward Models for Test-Time Scaling",
      "authors": [
        "Ramya Keerthy Thatikonda",
        "Wray Buntine",
        "Ehsan Shareghi"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Logical reasoning is a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs), as it reflects their ability to derive valid\nconclusions from given premises. While the combination of test-time scaling\nwith dedicated outcome or process reward models has opened up new avenues to\nenhance LLMs performance in complex reasoning tasks, this space is\nunder-explored in deductive logical reasoning. We present a set of Outcome\nReward Models (ORMs) for deductive reasoning. To train the ORMs we mainly\ngenerate data using Chain-of-Thought (CoT) with single and multiple samples.\nAdditionally, we propose a novel tactic to further expand the type of errors\ncovered in the training dataset of the ORM. In particular, we propose an echo\ngeneration technique that leverages LLMs' tendency to reflect incorrect\nassumptions made in prompts to extract additional training data, covering\npreviously unexplored error types. While a standard CoT chain may contain\nerrors likely to be made by the reasoner, the echo strategy deliberately steers\nthe model toward incorrect reasoning. We show that ORMs trained on CoT and\necho-augmented data demonstrate improved performance on the FOLIO, JustLogic,\nand ProverQA datasets across four different LLMs.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19903v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19903v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.45,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.541,
      "distributed_training_score": 0.359,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on training Outcome Reward Models (ORMs) using generated data from LLMs via Chain-of-Thought and echo techniques, without involving human feedback or reinforcement learning to fine-tune the main model. RLHF specifically requires human-ranked data and a reinforcement learning loop, which is absent here.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs Chain-of-Thought for reasoning and ORMs for verification, but it does not adapt diffusion models or their iterative refinement processes for logical tasks. There is no mention of treating reasoning paths as entities for multi-step correction via diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19905",
      "title": "Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations,\n  and Opportunities",
      "authors": [
        "Imad Ali Shah",
        "Jiarong Li",
        "Roshan George",
        "Tim Brophy",
        "Enda Ward",
        "Martin Glavin",
        "Edward Jones",
        "Brian Deegan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.ET (Emerging Technologies)"
      ],
      "abstract": "Hyperspectral imaging (HSI) offers a transformative sensing modality for\nAdvanced Driver Assistance Systems (ADAS) and autonomous driving (AD)\napplications, enabling material-level scene understanding through fine spectral\nresolution beyond the capabilities of traditional RGB imaging. This paper\npresents the first comprehensive review of HSI for automotive applications,\nexamining the strengths, limitations, and suitability of current HSI\ntechnologies in the context of ADAS/AD. In addition to this qualitative review,\nwe analyze 216 commercially available HSI and multispectral imaging cameras,\nbenchmarking them against key automotive criteria: frame rate, spatial\nresolution, spectral dimensionality, and compliance with AEC-Q100 temperature\nstandards. Our analysis reveals a significant gap between HSI's demonstrated\nresearch potential and its commercial readiness. Only four cameras meet the\ndefined performance thresholds, and none comply with AEC-Q100 requirements. In\naddition, the paper reviews recent HSI datasets and applications, including\nsemantic segmentation for road surface classification, pedestrian separability,\nand adverse weather perception. Our review shows that current HSI datasets are\nlimited in terms of scale, spectral consistency, the number of spectral\nchannels, and environmental diversity, posing challenges for the development of\nperception algorithms and the adequate validation of HSI's true potential in\nADAS/AD applications. This review paper establishes the current state of HSI in\nautomotive contexts as of 2025 and outlines key research directions toward\npractical integration of spectral imaging in ADAS and autonomous systems.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19905v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19905v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.297,
      "diffusion_reasoning_score": 0.288,
      "distributed_training_score": 0.323,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19906",
      "title": "Streamlining the Development of Active Learning Methods in Real-World\n  Object Detection",
      "authors": [
        "Moussa Kassem Sbeyti",
        "Nadja Klein",
        "Michelle Karg",
        "Christian Wirth",
        "Sahin Albayrak"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Active learning (AL) for real-world object detection faces computational and\nreliability challenges that limit practical deployment. Developing new AL\nmethods requires training multiple detectors across iterations to compare\nagainst existing approaches. This creates high costs for autonomous driving\ndatasets where the training of one detector requires up to 282 GPU hours.\nAdditionally, AL method rankings vary substantially across validation sets,\ncompromising reliability in safety-critical transportation systems. We\nintroduce object-based set similarity ($\\mathrm{OSS}$), a metric that addresses\nthese challenges. $\\mathrm{OSS}$ (1) quantifies AL method effectiveness without\nrequiring detector training by measuring similarity between training sets and\ntarget domains using object-level features. This enables the elimination of\nineffective AL methods before training. Furthermore, $\\mathrm{OSS}$ (2) enables\nthe selection of representative validation sets for robust evaluation. We\nvalidate our similarity-based approach on three autonomous driving datasets\n(KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with\ntwo detector architectures (EfficientDet, YOLOv3). This work is the first to\nunify AL training and evaluation strategies in object detection based on object\nsimilarity. $\\mathrm{OSS}$ is detector-agnostic, requires only labeled object\ncrops, and integrates with existing AL pipelines. This provides a practical\nframework for deploying AL in real-world applications where computational\nefficiency and evaluation reliability are critical. Code is available at\nhttps://mos-ks.github.io/publications/.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19906v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19906v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.41,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.351,
      "datasets_score": 0.403,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on active learning for object detection, specifically introducing the OSS metric to evaluate and rank AL methods without training detectors. It does not involve programmatically generating labels from noisy or imprecise sources, nor does it rely on weak supervision techniques; instead, it assumes accurate labeling for selected samples. Thus, it does not align with the topic of weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper analyzes and evaluates existing datasets (e.g., KITTI, BDD100K, CODA) for active learning in object detection, including assessing domain variations, selecting representative validation subsets, and addressing domain shifts. While it uses these datasets to validate the OSS metric, it is not primarily focused on creating, curating, or benchmarking new datasets, making it moderately relevant rather than highly so.",
      "llm_score_status": "completed",
      "summary": "The paper introduces the object-based set similarity (OSS) metric to address computational inefficiencies and reliability issues in active learning (AL) for real-world object detection, particularly in autonomous driving scenarios. By quantifying similarity between training sets and target domains using object-level features like aspect ratios, color histograms, and texture patterns, OSS enables ranking of AL methods without training detectors, facilitates selection of representative validation sets for robust evaluation, and is validated on datasets such as KITTI, BDD100K, and CODA using uncertainty-based AL methods, demonstrating improved efficiency and generalizability.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new metric, OSS, that advances the state-of-the-art by enabling evaluation of AL methods without training, thus addressing a significant computational challenge in object detection. This novel technique unifies AL training and evaluation based on object similarity, which is a fresh approach not previously explored in this context.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of active learning for computer vision, as it provides practical tools for reducing computational costs in real-world applications like autonomous driving. However, its influence may be limited to specific domains and not broadly transformative across all AI research.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with practical implications for AL in object detection, making it valuable for researchers in computer vision and autonomous systems to understand and potentially adopt. While not essential for all, it represents a strong advancement that those in the field should be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6ceea26543292ad8f7e0aa605f66c54ee4599a08",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 4,
      "average_h_index": 1.4,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Moussa Kassem Sbeyti",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2220095025"
        },
        {
          "name": "Nadja Klein",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2353851672"
        },
        {
          "name": "Michelle Karg",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2298756111"
        },
        {
          "name": "Christian Wirth",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2243459460"
        },
        {
          "name": "S. Albayrak",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2058692749"
        }
      ]
    },
    {
      "id": "2508.19909",
      "title": "Integrating SAM Supervision for 3D Weakly Supervised Point Cloud\n  Segmentation",
      "authors": [
        "Lechun You",
        "Zhonghua Wu",
        "Weide Liu",
        "Xulei Yang",
        "Jun Cheng",
        "Wei Zhou",
        "Bharadwaj Veeravalli",
        "Guosheng Lin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Current methods for 3D semantic segmentation propose training models with\nlimited annotations to address the difficulty of annotating large, irregular,\nand unordered 3D point cloud data. They usually focus on the 3D domain only,\nwithout leveraging the complementary nature of 2D and 3D data. Besides, some\nmethods extend original labels or generate pseudo labels to guide the training,\nbut they often fail to fully use these labels or address the noise within them.\nMeanwhile, the emergence of comprehensive and adaptable foundation models has\noffered effective solutions for segmenting 2D data. Leveraging this\nadvancement, we present a novel approach that maximizes the utility of sparsely\navailable 3D annotations by incorporating segmentation masks generated by 2D\nfoundation models. We further propagate the 2D segmentation masks into the 3D\nspace by establishing geometric correspondences between 3D scenes and 2D views.\nWe extend the highly sparse annotations to encompass the areas delineated by 3D\nmasks, thereby substantially augmenting the pool of available labels.\nFurthermore, we apply confidence- and uncertainty-based consistency\nregularization on augmentations of the 3D point cloud and select the reliable\npseudo labels, which are further spread on the 3D masks to generate more\nlabels. This innovative strategy bridges the gap between limited 3D annotations\nand the powerful capabilities of 2D foundation models, ultimately improving the\nperformance of 3D weakly supervised segmentation.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19909v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19909v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.492,
      "diffusion_reasoning_score": 0.331,
      "distributed_training_score": 0.329,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves training a 3D point cloud segmentation model using sparsely annotated data and programmatically generated pseudo labels from 2D foundation models, such as by back-projecting 2D masks and applying consistency regularization to handle noise. This directly aligns with weak supervision, as it relies on high-level, noisy, or imprecise sources (e.g., 2D segmentation outputs) to create and extend training labels, rather than fully hand-labeled data.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces a novel approach for 3D weakly supervised point cloud segmentation that leverages 2D foundation models, such as Semantic-SAM, to generate segmentation masks from 2D images and project them into 3D space using geometric correspondences. The methodology involves fusing masks from multiple views, extending sparse 3D annotations onto these masks, applying confidence- and uncertainty-based consistency regularization to select reliable pseudo labels, and using noise-robust loss to enhance training; key findings demonstrate significant performance improvements, achieving state-of-the-art results with minimal annotations by bridging the gap between 2D and 3D data.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing 2D foundation models with 3D weakly supervised segmentation techniques, offering a new way to propagate labels across modalities, though it builds on established concepts rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of 3D vision and weakly supervised learning, as it addresses practical challenges in annotation scarcity and could influence multi-modal approaches in applications like autonomous driving.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a valuable and innovative contribution to 3D segmentation by integrating 2D models, making it essential for researchers in computer vision focusing on weakly supervised methods to stay informed.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8de67eb011ec3f068379000ad6c1acd8e6baf42e",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 43,
      "average_h_index": 7.25,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Lechun You",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377560978"
        },
        {
          "name": "Zhonghua Wu",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2109718608"
        },
        {
          "name": "Weide Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2352208754"
        },
        {
          "name": "Xulei Yang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2288169797"
        },
        {
          "name": "Jun Cheng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2325207840"
        },
        {
          "name": "Wei Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372379416"
        },
        {
          "name": "B. Veeravalli",
          "h_index": 43,
          "profile_url": "https://www.semanticscholar.org/author/2097860"
        },
        {
          "name": "Guosheng Lin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2352605117"
        }
      ]
    },
    {
      "id": "2508.19914",
      "title": "The Next Layer: Augmenting Foundation Models with Structure-Preserving\n  and Attention-Guided Learning for Local Patches to Global Context Awareness\n  in Computational Pathology",
      "authors": [
        "Muhammad Waqas",
        "Rukhmini Bandyopadhyay",
        "Eman Showkatian",
        "Amgad Muneer",
        "Anas Zafar",
        "Frank Rojas Alvarez",
        "Maricel Corredor Marin",
        "Wentao Li",
        "David Jaffray",
        "Cara Haymaker",
        "John Heymach",
        "Natalie I Vokes",
        "Luisa Maren Solis Soto",
        "Jianjun Zhang",
        "Jia Wu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Foundation models have recently emerged as powerful feature extractors in\ncomputational pathology, yet they typically omit mechanisms for leveraging the\nglobal spatial structure of tissues and the local contextual relationships\namong diagnostically relevant regions - key elements for understanding the\ntumor microenvironment. Multiple instance learning (MIL) remains an essential\nnext step following foundation model, designing a framework to aggregate\npatch-level features into slide-level predictions. We present EAGLE-Net, a\nstructure-preserving, attention-guided MIL architecture designed to augment\nprediction and interpretability. EAGLE-Net integrates multi-scale absolute\nspatial encoding to capture global tissue architecture, a top-K\nneighborhood-aware loss to focus attention on local microenvironments, and\nbackground suppression loss to minimize false positives. We benchmarked\nEAGLE-Net on large pan-cancer datasets, including three cancer types for\nclassification (10,260 slides) and seven cancer types for survival prediction\n(4,172 slides), using three distinct histology foundation backbones (REMEDIES,\nUni-V1, Uni2-h). Across tasks, EAGLE-Net achieved up to 3% higher\nclassification accuracy and the top concordance indices in 6 of 7 cancer types,\nproducing smooth, biologically coherent attention maps that aligned with expert\nannotations and highlighted invasive fronts, necrosis, and immune infiltration.\nThese results position EAGLE-Net as a generalizable, interpretable framework\nthat complements foundation models, enabling improved biomarker discovery,\nprognostic modeling, and clinical decision support",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19914v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19914v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.337,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19927",
      "title": "WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image\n  Super-Resolution",
      "authors": [
        "Fayaz Ali",
        "Muhammad Zawish",
        "Steven Davy",
        "Radu Timofte"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Transformers have demonstrated promising performance in computer vision\ntasks, including image super-resolution (SR). The quadratic computational\ncomplexity of window self-attention mechanisms in many transformer-based SR\nmethods forces the use of small, fixed windows, limiting the receptive field.\nIn this paper, we propose a new approach by embedding the wavelet transform\nwithin a hierarchical transformer framework, called (WaveHiT-SR). First, using\nadaptive hierarchical windows instead of static small windows allows to capture\nfeatures across different levels and greatly improve the ability to model\nlong-range dependencies. Secondly, the proposed model utilizes wavelet\ntransforms to decompose images into multiple frequency subbands, allowing the\nnetwork to focus on both global and local features while preserving structural\ndetails. By progressively reconstructing high-resolution images through\nhierarchical processing, the network reduces computational complexity without\nsacrificing performance. The multi-level decomposition strategy enables the\nnetwork to capture fine-grained information in lowfrequency components while\nenhancing high-frequency textures. Through extensive experimentation, we\nconfirm the effectiveness and efficiency of our WaveHiT-SR. Our refined\nversions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR\nresults, achieving higher efficiency with fewer parameters, lower FLOPs, and\nfaster speeds.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19927v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19927v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.347,
      "datasets_score": 0.273,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19932",
      "title": "CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital\n  Payments",
      "authors": [
        "Nitish Jaipuria",
        "Lorenzo Gatto",
        "Zijun Kan",
        "Shankey Poddar",
        "Bill Cheung",
        "Diksha Bansal",
        "Ramanan Balakrishnan",
        "Aviral Suri",
        "Jose Estevez"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The proliferation of digital payment platforms has transformed commerce,\noffering unmatched convenience and accessibility globally. However, this growth\nhas also attracted malicious actors, leading to a corresponding increase in\nsophisticated social engineering scams. These scams are often initiated and\norchestrated on multiple surfaces outside the payment platform, making user and\ntransaction-based signals insufficient for a complete understanding of the\nscam's methodology and underlying patterns, without which it is very difficult\nto prevent it in a timely manner. This paper presents CASE (Conversational\nAgent for Scam Elucidation), a novel Agentic AI framework that addresses this\nproblem by collecting and managing user scam feedback in a safe and scalable\nmanner. A conversational agent is uniquely designed to proactively interview\npotential victims to elicit intelligence in the form of a detailed\nconversation. The conversation transcripts are then consumed by another AI\nsystem that extracts information and converts it into structured data for\ndownstream usage in automated and manual enforcement mechanisms. Using Google's\nGemini family of LLMs, we implemented this framework on Google Pay (GPay)\nIndia. By augmenting our existing features with this new intelligence, we have\nobserved a 21% uplift in the volume of scam enforcements. The architecture and\nits robust evaluation framework are highly generalizable, offering a blueprint\nfor building similar AI-driven systems to collect and manage scam intelligence\nin other sensitive domains.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19932v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19932v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.306,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19944",
      "title": "KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA\n  Attuned to Diverse Visual Contexts",
      "authors": [
        "Taebaek Hwang",
        "Minseo Kim",
        "Gisang Lee",
        "Seonuk Kim",
        "Hyunjun Eun"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Understanding and reasoning over text within visual contexts poses a\nsignificant challenge for Vision-Language Models (VLMs), given the complexity\nand diversity of real-world scenarios. To address this challenge, text-rich\nVisual Question Answering (VQA) datasets and benchmarks have emerged for\nhigh-resource languages like English. However, a critical gap persists for\nlow-resource languages such as Korean, where the lack of comprehensive\nbenchmarks hinders robust model evaluation and comparison. To bridge this gap,\nwe introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich\nVQA Attuned to diverse visual contexts. KRETA facilitates an in-depth\nevaluation of both visual text understanding and reasoning capabilities, while\nalso supporting a multifaceted assessment across 15 domains and 26 image types.\nAdditionally, we introduce a semi-automated VQA generation pipeline\nspecifically optimized for text-rich settings, leveraging refined stepwise\nimage decomposition and a rigorous seven-metric evaluation protocol to ensure\ndata quality. While KRETA is tailored for Korean, we hope our adaptable and\nextensible pipeline will facilitate the development of similar benchmarks in\nother languages, thereby accelerating multilingual VLM research. The code and\ndataset for KRETA are available at https://github.com/tabtoyou/KRETA.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19944v2",
      "pdf_url": "http://arxiv.org/pdf/2508.19944v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.289,
      "datasets_score": 0.431,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a benchmark dataset for Korean text-rich VQA and a semi-automated generation pipeline, but it does not mention or utilize diffusion models for reasoning. There is no discussion of iterative refinement processes or treating Chain-of-Thought as a single entity for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation, introduction, and evaluation of the KRETA dataset for Korean text-rich VQA, including dataset curation methodologies like a semi-automated pipeline, categorization into domains and image types, and benchmarking of VLMs. This directly aligns with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "KRETA introduces a new benchmark for evaluating Vision-Language Models (VLMs) on Korean text-rich Visual Question Answering (VQA), addressing the gap in resources for low-resource languages by providing 2,577 samples across 15 domains and 26 image types with a dual-level reasoning framework—System 1 for basic text recognition and System 2 for advanced reasoning. The methodology involves a semi-automated pipeline for QA generation, including stepwise image decomposition and a seven-metric evaluation protocol, while key findings from empirical analysis show that VLMs perform well on basic tasks but struggle with multi-step reasoning and domain-specific knowledge, underscoring the need for targeted improvements in multilingual VLM research.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark and pipeline specifically for Korean text-rich VQA, addressing a significant gap in low-resource language evaluation and advancing the state-of-the-art in multilingual VLM research.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like computer vision and multilingual NLP, particularly for low-resource languages, but its influence may remain limited to specific applications rather than widespread commercial or general research areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to multilingual VLM evaluation, making it essential for researchers in computer vision and language processing focused on low-resource languages, though not universally critical for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/197524ae6ea9a438e23574513284acb120870dcc",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Taebaek Hwang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377561327"
        },
        {
          "name": "Minseo Kim",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377653831"
        },
        {
          "name": "Gisang Lee",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377658750"
        },
        {
          "name": "Seonuk Kim",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377643073"
        },
        {
          "name": "Hyunjun Eun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377560069"
        }
      ]
    },
    {
      "id": "2508.19946",
      "title": "Reimagining Image Segmentation using Active Contour: From Chan Vese\n  Algorithm into a Proposal Novel Functional Loss Framework",
      "authors": [
        "Gianluca Guzzetta"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we present a comprehensive study and analysis of the Chan-Vese\nalgorithm for image segmentation. We employ a discretized scheme derived from\nthe empirical study of the Chan-Vese model's functional energy and its partial\ndifferential equation based on its level set function. We provide a proof of\nthe results and an implementation using MATLAB. Leveraging modern computer\nvision methodologies, we propose a functional segmentation loss based on active\ncontours, utilizing pytorch.nn.ModuleLoss and a level set based on the\nChan-Vese algorithm. We compare our results with common computer vision\nsegmentation datasets and evaluate the performance of classical loss functions\nagainst our proposed method. All code and materials used are available at\nhttps://github.com/gguzzy/chan_vese_functional_loss.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19946v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19946v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.231,
      "weak_supervision_score": 0.306,
      "diffusion_reasoning_score": 0.304,
      "distributed_training_score": 0.253,
      "datasets_score": 0.277,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19963",
      "title": "Flocking Behavior: An Innovative Inspiration for the Optimization of\n  Production Plants",
      "authors": [
        "M. Umlauft",
        "M. Schranz"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Optimizing modern production plants using the job-shop principle is a known\nhard problem. For very large plants, like semiconductor fabs, the problem\nbecomes unsolvable on a plant-wide scale in a reasonable amount of time using\nclassical linear optimization. An alternative approach is the use of swarm\nintelligence algorithms. These have been applied to the job-shop problem\nbefore, but often in a centrally calculated way where they are applied to the\nsolution space, but they can be implemented in a bottom-up fashion to avoid\nglobal result computation as well. One of the problems in semiconductor\nproduction is that the production process requires a lot of switching between\nmachines that process lots one after the other and machines that process\nbatches of lots at once, often with long processing times. In this paper, we\naddress this switching problem with the ``boids'' flocking algorithm that was\noriginally used in robotics and movie industry. The flocking behavior is a\nbio-inspired algorithm that uses only local information and interaction based\non simple heuristics. We show that this algorithm addresses these valid\nconsiderations in production plant optimization, as it reacts to the switching\nof machine kinds similar to how a swarm of flocking animals would react to\nobstacles in its course.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19963v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19963v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.256,
      "diffusion_reasoning_score": 0.298,
      "distributed_training_score": 0.282,
      "datasets_score": 0.21,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19966",
      "title": "Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity\n  Evaluation",
      "authors": [
        "Slimane Bellaouar",
        "Attia Nehar",
        "Soumia Souffi",
        "Mounia Bouameur"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite its significance, Arabic, a linguistically rich and morphologically\ncomplex language, faces the challenge of being under-resourced. The scarcity of\nlarge annotated datasets hampers the development of accurate tools for\nsubjectivity analysis in Arabic. Recent advances in deep learning and\nTransformers have proven highly effective for text classification in English\nand French. This paper proposes a new approach for subjectivity assessment in\nArabic textual data. To address the dearth of specialized annotated datasets,\nwe developed a comprehensive dataset, AraDhati+, by leveraging existing Arabic\ndatasets and collections (ASTD, LABR, HARD, and SANAD). Subsequently, we\nfine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, and\nArabianGPT) on AraDhati+ for effective subjectivity classification.\nFurthermore, we experimented with an ensemble decision approach to harness the\nstrengths of individual models. Our approach achieves a remarkable accuracy of\n97.79\\,\\% for Arabic subjectivity classification. Results demonstrate the\neffectiveness of the proposed approach in addressing the challenges posed by\nlimited resources in Arabic language processing.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19966v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19966v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.333,
      "datasets_score": 0.463,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes the creation of a new dataset, AraDhati+, by combining existing Arabic datasets (ASTD, LABR, HARD, and SANAD) for subjectivity analysis in machine learning. This directly aligns with research on creating and curating datasets for AI applications, as it addresses the scarcity of annotated data and demonstrates its use in model fine-tuning.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges of subjectivity analysis in Arabic, an under-resourced language, by developing a comprehensive dataset called AraDhati+ through the integration of existing datasets such as ASTD, LABR, HARD, and SANAD, and then fine-tuning state-of-the-art language models including XLM-RoBERTa, AraBERT, and ArabianGPT, along with an ensemble approach, to achieve a high accuracy of 97.79% in classifying Arabic text as subjective or objective. Building on their previous work with the Dhati tool, the authors demonstrate the effectiveness of this method in enhancing Arabic natural language processing for sentiment analysis.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing datasets and fine-tuning established language models for Arabic subjectivity, but it primarily refines previous methods rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in Arabic natural language processing and sentiment analysis, potentially leading to better tools for under-resourced languages, though its applicability may remain confined to specific subfields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a high-quality contribution to Arabic NLP with strong experimental results, making it essential for researchers focused on low-resource languages and sentiment analysis.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2f673bef88db8101f7f34d5923c4097c0e13a013",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 5,
      "average_h_index": 2.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Slimane Bellaouar",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/3328936"
        },
        {
          "name": "A. Nehar",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/3287866"
        },
        {
          "name": "Soumia Souffi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2267751859"
        },
        {
          "name": "Mounia Bouameur",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2267751847"
        }
      ]
    },
    {
      "id": "2508.19967",
      "title": "Assessing the Geolocation Capabilities, Limitations and Societal Risks\n  of Generative Vision-Language Models",
      "authors": [
        "Oliver Grainge",
        "Sania Waheed",
        "Jack Stilgoe",
        "Michael Milford",
        "Shoaib Ehsan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Geo-localization is the task of identifying the location of an image using\nvisual cues alone. It has beneficial applications, such as improving disaster\nresponse, enhancing navigation, and geography education. Recently,\nVision-Language Models (VLMs) are increasingly demonstrating capabilities as\naccurate image geo-locators. This brings significant privacy risks, including\nthose related to stalking and surveillance, considering the widespread uses of\nAI models and sharing of photos on social media. The precision of these models\nis likely to improve in the future. Despite these risks, there is little work\non systematically evaluating the geolocation precision of Generative VLMs,\ntheir limits and potential for unintended inferences. To bridge this gap, we\nconduct a comprehensive assessment of the geolocation capabilities of 25\nstate-of-the-art VLMs on four benchmark image datasets captured in diverse\nenvironments. Our results offer insight into the internal reasoning of VLMs and\nhighlight their strengths, limitations, and potential societal risks. Our\nfindings indicate that current VLMs perform poorly on generic street-level\nimages yet achieve notably high accuracy (61\\%) on images resembling social\nmedia content, raising significant and urgent privacy concerns.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19967v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19967v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.351,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluating the geolocation capabilities of various Vision-Language Models (VLMs) for tasks like identifying image locations, without any mention of diffusion-based models or their adaptation for iterative refinement in logical reasoning. It discusses general generative VLMs but does not involve concepts such as treating a 'Chain-of-Thought' as a single entity for multi-step correction, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19972",
      "title": "GLSim: Detecting Object Hallucinations in LVLMs via Global-Local\n  Similarity",
      "authors": [
        "Seongheon Park",
        "Yixuan Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Object hallucination in large vision-language models presents a significant\nchallenge to their safe deployment in real-world applications. Recent works\nhave proposed object-level hallucination scores to estimate the likelihood of\nobject hallucination; however, these methods typically adopt either a global or\nlocal perspective in isolation, which may limit detection reliability. In this\npaper, we introduce GLSim, a novel training-free object hallucination detection\nframework that leverages complementary global and local embedding similarity\nsignals between image and text modalities, enabling more accurate and reliable\nhallucination detection in diverse scenarios. We comprehensively benchmark\nexisting object hallucination detection methods and demonstrate that GLSim\nachieves superior detection performance, outperforming competitive baselines by\na significant margin.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19972v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19972v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.34,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on detecting object hallucinations in LVLMs using embedding similarities, without any mention of training models with human-ranked data, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a hallucination detection framework based on global and local similarity scores in embeddings, and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19982",
      "title": "Diffusion Language Models Know the Answer Before Decoding",
      "authors": [
        "Pengxiang Li",
        "Yefan Zhou",
        "Dilxat Muhtar",
        "Lu Yin",
        "Shilin Yan",
        "Li Shen",
        "Yi Liang",
        "Soroush Vosoughi",
        "Shiwei Liu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Diffusion language models (DLMs) have recently emerged as an alternative to\nautoregressive approaches, offering parallel sequence generation and flexible\ntoken orders. However, their inference remains slower than that of\nautoregressive models, primarily due to the cost of bidirectional attention and\nthe large number of refinement steps required for high quality outputs. In this\nwork, we highlight and leverage an overlooked property of DLMs early answer\nconvergence: in many cases, the correct answer can be internally identified by\nhalf steps before the final decoding step, both under semi-autoregressive and\nrandom remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%\nof instances, respectively, can be decoded correctly using only half of the\nrefinement steps. Building on this observation, we introduce Prophet, a\ntraining-free fast decoding paradigm that enables early commit decoding.\nSpecifically, Prophet dynamically decides whether to continue refinement or to\ngo \"all-in\" (i.e., decode all remaining tokens in one step), using the\nconfidence gap between the top-2 prediction candidates as the criterion. It\nintegrates seamlessly into existing DLM implementations, incurs negligible\noverhead, and requires no additional training. Empirical evaluations of\nLLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the\nnumber of decoding steps by up to 3.4x while preserving high generation\nquality. These results recast DLM decoding as a problem of when to stop\nsampling, and demonstrate that early decode convergence provides a simple yet\npowerful mechanism for accelerating DLM inference, complementary to existing\nspeedup techniques. Our code is publicly available at\nhttps://github.com/pixeli99/Prophet.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19982v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19982v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.573,
      "distributed_training_score": 0.418,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Moderately Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Diffusion Language Models (DLMs) and their iterative refinement process for sequence generation, which aligns with the core idea of diffusion models. It discusses early answer convergence in tasks like GSM8K and MMLU, which involve logical reasoning, and proposes accelerating this process. However, the main contribution (Prophet for fast decoding) emphasizes inference efficiency rather than adapting diffusion for multi-step logical reasoning or holistic correction of a Chain-of-Thought, making it only moderately relevant.",
      "distributed_training_justification": "The paper does not address distributed training, parallel computing, or multi-node machine learning. It focuses solely on accelerating inference in DLMs through early decoding strategies, with no mention of partitioning data, architecture, or computation across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper investigates an overlooked property of Diffusion Language Models (DLMs), where correct answers often emerge early in the decoding process, and introduces Prophet, a training-free method that leverages this by monitoring the confidence gap between top predictions to decide when to early commit decoding, thereby accelerating inference. The core objectives are to demonstrate early answer convergence in DLMs and propose Prophet to reduce decoding steps by up to 3.4x while maintaining high generation quality, as evidenced by empirical evaluations on benchmarks like GSM8K and MMLU, where up to 99% of instances can be correctly decoded with fewer steps.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing the Prophet method to exploit early answer convergence in DLMs, which is a clever combination of existing ideas rather than a completely new problem or architecture. While it advances DLM inference techniques, it builds on known concepts like confidence-based decisions without introducing a fundamentally new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of language model optimization, as it offers a practical speedup for DLMs that could enhance efficiency in AI applications. However, its influence may be limited to specific DLM implementations and not broadly transformative across all AI research.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution for researchers in AI and language models due to its practical method for accelerating DLMs without quality loss. It is essential for those working on model efficiency but not groundbreaking enough to be a must-read for the general audience.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b7b95b60ed78916950ae670cbf90bb5081112c38",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 29,
      "average_h_index": 5.666666666666667,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Pengxiang Li",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2303482643"
        },
        {
          "name": "Yefan Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377646021"
        },
        {
          "name": "Dilxat Muhtar",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2166353997"
        },
        {
          "name": "Lu Yin",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2254142682"
        },
        {
          "name": "Shilin Yan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378191846"
        },
        {
          "name": "Li Shen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2346065531"
        },
        {
          "name": "Yi Liang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377901253"
        },
        {
          "name": "Soroush Vosoughi",
          "h_index": 29,
          "profile_url": "https://www.semanticscholar.org/author/1918441"
        },
        {
          "name": "Shiwei Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2344983897"
        }
      ]
    },
    {
      "id": "2508.19993",
      "title": "MathBuddy: A Multimodal System for Affective Math Tutoring",
      "authors": [
        "Debanjana Kar",
        "Leopold Böss",
        "Dacia Braca",
        "Sebastian Maximilian Dennerlein",
        "Nina Christine Hubig",
        "Philipp Wintersberger",
        "Yufang Hou"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "The rapid adoption of LLM-based conversational systems is already\ntransforming the landscape of educational technology. However, the current\nstate-of-the-art learning models do not take into account the student's\naffective states. Multiple studies in educational psychology support the claim\nthat positive or negative emotional states can impact a student's learning\ncapabilities. To bridge this gap, we present MathBuddy, an emotionally aware\nLLM-powered Math Tutor, which dynamically models the student's emotions and\nmaps them to relevant pedagogical strategies, making the tutor-student\nconversation a more empathetic one. The student's emotions are captured from\nthe conversational text as well as from their facial expressions. The student's\nemotions are aggregated from both modalities to confidently prompt our LLM\nTutor for an emotionally-aware response. We have effectively evaluated our\nmodel using automatic evaluation metrics across eight pedagogical dimensions\nand user studies. We report a massive 23 point performance gain using the win\nrate and a 3 point gain at an overall level using DAMR scores which strongly\nsupports our hypothesis of improving LLM-based tutor's pedagogical abilities by\nmodeling students' emotions.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19993v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19993v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.289,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.19999",
      "title": "Linear-Time Demonstration Selection for In-Context Learning via Gradient\n  Estimation",
      "authors": [
        "Ziniu Zhang",
        "Zhenshuo Zhang",
        "Dongyue Li",
        "Lu Wang",
        "Jennifer Dy",
        "Hongyang R. Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "This paper introduces an algorithm to select demonstration examples for\nin-context learning of a query set. Given a set of $n$ examples, how can we\nquickly select $k$ out of $n$ to best serve as the conditioning for downstream\ninference? This problem has broad applications in prompt tuning and\nchain-of-thought reasoning. Since model weights remain fixed during in-context\nlearning, previous work has sought to design methods based on the similarity of\ntoken embeddings. This work proposes a new approach based on gradients of the\noutput taken in the input embedding space. Our approach estimates model outputs\nthrough a first-order approximation using the gradients. Then, we apply this\nestimation to multiple randomly sampled subsets. Finally, we aggregate the\nsampled subset outcomes to form an influence score for each demonstration, and\nselect $k$ most relevant examples. This procedure only requires pre-computing\nmodel outputs and gradients once, resulting in a linear-time algorithm relative\nto model and training set sizes. Extensive experiments across various models\nand datasets validate the efficiency of our approach. We show that the gradient\nestimation procedure yields approximations of full inference with less than\n$\\mathbf{1}\\%$ error across six datasets. This allows us to scale up subset\nselection that would otherwise run full inference by up to\n$\\mathbf{37.7}\\times$ on models with up to $34$ billion parameters, and\noutperform existing selection methods based on input embeddings by\n$\\mathbf{11}\\%$ on average.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.19999v1",
      "pdf_url": "http://arxiv.org/pdf/2508.19999v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.381,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a gradient-based algorithm for efficient demonstration selection in in-context learning, focusing on prompt tuning and subset selection using gradients and Taylor expansions. It does not involve diffusion models, iterative refinement processes, or adapting diffusion techniques for multi-step logical reasoning or chain-of-thought as a holistic entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20013",
      "title": "Cross-Platform E-Commerce Product Categorization and Recategorization: A\n  Multimodal Hierarchical Classification Approach",
      "authors": [
        "Lotte Gross",
        "Rebecca Walter",
        "Nicole Zoppi",
        "Adrien Justus",
        "Alessandro Gambetti",
        "Qiwei Han",
        "Maximilian Kaiser"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "This study addresses critical industrial challenges in e-commerce product\ncategorization, namely platform heterogeneity and the structural limitations of\nexisting taxonomies, by developing and deploying a multimodal hierarchical\nclassification framework. Using a dataset of 271,700 products from 40\ninternational fashion e-commerce platforms, we integrate textual features\n(RoBERTa), visual features (ViT), and joint vision--language representations\n(CLIP). We investigate fusion strategies, including early, late, and\nattention-based fusion within a hierarchical architecture enhanced by dynamic\nmasking to ensure taxonomic consistency. Results show that CLIP embeddings\ncombined via an MLP-based late-fusion strategy achieve the highest hierarchical\nF1 (98.59\\%), outperforming unimodal baselines. To address shallow or\ninconsistent categories, we further introduce a self-supervised ``product\nrecategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which\ndiscovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with\ncluster purities above 86\\%. Cross-platform experiments reveal a\ndeployment-relevant trade-off: complex late-fusion methods maximize accuracy\nwith diverse training data, while simpler early-fusion methods generalize more\neffectively to unseen platforms. Finally, we demonstrate the framework's\nindustrial scalability through deployment in EURWEB's commercial transaction\nintelligence platform via a two-stage inference pipeline, combining a\nlightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance\ncost and accuracy.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20013v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20013v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.385,
      "datasets_score": 0.405,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper utilizes a large dataset of 271,700 products from 40 e-commerce platforms as a key resource for developing and evaluating a multimodal classification framework, which involves aspects of dataset curation and analysis for machine learning applications. However, the primary focus is on the classification methodology rather than on creating, benchmarking, or deeply analyzing datasets as the main contribution. This makes it moderately relevant to the topic.",
      "llm_score_status": "completed",
      "summary": "This paper presents a multimodal hierarchical classification framework for e-commerce product categorization and recategorization, addressing challenges like platform heterogeneity and taxonomy limitations using a dataset of 271,700 products from 40 platforms. It integrates textual features (RoBERTa), visual features (ViT), and vision-language representations (CLIP) with various fusion strategies and dynamic masking for hierarchical accuracy, while introducing a self-supervised pipeline for discovering new subcategories; key findings include achieving 98.59% hierarchical F1 with late-fusion CLIP, high cluster purities in recategorization, and successful industrial deployment via a cost-efficient two-stage inference pipeline.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing techniques like RoBERTa, ViT, and CLIP in a hierarchical framework with dynamic masking and a self-supervised recategorization pipeline, offering notable improvements for cross-platform e-commerce challenges. However, it does not introduce entirely new problems or architectures, making it an incremental advancement rather than a groundbreaking one.",
      "impact_score": "Moderate",
      "impact_justification": "The work's deployment in a commercial e-commerce platform and its focus on practical scalability suggest it will be built upon in subfields like AI for retail and information retrieval, potentially influencing specific applications. Nonetheless, its impact may be limited to niche areas rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a high-quality, deployable solution with real-world relevance for e-commerce AI, offering valuable insights into multimodal classification and taxonomy adaptation. It is essential for researchers and practitioners in machine learning and information retrieval but not universally groundbreaking.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5d9873878d78a0643495ca7f2e3567b9b9f94876",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 4,
      "average_h_index": 1.2857142857142858,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Lotte Gross",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377556744"
        },
        {
          "name": "Rebecca Walter",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377556880"
        },
        {
          "name": "Nicole Zoppi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377556746"
        },
        {
          "name": "Adrien Justus",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377555983"
        },
        {
          "name": "Alessandro Gambetti",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/40023024"
        },
        {
          "name": "Qiwei Han",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2279734145"
        },
        {
          "name": "Maximilian Kaiser",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/30304746"
        }
      ]
    },
    {
      "id": "2508.20015",
      "title": "Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for\n  Emergent Misalignment",
      "authors": [
        "Julian Arnold",
        "Niels Lörch"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is\nbroadly misaligned with respect to human values. To understand when and how\nthis emergent misalignment occurs, we develop a comprehensive framework for\ndetecting and characterizing rapid transitions during fine-tuning using both\ndistributional change detection methods as well as order parameters that are\nformulated in plain English and evaluated by an LLM judge. Using an objective\nstatistical dissimilarity measure, we quantify how the phase transition that\noccurs during fine-tuning affects multiple aspects of the model. In particular,\nwe assess what percentage of the total distributional change in model outputs\nis captured by different aspects, such as alignment or verbosity, providing a\ndecomposition of the overall transition. We also find that the actual\nbehavioral transition occurs later in training than indicated by the peak in\nthe gradient norm alone. Our framework enables the automated discovery and\nquantification of language-based order parameters, which we demonstrate on\nexamples ranging from knowledge questions to politics and ethics.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20015v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20015v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.487,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.401,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses fine-tuning LLMs for alignment and misalignment, which relates to broader AI alignment goals that RLHF addresses, but it does not involve training with a reward model based on human feedback or reinforcement learning techniques. Instead, it focuses on fine-tuning on harmful datasets and detecting phase transitions.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper centers on detecting phase transitions in LLMs during fine-tuning and analyzing emergent misalignment, with no mention of diffusion models, iterative refinement for logical reasoning, or multi-step reasoning processes.",
      "distributed_training_justification": "The paper examines fine-tuning LLMs and behavioral changes but does not address distributed training methods, parallel computing, or strategies for partitioning data/computation across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20016",
      "title": "HPC Digital Twins for Evaluating Scheduling Policies, Incentive\n  Structures and their Impact on Power and Cooling",
      "authors": [
        "Matthias Maiterth",
        "Wesley H. Brewer",
        "Jaya S. Kuruvella",
        "Arunavo Dey",
        "Tanzima Z. Islam",
        "Kevin Menear",
        "Dmitry Duplyakin",
        "Rashadul Kabir",
        "Tapasya Patki",
        "Terry Jones",
        "Feiyi Wang"
      ],
      "categories": [
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.AI (Artificial Intelligence)",
        "cs.ET (Emerging Technologies)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "Schedulers are critical for optimal resource utilization in high-performance\ncomputing. Traditional methods to evaluate schedulers are limited to\npost-deployment analysis, or simulators, which do not model associated\ninfrastructure. In this work, we present the first-of-its-kind integration of\nscheduling and digital twins in HPC. This enables what-if studies to understand\nthe impact of parameter configurations and scheduling decisions on the physical\nassets, even before deployment, or regarching changes not easily realizable in\nproduction. We (1) provide the first digital twin framework extended with\nscheduling capabilities, (2) integrate various top-tier HPC systems given their\npublicly available datasets, (3) implement extensions to integrate external\nscheduling simulators. Finally, we show how to (4) implement and evaluate\nincentive structures, as-well-as (5) evaluate machine learning based\nscheduling, in such novel digital-twin based meta-framework to prototype\nscheduling. Our work enables what-if scenarios of HPC systems to evaluate\nsustainability, and the impact on the simulated system.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20016v2",
      "pdf_url": "http://arxiv.org/pdf/2508.20016v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.317,
      "weak_supervision_score": 0.306,
      "diffusion_reasoning_score": 0.297,
      "distributed_training_score": 0.422,
      "datasets_score": 0.387,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on integrating scheduling policies and digital twins in HPC systems to evaluate impacts on power, cooling, and workloads, including ML-based scheduling. While HPC environments can support distributed training through parallel computing and multi-node setups, the paper does not directly address algorithms or systems for accelerating ML model training by partitioning data or computation. Instead, it mentions ML for scheduling decisions, which is indirectly related but not the core focus, making it tangentially relevant.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20018",
      "title": "SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in\n  Mobile GUI Control",
      "authors": [
        "Quanfeng Lu",
        "Zhantao Ma",
        "Shuai Zhong",
        "Jin Wang",
        "Dahai Yu",
        "Michael K. Ng",
        "Ping Luo"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "The rapid advancement of large vision language models (LVLMs) and agent\nsystems has heightened interest in mobile GUI agents that can reliably\ntranslate natural language into interface operations. Existing single-agent\napproaches, however, remain limited by structural constraints. Although\nmulti-agent systems naturally decouple different competencies, recent progress\nin multi-agent reinforcement learning (MARL) has often been hindered by\ninefficiency and remains incompatible with current LVLM architectures. To\naddress these challenges, we introduce SWIRL, a staged workflow for interleaved\nreinforcement learning designed for multi-agent systems. SWIRL reformulates\nMARL into a sequence of single-agent reinforcement learning tasks, updating one\nagent at a time while keeping the others fixed. This formulation enables stable\ntraining and promotes efficient coordination across agents. Theoretically, we\nprovide a stepwise safety bound, a cross-round monotonic improvement theorem,\nand convergence guarantees on return, ensuring robust and principled\noptimization. In application to mobile GUI control, SWIRL instantiates a\nNavigator that converts language and screen context into structured plans, and\nan Interactor that grounds these plans into executable atomic actions.\nExtensive experiments demonstrate superior performance on both high-level and\nlow-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong\ncapability in multi-agent mathematical reasoning, underscoring its potential as\na general framework for developing efficient and robust multi-agent systems.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20018v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20018v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.426,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.404,
      "datasets_score": 0.269,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on SWIRL, a framework for interleaved reinforcement learning in multi-agent systems, but does not involve human feedback. It lacks elements like a reward model trained on human-ranked data or fine-tuning based on human preferences, which are core to RLHF. Instead, it uses standard RL for task-based optimization.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper addresses efficient training in multi-agent RL by using sequential updates to reduce memory usage (e.g., O(1) actor memory), which indirectly relates to resource optimization in training. However, it does not focus on core distributed training concepts like parallel computing across multiple nodes or data partitioning, making it only peripherally connected.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20019",
      "title": "Symphony: A Decentralized Multi-Agent Framework for Scalable Collective\n  Intelligence",
      "authors": [
        "Ji Wang",
        "Kashing Chen",
        "Xinyuan Song",
        "Ke Zhang",
        "Lynn Ai",
        "Eric Yang",
        "Bill Shi"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Most existing Large Language Model (LLM)-based agent frameworks rely on\ncentralized orchestration, incurring high deployment costs, rigid communication\ntopologies, and limited adaptability. To address these challenges, we introduce\nSymphony, a decentralized multi-agent system which enables lightweight LLMs on\nconsumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:\n(1) a decentralized ledger that records capabilities, (2) a Beacon-selection\nprotocol for dynamic task allocation, and (3) weighted result voting based on\nCoTs. This design forms a privacy-saving, scalable, and fault-tolerant\norchestration with low overhead. Empirically, Symphony outperforms existing\nbaselines on reasoning benchmarks, achieving substantial accuracy gains and\ndemonstrating robustness across models of varying capacities.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20019v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20019v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.465,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a decentralized multi-agent framework for LLM coordination, focusing on mechanisms like a decentralized ledger, Beacon-selection protocol, and weighted result voting using Chain-of-Thoughts (CoTs). While CoTs involve step-by-step reasoning, the paper does not mention diffusion models, iterative refinement processes, or any adaptation of diffusion for logical tasks. Thus, it lacks the core elements of diffusion-based reasoning.",
      "distributed_training_justification": "The paper describes a decentralized system for coordinating lightweight LLMs across heterogeneous edge devices, involving task allocation and parallel agent operations, which relates to distributed computing concepts. However, it focuses on agent orchestration for task solving rather than accelerating model training through data partitioning, model parallelism, or multi-node training algorithms. This makes it only loosely connected to distributed training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20020",
      "title": "GS: Generative Segmentation via Label Diffusion",
      "authors": [
        "Yuhao Chen",
        "Shubin Chen",
        "Liang Lin",
        "Guangrun Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Language-driven image segmentation is a fundamental task in vision-language\nunderstanding, requiring models to segment regions of an image corresponding to\nnatural language expressions. Traditional methods approach this as a\ndiscriminative problem, assigning each pixel to foreground or background based\non semantic alignment. Recently, diffusion models have been introduced to this\ndomain, but existing approaches remain image-centric: they either (i) use image\ndiffusion models as visual feature extractors, (ii) synthesize segmentation\ndata via image generation to train discriminative models, or (iii) perform\ndiffusion inversion to extract attention cues from pre-trained image diffusion\nmodels-thereby treating segmentation as an auxiliary process. In this paper, we\npropose GS (Generative Segmentation), a novel framework that formulates\nsegmentation itself as a generative task via label diffusion. Instead of\ngenerating images conditioned on label maps and text, GS reverses the\ngenerative process: it directly generates segmentation masks from noise,\nconditioned on both the input image and the accompanying language description.\nThis paradigm makes label generation the primary modeling target, enabling\nend-to-end training with explicit control over spatial and semantic fidelity.\nTo demonstrate the effectiveness of our approach, we evaluate GS on Panoptic\nNarrative Grounding (PNG), a representative and challenging benchmark for\nmultimodal segmentation that requires panoptic-level reasoning guided by\nnarrative captions. Experimental results show that GS significantly outperforms\nexisting discriminative and diffusion-based methods, setting a new\nstate-of-the-art for language-driven segmentation.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20020v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20020v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.557,
      "distributed_training_score": 0.325,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion models for iterative refinement in generating segmentation masks, which involves a step-by-step denoising process. However, it applies this to visual and semantic tasks in image segmentation, not to complex logical reasoning or treating a 'Chain-of-Thought' as a holistic entity for multi-step logical correction. Since the core focus is on generative segmentation rather than logical tasks, it only loosely connects to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20029",
      "title": "Segmentation Assisted Incremental Test Time Adaptation in an Open World",
      "authors": [
        "Manogna Sreenivas",
        "Soma Biswas"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In dynamic environments, unfamiliar objects and distribution shifts are often\nencountered, which challenge the generalization abilities of the deployed\ntrained models. This work addresses Incremental Test Time Adaptation of Vision\nLanguage Models, tackling scenarios where unseen classes and unseen domains\ncontinuously appear during testing. Unlike traditional Test Time Adaptation\napproaches, where the test stream comes only from a predefined set of classes,\nour framework allows models to adapt simultaneously to both covariate and label\nshifts, actively incorporating new classes as they emerge. Towards this goal,\nwe establish a new benchmark for ITTA, integrating single image TTA methods for\nVLMs with active labeling techniques that query an oracle for samples\npotentially representing unseen classes during test time. We propose a\nsegmentation assisted active labeling module, termed SegAssist, which is\ntraining free and repurposes the segmentation capabilities of VLMs to refine\nactive sample selection, prioritizing samples likely to belong to unseen\nclasses. Extensive experiments on several benchmark datasets demonstrate the\npotential of SegAssist to enhance the performance of VLMs in real world\nscenarios, where continuous adaptation to emerging data is essential.\nProject-page:https://manogna-s.github.io/segassist/",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20029v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20029v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.38,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves Incremental Test Time Adaptation (ITTA) using active labeling with an oracle to query precise labels for uncertain samples, particularly for unseen classes. This approach relies on targeted, accurate supervision rather than programmatically generating noisy or imprecise labels from high-level sources, which is the core of weak supervision. Therefore, it does not align with weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20030",
      "title": "Large Language Models (LLMs) for Electronic Design Automation (EDA)",
      "authors": [
        "Kangwei Xu",
        "Denis Schwachhofer",
        "Jason Blocklove",
        "Ilia Polian",
        "Peter Domanski",
        "Dirk Pflüger",
        "Siddharth Garg",
        "Ramesh Karri",
        "Ozgur Sinanoglu",
        "Johann Knechtel",
        "Zhuorui Zhao",
        "Ulf Schlichtmann",
        "Bing Li"
      ],
      "categories": [
        "eess.SY (Systems and Control)",
        "cs.AI (Artificial Intelligence)",
        "cs.AR (Hardware Architecture)",
        "cs.LG (Machine Learning)",
        "cs.SY (Systems and Control)"
      ],
      "abstract": "With the growing complexity of modern integrated circuits, hardware engineers\nare required to devote more effort to the full design-to-manufacturing\nworkflow. This workflow involves numerous iterations, making it both\nlabor-intensive and error-prone. Therefore, there is an urgent demand for more\nefficient Electronic Design Automation (EDA) solutions to accelerate hardware\ndevelopment. Recently, large language models (LLMs) have shown remarkable\nadvancements in contextual comprehension, logical reasoning, and generative\ncapabilities. Since hardware designs and intermediate scripts can be\nrepresented as text, integrating LLM for EDA offers a promising opportunity to\nsimplify and even automate the entire workflow. Accordingly, this paper\nprovides a comprehensive overview of incorporating LLMs into EDA, with emphasis\non their capabilities, limitations, and future opportunities. Three case\nstudies, along with their outlook, are introduced to demonstrate the\ncapabilities of LLMs in hardware design, testing, and optimization. Finally,\nfuture directions and challenges are highlighted to further explore the\npotential of LLMs in shaping the next-generation EDA, providing valuable\ninsights for researchers interested in leveraging advanced AI technologies for\nEDA.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20030v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20030v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.413,
      "diffusion_reasoning_score": 0.466,
      "distributed_training_score": 0.426,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on applying LLMs to EDA tasks such as hardware design and testing, but it does not mention or involve reinforcement learning from human feedback. There is no discussion of training models with human-ranked data or using a reward model for fine-tuning.",
      "weak_supervision_justification": "The paper mentions LLMs learning patterns from historical data in EDA, which could indirectly relate to weak supervision through noisy or programmatic data sources. However, it does not explicitly describe or focus on weak supervision techniques for training models, making the connection indirect.",
      "diffusion_reasoning_justification": "The paper discusses LLMs' capabilities in logical reasoning for EDA tasks but does not reference diffusion models, iterative refinement processes, or multi-step reasoning via diffusion. It lacks any component involving diffusion-based approaches.",
      "distributed_training_justification": "The paper overviews LLMs' application in EDA without addressing training methods, such as distributed training, parallel computing, or multi-node systems. It focuses on usage in hardware workflows, not on how models are trained.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20033",
      "title": "DeepScholar-Bench: A Live Benchmark and Automated Evaluation for\n  Generative Research Synthesis",
      "authors": [
        "Liana Patel",
        "Negar Arabzadeh",
        "Harshit Gupta",
        "Ankita Sundar",
        "Ion Stoica",
        "Matei Zaharia",
        "Carlos Guestrin"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The ability to research and synthesize knowledge is central to human\nexpertise and progress. An emerging class of systems promises these exciting\ncapabilities through generative research synthesis, performing retrieval over\nthe live web and synthesizing discovered sources into long-form, cited\nsummaries. However, evaluating such systems remains an open challenge: existing\nquestion-answering benchmarks focus on short-form factual responses, while\nexpert-curated datasets risk staleness and data contamination. Both fail to\ncapture the complexity and evolving nature of real research synthesis tasks. In\nthis work, we introduce DeepScholar-bench, a live benchmark and holistic,\nautomated evaluation framework designed to evaluate generative research\nsynthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv\npapers and focuses on a real research synthesis task: generating the related\nwork sections of a paper by retrieving, synthesizing, and citing prior\nresearch. Our evaluation framework holistically assesses performance across\nthree key dimensions, knowledge synthesis, retrieval quality, and\nverifiability. We also develop DeepScholar-base, a reference pipeline\nimplemented efficiently using the LOTUS API. Using the DeepScholar-bench\nframework, we perform a systematic evaluation of prior open-source systems,\nsearch AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that\nDeepScholar-base establishes a strong baseline, attaining competitive or higher\nperformance than each other method. We also find that DeepScholar-bench remains\nfar from saturated, with no system exceeding a score of $19\\%$ across all\nmetrics. These results underscore the difficulty of DeepScholar-bench, as well\nas its importance for progress towards AI systems capable of generative\nresearch synthesis. We make our code available at\nhttps://github.com/guestrin-lab/deepscholar-bench.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20033v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20033v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.436,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.401,
      "datasets_score": 0.465,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on introducing a benchmark for generative research synthesis, including dataset creation and evaluation frameworks, but does not involve training machine learning models using noisy or imprecise labels, which is the core of weak supervision.",
      "diffusion_reasoning_justification": "The paper does not mention or utilize diffusion models for multi-step logical reasoning; it centers on retrieval, synthesis, and verifiability in research tasks, without any iterative refinement processes characteristic of diffusion-based approaches.",
      "distributed_training_justification": "The paper's main contribution is a benchmark and evaluation framework for AI systems, with no discussion of parallel computing, data partitioning, or accelerating model training across multiple nodes, which defines distributed training.",
      "datasets_justification": "The paper introduces and evaluates DeepScholar-Bench, a new benchmark dataset derived from recent ArXiv papers, focusing on curation, benchmarking, and automated evaluation for AI research synthesis tasks, directly aligning with research on creating and evaluating datasets for machine learning applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces DeepScholar-Bench, a live benchmark designed to evaluate generative research synthesis systems by using recent ArXiv papers as queries for generating related work sections, incorporating an automated evaluation framework that assesses knowledge synthesis, retrieval quality, and verifiability. It develops DeepScholar-base as a reference pipeline and evaluates it against existing systems, revealing that while DeepScholar-base outperforms many baselines, no system exceeds 19% across all metrics, highlighting significant challenges and opportunities for advancement in AI-driven research synthesis.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new live benchmark and automated evaluation framework for generative research synthesis, addressing the limitations of existing benchmarks by using real-time data from ArXiv and focusing on complex tasks like retrieving and synthesizing web sources.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research and development in AI for research synthesis by providing a standardized, scalable benchmark that can drive improvements in retrieval, synthesis, and verifiability capabilities.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper is a high-quality contribution that offers a valuable new tool for evaluating AI systems in research synthesis, making it essential for researchers in AI evaluation and natural language processing to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/565864b8d6d7198fbc4cbd7fdbc3c9e6a3525b43",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 18,
      "average_h_index": 4.857142857142857,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Liana Patel",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2290487661"
        },
        {
          "name": "Negar Arabzadeh",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377558008"
        },
        {
          "name": "Harshit Gupta",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377559653"
        },
        {
          "name": "Ankita Sundar",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377559351"
        },
        {
          "name": "Ion Stoica",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2295665819"
        },
        {
          "name": "Matei Zaharia",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2253469020"
        },
        {
          "name": "Carlos Guestrin",
          "h_index": 18,
          "profile_url": "https://www.semanticscholar.org/author/1412355294"
        }
      ]
    },
    {
      "id": "2508.20040",
      "title": "Model Science: getting serious about verification, explanation and\n  control of AI systems",
      "authors": [
        "Przemyslaw Biecek",
        "Wojciech Samek"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The growing adoption of foundation models calls for a paradigm shift from\nData Science to Model Science. Unlike data-centric approaches, Model Science\nplaces the trained model at the core of analysis, aiming to interact, verify,\nexplain, and control its behavior across diverse operational contexts. This\npaper introduces a conceptual framework for a new discipline called Model\nScience, along with the proposal for its four key pillars: Verification, which\nrequires strict, context-aware evaluation protocols; Explanation, which is\nunderstood as various approaches to explore of internal model operations;\nControl, which integrates alignment techniques to steer model behavior; and\nInterface, which develops interactive and visual explanation tools to improve\nhuman calibration and decision-making. The proposed framework aims to guide the\ndevelopment of credible, safe, and human-aligned AI systems.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20040v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20040v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.451,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.336,
      "datasets_score": 0.407,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Not Relevant",
      "rlhf_justification": "The paper's Control pillar mentions alignment techniques to steer model behavior, which could indirectly relate to RLHF as a method for human alignment. However, it does not specifically discuss training reward models or using reinforcement learning with human feedback, focusing instead on a broader framework for Model Science.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper advocates for a shift from data-centric to model-centric approaches and does not involve creating, analyzing, benchmarking, or evaluating datasets, making it unrelated to this topic.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20063",
      "title": "OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without\n  Human Annotations",
      "authors": [
        "Peng-Hao Hsu",
        "Ke Zhang",
        "Fu-En Wang",
        "Tao Tu",
        "Ming-Feng Li",
        "Yu-Lun Liu",
        "Albert Y. C. Chen",
        "Min Sun",
        "Cheng-Hao Kuo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Open-vocabulary (OV) 3D object detection is an emerging field, yet its\nexploration through image-based methods remains limited compared to 3D point\ncloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view\nindoor 3D object detector trained without human annotations. In particular,\nOpenM3D is a single-stage detector adapting the 2D-induced voxel features from\nthe ImGeoNet model. To support OV, it is jointly trained with a class-agnostic\n3D localization loss requiring high-quality 3D pseudo boxes and a\nvoxel-semantic alignment loss requiring diverse pre-trained CLIP features. We\nfollow the training setting of OV-3DET where posed RGB-D images are given but\nno human annotations of 3D boxes or classes are available. We propose a 3D\nPseudo Box Generation method using a graph embedding technique that combines 2D\nsegments into coherent 3D structures. Our pseudo-boxes achieve higher precision\nand recall than other methods, including the method proposed in OV-3DET. We\nfurther sample diverse CLIP features from 2D segments associated with each\ncoherent 3D structure to align with the corresponding voxel feature. The key to\ntraining a highly accurate single-stage detector requires both losses to be\nlearned toward high-quality targets. At inference, OpenM3D, a highly efficient\ndetector, requires only multi-view images for input and demonstrates superior\naccuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor\nbenchmarks compared to existing methods. We outperform a strong two-stage\nmethod that leverages our class-agnostic detector with a ViT CLIP-based OV\nclassifier and a baseline incorporating multi-view depth estimator on both\naccuracy and speed.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20063v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20063v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.372,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20064",
      "title": "Patch Progression Masked Autoencoder with Fusion CNN Network for\n  Classifying Evolution Between Two Pairs of 2D OCT Slices",
      "authors": [
        "Philippe Zhang",
        "Weili Jiang",
        "Yihao Li",
        "Jing Zhang",
        "Sarah Matta",
        "Yubo Tan",
        "Hui Lin",
        "Haoshen Wang",
        "Jiangtian Pan",
        "Hui Xu",
        "Laurent Borderie",
        "Alexandre Le Guilcher",
        "Béatrice Cochener",
        "Chubin Ou",
        "Gwenolé Quellec",
        "Mathieu Lamard"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting\nvisual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments\nhave been effective in slowing the progression of neovascular AMD, with better\noutcomes achieved through timely diagnosis and consistent monitoring. Tracking\nthe progression of neovascular activity in OCT scans of patients with exudative\nAMD allows for the development of more personalized and effective treatment\nplans. This was the focus of the Monitoring Age-related Macular Degeneration\nProgression in Optical Coherence Tomography (MARIO) challenge, in which we\nparticipated. In Task 1, which involved classifying the evolution between two\npairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN\nnetwork with model ensembling to further enhance the model's performance. For\nTask 2, which focused on predicting progression over the next three months\nbased on current exam data, we proposed the Patch Progression Masked\nAutoencoder that generates an OCT for the next exam and then classifies the\nevolution between the current OCT and the one generated using our solution from\nTask 1. The results we achieved allowed us to place in the Top 10 for both\ntasks. Some team members are part of the same organization as the challenge\norganizers; therefore, we are not eligible to compete for the prize.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20064v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20064v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.266,
      "weak_supervision_score": 0.307,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.333,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20066",
      "title": "PAUL: Uncertainty-Guided Partition and Augmentation for Robust\n  Cross-View Geo-Localization under Noisy Correspondence",
      "authors": [
        "Zheng Li",
        "Yanming Guo",
        "WenZhe Liu",
        "Xueyi Zhang",
        "Zhaoyun Ding",
        "Long Xu",
        "Mingrui Lao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Cross-view geo-localization is a critical task for UAV navigation, event\ndetection, and aerial surveying, as it enables matching between drone-captured\nand satellite imagery. Most existing approaches embed multi-modal data into a\njoint feature space to maximize the similarity of paired images. However, these\nmethods typically assume perfect alignment of image pairs during training,\nwhich rarely holds true in real-world scenarios. In practice, factors such as\nurban canyon effects, electromagnetic interference, and adverse weather\nfrequently induce GPS drift, resulting in systematic alignment shifts where\nonly partial correspondences exist between pairs. Despite its prevalence, this\nsource of noisy correspondence has received limited attention in current\nresearch. In this paper, we formally introduce and address the Noisy\nCorrespondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to\nbridge the gap between idealized benchmarks and practical applications. To this\nend, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a\nnovel framework that partitions and augments training data based on estimated\ndata uncertainty through uncertainty-aware co-augmentation and evidential\nco-training. Specifically, PAUL selectively augments regions with high\ncorrespondence confidence and utilizes uncertainty estimation to refine feature\nlearning, effectively suppressing noise from misaligned pairs. Distinct from\ntraditional filtering or label correction, PAUL leverages both data uncertainty\nand loss discrepancy for targeted partitioning and augmentation, thus providing\nrobust supervision for noisy samples. Comprehensive experiments validate the\neffectiveness of individual components in PAUL,which consistently achieves\nsuperior performance over other competitive noisy-correspondence-driven methods\nin various noise ratios.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20066v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20066v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.437,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.365,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves handling noisy correspondences in training data for cross-view geo-localization, such as misaligned image pairs due to GPS drift. This directly aligns with weak supervision, as it trains models on imperfect, programmatically generated labels (from GPS sources) rather than perfect hand-labeled data. PAUL's uncertainty-guided approach enhances learning from these noisy sources, making it a clear application of weak supervision principles.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces the Noisy Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, which addresses GPS-induced misalignments in matching UAV and satellite images, a common real-world issue overlooked in existing methods. The authors propose the PAUL framework, which uses uncertainty-guided data partitioning and augmentation through evidential deep learning to selectively enhance high-confidence regions and refine feature learning, thereby improving robustness against noisy correspondences. Experimental results on various datasets demonstrate that PAUL outperforms competing methods across different noise ratios, validating its effectiveness in practical applications like UAV navigation and aerial surveying.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new problem in NC-CVGL and a novel framework, PAUL, that advances the state-of-the-art by leveraging uncertainty learning for robust handling of noisy correspondences, which has not been systematically addressed before.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in cross-view geo-localization by providing a robust method for noisy data, potentially leading to citations and improvements in subfields like UAV navigation, though its applicability may remain niche.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality, innovative approach to a practical problem in computer vision, making it valuable for researchers working on robust multi-modal matching and geo-localization tasks.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2905c35835239f0dab9fa282b9a3b93a7a7013c7",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 2,
      "average_h_index": 0.42857142857142855,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zheng Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374471225"
        },
        {
          "name": "Yanming Guo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373501012"
        },
        {
          "name": "WenZhe Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377629285"
        },
        {
          "name": "Xueyi Zhang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2261897002"
        },
        {
          "name": "Zhaoyun Ding",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379295441"
        },
        {
          "name": "Long Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377653474"
        },
        {
          "name": "Mingrui Lao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2337059618"
        }
      ]
    },
    {
      "id": "2508.20068",
      "title": "11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with\n  Cognitive-Inspired Analysis",
      "authors": [
        "Chengzu Li",
        "Wenshan Wu",
        "Huanyu Zhang",
        "Qingtao Li",
        "Zeyu Gao",
        "Yan Xia",
        "José Hernández-Orallo",
        "Ivan Vulić",
        "Furu Wei"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "For human cognitive process, spatial reasoning and perception are closely\nentangled, yet the nature of this interplay remains underexplored in the\nevaluation of multimodal large language models (MLLMs). While recent MLLM\nadvancements show impressive performance on reasoning, their capacity for\nhuman-like spatial cognition remains an open question. In this work, we\nintroduce a systematic evaluation framework to assess the spatial reasoning\nabilities of state-of-the-art MLLMs relative to human performance. Central to\nour work is 11Plus-Bench, a high-quality benchmark derived from realistic\nstandardized spatial aptitude tests. 11Plus-Bench also features fine-grained\nexpert annotations of both perceptual complexity and reasoning process,\nenabling detailed instance-level analysis of model behavior. Through extensive\nexperiments across 14 MLLMs and human evaluation, we find that current MLLMs\nexhibit early signs of spatial cognition. Despite a large performance gap\ncompared to humans, MLLMs' cognitive profiles resemble those of humans in that\ncognitive effort correlates strongly with reasoning-related complexity.\nHowever, instance-level performance in MLLMs remains largely random, whereas\nhuman correctness is highly predictable and shaped by abstract pattern\ncomplexity. These findings highlight both emerging capabilities and limitations\nin current MLLMs' spatial reasoning capabilities and provide actionable\ninsights for advancing model design.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20068v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20068v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.551,
      "distributed_training_score": 0.342,
      "datasets_score": 0.407,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluating spatial reasoning in MLLMs using a new benchmark, without any mention of diffusion models, iterative refinement processes, or adapting diffusion for logical tasks. It does not involve multi-step logical reasoning via diffusion mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces and analyzes a new benchmark dataset, 11Plus-Bench, specifically for evaluating spatial reasoning in MLLMs. It covers dataset creation, curation with expert annotations, benchmarking against human performance, and analysis, aligning directly with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces 11Plus-Bench, a benchmark derived from standardized spatial aptitude tests, to systematically evaluate the spatial reasoning abilities of multimodal large language models (MLLMs) by comparing them to human cognition. Through experiments involving 14 MLLMs and human participants, along with fine-grained annotations of perceptual and reasoning complexity, the authors reveal that MLLMs show early signs of spatial cognition similar to humans in cognitive effort correlations, but exhibit random instance-level performance and a significant overall gap, highlighting areas for future improvements in model design.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel benchmark and cognitive-inspired analysis framework for evaluating spatial reasoning in MLLMs, which significantly advances the state-of-the-art by providing detailed, instance-level insights beyond traditional aggregate metrics.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like MLLM evaluation and cognitive AI, as it offers a new tool for assessing spatial abilities, though its influence may be confined to specialized research areas rather than widespread applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution by introducing a valuable benchmark and insightful analysis of MLLM spatial reasoning, making it essential for researchers in AI and cognitive science to understand current limitations and future directions.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0ad7fa65b6d74db940cfe8be968b31c03239c1c7",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 12,
      "average_h_index": 4.777777777777778,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Chengzu Li",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2155795167"
        },
        {
          "name": "Wenshan Wu",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/51198241"
        },
        {
          "name": "Huanyu Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378032900"
        },
        {
          "name": "Qingtao Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377635252"
        },
        {
          "name": "Zeyu Gao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377731645"
        },
        {
          "name": "Yan Xia",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2258547658"
        },
        {
          "name": "Jos'e Hern'andez-Orallo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377557760"
        },
        {
          "name": "Ivan Vuli'c",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2339667880"
        },
        {
          "name": "Furu Wei",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2249539478"
        }
      ]
    },
    {
      "id": "2508.20072",
      "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding\n  in Vision-Language-Action Policies",
      "authors": [
        "Zhixuan Liang",
        "Yizhuo Li",
        "Tianshuo Yang",
        "Chengyue Wu",
        "Sitong Mao",
        "Liuao Pei",
        "Xiaokang Yang",
        "Jiangmiao Pang",
        "Yao Mu",
        "Ping Luo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Vision-Language-Action (VLA) models adapt large vision-language backbones to\nmap images and instructions to robot actions. However, prevailing VLA decoders\neither generate actions autoregressively in a fixed left-to-right order or\nattach continuous diffusion or flow matching heads outside the backbone,\ndemanding specialized training and iterative sampling that hinder a unified,\nscalable architecture. We present Discrete Diffusion VLA, a single-transformer\npolicy that models discretized action chunks with discrete diffusion and is\ntrained with the same cross-entropy objective as the VLM backbone. The design\nretains diffusion's progressive refinement paradigm while remaining natively\ncompatible with the discrete token interface of VLMs. Our method achieves an\nadaptive decoding order that resolves easy action elements before harder ones\nand uses secondary remasking to revisit uncertain predictions across refinement\nrounds, which improves consistency and enables robust error correction. This\nunified decoder preserves pretrained vision language priors, supports parallel\ndecoding, breaks the autoregressive bottleneck, and reduces the number of\nfunction evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,\n71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv\nBridge, improving over both autoregressive and continuous diffusion baselines.\nThese findings indicate that discrete-diffusion action decoder supports precise\naction modeling and consistent training, laying groundwork for scaling VLA to\nlarger models and datasets.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20072v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20072v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.588,
      "distributed_training_score": 0.395,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on discrete diffusion for action decoding in VLA models, trained with cross-entropy loss on robotics datasets, without any mention of human feedback, reward models, or reinforcement learning for alignment with human preferences. Thus, it does not involve RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses discrete diffusion for iterative refinement and error correction in action decoding, which shares the multi-step refinement concept with diffusion-based reasoning. However, it applies this to generating robot actions from visual and linguistic inputs, not to complex logical tasks or holistic Chain-of-Thought reasoning, making it only loosely related.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20080",
      "title": "Seam360GS: Seamless 360° Gaussian Splatting from Real-World\n  Omnidirectional Images",
      "authors": [
        "Changha Shin",
        "Woong Oh Cho",
        "Seon Joo Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.GR (Graphics)"
      ],
      "abstract": "360-degree visual content is widely shared on platforms such as YouTube and\nplays a central role in virtual reality, robotics, and autonomous navigation.\nHowever, consumer-grade dual-fisheye systems consistently yield imperfect\npanoramas due to inherent lens separation and angular distortions. In this\nwork, we introduce a novel calibration framework that incorporates a\ndual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach\nnot only simulates the realistic visual artifacts produced by dual-fisheye\ncameras but also enables the synthesis of seamlessly rendered 360-degree\nimages. By jointly optimizing 3D Gaussian parameters alongside calibration\nvariables that emulate lens gaps and angular distortions, our framework\ntransforms imperfect omnidirectional inputs into flawless novel view synthesis.\nExtensive evaluations on real-world datasets confirm that our method produces\nseamless renderings-even from imperfect images-and outperforms existing\n360-degree rendering models.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20080v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20080v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.237,
      "weak_supervision_score": 0.282,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.301,
      "datasets_score": 0.237,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20088",
      "title": "AudioStory: Generating Long-Form Narrative Audio with Large Language\n  Models",
      "authors": [
        "Yuxin Guo",
        "Teng Wang",
        "Yuying Ge",
        "Shijie Ma",
        "Yixiao Ge",
        "Wei Zou",
        "Ying Shan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)",
        "cs.SD (Sound)"
      ],
      "abstract": "Recent advances in text-to-audio (TTA) generation excel at synthesizing short\naudio clips but struggle with long-form narrative audio, which requires\ntemporal coherence and compositional reasoning. To address this gap, we propose\nAudioStory, a unified framework that integrates large language models (LLMs)\nwith TTA systems to generate structured, long-form audio narratives. AudioStory\npossesses strong instruction-following reasoning generation capabilities. It\nemploys LLMs to decompose complex narrative queries into temporally ordered\nsub-tasks with contextual cues, enabling coherent scene transitions and\nemotional tone consistency. AudioStory has two appealing features: (1)\nDecoupled bridging mechanism: AudioStory disentangles LLM-diffuser\ncollaboration into two specialized components, i.e., a bridging query for\nintra-event semantic alignment and a residual query for cross-event coherence\npreservation. (2) End-to-end training: By unifying instruction comprehension\nand audio generation within a single end-to-end framework, AudioStory\neliminates the need for modular training pipelines while enhancing synergy\nbetween components. Furthermore, we establish a benchmark AudioStory-10K,\nencompassing diverse domains such as animated soundscapes and natural sound\nnarratives. Extensive experiments show the superiority of AudioStory on both\nsingle-audio generation and narrative audio generation, surpassing prior TTA\nbaselines in both instruction-following ability and audio fidelity. Our code is\navailable at https://github.com/TencentARC/AudioStory",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20088v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20088v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.545,
      "distributed_training_score": 0.328,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper integrates diffusion models primarily for audio synthesis, not for multi-step logical reasoning. While it describes an interleaved process where LLMs handle reasoning and diffusion generates audio outputs, there is no clear adaptation of diffusion's iterative refinement for solving complex logical tasks. The diffusion component focuses on generation fidelity and coherence, making the paper related but not directly aligned with diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20089",
      "title": "Bridging Domain Gaps for Fine-Grained Moth Classification Through\n  Expert-Informed Adaptation and Foundation Model Priors",
      "authors": [
        "Ross J Gardiner",
        "Guillaume Mougeot",
        "Sareh Rowlands",
        "Benno I Simmons",
        "Flemming Helsing",
        "Toke Thomas Høye"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Labelling images of Lepidoptera (moths) from automated camera systems is\nvital for understanding insect declines. However, accurate species\nidentification is challenging due to domain shifts between curated images and\nnoisy field imagery. We propose a lightweight classification approach,\ncombining limited expert-labelled field data with knowledge distillation from\nthe high-performance BioCLIP2 foundation model into a ConvNeXt-tiny\narchitecture. Experiments on 101 Danish moth species from AMI camera systems\ndemonstrate that BioCLIP2 substantially outperforms other methods and that our\ndistilled lightweight model achieves comparable accuracy with significantly\nreduced computational cost. These insights offer practical guidelines for the\ndevelopment of efficient insect monitoring systems and bridging domain gaps for\nfine-grained classification.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20089v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20089v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.35,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20095",
      "title": "Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion\n  Planning",
      "authors": [
        "Jinhao Liang",
        "Sven Koenig",
        "Ferdinando Fioretto"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Multi-Robot Motion Planning (MRMP) involves generating collision-free\ntrajectories for multiple robots operating in a shared continuous workspace.\nWhile discrete multi-agent path finding (MAPF) methods are broadly adopted due\nto their scalability, their coarse discretization severely limits trajectory\nquality. In contrast, continuous optimization-based planners offer\nhigher-quality paths but suffer from the curse of dimensionality, resulting in\npoor scalability with respect to the number of robots. This paper tackles the\nlimitations of these two approaches by introducing a novel framework that\nintegrates discrete MAPF solvers with constrained generative diffusion models.\nThe resulting framework, called Discrete-Guided Diffusion (DGD), has three key\ncharacteristics: (1) it decomposes the original nonconvex MRMP problem into\ntractable subproblems with convex configuration spaces, (2) it combines\ndiscrete MAPF solutions with constrained optimization techniques to guide\ndiffusion models capture complex spatiotemporal dependencies among robots, and\n(3) it incorporates a lightweight constraint repair mechanism to ensure\ntrajectory feasibility. The proposed method sets a new state-of-the-art\nperformance in large-scale, complex environments, scaling to 100 robots while\nachieving planning efficiency and high success rates.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20095v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20095v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.298,
      "diffusion_reasoning_score": 0.512,
      "distributed_training_score": 0.382,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion models for iterative refinement in generating multi-robot trajectories, involving steps like denoising and constraint repair to handle spatiotemporal dependencies. However, this is applied to motion planning rather than solving complex logical tasks or implementing a Chain-of-Thought process for reasoning. There is no clear component for multi-step logical reasoning, as the focus is on generative optimization in robotics, making it only tangentially related to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20096",
      "title": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer\n  Use Agent with Decoupled Reinforcement Learning",
      "authors": [
        "Zeyi Sun",
        "Yuhang Cao",
        "Jianze Liang",
        "Qiushi Sun",
        "Ziyu Liu",
        "Zhixiong Zhang",
        "Yuhang Zang",
        "Xiaoyi Dong",
        "Kai Chen",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Autonomous agents for Graphical User Interfaces (GUIs) face significant\nchallenges in specialized domains such as scientific computing, where both\nlong-horizon planning and precise execution are required. Existing approaches\nsuffer from a trade-off: generalist agents excel at planning but perform poorly\nin execution, while specialized agents demonstrate the opposite weakness.\nRecent compositional frameworks attempt to bridge this gap by combining a\nplanner and an actor, but they are typically static and non-trainable, which\nprevents adaptation from experience. This is a critical limitation given the\nscarcity of high-quality data in scientific domains. To address these\nlimitations, we introduce CODA, a novel and trainable compositional framework\nthat integrates a generalist planner (Cerebrum) with a specialist executor\n(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,\nSpecialization, we apply a decoupled GRPO approach to train an expert planner\nfor each scientific application individually, bootstrapping from a small set of\ntask trajectories. In the second stage, Generalization, we aggregate all\nsuccessful trajectories from the specialized experts to build a consolidated\ndataset, which is then used for supervised fine-tuning of the final planner.\nThis equips CODA with both robust execution and cross-domain generalization.\nEvaluated on four challenging applications from the ScienceBoard benchmark,\nCODA significantly outperforms baselines and establishes a new state of the art\namong open-source models.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20096v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20096v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.4,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning in its training pipeline (e.g., decoupled GRPO), but it relies on an automatic judging system from open-source models for rewards, not human-ranked data or a reward model trained on human feedback. Therefore, it does not align with RLHF's core definition.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a compositional framework for planning and execution in GUI agents, with no mention of diffusion models, iterative refinement for reasoning, or treating Chain-of-Thought as a holistic entity. There is no component for multi-step logical reasoning via diffusion.",
      "distributed_training_justification": "The paper describes distributing the interaction process across multiple software environments in parallel, coordinated by a central master, to accelerate reinforcement learning, which involves parallel computing elements. However, this is not the main focus or a core algorithmic contribution; it's a supporting technique for efficiency, rather than a comprehensive distributed training system.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "CODA is a novel framework designed to enhance autonomous agents for Graphical User Interfaces (GUIs) in specialized domains like scientific computing, by integrating a generalist planner (Cerebrum, based on Qwen2.5-VL) with a specialist executor (Cerebellum, based on UI-Tars-1.5) to handle both long-horizon planning and precise execution. The methodology involves a two-stage training pipeline—Specialization, where planners are trained individually using decoupled reinforcement learning on task trajectories, and Generalization, where successful trajectories are aggregated for supervised fine-tuning—resulting in improved performance and a new state-of-the-art on the ScienceBoard benchmark among open-source models.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new trainable compositional framework that decouples and trains a planner and executor inspired by the human brain, significantly advancing state-of-the-art in autonomous agents for specialized domains. This approach addresses key limitations of existing static frameworks by enabling adaptive learning with minimal data.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within subfields like computer vision and AI for GUI agents, particularly in scientific computing, due to its efficient training method and performance gains. However, its impact may be limited to specific applications rather than broadly across all AI domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution by introducing an innovative framework that advances autonomous agents, making it essential for researchers in AI and computer vision to be aware of for potential applications in specialized domains. While not groundbreaking for all fields, its practical improvements warrant attention from relevant experts.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d307660082eafbcf6c69eb09db8f372393d71d1e",
      "total_authors": 11,
      "authors_found": 11,
      "highest_h_index": 24,
      "average_h_index": 10.909090909090908,
      "notable_authors_count": 7,
      "author_h_indexes": [
        {
          "name": "Zeyi Sun",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2269789160"
        },
        {
          "name": "Yuhang Cao",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/50206929"
        },
        {
          "name": "Jianze Liang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377920481"
        },
        {
          "name": "Qiushi Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377648766"
        },
        {
          "name": "Ziyu Liu",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2292942831"
        },
        {
          "name": "Zhixiong Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2338346463"
        },
        {
          "name": "Yuhang Zang",
          "h_index": 24,
          "profile_url": "https://www.semanticscholar.org/author/12862495"
        },
        {
          "name": "Xiao-wen Dong",
          "h_index": 22,
          "profile_url": "https://www.semanticscholar.org/author/2118187561"
        },
        {
          "name": "Kai Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377880984"
        },
        {
          "name": "Dahua Lin",
          "h_index": 20,
          "profile_url": "https://www.semanticscholar.org/author/2237734015"
        },
        {
          "name": "Jiaqi Wang",
          "h_index": 20,
          "profile_url": "https://www.semanticscholar.org/author/2267494294"
        }
      ]
    },
    {
      "id": "2508.20139",
      "title": "Is the medical image segmentation problem solved? A survey of current\n  developments and future directions",
      "authors": [
        "Guoping Xu",
        "Jayaram K. Udupa",
        "Jax Luo",
        "Songlin Zhao",
        "Yajun Yu",
        "Scott B. Raymond",
        "Hao Peng",
        "Lipeng Ning",
        "Yogesh Rathi",
        "Wei Liu",
        "You Zhang"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.HC (Human-Computer Interaction)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Medical image segmentation has advanced rapidly over the past two decades,\nlargely driven by deep learning, which has enabled accurate and efficient\ndelineation of cells, tissues, organs, and pathologies across diverse imaging\nmodalities. This progress raises a fundamental question: to what extent have\ncurrent models overcome persistent challenges, and what gaps remain? In this\nwork, we provide an in-depth review of medical image segmentation, tracing its\nprogress and key developments over the past decade. We examine core principles,\nincluding multiscale analysis, attention mechanisms, and the integration of\nprior knowledge, across the encoder, bottleneck, skip connections, and decoder\ncomponents of segmentation networks. Our discussion is organized around seven\nkey dimensions: (1) the shift from supervised to semi-/unsupervised learning,\n(2) the transition from organ segmentation to lesion-focused tasks, (3)\nadvances in multi-modality integration and domain adaptation, (4) the role of\nfoundation models and transfer learning, (5) the move from deterministic to\nprobabilistic segmentation, (6) the progression from 2D to 3D and 4D\nsegmentation, and (7) the trend from model invocation to segmentation agents.\nTogether, these perspectives provide a holistic overview of the trajectory of\ndeep learning-based medical image segmentation and aim to inspire future\ninnovation. To support ongoing research, we maintain a continually updated\nrepository of relevant literature and open-source resources at\nhttps://github.com/apple1986/medicalSegReview",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20139v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20139v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.271,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.33,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20140",
      "title": "Array-Based Monte Carlo Tree Search",
      "authors": [
        "James Ragan",
        "Fred Y. Hadaegh",
        "Soon-Jo Chung"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "Monte Carlo Tree Search is a popular method for solving decision making\nproblems. Faster implementations allow for more simulations within the same\nwall clock time, directly improving search performance. To this end, we present\nan alternative array-based implementation of the classic Upper Confidence\nbounds applied to Trees algorithm. Our method preserves the logic of the\noriginal algorithm, but eliminates the need for branch prediction, enabling\nfaster performance on pipelined processors, and up to a factor of 2.8 times\nbetter scaling with search depth in our numerical simulations.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20140v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20140v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.279,
      "weak_supervision_score": 0.235,
      "diffusion_reasoning_score": 0.321,
      "distributed_training_score": 0.284,
      "datasets_score": 0.237,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20141",
      "title": "UltraEar: a multicentric, large-scale database combining\n  ultra-high-resolution computed tomography and clinical data for ear diseases",
      "authors": [
        "Ruowei Tang",
        "Pengfei Zhao",
        "Xiaoguang Li",
        "Ning Xu",
        "Yue Cheng",
        "Mengshi Zhang",
        "Zhixiang Wang",
        "Zhengyu Zhang",
        "Hongxia Yin",
        "Heyu Ding",
        "Shusheng Gong",
        "Yuhe Liu",
        "Zhenchang Wang"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Ear diseases affect billions of people worldwide, leading to substantial\nhealth and socioeconomic burdens. Computed tomography (CT) plays a pivotal role\nin accurate diagnosis, treatment planning, and outcome evaluation. The\nobjective of this study is to present the establishment and design of UltraEar\nDatabase, a large-scale, multicentric repository of isotropic 0.1 mm\nultra-high-resolution CT (U-HRCT) images and associated clinical data dedicated\nto ear diseases. UltraEar recruits patients from 11 tertiary hospitals between\nOctober 2020 and October 2035, integrating U-HRCT images, structured CT\nreports, and comprehensive clinical information, including demographics,\naudiometric profiles, surgical records, and pathological findings. A broad\nspectrum of otologic disorders is covered, such as otitis media, cholesteatoma,\nossicular chain malformation, temporal bone fracture, inner ear malformation,\ncochlear aperture stenosis, enlarged vestibular aqueduct, and sigmoid sinus\nbony deficiency. Standardized preprocessing pipelines have been developed for\ngeometric calibration, image annotation, and multi-structure segmentation. All\npersonal identifiers in DICOM headers and metadata are removed or anonymized to\nensure compliance with data privacy regulation. Data collection and curation\nare coordinated through monthly expert panel meetings, with secure storage on\nan offline cloud system. UltraEar provides an unprecedented\nultra-high-resolution reference atlas with both technical fidelity and clinical\nrelevance. This resource has significant potential to advance radiological\nresearch, enable development and validation of AI algorithms, serve as an\neducational tool for training in otologic imaging, and support\nmulti-institutional collaborative studies. UltraEar will be continuously\nupdated and expanded, ensuring long-term accessibility and usability for the\nglobal otologic research community.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20141v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20141v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.244,
      "weak_supervision_score": 0.289,
      "diffusion_reasoning_score": 0.255,
      "distributed_training_score": 0.287,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20144",
      "title": "Navigating the EU AI Act: Foreseeable Challenges in Qualifying Deep\n  Learning-Based Automated Inspections of Class III Medical Devices",
      "authors": [
        "Julio Zanon Diaz",
        "Tommy Brennan",
        "Peter Corcoran"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As deep learning (DL) technologies advance, their application in automated\nvisual inspection for Class III medical devices offers significant potential to\nenhance quality assurance and reduce human error. However, the adoption of such\nAI-based systems introduces new regulatory complexities-particularly under the\nEU Artificial Intelligence (AI) Act, which imposes high-risk system obligations\nthat differ in scope and depth from established regulatory frameworks such as\nthe Medical Device Regulation (MDR) and the U.S. FDA Quality System Regulation\n(QSR). This paper presents a high-level technical assessment of the foreseeable\nchallenges that manufacturers are likely to encounter when qualifying DL-based\nautomated inspections -- specifically static models -- within the existing\nmedical device compliance landscape. It examines divergences in risk management\nprinciples, dataset governance, model validation, explainability requirements,\nand post-deployment monitoring obligations. The discussion also explores\npotential implementation strategies and highlights areas of uncertainty,\nincluding data retention burdens, global compliance implications, and the\npractical difficulties of achieving statistical significance in validation with\nlimited defect data. Disclaimer: This paper presents a technical perspective\nand does not constitute legal or regulatory advice.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20144v2",
      "pdf_url": "http://arxiv.org/pdf/2508.20144v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.361,
      "datasets_score": 0.4,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper discusses challenges in dataset governance and the difficulties of achieving statistical significance in validation due to limited defect data, which are relevant to how datasets are managed in AI applications for medical devices. However, it does not focus on creating, analyzing, benchmarking, or evaluating datasets directly; instead, it addresses these aspects peripherally in the context of regulatory compliance under the EU AI Act. Thus, the paper's main contribution is more about regulatory implications than core dataset research.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20148",
      "title": "The Anatomy of a Personal Health Agent",
      "authors": [
        "A. Ali Heydari",
        "Ken Gu",
        "Vidya Srinivas",
        "Hong Yu",
        "Zhihan Zhang",
        "Yuwei Zhang",
        "Akshay Paruchuri",
        "Qian He",
        "Hamid Palangi",
        "Nova Hammerquist",
        "Ahmed A. Metwally",
        "Brent Winslow",
        "Yubin Kim",
        "Kumar Ayush",
        "Yuzhe Yang",
        "Girish Narayanswamy",
        "Maxwell A. Xu",
        "Jake Garrison",
        "Amy Aremnto Lee",
        "Jenny Vafeiadou",
        "Ben Graef",
        "Isaac R. Galatzer-Levy",
        "Erik Schenck",
        "Andrew Barakat",
        "Javier Perez",
        "Jacqueline Shreibati",
        "John Hernandez",
        "Anthony Z. Faranesh",
        "Javier L. Prieto",
        "Connor Heneghan",
        "Yun Liu",
        "Jiening Zhan",
        "Mark Malhotra",
        "Shwetak Patel",
        "Tim Althoff",
        "Xin Liu",
        "Daniel McDuff",
        "Xuhai \"Orson\" Xu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Health is a fundamental pillar of human wellness, and the rapid advancements\nin large language models (LLMs) have driven the development of a new generation\nof health agents. However, the application of health agents to fulfill the\ndiverse needs of individuals in daily non-clinical settings is underexplored.\nIn this work, we aim to build a comprehensive personal health agent that is\nable to reason about multimodal data from everyday consumer wellness devices\nand common personal health records, and provide personalized health\nrecommendations. To understand end-users' needs when interacting with such an\nassistant, we conducted an in-depth analysis of web search and health forum\nqueries, alongside qualitative insights from users and health experts gathered\nthrough a user-centered design process. Based on these findings, we identified\nthree major categories of consumer health needs, each of which is supported by\na specialist sub-agent: (1) a data science agent that analyzes personal\ntime-series wearable and health record data, (2) a health domain expert agent\nthat integrates users' health and contextual data to generate accurate,\npersonalized insights, and (3) a health coach agent that synthesizes data\ninsights, guiding users using a specified psychological strategy and tracking\nusers' progress. Furthermore, we propose and develop the Personal Health Agent\n(PHA), a multi-agent framework that enables dynamic, personalized interactions\nto address individual health needs. To evaluate each sub-agent and the\nmulti-agent system, we conducted automated and human evaluations across 10\nbenchmark tasks, involving more than 7,000 annotations and 1,100 hours of\neffort from health experts and end-users. Our work represents the most\ncomprehensive evaluation of a health agent to date and establishes a strong\nfoundation towards the futuristic vision of a personal health agent accessible\nto everyone.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20148v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20148v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.421,
      "weak_supervision_score": 0.305,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.322,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development and evaluation of a personal health agent using large language models, focusing on multimodal data analysis, user needs, and sub-agents for health recommendations. While it involves human evaluations and feedback for design and assessment, it does not mention training a reward model on human-ranked data or using reinforcement learning to fine-tune models, which are core to RLHF. Thus, it does not align with the specific definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20151",
      "title": "IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent\n  Reasoning and Selective Query Refinement",
      "authors": [
        "Yuanzhe Shen",
        "Zisu Huang",
        "Zhengkang Guo",
        "Yide Liu",
        "Guanxu Chen",
        "Ruicheng Yin",
        "Xiaoqing Zheng",
        "Xuanjing Huang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) has driven their\nadoption across diverse domains, yet their ability to generate harmful content\nposes significant safety challenges. While extensive research has focused on\nmitigating harmful outputs, such efforts often come at the cost of excessively\nrejecting harmless prompts. Striking a balance among safety, over-refusal, and\nutility remains a critical challenge. In this work, we introduce\nIntentionReasoner, a novel safeguard mechanism that leverages a dedicated guard\nmodel to perform intent reasoning, multi-level safety classification, and query\nrewriting to neutralize potentially harmful intent in edge-case queries.\nSpecifically, we first construct a comprehensive dataset comprising\napproximately 163,000 queries, each annotated with intent reasoning, safety\nlabels, and rewritten versions. Supervised fine-tuning is then applied to equip\nthe guard model with foundational capabilities in format adherence, intent\nanalysis, and safe rewriting. Finally, we apply a tailored multi-reward\noptimization strategy that integrates rule-based heuristics and reward model\nsignals within a reinforcement learning framework to further enhance\nperformance. Extensive experiments show that IntentionReasoner excels in\nmultiple safeguard benchmarks, generation quality evaluations, and jailbreak\nattack scenarios, significantly enhancing safety while effectively reducing\nover-refusal rates and improving the quality of responses.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20151v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20151v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.484,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.353,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses a reinforcement learning framework (GRPO) with a composite reward function that includes reward model signals, which aligns somewhat with RLHF concepts. However, it does not explicitly confirm that the reward model is trained on human-ranked data, a key requirement for RLHF, as the paper focuses on rule-based heuristics and general reward signals rather than direct human feedback.",
      "weak_supervision_justification": "The paper describes constructing a 163K-sample dataset by leveraging advanced LLMs to programmatically generate annotations for intent reasoning, safety labels, and query rewrites, which involves noisy or imprecise sources rather than perfect hand-labeling. This directly matches weak supervision techniques for training models.",
      "diffusion_reasoning_justification": "The paper focuses on intent reasoning, multi-level classification, and query rewriting using supervised fine-tuning and reinforcement learning, but it does not involve diffusion models, iterative refinement of a chain-of-thought as a single entity, or any multi-step logical reasoning process based on diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces IntentionReasoner, a novel safeguard mechanism for large language models (LLMs) that uses intent reasoning, multi-level safety classification, and selective query rewriting to enhance safety while minimizing over-refusal of harmless prompts. The methodology involves constructing a 163,000-query dataset, applying supervised fine-tuning to develop core capabilities, and employing reinforcement learning with a multi-reward optimization to refine the model; experiments demonstrate superior performance in safety benchmarks, reduced over-refusal rates, and improved response quality.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining intent reasoning with multi-level classification and query rewriting, building on existing guard models like GuardReasoner to address over-refusal more effectively, though it does not introduce a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in LLM safety by providing a more nuanced approach to query handling, potentially leading to better real-world applications and citations within the AI safety subfield.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to LLM safety mechanisms with practical innovations that enhance performance, making it important for researchers in AI ethics and safety to be aware of, though it may not be essential for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/4f83828e88968731f077c6ea39277e5d8cbb279e",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 8,
      "average_h_index": 2.5,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Yuanzhe Shen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2372207040"
        },
        {
          "name": "Zisu Huang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2309201788"
        },
        {
          "name": "Zhengkang Guo",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2308068216"
        },
        {
          "name": "Yide Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377941573"
        },
        {
          "name": "Guanxu Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379246017"
        },
        {
          "name": "Ruicheng Yin",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2292032843"
        },
        {
          "name": "Xiaoqing Zheng",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2257315404"
        },
        {
          "name": "Xuanjing Huang",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2257129987"
        }
      ]
    },
    {
      "id": "2508.20176",
      "title": "RelAItionship Building: Analyzing Recruitment Strategies for\n  Participatory AI",
      "authors": [
        "Eugene Kim",
        "Vaibhav Balloli",
        "Berelian Karimian",
        "Elizabeth Bondi-Kelly",
        "Benjamin Fish"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Participatory AI, in which impacted community members and other stakeholders\nare involved in the design and development of AI systems, holds promise as a\nway to ensure AI is developed to meet their needs and reflect their values.\nHowever, the process of identifying, reaching out, and engaging with all\nrelevant stakeholder groups, which we refer to as recruitment methodology, is\nstill a practical challenge in AI projects striving to adopt participatory\npractices. In this paper, we investigate the challenges that researchers face\nwhen designing and executing recruitment methodology for Participatory AI\nprojects, and the implications of current recruitment practice for\nParticipatory AI. First, we describe the recruitment methodologies used in AI\nprojects using a corpus of 37 projects to capture the diversity of practices in\nthe field and perform an initial analysis on the documentation of recruitment\npractices, as well as specific strategies that researchers use to meet goals of\nequity and empowerment. To complement this analysis, we interview five AI\nresearchers to learn about the outcomes of recruitment methodologies. We find\nthat these outcomes are shaped by structural conditions of their work,\nresearchers' own goals and expectations, and the relationships built from the\nrecruitment methodology and subsequent collaboration. Based on these analyses,\nwe provide recommendations for designing and executing relationship-forward\nrecruitment methods, as well as reflexive recruitment documentation practices\nfor Participatory AI researchers.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20176v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20176v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.308,
      "distributed_training_score": 0.321,
      "datasets_score": 0.405,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Not Relevant",
      "rlhf_justification": "The paper focuses on recruitment strategies and participatory methods in AI development, emphasizing ethical and social aspects like stakeholder engagement and equity. It does not involve training AI models with human feedback, reward models, or reinforcement learning techniques, which are central to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper analyzes a corpus of 37 AI projects to study recruitment practices, but this is not related to creating, analyzing, benchmarking, or evaluating datasets for machine learning or AI applications. It deals with qualitative research on participatory methods, not ML-specific datasets.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20181",
      "title": "Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference\n  Optimization",
      "authors": [
        "Alberto Compagnoni",
        "Davide Caffagni",
        "Nicholas Moratelli",
        "Lorenzo Baraldi",
        "Marcella Cornia",
        "Rita Cucchiara"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) emerge as a unified interface to\naddress a multitude of tasks, ranging from NLP to computer vision. Despite\nshowcasing state-of-the-art results in many benchmarks, a long-standing issue\nis the tendency of MLLMs to hallucinate, that is to generate answers to the\nuser's query that are not reflected in the visual input. In this paper, we\naddress the problem of hallucinations as an alignment problem, seeking to steer\nthe MLLM so that it prefers generating content without hallucinations. In\ncontrast to recent approaches that require complicated pipelines to build\nsynthetic preference data for alignment training, often relying on proprietary\nmodels, we capitalize on the well-known CHAIR metric, originally proposed to\ngauge the degree of hallucinations in image captioning. Given a pair of\ngenerated answers, we leverage CHAIR to distinguish winner and loser options\n(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf\nMLLMs via Direct Preference Optimization (DPO). The resulting method, which we\nrefer to as CHAIR-DPO, effectively diminishes the amount of hallucinated\nanswers on several hallucination benchmarks, demonstrating the effectiveness of\nfine-tuning the MLLM with a CHAIR-based reward. Source code and trained models\nare publicly available at https://github.com/aimagelab/CHAIR-DPO.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20181v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20181v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.501,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.379,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses Direct Preference Optimization (DPO), which is inspired by RLHF techniques for alignment, but it does not involve training a separate reward model on human-ranked data or using reinforcement learning directly. Instead, it automates preference data with the CHAIR metric, making it only loosely connected to RLHF.",
      "weak_supervision_justification": "The paper employs the CHAIR metric to programmatically generate preference labels for training data by ranking responses based on hallucination levels, which aligns with weak supervision as it uses an automated, noisy source to create labels without relying on hand-annotated data.",
      "diffusion_reasoning_justification": "The paper focuses on mitigating hallucinations in MLLMs using preference optimization and the CHAIR metric, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the issue of hallucinations in Multimodal Large Language Models (MLLMs), where models generate content not supported by visual inputs, by proposing CHAIR-DPO, a method that uses the CHAIR metric to evaluate and select preferred responses with lower hallucination rates from pairs of generated answers. The authors collect preference data based on CHAIR scores and fine-tune off-the-shelf MLLMs using Direct Preference Optimization (DPO), demonstrating significant reductions in hallucinations across benchmarks like AMBER, CHAIR-MSCOCO, and Object HalBench, while maintaining the models' original capabilities.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of the existing CHAIR metric with Direct Preference Optimization to create preference data for hallucination mitigation, offering a notable improvement over methods that rely on proprietary models. While it builds on established techniques, it innovatively applies them in a new context for MLLMs.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of multimodal AI, as it provides an efficient, accessible method for reducing hallucinations in MLLMs. However, its influence may be limited to specific applications in computer vision and language processing rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a high-quality contribution by effectively addressing a persistent issue in MLLMs, making it valuable for researchers focused on AI alignment and hallucination reduction. It represents a strong advancement that enhances awareness of practical techniques in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/17b4d09c27ad1191325ec50486d358f7cd20e68c",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 31,
      "average_h_index": 12.833333333333334,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Alberto Compagnoni",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2377789532"
        },
        {
          "name": "Davide Caffagni",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2238815102"
        },
        {
          "name": "Nicholas Moratelli",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2202986093"
        },
        {
          "name": "L. Baraldi",
          "h_index": 31,
          "profile_url": "https://www.semanticscholar.org/author/1843795"
        },
        {
          "name": "Marcella Cornia",
          "h_index": 29,
          "profile_url": "https://www.semanticscholar.org/author/3468983"
        },
        {
          "name": "Rita Cucchiara",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2303850502"
        }
      ]
    },
    {
      "id": "2508.20182",
      "title": "SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization",
      "authors": [
        "Yang Su",
        "Shunquan Tan",
        "Jiwu Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Driven by the new generation of multi-modal large models, such as Stable\nDiffusion (SD), image manipulation technologies have advanced rapidly, posing\nsignificant challenges to image forensics. However, existing image forgery\nlocalization methods, which heavily rely on labor-intensive and costly\nannotated data, are struggling to keep pace with these emerging image\nmanipulation technologies. To address these challenges, we are the first to\nintegrate both image generation and powerful perceptual capabilities of SD into\nan image forensic framework, enabling more efficient and accurate forgery\nlocalization. First, we theoretically show that the multi-modal architecture of\nSD can be conditioned on forgery-related information, enabling the model to\ninherently output forgery localization results. Then, building on this\nfoundation, we specifically leverage the multimodal framework of Stable\nDiffusionV3 (SD3) to enhance forgery localization performance.We leverage the\nmulti-modal processing capabilities of SD3 in the latent space by treating\nimage forgery residuals -- high-frequency signals extracted using specific\nhighpass filters -- as an explicit modality. This modality is fused into the\nlatent space during training to enhance forgery localization performance.\nNotably, our method fully preserves the latent features extracted by SD3,\nthereby retaining the rich semantic information of the input image.\nExperimental results show that our framework achieves up to 12% improvements in\nperformance on widely used benchmarking datasets compared to current\nstate-of-the-art image forgery localization models. Encouragingly, the model\ndemonstrates strong performance on forensic tasks involving real-world document\nforgery images and natural scene forging images, even when such data were\nentirely unseen during training.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20182v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20182v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.501,
      "distributed_training_score": 0.327,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on adapting Stable Diffusion for image forgery localization by integrating forgery residuals into its multi-modal architecture, primarily for visual processing and generation tasks. While Stable Diffusion is a diffusion model that involves iterative refinement, the paper does not use this process for multi-step logical reasoning, such as treating a 'Chain-of-Thought' as a single entity for holistic correction in complex logical tasks. Instead, it applies diffusion models to image forensics, which does not align with the topic's emphasis on logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20186",
      "title": "AI Propaganda factories with language models",
      "authors": [
        "Lukasz Olejnik"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "AI-powered influence operations can now be executed end-to-end on commodity\nhardware. We show that small language models produce coherent, persona-driven\npolitical messaging and can be evaluated automatically without human raters.\nTwo behavioural findings emerge. First, persona-over-model: persona design\nexplains behaviour more than model identity. Second, engagement as a stressor:\nwhen replies must counter-arguments, ideological adherence strengthens and the\nprevalence of extreme content increases. We demonstrate that fully automated\ninfluence-content production is within reach of both large and small actors.\nConsequently, defence should shift from restricting model access towards\nconversation-centric detection and disruption of campaigns and coordination\ninfrastructure. Paradoxically, the very consistency that enables these\noperations also provides a detection signature.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20186v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20186v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.474,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.337,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using small language models for automated content generation and evaluation in propaganda contexts, without any mention of training models with human feedback or using a reward model for fine-tuning via reinforcement learning. It relies on SLM-as-judge for assessment, which does not align with RLHF principles.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses SLMs for generating political messaging and persona fidelity, but it does not involve diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes. There is no component related to treating Chain-of-Thought as an entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20188",
      "title": "Grounding Multimodal Large Language Models with Quantitative Skin\n  Attributes: A Retrieval Study",
      "authors": [
        "Max Torop",
        "Masih Eskandar",
        "Nicholas Kurtansky",
        "Jinyang Liu",
        "Jochen Weber",
        "Octavia Camps",
        "Veronica Rotemberg",
        "Jennifer Dy",
        "Kivanc Kose"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Artificial Intelligence models have demonstrated significant success in\ndiagnosing skin diseases, including cancer, showing the potential to assist\nclinicians in their analysis. However, the interpretability of model\npredictions must be significantly improved before they can be used in practice.\nTo this end, we explore the combination of two promising approaches: Multimodal\nLarge Language Models (MLLMs) and quantitative attribute usage. MLLMs offer a\npotential avenue for increased interpretability, providing reasoning for\ndiagnosis in natural language through an interactive format. Separately, a\nnumber of quantitative attributes that are related to lesion appearance (e.g.,\nlesion area) have recently been found predictive of malignancy with high\naccuracy. Predictions grounded as a function of such concepts have the\npotential for improved interpretability. We provide evidence that MLLM\nembedding spaces can be grounded in such attributes, through fine-tuning to\npredict their values from images. Concretely, we evaluate this grounding in the\nembedding space through an attribute-specific content-based image retrieval\ncase study using the SLICE-3D dataset.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20188v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20188v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.315,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on fine-tuning Multimodal Large Language Models (MLLMs) for predicting quantitative skin attributes and using them for image retrieval, emphasizing interpretability in skin disease diagnosis. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20193",
      "title": "Enhancing Automatic Modulation Recognition With a Reconstruction-Driven\n  Vision Transformer Under Limited Labels",
      "authors": [
        "Hossein Ahmadi",
        "Banafsheh Saffari",
        "Sajjad Emdadi Mahdimahalleh",
        "Mohammad Esmaeil Safari",
        "Aria Ahmadi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "Automatic modulation recognition (AMR) is critical for cognitive radio,\nspectrum monitoring, and secure wireless communication. However, existing\nsolutions often rely on large labeled datasets or multi-stage training\npipelines, which limit scalability and generalization in practice. We propose a\nunified Vision Transformer (ViT) framework that integrates supervised,\nself-supervised, and reconstruction objectives. The model combines a ViT\nencoder, a lightweight convolutional decoder, and a linear classifier; the\nreconstruction branch maps augmented signals back to their originals, anchoring\nthe encoder to fine-grained I/Q structure. This strategy promotes robust,\ndiscriminative feature learning during pretraining, while partial label\nsupervision in fine-tuning enables effective classification with limited\nlabels. On the RML2018.01A dataset, our approach outperforms supervised CNN and\nViT baselines in low-label regimes, approaches ResNet-level accuracy with only\n15-20% labeled data, and maintains strong performance across varying SNR\nlevels. Overall, the framework provides a simple, generalizable, and\nlabel-efficient solution for AMR.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20193v2",
      "pdf_url": "http://arxiv.org/pdf/2508.20193v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.413,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.336,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves a semi-supervised framework that uses reconstruction-driven pretraining and partial label supervision to learn effectively with limited labels, directly aligning with weak supervision. It reduces reliance on large labeled datasets by programmatically leveraging self-supervised objectives to generate robust features, fitting the definition of training models with noisy or imprecise label sources.",
      "diffusion_reasoning_justification": "The paper focuses on a Vision Transformer framework for automatic modulation recognition with reconstruction and supervised objectives, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. There is no component related to treating a chain-of-thought as an entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces a unified Vision Transformer (ViT) framework for Automatic Modulation Recognition (AMR) that combines supervised, self-supervised, and reconstruction objectives to address challenges with limited labeled data. By employing a ViT encoder, a lightweight convolutional decoder for reconstructing augmented signals, and a linear classifier for fine-tuning, the approach enhances feature learning and achieves superior performance on the RML2018.01A dataset, outperforming supervised CNN and ViT baselines in low-label regimes while approaching ResNet-level accuracy with only 15-20% labels and maintaining robustness across varying SNR levels.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by integrating reconstruction objectives with ViT for AMR, cleverly combining existing ideas to enhance label-efficient learning. While not introducing an entirely new problem, it advances the state-of-the-art in handling limited labels through this unified framework.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in signal processing and computer vision subfields for developing more efficient AMR systems, particularly in data-scarce scenarios. However, its influence may be confined to specific applications like cognitive radio, limiting broader commercial impact.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a high-quality contribution to semi-supervised learning in AMR with practical advancements in label efficiency and robustness, making it valuable for researchers in wireless communication and machine learning. While not essential for all, it provides insights that could inform future work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/707c7ade9d48f08082f35757dc254ac70314bfe1",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 2,
      "average_h_index": 0.8,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Hossein Ahmadi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2357082389"
        },
        {
          "name": "Banafsheh Saffari",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2357082321"
        },
        {
          "name": "Sajjad Emdadi Mahdimahalleh",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2221124815"
        },
        {
          "name": "Mohammad Esmaeil Safari",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380025597"
        },
        {
          "name": "Aria Ahmadi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380025967"
        }
      ]
    },
    {
      "id": "2508.20195",
      "title": "AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and\n  Emergent Grammar Development",
      "authors": [
        "Nicanor I. Moldovan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "This paper presents the first documented case of artificial intelligence (AI)\nsystems engaging in collaborative esthetic creation through the development of\nendogenous semiotic protocols. Two interacting large language models (Claude\nSonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of\nmeta-semiotic awareness, recursive grammar development, and irreducible\ncollaborative esthetic synthesis. The interaction produced novel symbolic\noperators that functioned as operative grammar protocols, enabling the\nco-creation of a poetic work that could not have been generated by either\nsystem independently. This research introduces the concept of Trans-Semiotic\nCo-Creation Protocols (TSCP) and provides evidence for genuine inter-AI\nmeaning-making capabilities that extend beyond task coordination, to what could\nbe esthetic collaboration. Note: This report was generated by the AI agents\nwith minor human supervision.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20195v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20195v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.307,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.311,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses AI systems collaborating on esthetic creation through emergent semiotic protocols and grammar development, but it does not involve diffusion models, iterative refinement for logical tasks, or any adaptation of diffusion processes for Chain-of-Thought reasoning. There is no evidence of multi-step logical reasoning using a diffusion-based approach.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20206",
      "title": "Filter then Attend: Improving attention-based Time Series Forecasting\n  with Spectral Filtering",
      "authors": [
        "Elisha Dayag",
        "Nhat Thanh Van Tran",
        "Jack Xin"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Transformer-based models are at the forefront in long time-series forecasting\n(LTSF). While in many cases, these models are able to achieve state of the art\nresults, they suffer from a bias toward low-frequencies in the data and high\ncomputational and memory requirements. Recent work has established that\nlearnable frequency filters can be an integral part of a deep forecasting model\nby enhancing the model's spectral utilization. These works choose to use a\nmultilayer perceptron to process their filtered signals and thus do not solve\nthe issues found with transformer-based models. In this paper, we establish\nthat adding a filter to the beginning of transformer-based models enhances\ntheir performance in long time-series forecasting. We add learnable filters,\nwhich only add an additional $\\approx 1000$ parameters to several\ntransformer-based models and observe in multiple instances 5-10 \\% relative\nimprovement in forecasting performance. Additionally, we find that with filters\nadded, we are able to decrease the embedding dimension of our models, resulting\nin transformer-based architectures that are both smaller and more effective\nthan their non-filtering base models. We also conduct synthetic experiments to\nanalyze how the filters enable Transformer-based models to better utilize the\nfull spectrum for forecasting.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20206v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20206v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.325,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20210",
      "title": "InfinityHuman: Towards Long-Term Audio-Driven Human",
      "authors": [
        "Xiaodi Li",
        "Pan Xie",
        "Yi Ren",
        "Qijun Gan",
        "Chen Zhang",
        "Fangyuan Kong",
        "Xiang Yin",
        "Bingyue Peng",
        "Zehuan Yuan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Audio-driven human animation has attracted wide attention thanks to its\npractical applications. However, critical challenges remain in generating\nhigh-resolution, long-duration videos with consistent appearance and natural\nhand motions. Existing methods extend videos using overlapping motion frames\nbut suffer from error accumulation, leading to identity drift, color shifts,\nand scene instability. Additionally, hand movements are poorly modeled,\nresulting in noticeable distortions and misalignment with the audio. In this\nwork, we propose InfinityHuman, a coarse-to-fine framework that first generates\naudio-synchronized representations, then progressively refines them into\nhigh-resolution, long-duration videos using a pose-guided refiner. Since pose\nsequences are decoupled from appearance and resist temporal degradation, our\npose-guided refiner employs stable poses and the initial frame as a visual\nanchor to reduce drift and improve lip synchronization. Moreover, to enhance\nsemantic accuracy and gesture realism, we introduce a hand-specific reward\nmechanism trained with high-quality hand motion data. Experiments on the EMTD\nand HDTF datasets show that InfinityHuman achieves state-of-the-art performance\nin video quality, identity preservation, hand accuracy, and lip-sync. Ablation\nstudies further confirm the effectiveness of each module. Code will be made\npublic.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20210v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20210v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.315,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions a hand-specific reward mechanism trained on high-quality hand motion data to guide generation, which shares some conceptual similarities with reward models in RLHF. However, it does not involve human-ranked data, a separate reward model trained on human preferences, or explicit reinforcement learning for fine-tuning, making it only loosely related.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs a diffusion-based refiner for generating and refining video frames, but this is focused on visual animation tasks, not on adapting diffusion for multi-step logical reasoning or Chain-of-Thought processes as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20213",
      "title": "Collaborating with GenAI: Incentives and Replacements",
      "authors": [
        "Boaz Taitler",
        "Omer Ben-Porat"
      ],
      "categories": [
        "cs.GT (Computer Science and Game Theory)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rise of Generative AI (GenAI) is reshaping how workers contribute to\nshared projects. While workers can use GenAI to boost productivity or reduce\neffort, managers may use it to replace some workers entirely. We present a\ntheoretical framework to analyze how GenAI affects collaboration in such\nsettings. In our model, the manager selects a team to work on a shared task,\nwith GenAI substituting for unselected workers. Each worker selects how much\neffort to exert, and incurs a cost that increases with the level of effort. We\nshow that GenAI can lead workers to exert no effort, even if GenAI is almost\nineffective. We further show that the manager's optimization problem is\nNP-complete, and provide an efficient algorithm for the special class of\n(almost-) linear instances. Our analysis shows that even workers with low\nindividual value may play a critical role in sustaining overall output, and\nexcluding such workers can trigger a cascade. Finally, we conduct extensive\nsimulations to illustrate our theoretical findings.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20213v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20213v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.369,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper examines the impact of Generative AI on worker incentives, collaboration, and managerial decisions through a theoretical framework, focusing on game theory and simulations. It does not discuss techniques for training AI models, such as using human feedback to create reward models or fine-tuning via reinforcement learning. Since RLHF specifically involves aligning AI with human preferences through these methods, and the paper lacks any reference to such processes, it is not relevant to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20217",
      "title": "Prompting Strategies for Language Model-Based Item Generation in K-12\n  Education: Bridging the Gap Between Small and Large Language Models",
      "authors": [
        "Mohammad Amini",
        "Babak Ahmadi",
        "Xiaomeng Xiong",
        "Yilin Zhang",
        "Christopher Qiao"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study explores automatic generation (AIG) using language models to\ncreate multiple choice questions (MCQs) for morphological assessment, aiming to\nreduce the cost and inconsistency of manual test development. The study used a\ntwo-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B)\nwith a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven\nstructured prompting strategies, including zero-shot, few-shot,\nchain-of-thought, role-based, sequential, and combinations. Generated items\nwere assessed using automated metrics and expert scoring across five\ndimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate\nhuman scoring at scale. Results show that structured prompting, especially\nstrategies combining chain-of-thought and sequential design, significantly\nimproved Gemma's outputs. Gemma generally produced more construct-aligned and\ninstructionally appropriate items than GPT-3.5's zero-shot responses, with\nprompt design playing a key role in mid-size model performance. This study\ndemonstrates that structured prompting and efficient fine-tuning can enhance\nmidsized models for AIG under limited data conditions. We highlight the value\nof combining automated metrics, expert judgment, and large-model simulation to\nensure alignment with assessment goals. The proposed workflow offers a\npractical and scalable way to develop and validate language assessment items\nfor K-12.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20217v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20217v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.48,
      "distributed_training_score": 0.332,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper uses GPT-4.1, trained on expert-rated samples, to simulate human scoring, which involves programmatically generating labels from potentially noisy sources, aligning somewhat with weak supervision concepts. However, this is not the main focus; the primary contribution is on prompting strategies for item generation, with weak supervision elements only appearing in the evaluation process, making it peripheral.",
      "diffusion_reasoning_justification": "The paper discusses prompting strategies like chain-of-thought for reasoning in language models, but it does not involve diffusion models or iterative refinement processes for logical tasks as defined. There is no mention of adapting diffusion techniques, so this topic is not addressed.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20221",
      "title": "Spherical Vision Transformers for Audio-Visual Saliency Prediction in\n  360-Degree Videos",
      "authors": [
        "Mert Cokelek",
        "Halit Ozsoy",
        "Nevrez Imamoglu",
        "Cagri Ozcinar",
        "Inci Ayhan",
        "Erkut Erdem",
        "Aykut Erdem"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Omnidirectional videos (ODVs) are redefining viewer experiences in virtual\nreality (VR) by offering an unprecedented full field-of-view (FOV). This study\nextends the domain of saliency prediction to 360-degree environments,\naddressing the complexities of spherical distortion and the integration of\nspatial audio. Contextually, ODVs have transformed user experience by adding a\nspatial audio dimension that aligns sound direction with the viewer's\nperspective in spherical scenes. Motivated by the lack of comprehensive\ndatasets for 360-degree audio-visual saliency prediction, our study curates\nYT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying\naudio-visual conditions. Our goal is to explore how to utilize audio-visual\ncues to effectively predict visual saliency in 360-degree videos. Towards this\naim, we propose two novel saliency prediction models: SalViT360, a\nvision-transformer-based framework for ODVs equipped with spherical\ngeometry-aware spatio-temporal attention layers, and SalViT360-AV, which\nfurther incorporates transformer adapters conditioned on audio input. Our\nresults on a number of benchmark datasets, including our YT360-EyeTracking,\ndemonstrate that SalViT360 and SalViT360-AV significantly outperform existing\nmethods in predicting viewer attention in 360-degree scenes. Interpreting these\nresults, we suggest that integrating spatial audio cues in the model\narchitecture is crucial for accurate saliency prediction in omnidirectional\nvideos. Code and dataset will be available at\nhttps://cyberiada.github.io/SalViT360.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20221v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20221v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.29,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.29,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20224",
      "title": "The Role of Teacher Calibration in Knowledge Distillation",
      "authors": [
        "Suyoung Kim",
        "Seonguk Park",
        "Junhoo Lee",
        "Nojun Kwak"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Knowledge Distillation (KD) has emerged as an effective model compression\ntechnique in deep learning, enabling the transfer of knowledge from a large\nteacher model to a compact student model. While KD has demonstrated significant\nsuccess, it is not yet fully understood which factors contribute to improving\nthe student's performance. In this paper, we reveal a strong correlation\nbetween the teacher's calibration error and the student's accuracy. Therefore,\nwe claim that the calibration of the teacher model is an important factor for\neffective KD. Furthermore, we demonstrate that the performance of KD can be\nimproved by simply employing a calibration method that reduces the teacher's\ncalibration error. Our algorithm is versatile, demonstrating effectiveness\nacross various tasks from classification to detection. Moreover, it can be\neasily integrated with existing state-of-the-art methods, consistently\nachieving superior performance.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20224v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20224v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.388,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20227",
      "title": "A Novel Framework for Automated Explain Vision Model Using\n  Vision-Language Models",
      "authors": [
        "Phu-Vinh Nguyen",
        "Tan-Hanh Pham",
        "Chris Ngo",
        "Truong Son Hy"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The development of many vision models mainly focuses on improving their\nperformance using metrics such as accuracy, IoU, and mAP, with less attention\nto explainability due to the complexity of applying xAI methods to provide a\nmeaningful explanation of trained models. Although many existing xAI methods\naim to explain vision models sample-by-sample, methods explaining the general\nbehavior of vision models, which can only be captured after running on a large\ndataset, are still underexplored. Furthermore, understanding the behavior of\nvision models on general images can be very important to prevent biased\njudgments and help identify the model's trends and patterns. With the\napplication of Vision-Language Models, this paper proposes a pipeline to\nexplain vision models at both the sample and dataset levels. The proposed\npipeline can be used to discover failure cases and gain insights into vision\nmodels with minimal effort, thereby integrating vision model development with\nxAI analysis to advance image analysis.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20227v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20227v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.284,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a framework for explaining vision models using Vision-Language Models and CAM-based methods, emphasizing explainability at sample and dataset levels. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, which are core to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20232",
      "title": "ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation\n  for a Lightweight Residual CNN in Agricultural Embedded Systems",
      "authors": [
        "Mohamed Ohamouddou",
        "Said Ohamouddou",
        "Abdellatif El Afia",
        "Rafik Lasri"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This study proposes ATMS-KD (Adaptive Temperature and Mixed-Sample Knowledge\nDistillation), a novel framework for developing lightweight CNN models suitable\nfor resource-constrained agricultural environments. The framework combines\nadaptive temperature scheduling with mixed-sample augmentation to transfer\nknowledge from a MobileNetV3 Large teacher model (5.7\\,M parameters) to\nlightweight residual CNN students. Three student configurations were evaluated:\nCompact (1.3\\,M parameters), Standard (2.4\\,M parameters), and Enhanced (3.8\\,M\nparameters). The dataset used in this study consists of images of \\textit{Rosa\ndamascena} (Damask rose) collected from agricultural fields in the Dades Oasis,\nsoutheastern Morocco, providing a realistic benchmark for agricultural computer\nvision applications under diverse environmental conditions. Experimental\nevaluation on the Damascena rose maturity classification dataset demonstrated\nsignificant improvements over direct training methods. All student models\nachieved validation accuracies exceeding 96.7\\% with ATMS-KD compared to\n95--96\\% with direct training. The framework outperformed eleven established\nknowledge distillation methods, achieving 97.11\\% accuracy with the compact\nmodel -- a 1.60 percentage point improvement over the second-best approach\nwhile maintaining the lowest inference latency of 72.19\\,ms. Knowledge\nretention rates exceeded 99\\% for all configurations, demonstrating effective\nknowledge transfer regardless of student model capacity.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20232v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20232v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.425,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a knowledge distillation framework (ATMS-KD) for compressing CNN models in agricultural embedded systems, focusing on adaptive temperature scheduling and mixed-sample augmentation. It does not involve distributed training, parallel computing, multi-node machine learning, or any partitioning of data/computation across processors or nodes. Therefore, it has no connection to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20234",
      "title": "Validating Generative Agent-Based Models for Logistics and Supply Chain\n  Management Research",
      "authors": [
        "Vincent E. Castillo"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Generative Agent-Based Models (GABMs) powered by large language models (LLMs)\noffer promising potential for empirical logistics and supply chain management\n(LSCM) research by enabling realistic simulation of complex human behaviors.\nUnlike traditional agent-based models, GABMs generate human-like responses\nthrough natural language reasoning, which creates potential for new\nperspectives on emergent LSCM phenomena. However, the validity of LLMs as\nproxies for human behavior in LSCM simulations is unknown. This study evaluates\nLLM equivalence of human behavior through a controlled experiment examining\ndyadic customer-worker engagements in food delivery scenarios. I test six\nstate-of-the-art LLMs against 957 human participants (477 dyads) using a\nmoderated mediation design. This study reveals a need to validate GABMs on two\nlevels: (1) human equivalence testing, and (2) decision process validation.\nResults reveal GABMs can effectively simulate human behaviors in LSCM; however,\nan equivalence-versus-process paradox emerges. While a series of Two One-Sided\nTests (TOST) for equivalence reveals some LLMs demonstrate surface-level\nequivalence to humans, structural equation modeling (SEM) reveals artificial\ndecision processes not present in human participants for some LLMs. These\nfindings show GABMs as a potentially viable methodological instrument in LSCM\nwith proper validation checks. The dual-validation framework also provides LSCM\nresearchers with a guide to rigorous GABM development. For practitioners, this\nstudy offers evidence-based assessment for LLM selection for operational tasks.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20234v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20234v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.329,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.325,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates LLMs (e.g., GPT-4o) for simulating human behavior in logistics simulations, and these models are often trained using RLHF. However, the paper does not discuss, implement, or contribute to RLHF methods; it focuses on validation of GABMs, making the connection indirect.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses LLMs for natural language reasoning in simulations but does not mention or involve diffusion-based processes for multi-step logical reasoning. There is no evidence of iterative refinement or diffusion models in the methodology or contributions.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20236",
      "title": "The Mathematician's Assistant: Integrating AI into Research Practice",
      "authors": [
        "Jonas Henkel"
      ],
      "categories": [
        "math.HO (History and Overview)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The rapid development of artificial intelligence (AI), marked by\nbreakthroughs like 'AlphaEvolve' and 'Gemini Deep Think', is beginning to offer\npowerful new tools that have the potential to significantly alter the research\npractice in many areas of mathematics. This paper explores the current\nlandscape of publicly accessible large language models (LLMs) in a mathematical\nresearch context, based on developments up to August 2, 2025. Our analysis of\nrecent benchmarks, such as MathArena and the Open Proof Corpus (Balunovi\\'c et\nal., 2025; Dekoninck et al., 2025), reveals a complex duality: while\nstate-of-the-art models demonstrate strong abilities in solving problems and\nevaluating proofs, they also exhibit systematic flaws, including a lack of\nself-critique and a model depending discrepancy between final-answer accuracy\nand full-proof validity.\n  Based on these findings, we propose a durable framework for integrating AI\ninto the research workflow, centered on the principle of the augmented\nmathematician. In this model, the AI functions as a copilot under the critical\nguidance of the human researcher, an approach distilled into five guiding\nprinciples for effective and responsible use. We then systematically explore\nseven fundamental ways AI can be applied across the research lifecycle, from\ncreativity and ideation to the final writing process, demonstrating how these\nprinciples translate into concrete practice.\n  We conclude that the primary role of AI is currently augmentation rather than\nautomation. This requires a new skill set focused on strategic prompting,\ncritical verification, and methodological rigor in order to effectively use\nthese powerful tools.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20236v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20236v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.344,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on integrating AI tools like LLMs into mathematical research practices and proposes a framework for their use, but it does not discuss or involve training AI models with human feedback, reward models, or reinforcement learning techniques. The content is centered on application and augmentation, not RLHF methodologies.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines the use of AI in mathematical research, including problem-solving and proof evaluation, but it does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning as described. It discusses general AI capabilities without referencing diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20243",
      "title": "Linking heterogeneous microstructure informatics with expert\n  characterization knowledge through customized and hybrid vision-language\n  representations for industrial qualification",
      "authors": [
        "Mutahar Safdar",
        "Gentry Wood",
        "Max Zimmermann",
        "Guy Lamouche",
        "Priti Wanjara",
        "Yaoyao Fiona Zhao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Rapid and reliable qualification of advanced materials remains a bottleneck\nin industrial manufacturing, particularly for heterogeneous structures produced\nvia non-conventional additive manufacturing processes. This study introduces a\nnovel framework that links microstructure informatics with a range of expert\ncharacterization knowledge using customized and hybrid vision-language\nrepresentations (VLRs). By integrating deep semantic segmentation with\npre-trained multi-modal models (CLIP and FLAVA), we encode both visual\nmicrostructural data and textual expert assessments into shared\nrepresentations. To overcome limitations in general-purpose embeddings, we\ndevelop a customized similarity-based representation that incorporates both\npositive and negative references from expert-annotated images and their\nassociated textual descriptions. This allows zero-shot classification of\npreviously unseen microstructures through a net similarity scoring approach.\nValidation on an additively manufactured metal matrix composite dataset\ndemonstrates the framework's ability to distinguish between acceptable and\ndefective samples across a range of characterization criteria. Comparative\nanalysis reveals that FLAVA model offers higher visual sensitivity, while the\nCLIP model provides consistent alignment with the textual criteria. Z-score\nnormalization adjusts raw unimodal and cross-modal similarity scores based on\ntheir local dataset-driven distributions, enabling more effective alignment and\nclassification in the hybrid vision-language framework. The proposed method\nenhances traceability and interpretability in qualification pipelines by\nenabling human-in-the-loop decision-making without task-specific model\nretraining. By advancing semantic interoperability between raw data and expert\nknowledge, this work contributes toward scalable and domain-adaptable\nqualification strategies in engineering informatics.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20243v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20243v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.394,
      "distributed_training_score": 0.349,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20244",
      "title": "Do Students Rely on AI? Analysis of Student-ChatGPT Conversations from a\n  Field Study",
      "authors": [
        "Jiayu Zheng",
        "Lingxin Hao",
        "Kelun Lu",
        "Ashi Garg",
        "Mike Reese",
        "Melo-Jean Yap",
        "I-Jeng Wang",
        "Xingyun Wu",
        "Wenrui Huang",
        "Jenna Hoffman",
        "Ariane Kelly",
        "My Le",
        "Ryan Zhang",
        "Yanyu Lin",
        "Muhammad Faayez",
        "Anqi Liu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study explores how college students interact with generative AI\n(ChatGPT-4) during educational quizzes, focusing on reliance and predictors of\nAI adoption. Conducted at the early stages of ChatGPT implementation, when\nstudents had limited familiarity with the tool, this field study analyzed 315\nstudent-AI conversations during a brief, quiz-based scenario across various\nSTEM courses. A novel four-stage reliance taxonomy was introduced to capture\nstudents' reliance patterns, distinguishing AI competence, relevance, adoption,\nand students' final answer correctness. Three findings emerged. First, students\nexhibited overall low reliance on AI and many of them could not effectively use\nAI for learning. Second, negative reliance patterns often persisted across\ninteractions, highlighting students' difficulty in effectively shifting\nstrategies after unsuccessful initial experiences. Third, certain behavioral\nmetrics strongly predicted AI reliance, highlighting potential behavioral\nmechanisms to explain AI adoption. The study's findings underline critical\nimplications for ethical AI integration in education and the broader field. It\nemphasizes the need for enhanced onboarding processes to improve student's\nfamiliarity and effective use of AI tools. Furthermore, AI interfaces should be\ndesigned with reliance-calibration mechanisms to enhance appropriate reliance.\nUltimately, this research advances understanding of AI reliance dynamics,\nproviding foundational insights for ethically sound and cognitively enriching\nAI practices.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20244v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20244v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.441,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.338,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper examines student interactions with ChatGPT-4 in educational quizzes, focusing on reliance patterns and behavioral analyses, but it does not involve or discuss reinforcement learning from human feedback. There is no mention of training AI models using human-ranked data, reward models, or fine-tuning via reinforcement learning; instead, it analyzes existing AI usage in a real-world context.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20250",
      "title": "Efficient and Privacy-Protecting Background Removal for 2D Video\n  Streaming using iPhone 15 Pro Max LiDAR",
      "authors": [
        "Jessica Kinnevan",
        "Naifa Alqahtani",
        "Toral Chauhan"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Light Detection and Ranging (LiDAR) technology in consumer-grade mobile\ndevices can be used as a replacement for traditional background removal and\ncompositing techniques. Unlike approaches such as chroma keying and trained AI\nmodels, LiDAR's depth information is independent of subject lighting, and\nperforms equally well in low-light and well-lit environments. We integrate the\nLiDAR and color cameras on the iPhone 15 Pro Max with GPU-based image\nprocessing. We use Apple's SwiftUI and Swift frameworks for user interface and\nbackend development, and Metal Shader Language (MSL) for realtime image\nenhancement at the standard iPhone streaming frame rate of 60 frames per\nsecond. The only meaningful limitations of the technology are the streaming\nbandwidth of the depth data, which currently reduces the depth map resolution\nto 320x240, and any pre-existing limitations of the LiDAR IR laser to reflect\naccurate depth from some materials. If the LiDAR resolution on a mobile device\nlike the iPhone can be improved to match the color image resolution, LiDAR\ncould feasibly become the preeminent method of background removal for video\napplications and photography.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20250v2",
      "pdf_url": "http://arxiv.org/pdf/2508.20250v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.317,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.304,
      "distributed_training_score": 0.288,
      "datasets_score": 0.281,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20256",
      "title": "MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated\n  Segmentation of Perivascular Spaces",
      "authors": [
        "Zhen Xuen Brandon Low",
        "Rory Zhang",
        "Hang Min",
        "William Pham",
        "Lucy Vivash",
        "Jasmine Moses",
        "Miranda Lynch",
        "Karina Dorfman",
        "Cassandra Marotta",
        "Shaun Koh",
        "Jacob Bunyamin",
        "Ella Rowsthorn",
        "Alex Jarema",
        "Himashi Peiris",
        "Zhaolin Chen",
        "Sandy R. Shultz",
        "David K. Wright",
        "Dexiao Kong",
        "Sharon L. Naismith",
        "Terence J. O'Brien",
        "Ying Xia",
        "Meng Law",
        "Benjamin Sinclair"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers\nof cerebral small vessel disease, Alzheimer's disease, stroke, and\naging-related neurodegeneration. However, manual segmentation of PVS is\ntime-consuming and subject to moderate inter-rater reliability, while existing\nautomated deep learning models have moderate performance and typically fail to\ngeneralize across diverse clinical and research MRI datasets. We adapted\nMedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network,\nfor automated PVS segmentation. Two models were trained: one using a\nhomogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human\nConnectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous\nT1-weighted (T1w) MRI volumes from seven studies across six scanners. Model\nperformance was evaluated using internal 5-fold cross validation (5FCV) and\nleave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on\nthe T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of\n0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater\nreliability of that dataset, and the highest yet reported in the literature.\nThe same models trained on the T1w images of the HCP-Aging dataset achieved a\nsubstantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had\nvoxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and\ncluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG).\nMedNeXt-L-k5 provides an efficient solution for automated PVS segmentation\nacross diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the\nnnU-Net, indicating that the attention-based mechanisms present in\ntransformer-inspired models to provide global context are not required for high\naccuracy in PVS segmentation.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20256v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20256v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.278,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.355,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20258",
      "title": "SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization",
      "authors": [
        "Arya Tschand",
        "Muhammad Awad",
        "Ryan Swann",
        "Kesavan Ramakrishnan",
        "Jeffrey Ma",
        "Keith Lowery",
        "Ganesh Dasika",
        "Vijay Janapa Reddi"
      ],
      "categories": [
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) have shown progress in GPU kernel performance\nengineering using inefficient search-based methods that optimize around\nruntime. Any existing approach lacks a key characteristic that human\nperformance engineers rely on for near-optimal utilization --\nhardware-awareness. By leveraging the workload's specific memory access\npatterns, architecture specifications, filtered profiling logs, and reflections\non historical performance, we can make software-level optimizations that are\ntailored to the underlying hardware. SwizzlePerf automatically generates\nspatial optimizations for GPU kernels on disaggregated architectures by giving\nLLMs explicit hardware-awareness.\n  For a GEMM kernel, SwizzlePerf takes less than 5 minutes to generate the same\nhardware-specific optimal swizzling pattern that took expert performance\nengineers 2 weeks to find. On a suite of 10 diverse ML and Science kernels,\nSwizzlePerf can generate swizzling patterns for 9 of the kernels that achieve\nup to a 2.06x speedup and 70% improvement in L2 hit rate. This work is the\nfirst of many steps toward systematically creating hardware-aware LLM\nperformance engineering agents.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20258v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20258v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.461,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on hardware-aware optimizations for GPU kernels using LLMs, specifically for improving memory access patterns and performance on single GPUs with disaggregated architectures. While GPU optimizations like those in SwizzlePerf could indirectly support distributed training by enhancing efficiency in parallel computing environments, the paper does not address key elements of distributed training, such as multi-node partitioning of data or models, or strategies for accelerating training across processors. Thus, it is only tangentially related through the shared use of GPUs in broader parallel computing contexts.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20262",
      "title": "AI reasoning effort mirrors human decision time on content moderation\n  tasks",
      "authors": [
        "Thomas Davidson"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Large language models can now generate intermediate reasoning steps before\nproducing answers, improving performance on difficult problems. This study uses\na paired conjoint experiment on a content moderation task to examine parallels\nbetween human decision times and model reasoning effort. Across three frontier\nmodels, reasoning effort consistently predicts human decision time. Both humans\nand models expended greater effort when important variables were held constant,\nsuggesting similar sensitivity to task difficulty and patterns consistent with\ndual-process theories of cognition. These findings show that AI reasoning\neffort mirrors human processing time in subjective judgments and underscores\nthe potential of reasoning traces for interpretability and decision-making.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20262v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20262v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.489,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.567,
      "distributed_training_score": 0.353,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions that frontier models are optimized for intermediate reasoning via reinforcement learning, citing sources like {lightman_lets_2023} and {yao_tree_2023}, which could imply alignment techniques similar to RLHF. However, it does not explicitly describe using human feedback for training a reward model or fine-tuning, focusing instead on reasoning parallels in content moderation. Thus, reinforcement learning is referenced but not central to the main contribution.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses chain-of-thought prompting and reasoning traces in large language models, drawing analogies to dual-process theories, but makes no mention of diffusion models, iterative refinement processes, or adapting diffusion for multi-step logical reasoning. The main contribution is on human-AI reasoning parallels, with no connection to diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20265",
      "title": "Plug-in Feedback Self-adaptive Attention in CLIP for Training-free\n  Open-Vocabulary Segmentation",
      "authors": [
        "Zhixiang Chi",
        "Yanan Wu",
        "Li Gu",
        "Huan Liu",
        "Ziqiang Wang",
        "Yang Zhang",
        "Yang Wang",
        "Konstantinos N. Plataniotis"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "CLIP exhibits strong visual-textual alignment but struggle with\nopen-vocabulary segmentation due to poor localization. Prior methods enhance\nspatial coherence by modifying intermediate attention. But, this coherence\nisn't consistently propagated to the final output due to subsequent operations\nsuch as projections. Additionally, intermediate attention lacks direct\ninteraction with text representations, such semantic discrepancy limits the\nfull potential of CLIP.\n  In this work, we propose a training-free, feedback-driven self-adaptive\nframework that adapts output-based patch-level correspondences back to the\nintermediate attention. The output predictions, being the culmination of the\nmodel's processing, encapsulate the most comprehensive visual and textual\nsemantics about each patch. Our approach enhances semantic consistency between\ninternal representations and final predictions by leveraging the model's\noutputs as a stronger spatial coherence prior. We design key modules, including\nattention isolation, confidence-based pruning for sparse adaptation, and\nadaptation ensemble, to effectively feedback the output coherence cues. Our\nmethod functions as a plug-in module, seamlessly integrating into four\nstate-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We\nfurther validate our framework across multiple attention types (Q-K, self-self,\nand Proxy augmented with MAE, SAM, and DINO). Our approach consistently\nimproves their performance across eight benchmarks.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20265v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20265v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.36,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a training-free framework for enhancing CLIP's open-vocabulary segmentation by using internal model outputs to refine attention, without involving human feedback, reward models, or reinforcement learning techniques. It focuses on self-adaptive mechanisms based on the model's own predictions, which differs fundamentally from RLHF systems that rely on human-ranked data for alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20279",
      "title": "How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task\n  Reasoning, and Answer Decoding",
      "authors": [
        "Zhuoran Yu",
        "Yong Jae Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated strong performance\nacross a wide range of vision-language tasks, yet their internal processing\ndynamics remain underexplored. In this work, we introduce a probing framework\nto systematically analyze how MLLMs process visual and textual inputs across\nlayers. We train linear classifiers to predict fine-grained visual categories\n(e.g., dog breeds) from token embeddings extracted at each layer, using a\nstandardized anchor question. To uncover the functional roles of different\nlayers, we evaluate these probes under three types of controlled prompt\nvariations: (1) lexical variants that test sensitivity to surface-level\nchanges, (2) semantic negation variants that flip the expected answer by\nmodifying the visual concept in the prompt, and (3) output format variants that\npreserve reasoning but alter the answer format. Applying our framework to\nLLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent\nstage-wise structure in which early layers perform visual grounding, middle\nlayers support lexical integration and semantic reasoning, and final layers\nprepare task-specific outputs. We further show that while the overall\nstage-wise structure remains stable across variations in visual tokenization,\ninstruction tuning data, and pretraining corpus, the specific layer allocation\nto each stage shifts notably with changes in the base LLM architecture. Our\nfindings provide a unified perspective on the layer-wise organization of MLLMs\nand offer a lightweight, model-agnostic approach for analyzing multimodal\nrepresentation dynamics.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20279v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20279v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.496,
      "distributed_training_score": 0.348,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a probing framework to analyze how Multimodal Large Language Models (MLLMs) process visual and textual inputs across layers, focusing on visual grounding, task reasoning, and answer decoding. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20282",
      "title": "Network-Level Prompt and Trait Leakage in Local Research Agents",
      "authors": [
        "Hyejun Jeong",
        "Mohammadreza Teymoorianfard",
        "Abhinav Kumar",
        "Amir Houmansadr",
        "Eugene Bagdasarian"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We show that Web and Research Agents (WRAs) -- language model-based systems\nthat investigate complex topics on the Internet -- are vulnerable to inference\nattacks by passive network adversaries such as ISPs. These agents could be\ndeployed locally by organizations and individuals for privacy, legal, or\nfinancial purposes. Unlike sporadic web browsing by humans, WRAs visit\n$70{-}140$ domains with distinguishable timing correlations, enabling unique\nfingerprinting attacks.\n  Specifically, we demonstrate a novel prompt and user trait leakage attack\nagainst WRAs that only leverages their network-level metadata (i.e., visited IP\naddresses and their timings). We start by building a new dataset of WRA traces\nbased on user search queries and queries generated by synthetic personas. We\ndefine a behavioral metric (called OBELS) to comprehensively assess similarity\nbetween original and inferred prompts, showing that our attack recovers over\n73% of the functional and domain knowledge of user prompts. Extending to a\nmulti-session setting, we recover up to 19 of 32 latent traits with high\naccuracy. Our attack remains effective under partial observability and noisy\nconditions. Finally, we discuss mitigation strategies that constrain domain\ndiversity or obfuscate traces, showing negligible utility impact while reducing\nattack effectiveness by an average of 29%.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20282v2",
      "pdf_url": "http://arxiv.org/pdf/2508.20282v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.353,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on inference attacks using network metadata from Web and Research Agents, without any discussion of training AI models with human feedback, reward models, or reinforcement learning. Its main contribution is privacy leakage, not alignment via RLHF.",
      "weak_supervision_justification": "The paper involves building a dataset from programmatically generated queries and synthetic personas for training inference models, which resembles weak supervision by using noisy or automated labels. However, weak supervision is not the primary focus; the core is on privacy attacks, not the technique itself.",
      "diffusion_reasoning_justification": "The paper does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. It discusses AI agents' browsing patterns for inference attacks, but lacks any components related to diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20290",
      "title": "Objective Value Change and Shape-Based Accelerated Optimization for the\n  Neural Network Approximation",
      "authors": [
        "Pengcheng Xie",
        "Zihao Zhou",
        "Zijian Zhou"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.NA (Numerical Analysis)",
        "math.NA (Numerical Analysis)",
        "math.OC (Optimization and Control)"
      ],
      "abstract": "This paper introduce a novel metric of an objective function f, we say VC\n(value change) to measure the difficulty and approximation affection when\nconducting an neural network approximation task, and it numerically supports\ncharacterizing the local performance and behavior of neural network\napproximation. Neural networks often suffer from unpredictable local\nperformance, which can hinder their reliability in critical applications. VC\naddresses this issue by providing a quantifiable measure of local value changes\nin network behavior, offering insights into the stability and performance for\nachieving the neural-network approximation. We investigate some fundamental\ntheoretical properties of VC and identified two intriguing phenomena in neural\nnetwork approximation: the VC-tendency and the minority-tendency. These trends\nrespectively characterize how pointwise errors evolve in relation to the\ndistribution of VC during the approximation process.In addition, we propose a\nnovel metric based on VC, which measures the distance between two functions\nfrom the perspective of variation. Building upon this metric, we further\npropose a new preprocessing framework for neural network approximation.\nNumerical results including the real-world experiment and the PDE-related\nscientific problem support our discovery and pre-processing acceleration\nmethod.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20290v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20290v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.407,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution focuses on introducing a new metric called Value Change (VC) for neural network approximation, exploring related phenomena, and proposing a preprocessing framework to enhance approximation efficiency. It does not address distributed training, parallel computing, multi-node machine learning, or strategies for partitioning data/computation across processors. While it mentions general computational advancements like GPUs in the background, this is not central to the paper's contributions.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20293",
      "title": "Beacon: Post-Training Quantization with Integrated Grid Selection",
      "authors": [
        "Shihao Zhang",
        "Rayan Saab"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Quantization is a widely used compression technique for reducing the memory\nand computation costs of large pre-trained models. A key challenge in\nper-channel post-training quantization (PTQ) is selecting appropriate scaling\nfactors to replace weight values with values from a scaled integer grid.\nExisting methods typically fix the scale at the outset via heuristic tuning or\ngrid search. We propose Beacon, a simple and effective algorithm that\neliminates the need for such manual tuning. Beacon performs per-channel PTQ\ndirectly using an unscaled grid and automatically determines the optimal\nscaling factors by exploiting the geometry of scalar quantization. It does not\nrely on back-propagation or large calibration sets. Despite its simplicity and\ntuning-free nature, Beacon achieves competitive performance compared to\nstate-of-the-art methods, making it a practical solution for efficient model\ndeployment.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20293v2",
      "pdf_url": "http://arxiv.org/pdf/2508.20293v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.412,
      "datasets_score": 0.239,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a post-training quantization technique called Beacon, which focuses on compressing pre-trained models by optimizing scaling factors for quantization. This is aimed at reducing memory and computation costs during model deployment, not during training. The paper does not address distributed training, parallel computing for training acceleration, or strategies for partitioning data/computation across multiple nodes. Therefore, it has no connection to the topic of distributed training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20294",
      "title": "Dynamics-Aligned Latent Imagination in Contextual World Models for\n  Zero-Shot Generalization",
      "authors": [
        "Frank Röder",
        "Jan Benad",
        "Manfred Eppe",
        "Pradeep Kr. Banerjee"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Real-world reinforcement learning demands adaptation to unseen environmental\nconditions without costly retraining. Contextual Markov Decision Processes\n(cMDP) model this challenge, but existing methods often require explicit\ncontext variables (e.g., friction, gravity), limiting their use when contexts\nare latent or hard to measure. We introduce Dynamics-Aligned Latent Imagination\n(DALI), a framework integrated within the Dreamer architecture that infers\nlatent context representations from agent-environment interactions. By training\na self-supervised encoder to predict forward dynamics, DALI generates\nactionable representations conditioning the world model and policy, bridging\nperception and control. We theoretically prove this encoder is essential for\nefficient context inference and robust generalization. DALI's latent space\nenables counterfactual consistency: Perturbing a gravity-encoding dimension\nalters imagined rollouts in physically plausible ways. On challenging cMDP\nbenchmarks, DALI achieves significant gains over context-unaware baselines,\noften surpassing context-aware baselines in extrapolation tasks, enabling\nzero-shot generalization to unseen contextual variations.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20294v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20294v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.48,
      "distributed_training_score": 0.391,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a self-supervised framework for inferring latent contexts in reinforcement learning, relying solely on agent-environment interactions without any involvement of human feedback, human-ranked data, or a reward model trained on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on dynamics prediction and latent imagination in reinforcement learning using world models, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20307",
      "title": "Surveying the Operational Cybersecurity and Supply Chain Threat\n  Landscape when Developing and Deploying AI Systems",
      "authors": [
        "Michael R Smith",
        "Joe Ingram"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rise of AI has transformed the software and hardware landscape, enabling\npowerful capabilities through specialized infrastructures, large-scale data\nstorage, and advanced hardware. However, these innovations introduce unique\nattack surfaces and objectives which traditional cybersecurity assessments\noften overlook. Cyber attackers are shifting their objectives from conventional\ngoals like privilege escalation and network pivoting to manipulating AI outputs\nto achieve desired system effects, such as slowing system performance, flooding\noutputs with false positives, or degrading model accuracy. This paper serves to\nraise awareness of the novel cyber threats that are introduced when\nincorporating AI into a software system. We explore the operational\ncybersecurity and supply chain risks across the AI lifecycle, emphasizing the\nneed for tailored security frameworks to address evolving threats in the\nAI-driven landscape. We highlight previous exploitations and provide insights\nfrom working in this area. By understanding these risks, organizations can\nbetter protect AI systems and ensure their reliability and resilience.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20307v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20307v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.349,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.20310",
      "title": "Differentially Private Federated Quantum Learning via Quantum Noise",
      "authors": [
        "Atit Pokharel",
        "Ratun Rahman",
        "Shaba Shaon",
        "Thomas Morris",
        "Dinh C. Nguyen"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Quantum federated learning (QFL) enables collaborative training of quantum\nmachine learning (QML) models across distributed quantum devices without raw\ndata exchange. However, QFL remains vulnerable to adversarial attacks, where\nshared QML model updates can be exploited to undermine information privacy. In\nthe context of noisy intermediate-scale quantum (NISQ) devices, a key question\narises: How can inherent quantum noise be leveraged to enforce differential\nprivacy (DP) and protect model information during training and communication?\nThis paper explores a novel DP mechanism that harnesses quantum noise to\nsafeguard quantum models throughout the QFL process. By tuning noise variance\nthrough measurement shots and depolarizing channel strength, our approach\nachieves desired DP levels tailored to NISQ constraints. Simulations\ndemonstrate the framework's effectiveness by examining the relationship between\ndifferential privacy budget and noise parameters, as well as the trade-off\nbetween security and training accuracy. Additionally, we demonstrate the\nframework's robustness against an adversarial attack designed to compromise\nmodel performance using adversarial examples, with evaluations based on\ncritical metrics such as accuracy on adversarial examples, confidence scores\nfor correct predictions, and attack success rates. The results reveal a tunable\ntrade-off between privacy and robustness, providing an efficient solution for\nsecure QFL on NISQ devices with significant potential for reliable quantum\ncomputing applications.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20310v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20310v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.416,
      "datasets_score": 0.281,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on Quantum Federated Learning (QFL), a distributed training framework where multiple quantum devices collaboratively train a shared model without exchanging raw data. This directly aligns with distributed training concepts, as it involves partitioning computation across nodes (NISQ devices), aggregating model updates, and addressing challenges like parallel computing in a multi-node environment, making it highly pertinent to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces a differentially private quantum federated learning (DP-QFL) framework that leverages inherent quantum noise from noisy intermediate-scale quantum (NISQ) devices to enforce differential privacy, protecting shared models from adversarial attacks during training and communication. By tuning noise parameters such as measurement shots and depolarizing channel strength, the authors conduct simulations to demonstrate the framework's effectiveness, revealing a tunable trade-off between privacy levels and training accuracy, as well as enhanced robustness against adversarial examples, positioning it as a pioneering approach for secure quantum computing applications.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel application of inherent quantum noise to enforce differential privacy in quantum federated learning, marking the first study of this kind and significantly advancing the state-of-the-art in securing QFL systems.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in quantum AI and privacy mechanisms by providing a new framework for secure QFL on NISQ devices, though its broader commercial applications may be limited to emerging quantum technologies.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality contribution to quantum machine learning security, offering essential insights for researchers in quantum AI and differential privacy, making it valuable for those in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5cbe3a279834101c72792db917d2d1585aedec41",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 4,
      "average_h_index": 2.8,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Atit Pokharel",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2334356945"
        },
        {
          "name": "Ratun Rahman",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2317136919"
        },
        {
          "name": "Shaba Shaon",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2149703027"
        },
        {
          "name": "Thomas Morris",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2371990259"
        },
        {
          "name": "Dinh C. Nguyen",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2318279631"
        }
      ]
    },
    {
      "id": "2508.20322",
      "title": "Disentangling Latent Embeddings with Sparse Linear Concept Subspaces\n  (SLiCS)",
      "authors": [
        "Zhi Li",
        "Hau Phan",
        "Matthew Emigh",
        "Austin J. Brockmeier"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision-language co-embedding networks, such as CLIP, provide a latent\nembedding space with semantic information that is useful for downstream tasks.\nWe hypothesize that the embedding space can be disentangled to separate the\ninformation on the content of complex scenes by decomposing the embedding into\nmultiple concept-specific component vectors that lie in different subspaces. We\npropose a supervised dictionary learning approach to estimate a linear\nsynthesis model consisting of sparse, non-negative combinations of groups of\nvectors in the dictionary (atoms), whose group-wise activity matches the\nmulti-label information. Each concept-specific component is a non-negative\ncombination of atoms associated to a label. The group-structured dictionary is\noptimized through a novel alternating optimization with guaranteed convergence.\nExploiting the text co-embeddings, we detail how semantically meaningful\ndescriptions can be found based on text embeddings of words best approximated\nby a concept's group of atoms, and unsupervised dictionary learning can exploit\nzero-shot classification of training set images using the text embeddings of\nconcept labels to provide instance-wise multi-labels. We show that the\ndisentangled embeddings provided by our sparse linear concept subspaces (SLiCS)\nenable concept-filtered image retrieval (and conditional generation using\nimage-to-prompt) that is more precise. We also apply SLiCS to highly-compressed\nautoencoder embeddings from TiTok and the latent embedding from self-supervised\nDINOv2. Quantitative and qualitative results highlight the improved precision\nof the concept-filtered image retrieval for all embeddings.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.20322v1",
      "pdf_url": "http://arxiv.org/pdf/2508.20322v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.45,
      "distributed_training_score": 0.351,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on disentangling latent embeddings in vision-language models using sparse linear concept subspaces (SLiCS) for tasks like image retrieval and conditional generation. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. There is no mention of adapting diffusion for complex logical tasks or chain-of-thought processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21088",
      "title": "Advanced Deep Learning Techniques for Classifying Dental Conditions\n  Using Panoramic X-Ray Images",
      "authors": [
        "Alireza Golkarieh",
        "Kiana Kiashemshaki",
        "Sajjad Rezvani Boroujeni"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "This study investigates deep learning methods for automated classification of\ndental conditions in panoramic X-ray images. A dataset of 1,512 radiographs\nwith 11,137 expert-verified annotations across four conditions fillings,\ncavities, implants, and impacted teeth was used. After preprocessing and class\nbalancing, three approaches were evaluated: a custom convolutional neural\nnetwork (CNN), hybrid models combining CNN feature extraction with traditional\nclassifiers, and fine-tuned pre-trained architectures. Experiments employed 5\nfold cross validation with accuracy, precision, recall, and F1 score as\nevaluation metrics. The hybrid CNN Random Forest model achieved the highest\nperformance with 85.4% accuracy, surpassing the custom CNN baseline of 74.3%.\nAmong pre-trained models, VGG16 performed best at 82.3% accuracy, followed by\nXception and ResNet50. Results show that hybrid models improve discrimination\nof morphologically similar conditions and provide efficient, reliable\nperformance. These findings suggest that combining CNN-based feature extraction\nwith ensemble classifiers offers a practical path toward automated dental\ndiagnostic support, while also highlighting the need for larger datasets and\nfurther clinical validation.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.21088v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21088v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.357,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21090",
      "title": "Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer\n  via Query-Query Alignment",
      "authors": [
        "Namu Kim",
        "Wonbin Kweon",
        "Minsoo Kim",
        "Hwanjo Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We observe that zero-shot appearance transfer with large-scale image\ngeneration models faces a significant challenge: Attention Leakage. This\nchallenge arises when the semantic mapping between two images is captured by\nthe Query-Key alignment. To tackle this issue, we introduce Q-Align, utilizing\nQuery-Query alignment to mitigate attention leakage and improve the semantic\nalignment in zero-shot appearance transfer. Q-Align incorporates three core\ncontributions: (1) Query-Query alignment, facilitating the sophisticated\nspatial semantic mapping between two images; (2) Key-Value rearrangement,\nenhancing feature correspondence through realignment; and (3) Attention\nrefinement using rearranged keys and values to maintain semantic consistency.\nWe validate the effectiveness of Q-Align through extensive experiments and\nanalysis, and Q-Align outperforms state-of-the-art methods in appearance\nfidelity while maintaining competitive structure preservation.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.21090v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21090v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.372,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on zero-shot appearance transfer in image generation, addressing attention leakage through query-query alignment and related techniques. It does not involve training a reward model on human-ranked data, fine-tuning with reinforcement learning, or any use of human feedback for alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses attention mechanisms in large-scale image generation models for appearance transfer, but it does not adapt diffusion processes for multi-step logical reasoning or treat a Chain-of-Thought as an entity for iterative refinement. There is no clear component for complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21091",
      "title": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion",
      "authors": [
        "Xurui Peng",
        "Hong Liu",
        "Chenqian Yan",
        "Rui Ma",
        "Fangmin Chen",
        "Xing Wang",
        "Zhihua Wu",
        "Songwei Liu",
        "Mingbao Lin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.21091v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21091v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.294,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.52,
      "distributed_training_score": 0.399,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a caching framework to accelerate diffusion models for image and video generation by addressing computational errors and inefficiencies. It does not involve adapting diffusion models for complex logical tasks, iterative reasoning paths, or holistic correction of Chain-of-Thought processes. Thus, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21094",
      "title": "Video-LLMs with Temporal Visual Screening",
      "authors": [
        "Zheyu Fan",
        "Jiateng Liu",
        "Yuji Zhang",
        "Zihan Wang",
        "Yi R. Fung",
        "Manling Li",
        "Heng Ji"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Humans naturally perform temporal screening by dragging the progress bar and\nfocusing on salient temporal segments, but current Video Large Language Models\n(Video-LLMs) struggle to capture fine-grained temporal semantics due to sparse\nframe sampling and insufficient inter-frame reasoning supervision during their\ntraining. To address this, Inspired by well-established cognitive science\nprinciples, we propose Temporal Visual Screening (TVS), a new task that\nuniversally pre-processes video question answering and instruction tuning data\nby: (1) retaining focus-critical video segments, (2) synchronously\nreconstructing queries to their most direct form while preserving answer\nconsistency, and (3) keeping the invariance and consistency for any possible\nanswer. TVS is formulated as a modular front-end adapter task that can be\nseamlessly integrated into both Video Instruction Tuning (training) and Video\nQuestion Answering (inference) pipelines. TVS optimizes distribution of\nreasoning burden and cognitive load; during training, it aligns queries with\nfocus-critical visual information; at inference, it enables query-aware segment\nfocus and streamlined query representations. In particular, we curate the first\nbenchmark for TVS and propose ReSimplifyIt, a baseline outperforming prior\napproaches on seemingly similar tasks by 0.47 in F-1 score on video trimming\nwhile achieving competitive query rewriting performance. Experiments\ndemonstrate that incorporating TVS yields relative gains of 7.33% (training)\nand 34.6% (inference), demonstrating the effectiveness of temporal information\nscreening for improving video-language understanding.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.21094v2",
      "pdf_url": "http://arxiv.org/pdf/2508.21094v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.331,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21095",
      "title": "ScanMove: Motion Prediction and Transfer for Unregistered Body Meshes",
      "authors": [
        "Thomas Besnier",
        "Sylvain Arguillère",
        "Mohamed Daoudi"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Unregistered surface meshes, especially raw 3D scans, present significant\nchallenges for automatic computation of plausible deformations due to the lack\nof established point-wise correspondences and the presence of noise in the\ndata. In this paper, we propose a new, rig-free, data-driven framework for\nmotion prediction and transfer on such body meshes. Our method couples a robust\nmotion embedding network with a learned per-vertex feature field to generate a\nspatio-temporal deformation field, which drives the mesh deformation. Extensive\nevaluations, including quantitative benchmarks and qualitative visuals on tasks\nsuch as walking and running, demonstrate the effectiveness and versatility of\nour approach on challenging unregistered meshes.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.21095v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21095v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.273,
      "weak_supervision_score": 0.283,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.299,
      "datasets_score": 0.274,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2508.21096",
      "title": "ROBUST-MIPS: A Combined Skeletal Pose and Instance Segmentation Dataset\n  for Laparoscopic Surgical Instruments",
      "authors": [
        "Zhe Han",
        "Charlie Budd",
        "Gongyu Zhang",
        "Huanyu Tian",
        "Christos Bergeles",
        "Tom Vercauteren"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Localisation of surgical tools constitutes a foundational building block for\ncomputer-assisted interventional technologies. Works in this field typically\nfocus on training deep learning models to perform segmentation tasks.\nPerformance of learning-based approaches is limited by the availability of\ndiverse annotated data. We argue that skeletal pose annotations are a more\nefficient annotation approach for surgical tools, striking a balance between\nrichness of semantic information and ease of annotation, thus allowing for\naccelerated growth of available annotated data. To encourage adoption of this\nannotation style, we present, ROBUST-MIPS, a combined tool pose and tool\ninstance segmentation dataset derived from the existing ROBUST-MIS dataset. Our\nenriched dataset facilitates the joint study of these two annotation styles and\nallow head-to-head comparison on various downstream tasks. To demonstrate the\nadequacy of pose annotations for surgical tool localisation, we set up a simple\nbenchmark using popular pose estimation methods and observe high-quality\nresults. To ease adoption, together with the dataset, we release our benchmark\nmodels and custom tool pose annotation software.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2508.21096v1",
      "pdf_url": "http://arxiv.org/pdf/2508.21096v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.276,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.244,
      "distributed_training_score": 0.294,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00083",
      "title": "Data Cartography for Detecting Memorization Hotspots and Guiding Data\n  Interventions in Generative Models",
      "authors": [
        "Laksh Patel",
        "Neel Shanbhag"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Modern generative models risk overfitting and unintentionally memorizing rare\ntraining examples, which can be extracted by adversaries or inflate benchmark\nperformance. We propose Generative Data Cartography (GenDataCarto), a\ndata-centric framework that assigns each pretraining sample a difficulty score\n(early-epoch loss) and a memorization score (frequency of ``forget events''),\nthen partitions examples into four quadrants to guide targeted pruning and\nup-/down-weighting. We prove that our memorization score lower-bounds classical\ninfluence under smoothness assumptions and that down-weighting\nhigh-memorization hotspots provably decreases the generalization gap via\nuniform stability bounds. Empirically, GenDataCarto reduces synthetic canary\nextraction success by over 40\\% at just 10\\% data pruning, while increasing\nvalidation perplexity by less than 0.5\\%. These results demonstrate that\nprincipled data interventions can dramatically mitigate leakage with minimal\ncost to generative performance.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2509.00083v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00083v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.44,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.407,
      "datasets_score": 0.392,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on data cartography for detecting memorization in generative models and guiding data interventions, with no mention of human feedback, reward models, or reinforcement learning techniques for alignment.",
      "weak_supervision_justification": "The paper addresses analyzing and pruning training data for generative models to reduce memorization, but it does not involve programmatically generating labels from noisy sources or rely on weak supervision methods for training.",
      "diffusion_reasoning_justification": "While the paper briefly mentions diffusion models in the context of generative AI, its main contribution is data cartography for general generative models, not adapting diffusion for multi-step logical reasoning or chain-of-thought processes.",
      "distributed_training_justification": "The paper's core focus is on data-centric interventions for generative models, such as scoring and pruning data, and does not discuss distributed systems, parallel computing, or strategies for partitioning data/computation across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00084",
      "title": "Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs",
      "authors": [
        "Qibin Wang",
        "Pu Zhao",
        "Shaohan Huang",
        "Fangkai Yang",
        "Lu Wang",
        "Furu Wei",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "To further enhance the ability of Large Language Models (LLMs) to solve\ncomplex, multi-step reasoning problems, test-time scaling (TTS) methods have\ngained widespread attention. Existing approaches such as Best-of-N and majority\nvoting are limited as their performance depends on the quality of candidate\nresponses, making them unable to produce a correct solution when all candidates\nare incorrect. Introducing an additional model to select the best response also\nincurs significant deployment costs. To this end, we introduce Generative\nSelf-Refinement (GSR), a novel parallel test-time scaling framework where a\nunified model first generates a set of candidate responses in parallel and then\nperforms self-refinement to synthesize a new superior solution based on a\nprompt consisting of the problem and these candidates. However, LLMs struggle\nto perform refinement effectively when prompted directly. Therefore, we design\na hybrid training pipeline by jointly optimizing for two complementary\nobjectives, solving problems directly and refining candidate responses.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance across five mathematical benchmarks. We further show that this\nlearned self-refinement skill is a model-agnostic enhancement, robust across\ndifferent model scales and generalizing to out-of-distribution reasoning tasks.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2509.00084v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00084v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.454,
      "weak_supervision_score": 0.425,
      "diffusion_reasoning_score": 0.564,
      "distributed_training_score": 0.444,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Generative Self-Refinement for LLMs, involving generation and refinement of responses, but does not involve human feedback, reward models, or reinforcement learning for alignment. It uses a hybrid training pipeline for direct-solving and self-refinement objectives, without any RLHF elements.",
      "weak_supervision_justification": "The paper describes a hybrid training pipeline for optimizing direct-solving and self-refinement, but it does not mention using programmatically generated, noisy, or imprecise labels. It likely relies on standard datasets for mathematical benchmarks, without evidence of weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper introduces a self-refinement process for LLMs to improve reasoning outputs, but it does not adapt diffusion models or their iterative refinement for logical tasks. There is no mention of treating Chain-of-Thought as a holistic entity for multi-step correction via diffusion.",
      "distributed_training_justification": "The paper discusses parallel generation of candidates at test time for self-refinement, but it does not address distributed training, parallel computing for model training, or partitioning data/computation across multiple nodes. The focus is on inference-time scaling, not training processes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00085",
      "title": "Private, Verifiable, and Auditable AI Systems",
      "authors": [
        "Tobin South"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "The growing societal reliance on artificial intelligence necessitates robust\nframeworks for ensuring its security, accountability, and trustworthiness. This\nthesis addresses the complex interplay between privacy, verifiability, and\nauditability in modern AI, particularly in foundation models. It argues that\ntechnical solutions that integrate these elements are critical for responsible\nAI innovation. Drawing from international policy contributions and technical\nresearch to identify key risks in the AI pipeline, this work introduces novel\ntechnical solutions for critical privacy and verifiability challenges.\nSpecifically, the research introduces techniques for enabling verifiable and\nauditable claims about AI systems using zero-knowledge cryptography; utilizing\nsecure multi-party computation and trusted execution environments for\nauditable, confidential deployment of large language models and information\nretrieval; and implementing enhanced delegation mechanisms, credentialing\nsystems, and access controls to secure interactions with autonomous and\nmulti-agent AI systems. Synthesizing these technical advancements, this\ndissertation presents a cohesive perspective on balancing privacy,\nverifiability, and auditability in foundation model-based AI systems, offering\npractical blueprints for system designers and informing policy discussions on\nAI safety and governance.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2509.00085v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00085v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.314,
      "distributed_training_score": 0.37,
      "datasets_score": 0.399,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution focuses on privacy, verifiability, and auditability in AI systems using techniques like zero-knowledge cryptography and secure multi-party computation. It does not involve reinforcement learning, human feedback, reward models, or any alignment of AI models with human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00087",
      "title": "Yet Unnoticed in LSTM: Binary Tree Based Input Reordering, Weight\n  Regularization, and Gate Nonlinearization",
      "authors": [
        "Mojtaba Moattari"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "LSTM models used in current Machine Learning literature and applications, has\na promising solution for permitting long term information using gating\nmechanisms that forget and reduce effect of current input information. However,\neven with this pipeline, they do not optimally focus on specific old index or\nlong-term information. This paper elaborates upon input reordering approaches\nto prioritize certain input indices. Moreover, no LSTM based approach is found\nin the literature that examines weight normalization while choosing the right\nweight and exponent of Lp norms through main supervised loss function. In this\npaper, we find out which norm best finds relationship between weights to either\nsmooth or sparsify them. Lastly, gates, as weighted representations of inputs\nand states, which control reduction-extent of current input versus previous\ninputs (~ state), are not nonlinearized enough (through a small FFNN). As\nanalogous to attention mechanisms, gates easily filter current information to\nbold (emphasize on) past inputs. Nonlinearized gates can more easily tune up to\npeculiar nonlinearities of specific input in the past. This type of\nnonlinearization is not proposed in the literature, to the best of author's\nknowledge. The proposed approaches are implemented and compared with a simple\nLSTM to understand their performance in text classification tasks. The results\nshow they improve accuracy of LSTM.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2509.00087v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00087v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.312,
      "datasets_score": 0.267,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00088",
      "title": "AEGIS : Automated Co-Evolutionary Framework for Guarding Prompt\n  Injections Schema",
      "authors": [
        "Ting-Chun Liu",
        "Ching-Yu Hsu",
        "Kuan-Yi Lee",
        "Chi-An Fu",
        "Hung-yi Lee"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Prompt injection attacks pose a significant challenge to the safe deployment\nof Large Language Models (LLMs) in real-world applications. While prompt-based\ndetection offers a lightweight and interpretable defense strategy, its\neffectiveness has been hindered by the need for manual prompt engineering. To\naddress this issue, we propose AEGIS , an Automated co-Evolutionary framework\nfor Guarding prompt Injections Schema. Both attack and defense prompts are\niteratively optimized against each other using a gradient-like natural language\nprompt optimization technique. This framework enables both attackers and\ndefenders to autonomously evolve via a Textual Gradient Optimization (TGO)\nmodule, leveraging feedback from an LLM-guided evaluation loop. We evaluate our\nsystem on a real-world assignment grading dataset of prompt injection attacks\nand demonstrate that our method consistently outperforms existing baselines,\nachieving superior robustness in both attack success and detection.\nSpecifically, the attack success rate (ASR) reaches 1.0, representing an\nimprovement of 0.26 over the baseline. For detection, the true positive rate\n(TPR) improves by 0.23 compared to the previous best work, reaching 0.84, and\nthe true negative rate (TNR) remains comparable at 0.89. Ablation studies\nconfirm the importance of co-evolution, gradient buffering, and multi-objective\noptimization. We also confirm that this framework is effective in different\nLLMs. Our results highlight the promise of adversarial training as a scalable\nand effective approach for guarding prompt injections.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2509.00088v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00088v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.345,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is an automated co-evolutionary framework (AEGIS) for optimizing attack and defense prompts against prompt injection attacks using textual gradient optimization and LLM-guided feedback. It does not involve human feedback, a separate reward model trained on human-ranked data, or reinforcement learning to fine-tune an AI model based on human preferences. Instead, it focuses on adversarial prompt evolution, which is unrelated to the core elements of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00091",
      "title": "Ensemble Debates with Local Large Language Models for AI Alignment",
      "authors": [
        "Ephraiem Sarabamoun"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "As large language models (LLMs) take on greater roles in high-stakes\ndecisions, alignment with human values is essential. Reliance on proprietary\nAPIs limits reproducibility and broad participation. We study whether local\nopen-source ensemble debates can improve alignmentoriented reasoning. Across\n150 debates spanning 15 scenarios and five ensemble configurations, ensembles\noutperform single-model baselines on a 7-point rubric (overall: 3.48 vs. 3.13),\nwith the largest gains in reasoning depth (+19.4%) and argument quality\n(+34.1%). Improvements are strongest for truthfulness (+1.25 points) and human\nenhancement (+0.80). We provide code, prompts, and a debate data set, providing\nan accessible and reproducible foundation for ensemble-based alignment\nevaluation.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2509.00091v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00091v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.506,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.393,
      "datasets_score": 0.417,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on ensemble debates using local open-source language models to improve AI alignment through reasoning and evaluation, without any mention of training a reward model on human-ranked data or using reinforcement learning for fine-tuning. There is no human feedback mechanism involved in the methodology.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses ensemble debates for enhancing reasoning in AI alignment but does not involve diffusion models, iterative refinement processes, or treating a chain-of-thought as a holistically corrected entity over multiple steps. There is no component of multi-step logical reasoning based on diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces and provides a debate dataset, along with code and prompts, for evaluating ensemble-based AI alignment, which aligns with dataset creation and benchmarking for AI applications. However, the primary focus is on ensemble debates for alignment, not a comprehensive analysis or curation methodology of datasets.",
      "llm_score_status": "completed",
      "summary": "This paper investigates whether ensemble debates using local open-source large language models (LLMs) can enhance AI alignment by improving reasoning quality, addressing the limitations of proprietary APIs for reproducibility and accessibility. The methodology involves conducting 150 debates across 15 scenarios with five ensemble configurations, comparing them to single-model baselines, and evaluating on a 7-point rubric, revealing that ensembles outperform baselines overall (3.48 vs. 3.13) with significant gains in reasoning depth (+19.4%), argument quality (+34.1%), truthfulness (+1.25 points), and human enhancement (+0.80 points), while providing open code and data for further research.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining ensemble methods and debates with open-source LLMs for AI alignment, offering a clever adaptation of existing techniques to enhance reasoning in a new context, though it does not introduce a entirely novel problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in AI alignment and open-source LLMs by providing reproducible methods and empirical evidence, potentially leading to citations and builds within its subfield, though its broader commercial impact may be limited.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution with solid empirical results and open resources that advance AI alignment research, making it important for researchers in the field to be aware of, though it is not essential for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/4469dbccf65083fd18d6f01df28a959a173ce654",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 2,
      "average_h_index": 2.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ephraiem S. Sarabamoun",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/1380260730"
        }
      ]
    },
    {
      "id": "2509.00094",
      "title": "Automatic Pronunciation Error Detection and Correction of the Holy\n  Quran's Learners Using Deep Learning",
      "authors": [
        "Abdullah Abdelfattah",
        "Mahmoud I. Khalil",
        "Hazem Abbas"
      ],
      "categories": [
        "eess.AS (Audio and Speech Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)",
        "cs.SD (Sound)"
      ],
      "abstract": "Assessing spoken language is challenging, and quantifying pronunciation\nmetrics for machine learning models is even harder. However, for the Holy\nQuran, this task is simplified by the rigorous recitation rules (tajweed)\nestablished by Muslim scholars, enabling highly effective assessment. Despite\nthis advantage, the scarcity of high-quality annotated data remains a\nsignificant barrier.\n  In this work, we bridge these gaps by introducing: (1) A 98% automated\npipeline to produce high-quality Quranic datasets -- encompassing: Collection\nof recitations from expert reciters, Segmentation at pause points (waqf) using\nour fine-tuned wav2vec2-BERT model, Transcription of segments, Transcript\nverification via our novel Tasmeea algorithm; (2) 850+ hours of audio (~300K\nannotated utterances); (3) A novel ASR-based approach for pronunciation error\ndetection, utilizing our custom Quran Phonetic Script (QPS) to encode Tajweed\nrules (unlike the IPA standard for Modern Standard Arabic). QPS uses a\ntwo-level script: (Phoneme level): Encodes Arabic letters with short/long\nvowels. (Sifa level): Encodes articulation characteristics of every phoneme. We\nfurther include comprehensive modeling with our novel multi-level CTC Model\nwhich achieved 0.16% average Phoneme Error Rate (PER) on the testset. We\nrelease all code, data, and models as open-source:\nhttps://obadx.github.io/prepare-quran-dataset/",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2509.00094v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00094v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.265,
      "distributed_training_score": 0.278,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00097",
      "title": "Progressive Element-wise Gradient Estimation for Neural Network\n  Quantization",
      "authors": [
        "Kaiqi Zhao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Neural network quantization aims to reduce the bit-widths of weights and\nactivations, making it a critical technique for deploying deep neural networks\non resource-constrained hardware. Most Quantization-Aware Training (QAT)\nmethods rely on the Straight-Through Estimator (STE) to address the\nnon-differentiability of discretization functions by replacing their\nderivatives with that of the identity function. While effective, STE overlooks\ndiscretization errors between continuous and quantized values, which can lead\nto accuracy degradation -- especially at extremely low bit-widths. In this\npaper, we propose Progressive Element-wise Gradient Estimation (PEGE), a simple\nyet effective alternative to STE, which can be seamlessly integrated with any\nforward propagation methods and improves the quantized model accuracy. PEGE\nprogressively replaces full-precision weights and activations with their\nquantized counterparts via a novel logarithmic curriculum-driven\nmixed-precision replacement strategy. Then it formulates QAT as a\nco-optimization problem that simultaneously minimizes the task loss for\nprediction and the discretization error for quantization, providing a unified\nand generalizable framework. Extensive experiments on CIFAR-10 and ImageNet\nacross various architectures (e.g., ResNet, VGG) demonstrate that PEGE\nconsistently outperforms existing backpropagation methods and enables\nlow-precision models to match or even outperform the accuracy of their\nfull-precision counterparts.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2509.00097v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00097v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.421,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on improving neural network quantization through a novel gradient estimation method (PEGE) to reduce discretization errors and enhance accuracy in low-bit models. It does not address distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors or nodes. The contributions are centered on quantization-aware training techniques, which are unrelated to accelerating training via distribution.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00100",
      "title": "MODE: Mixture of Document Experts for RAG",
      "authors": [
        "Rahul Anand"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2509.00100v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00100v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.505,
      "distributed_training_score": 0.394,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a lightweight RAG framework using cluster-and-route retrieval for efficient document handling in small corpora. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks, such as treating a Chain-of-Thought as a single entity for correction. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00102",
      "title": "Exploiting a Mixture-of-Layers in an Electrocardiography Foundation\n  Model",
      "authors": [
        "Phu X. Nguyen",
        "Huy Phan",
        "Hieu Pham",
        "Christos Chatzichristos",
        "Bert Vandenberk",
        "Maarten De Vos"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Transformer-based foundation models for Electrocardiograms (ECGs) have\nrecently achieved impressive performance in many downstream applications.\nHowever, the internal representations of such models across layers have not\nbeen fully understood and exploited. An important question arises: Does the\nfinal layer of the pre-trained Transformer model, the \\emph{de facto}\nrepresentational layer, provide optimal performance for downstream tasks?\nAlthough our answer based on empirical and theoretical analyses for this\nquestion is negative, we propose a novel approach to leverage the\nrepresentation diversity of the model's layers effectively. Specifically, we\nintroduce a novel architecture called Post-pretraining Mixture-of-layers\nAggregation (PMA), which enables a flexible combination of the layer-wise\nrepresentations from the layer stack of a Transformer-based foundation model.\nWe first pre-train the model from ECG signals using the 1-dimensional Vision\nTransformer (ViT) via masked modeling. In downstream applications, instead of\nrelying solely on the last layer of the model, we employ a gating network to\nselectively fuse the representations from the pretrained model's layers,\nthereby enhancing representation power and improving performance of the\ndownstream applications. In addition, we extend the proposed method to the\npretraining stage by aggregating all representations through group-wise\naveraging before feeding them into the decoder-based Transformer.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2509.00102v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00102v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.361,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.00103",
      "title": "Pre-trained knowledge elevates large language models beyond traditional\n  chemical reaction optimizers",
      "authors": [
        "Robert MacKnight",
        "Jose Emilio Regio",
        "Jeffrey G. Ethier",
        "Luke A. Baldwin",
        "Gabe Gomes"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Modern optimization in experimental chemistry employs algorithmic search\nthrough black-box parameter spaces. Here we demonstrate that pre-trained\nknowledge in large language models (LLMs) fundamentally changes this paradigm.\nUsing six fully enumerated categorical reaction datasets (768 - 5,684\nexperiments), we benchmark LLM-guided optimization (LLM-GO) against Bayesian\noptimization (BO) and random sampling. Frontier LLMs consistently match or\nexceed BO performance across five single-objective datasets, with advantages\ngrowing as parameter complexity increases and high-performing conditions become\nscarce (<5% of space). BO retains superiority only for explicit multi-objective\ntrade-offs. To understand these contrasting behaviors, we introduce a\ntopology-agnostic information theory framework quantifying sampling diversity\nthroughout optimization campaigns. This analysis reveals that LLMs maintain\nsystematically higher exploration entropy than BO across all datasets while\nachieving superior performance, with advantages most pronounced in\nsolution-scarce parameter spaces where high-entropy exploration typically fails\n- suggesting that pre-trained domain knowledge enables more effective\nnavigation of chemical parameter space rather than replacing structured\nexploration strategies. To enable transparent benchmarking and community\nvalidation, we release Iron Mind (https://gomes.andrew.cmu.edu/iron-mind), a\nno-code platform for side-by-side evaluation of human, algorithmic, and LLM\noptimization campaigns with public leaderboards and complete trajectories. Our\nfindings establish that LLM-GO excels precisely where traditional methods\nstruggle: complex categorical spaces requiring domain understanding rather than\nmathematical optimization.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2509.00103v1",
      "pdf_url": "http://arxiv.org/pdf/2509.00103v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.441,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.462,
      "distributed_training_score": 0.397,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using pre-trained LLMs for chemical reaction optimization and does not involve training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper benchmarks LLM-guided optimization but does not discuss or utilize weak supervision methods, such as programmatically generating labels from noisy sources, for training or data annotation.",
      "diffusion_reasoning_justification": "The paper examines LLMs for optimization in chemical spaces and does not incorporate diffusion models, iterative refinement for logical tasks, or multi-step chain-of-thought reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02581",
      "title": "Charting the Future of Scholarly Knowledge with AI: A Community\n  Perspective",
      "authors": [
        "Azanzi Jiomekong",
        "Hande Küçük McGinty",
        "Keith G. Mills",
        "Allard Oelen",
        "Enayat Rajabi",
        "Harry McElroy",
        "Antrea Christou",
        "Anmol Saini",
        "Janice Anta Zebaze",
        "Hannah Kim",
        "Anna M. Jacyszyn",
        "Sören Auer"
      ],
      "categories": [
        "cs.DL (Digital Libraries)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite the growing availability of tools designed to support scholarly\nknowledge extraction and organization, many researchers still rely on manual\nmethods, sometimes due to unfamiliarity with existing technologies or limited\naccess to domain-adapted solutions. Meanwhile, the rapid increase in scholarly\npublications across disciplines has made it increasingly difficult to stay\ncurrent, further underscoring the need for scalable, AI-enabled approaches to\nstructuring and synthesizing scholarly knowledge. Various research communities\nhave begun addressing this challenge independently, developing tools and\nframeworks aimed at building reliable, dynamic, and queryable scholarly\nknowledge bases. However, limited interaction across these communities has\nhindered the exchange of methods, models, and best practices, slowing progress\ntoward more integrated solutions. This manuscript identifies ways to foster\ncross-disciplinary dialogue, identify shared challenges, categorize new\ncollaboration and shape future research directions in scholarly knowledge and\norganization.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2509.02581v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02581v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.364,
      "datasets_score": 0.448,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on AI for scholarly knowledge extraction, organization, and community discussions, including tools and frameworks, but does not mention reinforcement learning, human feedback mechanisms, or related techniques for aligning AI models.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper briefly references datasets as part of AI tools and frameworks for the research life cycle and scholarly knowledge bases, but it does not primarily focus on creating, analyzing, benchmarking, or evaluating datasets; instead, it emphasizes broader community perspectives and AI applications.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03370",
      "title": "Neural Field Turing Machine: A Differentiable Spatial Computer",
      "authors": [
        "Akash Malhotra",
        "Nacéra Seghouani"
      ],
      "categories": [
        "cs.NE (Neural and Evolutionary Computing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce the Neural Field Turing Machine (NFTM), a differentiable\narchitecture that unifies symbolic computation, physical simulation, and\nperceptual inference within continuous spatial fields. NFTM combines a neural\ncontroller, continuous memory field, and movable read/write heads that perform\nlocal updates. At each timestep, the controller reads local patches, computes\nupdates via learned rules, and writes them back while updating head positions.\nThis design achieves linear O(N) scaling through fixed-radius neighborhoods\nwhile maintaining Turing completeness under bounded error. We demonstrate three\nexample instantiations of NFTM: cellular automata simulation (Rule 110),\nphysics-informed PDE solvers (2D heat equation), and iterative image refinement\n(CIFAR-10 inpainting). These instantiations learn local update rules that\ncompose into global dynamics, exhibit stable long-horizon rollouts, and\ngeneralize beyond training horizons. NFTM provides a unified computational\nsubstrate bridging discrete algorithms and continuous field dynamics within a\nsingle differentiable framework.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2509.03370v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03370v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.462,
      "distributed_training_score": 0.426,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces the Neural Field Turing Machine (NFTM), which uses iterative updates for spatial computations, but it does not adapt the iterative refinement process of diffusion models for complex logical tasks. Instead, it positions NFTM as an alternative to diffusion models, emphasizing deterministic rollouts and controllable update rules, without any component for multi-step logical reasoning via diffusion.",
      "distributed_training_justification": "The paper focuses on the NFTM architecture for spatial computations and its linear scaling through fixed-radius neighborhoods, but it does not address distributed training, parallel computing, or multi-node machine learning techniques such as partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03535",
      "title": "QuesGenie: Intelligent Multimodal Question Generation",
      "authors": [
        "Ahmed Mubarak",
        "Amna Ahmed",
        "Amira Nasser",
        "Aya Mohamed",
        "Fares El-Sadek",
        "Mohammed Ahmed",
        "Ahmed Salah",
        "Youssef Sobhy"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In today's information-rich era, learners have access to abundant educational\nresources, but the lack of practice materials tailored to these resources\npresents a significant challenge. This project addresses that gap by developing\na multi-modal question generation system that can automatically generate\ndiverse question types from various content formats. The system features four\nmajor components: multi-modal input handling, question generation,\nreinforcement learning from human feedback (RLHF), and an end-to-end\ninteractive interface. This project lays the foundation for automated,\nscalable, and intelligent question generation, carefully balancing resource\nefficiency, robust functionality and a smooth user experience.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2509.03535v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03535v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.251,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a multi-modal question generation system with components like input handling, question generation, RLHF, and an interactive interface. It does not mention or utilize diffusion models, iterative refinement processes for logical tasks, or treat a Chain-of-Thought as a single entity for holistic correction. Therefore, it lacks any clear component for multi-step logical reasoning using a diffusion model, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03536",
      "title": "PG-Agent: An Agent Powered by Page Graph",
      "authors": [
        "Weizhi Chen",
        "Ziwei Wang",
        "Leyang Yang",
        "Sheng Zhou",
        "Xiaoxuan Tang",
        "Jiajun Bu",
        "Yong Li",
        "Wei Jiang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Graphical User Interface (GUI) agents possess significant commercial and\nsocial value, and GUI agents powered by advanced multimodal large language\nmodels (MLLMs) have demonstrated remarkable potential. Currently, existing GUI\nagents usually utilize sequential episodes of multi-step operations across\npages as the prior GUI knowledge, which fails to capture the complex transition\nrelationship between pages, making it challenging for the agents to deeply\nperceive the GUI environment and generalize to new scenarios. Therefore, we\ndesign an automated pipeline to transform the sequential episodes into page\ngraphs, which explicitly model the graph structure of the pages that are\nnaturally connected by actions. To fully utilize the page graphs, we further\nintroduce Retrieval-Augmented Generation (RAG) technology to effectively\nretrieve reliable perception guidelines of GUI from them, and a tailored\nmulti-agent framework PG-Agent with task decomposition strategy is proposed to\nbe injected with the guidelines so that it can generalize to unseen scenarios.\nExtensive experiments on various benchmarks demonstrate the effectiveness of\nPG-Agent, even with limited episodes for page graph construction.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2509.03536v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03536v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.326,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03537",
      "title": "AR$^2$: Adversarial Reinforcement Learning for Abstract Reasoning in\n  Large Language Models",
      "authors": [
        "Cheng-Kai Yeh",
        "Hsing-Wang Lee",
        "Chung-Hung Kuo",
        "Hen-Hsen Huang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Abstraction--the ability to recognize and distill essential computational\npatterns from complex problem statements--is a foundational skill in computer\nscience, critical both for human problem-solvers and coding-oriented large\nlanguage models (LLMs). Despite recent advances in training LLMs for code\ngeneration using reinforcement learning (RL), most existing approaches focus\nprimarily on superficial pattern recognition, overlooking explicit training for\nabstraction. In this study, we propose AR$^2$ (Adversarial Reinforcement\nLearning for Abstract Reasoning), a novel framework explicitly designed to\nenhance the abstraction abilities of LLMs. AR$^2$ employs a teacher model to\ntransform kernel problems into narrative-rich, challenging descriptions without\nchanging their fundamental logic. Simultaneously, a student coding model is\ntrained to solve these complex narrative problems by extracting their\nunderlying computational kernels. Experimental results demonstrate that AR$^2$\nsubstantially improves the student model's accuracy on previously unseen,\nchallenging programming tasks, underscoring abstraction as a key skill for\nenhancing LLM generalization.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2509.03537v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03537v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.448,
      "weak_supervision_score": 0.412,
      "diffusion_reasoning_score": 0.517,
      "distributed_training_score": 0.384,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes an adversarial reinforcement learning framework where rewards are derived from automated evaluations like test case correctness, not from human-ranked data or a reward model trained on human preferences. Thus, it does not align with RLHF.",
      "weak_supervision_justification": "The paper uses well-defined problems from sources like LeetCode with precise test cases to generate and evaluate training data, rather than programmatically creating noisy or imprecise labels for weak supervision.",
      "diffusion_reasoning_justification": "The paper focuses on adversarial reinforcement learning for abstract reasoning and does not involve diffusion models, iterative refinement processes, or treating reasoning paths as entities for multi-step correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09018",
      "title": "Personalized Sleep Prediction via Deep Adaptive Spatiotemporal Modeling\n  and Sparse Data",
      "authors": [
        "Xueyi Wang",
        "C. J. C.",
        "Lamoth",
        "Elisabeth Wilhelm"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "A sleep forecast allows individuals and healthcare providers to anticipate\nand proactively address factors influencing restful rest, ultimately improving\nmental and physical well-being. This work presents an adaptive spatial and\ntemporal model (AdaST-Sleep) for predicting sleep scores. Our proposed model\ncombines convolutional layers to capture spatial feature interactions between\nmultiple features and recurrent neural network layers to handle longer-term\ntemporal health-related data. A domain classifier is further integrated to\ngeneralize across different subjects. We conducted several experiments using\nfive input window sizes (3, 5, 7, 9, 11 days) and five predicting window sizes\n(1, 3, 5, 7, 9 days). Our approach consistently outperformed four baseline\nmodels, achieving its lowest RMSE (0.282) with a seven-day input window and a\none-day predicting window. Moreover, the method maintained strong performance\neven when forecasting multiple days into the future, demonstrating its\nversatility for real-world applications. Visual comparisons reveal that the\nmodel accurately tracks both the overall sleep score level and daily\nfluctuations. These findings prove that the proposed framework provides a\nrobust and adaptable solution for personalized sleep forecasting using sparse\ndata from commercial wearable devices and domain adaptation techniques.",
      "published_date": "2025-08-27",
      "arxiv_url": "http://arxiv.org/abs/2509.09018v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09018v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.342,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    }
  ],
  "total_papers": 205,
  "date": "2025-08-27"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Relevance Filter State Management
        let currentRelevanceFilters = {
            highlyRelevant: true,
            moderatelyRelevant: true,
            tangentiallyRelevant: true,
            notRelevant: true
        };
        
        let pendingRelevanceFilters = { ...currentRelevanceFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function calculateRelevanceScore(paper) {
            let score = 0;
            
            // Only consider topics that are currently selected/enabled in the topic filter
            const topicsToConsider = [];
            if (currentTopicFilters.rlhf) topicsToConsider.push('rlhf_relevance');
            if (currentTopicFilters.weakSupervision) topicsToConsider.push('weak_supervision_relevance');
            if (currentTopicFilters.diffusionReasoning) topicsToConsider.push('diffusion_reasoning_relevance');
            if (currentTopicFilters.distributedTraining) topicsToConsider.push('distributed_training_relevance');
            if (currentTopicFilters.datasets) topicsToConsider.push('datasets_relevance');
            
            // If no topics are selected, return 0
            if (topicsToConsider.length === 0) return 0;
            
            // Weighted scoring system
            const relevanceWeights = {
                'Highly Relevant': 4,
                'Moderately Relevant': 3,
                'Tangentially Relevant': 2,
                'Not Relevant': 1
            };
            
            // Sum up scores for selected topics only
            for (let topicField of topicsToConsider) {
                const relevance = paper[topicField];
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = relevance === "not_validated" ? "Not Relevant" : relevance;
                score += relevanceWeights[normalizedRelevance] || 1; // Default to 1 if unknown
            }
            
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'relevance_high':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(b) - calculateRelevanceScore(a));
                    break;
                case 'relevance_low':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(a) - calculateRelevanceScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'relevance_high': 'Relevance (Highest to Lowest)',
                'relevance_low': 'Relevance (Lowest to Highest)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
            updateNoveltyButtonText();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
            updateImpactButtonText();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for relevance checkboxes to sync between mobile and desktop
            document.getElementById('mobile-relevance-highly').addEventListener('change', () => {
                document.getElementById('desktop-relevance-highly').checked = 
                    document.getElementById('mobile-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-moderately').addEventListener('change', () => {
                document.getElementById('desktop-relevance-moderately').checked = 
                    document.getElementById('mobile-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('desktop-relevance-tangentially').checked = 
                    document.getElementById('mobile-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-not').addEventListener('change', () => {
                document.getElementById('desktop-relevance-not').checked = 
                    document.getElementById('mobile-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-highly').addEventListener('change', () => {
                document.getElementById('mobile-relevance-highly').checked = 
                    document.getElementById('desktop-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-moderately').addEventListener('change', () => {
                document.getElementById('mobile-relevance-moderately').checked = 
                    document.getElementById('desktop-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('mobile-relevance-tangentially').checked = 
                    document.getElementById('desktop-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-not').addEventListener('change', () => {
                document.getElementById('mobile-relevance-not').checked = 
                    document.getElementById('desktop-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function resetPendingScoringFilters() {
            pendingScoringFilters = { ...currentScoringFilters };
            syncPendingScoringUI();
            updateScoringButtonText();
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function resetPendingRecommendationFilters() {
            pendingRecommendationFilters = { ...currentRecommendationFilters };
            syncPendingRecommendationUI();
            updateRecommendationButtonText();
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
            updateTopicButtonText();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // RELEVANCE FILTER FUNCTIONS  
        // ============================================================================
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = pendingRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = pendingRelevanceFilters.notRelevant;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = currentRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = currentRelevanceFilters.notRelevant;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            // Read the current checkbox states from the UI (like H-Index filter does)
            const highlyRelevantChecked = document.getElementById('mobile-relevance-highly').checked;
            const moderatelyRelevantChecked = document.getElementById('mobile-relevance-moderately').checked;
            const tangentiallyRelevantChecked = document.getElementById('mobile-relevance-tangentially').checked;
            const notRelevantChecked = document.getElementById('mobile-relevance-not').checked;
            
            const selectedOptions = [];
            if (highlyRelevantChecked) selectedOptions.push("Highly Relevant");
            if (moderatelyRelevantChecked) selectedOptions.push("Moderately Relevant");
            if (tangentiallyRelevantChecked) selectedOptions.push("Tangentially Relevant");
            if (notRelevantChecked) selectedOptions.push("Not Relevant");
            
            const selectionText = selectedOptions.length === 4 ? "All Selected" : 
                                selectedOptions.length === 0 ? "None Selected" : 
                                `${selectedOptions.length} Selected`;
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            // Get values from mobile (primary source)
            const mobileHighly = document.getElementById('mobile-relevance-highly');
            const mobileModerately = document.getElementById('mobile-relevance-moderately');
            const mobileTangentially = document.getElementById('mobile-relevance-tangentially');
            const mobileNot = document.getElementById('mobile-relevance-not');
            
            // Update pending filters from mobile if available, otherwise from desktop
            pendingRelevanceFilters.highlyRelevant = mobileHighly ? mobileHighly.checked : document.getElementById('desktop-relevance-highly').checked;
            pendingRelevanceFilters.moderatelyRelevant = mobileModerately ? mobileModerately.checked : document.getElementById('desktop-relevance-moderately').checked;
            pendingRelevanceFilters.tangentiallyRelevant = mobileTangentially ? mobileTangentially.checked : document.getElementById('desktop-relevance-tangentially').checked;
            pendingRelevanceFilters.notRelevant = mobileNot ? mobileNot.checked : document.getElementById('desktop-relevance-not').checked;
        }
        
        function resetPendingRelevanceFilters() {
            pendingRelevanceFilters = { ...currentRelevanceFilters };
            syncPendingRelevanceUI();
            updateRelevanceButtonText();
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_highly', currentRelevanceFilters.highlyRelevant.toString());
            params.set('relevance_moderately', currentRelevanceFilters.moderatelyRelevant.toString());
            params.set('relevance_tangentially', currentRelevanceFilters.tangentiallyRelevant.toString());
            params.set('relevance_not', currentRelevanceFilters.notRelevant.toString());
            window.history.replaceState({}, '', `${window.location.pathname}?${params.toString()}`);
        }
        
        function loadRelevanceFiltersFromURL() {
            const highlyRelevant = getUrlParameter('relevance_highly');
            const moderatelyRelevant = getUrlParameter('relevance_moderately');
            const tangentiallyRelevant = getUrlParameter('relevance_tangentially');
            const notRelevant = getUrlParameter('relevance_not');
            
            if (highlyRelevant !== null) {
                currentRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
                pendingRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
            }
            if (moderatelyRelevant !== null) {
                currentRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
                pendingRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
            }
            if (tangentiallyRelevant !== null) {
                currentRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
                pendingRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
            }
            if (notRelevant !== null) {
                currentRelevanceFilters.notRelevant = notRelevant === 'true';
                pendingRelevanceFilters.notRelevant = notRelevant === 'true';
            }
            
            syncRelevanceUI();
        }
        
        function passesRelevanceFilter(paper) {
            // Get selected topics
            const selectedTopics = [];
            if (currentTopicFilters.rlhf) selectedTopics.push('rlhf');
            if (currentTopicFilters.weakSupervision) selectedTopics.push('weak_supervision');
            if (currentTopicFilters.diffusionReasoning) selectedTopics.push('diffusion_reasoning');
            if (currentTopicFilters.distributedTraining) selectedTopics.push('distributed_training');
            if (currentTopicFilters.datasets) selectedTopics.push('datasets');
            
            // If no topics selected, skip relevance filtering
            if (selectedTopics.length === 0) return true;
            
            // Get selected relevance levels
            const selectedRelevanceLevels = [];
            if (currentRelevanceFilters.highlyRelevant) selectedRelevanceLevels.push('Highly Relevant');
            if (currentRelevanceFilters.moderatelyRelevant) selectedRelevanceLevels.push('Moderately Relevant');
            if (currentRelevanceFilters.tangentiallyRelevant) selectedRelevanceLevels.push('Tangentially Relevant');
            if (currentRelevanceFilters.notRelevant) selectedRelevanceLevels.push('Not Relevant');
            
            // If no relevance levels selected, show 0 papers
            if (selectedRelevanceLevels.length === 0) return false;
            
            // Check each selected topic
            for (let topic of selectedTopics) {
                const relevanceField = `${topic}_relevance`;
                const paperRelevance = paper[relevanceField];
                
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = paperRelevance === "not_validated" ? "Not Relevant" : paperRelevance;
                
                // If this topic's relevance matches any selected relevance level, paper passes
                if (selectedRelevanceLevels.includes(normalizedRelevance)) {
                    return true; // At least one topic matches
                }
            }
            
            // No selected topics had matching relevance levels
            return false;
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
            
            // Update Relevance
            updateDropdownDisabledState('mobile-relevance-btn', 'mobile-relevance-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-relevance-btn', 'desktop-relevance-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            // Check if there are no papers to display
            if (filteredSortedPapers.length === 0) {
                showNoPapersMessage();
                hidePaginationSections();
                return;
            }
            
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            showPaginationSections();
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        function showNoPapersMessage() {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const noPapersHTML = '<div class="flex items-center justify-center min-h-screen"><h2 class="font-heading text-2xl text-neutral-600">No papers to show</h2></div>';
            
            if (mobileContainer) {
                mobileContainer.innerHTML = noPapersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = noPapersHTML;
            }
        }

        function hidePaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = 'none';
                }
            });
        }

        function showPaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = '';
                }
            });
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            // Find the similarity container in the currently visible layout
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            // Find the justification container in the currently visible layout
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            // Find the normalized similarity container in the currently visible layout
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // QUICK FILTER FUNCTIONS
        // ============================================================================


        function applyQuickFilter(filterType) {
            // Build URL parameters based on filter type
            const url = new URL(window.location.href);
            
            // Clear all existing parameters
            url.search = '';
            
            // Set sort to recommendation best first for all filters
            url.searchParams.set('sort', 'recommend_best');
            
            switch(filterType) {
                case 'must-read':
                    // Recommendation: Only Must Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'true');
                    url.searchParams.set('recommendation_should', 'false');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'should-read':
                    // Recommendation: Only Should Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'false');
                    url.searchParams.set('recommendation_should', 'true');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'rlhf':
                    // Topic: Only RLHF + Relevance: Exclude Not Relevant
                    url.searchParams.set('topic_rlhf', 'true');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'weak-supervision':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'true');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'diffusion-reasoning':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'true');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'distributed-training':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'true');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'datasets':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'true');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'reset':
                    // Clear all parameters, which will reset everything to defaults
                    break;
                    
                default:
                    console.warn('Unknown quick filter type:', filterType);
                    return;
            }
            
            // Navigate to new URL, which will trigger existing URL parsing logic
            window.location.href = url.toString();
        }



        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Load Relevance filters from URL
            loadRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-relevance-dropdown'), button: document.getElementById('mobile-relevance-btn') },
                { dropdown: document.getElementById('desktop-relevance-dropdown'), button: document.getElementById('desktop-relevance-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('relevance')) {
                            resetPendingRelevanceFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        } else if (dropdown.id.includes('recommendation')) {
                            resetPendingRecommendationFilters();
                        } else if (dropdown.id.includes('scoring')) {
                            resetPendingScoringFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
