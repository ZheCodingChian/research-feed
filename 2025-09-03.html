<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 03 September 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: 'rgba(107, 106, 101, 0.7)' // #6B6A65 with 70% opacity
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 03 September 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100 min-h-screen">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="mobile-quick-must-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="mobile-quick-should-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="mobile-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="mobile-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="mobile-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="mobile-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="mobile-quick-datasets" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="mobile-quick-reset" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-highly" checked>
                                            <label for="mobile-relevance-highly"></label>
                                        </div>
                                        <label for="mobile-relevance-highly" class="text-neutral-10 text-xl font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-moderately" checked>
                                            <label for="mobile-relevance-moderately"></label>
                                        </div>
                                        <label for="mobile-relevance-moderately" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-tangentially" checked>
                                            <label for="mobile-relevance-tangentially"></label>
                                        </div>
                                        <label for="mobile-relevance-tangentially" class="text-neutral-10 text-xl font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-not" checked>
                                            <label for="mobile-relevance-not"></label>
                                        </div>
                                        <label for="mobile-relevance-not" class="text-neutral-10 text-xl font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="desktop-quick-must-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="desktop-quick-should-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="desktop-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="desktop-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="desktop-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="desktop-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="desktop-quick-datasets" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="desktop-quick-reset" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-highly" checked>
                                            <label for="desktop-relevance-highly"></label>
                                        </div>
                                        <label for="desktop-relevance-highly" class="text-neutral-10 text-lg font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-moderately" checked>
                                            <label for="desktop-relevance-moderately"></label>
                                        </div>
                                        <label for="desktop-relevance-moderately" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-tangentially" checked>
                                            <label for="desktop-relevance-tangentially"></label>
                                        </div>
                                        <label for="desktop-relevance-tangentially" class="text-neutral-10 text-lg font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-not" checked>
                                            <label for="desktop-relevance-not"></label>
                                        </div>
                                        <label for="desktop-relevance-not" class="text-neutral-10 text-lg font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 03 September 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl min-h-screen">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2509.02902",
      "title": "LiGuard: A Streamlined Open-Source Framework for Rapid & Interactive\n  Lidar Research",
      "authors": [
        "Muhammad Shahbaz",
        "Shaurya Agarwal"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "There is a growing interest in the development of lidar-based autonomous\nmobility and Intelligent Transportation Systems (ITS). To operate and research\non lidar data, researchers often develop code specific to application niche.\nThis approach leads to duplication of efforts across studies that, in many\ncases, share multiple methodological steps such as data input/output (I/O),\npre/post processing, and common algorithms in multi-stage solutions. Moreover,\nslight changes in data, algorithms, and/or research focus may force major\nrevisions in the code. To address these challenges, we present LiGuard, an\nopen-source software framework that allows researchers to: 1) rapidly develop\ncode for their lidar-based projects by providing built-in support for data I/O,\npre/post processing, and commonly used algorithms, 2) interactively\nadd/remove/reorder custom algorithms and adjust their parameters, and 3)\nvisualize results for classification, detection, segmentation, and tracking\ntasks. Moreover, because it creates all the code files in structured\ndirectories, it allows easy sharing of entire projects or even the individual\ncomponents to be reused by other researchers. The effectiveness of LiGuard is\ndemonstrated via case studies.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02902v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02902v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.318,
      "distributed_training_score": 0.352,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02903",
      "title": "PercepTwin: Modeling High-Fidelity Digital Twins for Sim2Real\n  LiDAR-based Perception for Intelligent Transportation Systems",
      "authors": [
        "Muhammad Shahbaz",
        "Shaurya Agarwal"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "LiDAR-based perception in intelligent transportation systems (ITS), for tasks\nsuch as object detection, tracking, and semantic and instance segmentation, is\npredominantly solved by deep neural network models which often require\nlarge-scale labeled datasets during training to achieve generalization.\nHowever, creating these datasets is costly. time consuming and require human\nlabor before the datasets are ready for training models. This hinders\nscalability of the LiDAR-based perception systems in ITS. Sim2Real learning\noffers scalable alternative, however, its effectiveness is dependent on the\nfidelity of the source simulation(s) to real-world, in terms of environment\nstructure, actor dynamics, and sensor emulations. In response, this paper\nintroduces a rigorous and reproducible methodology for creating large-scale,\nhigh-quality synthetic datasets using High-Fidelity Digital Twins (HiFi DTs).\nThe proposed workflow outlines the steps, tools, and best practices for\ndigitally replicating real-world environments, encompassing static geometry\nmodeling, road infrastructure replication, and dynamic traffic scenario\ngeneration. Leveraging open-source and readily available resources such as\nsatellite imagery and OpenStreetMap data, alongside specific sensor\nconfigurations, this paper provides practical, detailed guidance for\nconstructing robust synthetic environments. These environments subsequently\nfacilitate scalable, cost-effective, and diverse dataset generation, forming a\nreliable foundation for robust Sim2Real learning.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02903v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02903v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.399,
      "datasets_score": 0.401,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the development of a methodology for creating high-fidelity digital twins to generate large-scale synthetic datasets for LiDAR-based perception in intelligent transportation systems. This directly aligns with research on creating and curating datasets for machine learning and AI applications, as it addresses the challenges of dataset generation, provides a systematic approach using public data sources, and aims to produce scalable, cost-effective datasets for Sim2Real learning. While the paper focuses on the creation process rather than in-depth analysis or benchmarking (which is deferred to subsequent works), its emphasis on dataset generation methodologies makes it highly relevant to the topic.",
      "llm_score_status": "completed",
      "summary": "The paper, titled PercepTwin, addresses the challenges of creating large-scale labeled datasets for LiDAR-based perception in intelligent transportation systems by proposing a methodology to build high-fidelity digital twins (HiFi DTs) using publicly available geospatial data such as satellite imagery and OpenStreetMap. It outlines a reproducible workflow for replicating real-world environments, including static geometry modeling, road infrastructure replication, and dynamic traffic scenario generation, which enables the creation of scalable synthetic datasets for Sim2Real learning, thereby reducing costs and improving generalization in deep neural network models for tasks like object detection and segmentation.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing tools and data sources into a systematic, reproducible methodology for creating high-fidelity digital twins, addressing a known problem in Sim2Real learning without introducing entirely new techniques.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of LiDAR-based perception for ITS, as it provides practical guidance for generating synthetic datasets, though its influence may be limited to specific applications rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper represents a strong, valuable contribution with detailed methodology that could benefit researchers in Sim2Real learning and autonomous systems, making it important for those in the field to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/43107e8f3c982cafeb7f47651ac636f02b3a4e3d",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Muhammad Shahbaz",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2342503919"
        },
        {
          "name": "Shaurya Agarwal",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379989036"
        }
      ]
    },
    {
      "id": "2509.02904",
      "title": "High-Fidelity Digital Twins for Bridging the Sim2Real Gap in LiDAR-Based\n  ITS Perception",
      "authors": [
        "Muhammad Shahbaz",
        "Shaurya Agarwal"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Sim2Real domain transfer offers a cost-effective and scalable approach for\ndeveloping LiDAR-based perception (e.g., object detection, tracking,\nsegmentation) in Intelligent Transportation Systems (ITS). However, perception\nmodels trained in simulation often under perform on real-world data due to\ndistributional shifts. To address this Sim2Real gap, this paper proposes a\nhigh-fidelity digital twin (HiFi DT) framework that incorporates real-world\nbackground geometry, lane-level road topology, and sensor-specific\nspecifications and placement. We formalize the domain adaptation challenge\nunderlying Sim2Real learning and present a systematic method for constructing\nsimulation environments that yield in-domain synthetic data. An off-the-shelf\n3D object detector is trained on HiFi DT-generated synthetic data and evaluated\non real data. Our experiments show that the DT-trained model outperforms the\nequivalent model trained on real data by 4.8%. To understand this gain, we\nquantify distributional alignment between synthetic and real data using\nmultiple metrics, including Chamfer Distance (CD), Maximum Mean Discrepancy\n(MMD), Earth Mover's Distance (EMD), and Fr'echet Distance (FD), at both\nraw-input and latent-feature levels. Results demonstrate that HiFi DTs\nsubstantially reduce domain shift and improve generalization across diverse\nevaluation scenarios. These findings underscore the significant role of digital\ntwins in enabling reliable, simulation-based LiDAR perception for real-world\nITS applications.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02904v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02904v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.406,
      "datasets_score": 0.398,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is on creating high-fidelity digital twins to bridge the Sim2Real gap for LiDAR-based perception in Intelligent Transportation Systems, focusing on simulation, domain adaptation, and training perception models with synthetic data. It does not address distributed training, parallel computing, multi-node machine learning, or any techniques for partitioning data/computation across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02910",
      "title": "The Basic B*** Effect: The Use of LLM-based Agents Reduces the\n  Distinctiveness and Diversity of People's Choices",
      "authors": [
        "Sandra C. Matz",
        "C. Blaine Horton",
        "Sofie Goethals"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Large language models (LLMs) increasingly act on people's behalf: they write\nemails, buy groceries, and book restaurants. While the outsourcing of human\ndecision-making to AI can be both efficient and effective, it raises a\nfundamental question: how does delegating identity-defining choices to AI\nreshape who people become? We study the impact of agentic LLMs on two\nidentity-relevant outcomes: interpersonal distinctiveness - how unique a\nperson's choices are relative to others - and intrapersonal diversity - the\nbreadth of a single person's choices over time. Using real choices drawn from\nsocial-media behavior of 1,000 U.S. users (110,000 choices in total), we\ncompare a generic and personalized agent to a human baseline. Both agents shift\npeople's choices toward more popular options, reducing the distinctiveness of\ntheir behaviors and preferences. While the use of personalized agents tempers\nthis homogenization (compared to the generic AI), it also more strongly\ncompresses the diversity of people's preference portfolios by narrowing what\nthey explore across topics and psychological affinities. Understanding how AI\nagents might flatten human experience, and how using generic versus\npersonalized agents involves distinctiveness-diversity trade-offs, is critical\nfor designing systems that augment rather than constrain human agency, and for\nsafeguarding diversity in thought, taste, and expression.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02910v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02910v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.457,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.299,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper examines the impact of LLM-based agents on human choice distinctiveness and diversity, using real-world data to analyze behavioral shifts. It does not discuss or involve reinforcement learning techniques, human feedback for training AI models, or any related methods for aligning AI with preferences. Therefore, it has no connection to Reinforcement Learning from Human Feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02918",
      "title": "Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic\n  Learning Approach",
      "authors": [
        "Midhat Urooj",
        "Ayan Banerjee",
        "Farhat Shaikh",
        "Kuntal Thakur",
        "Sandeep Gupta"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Domain generalization remains a critical challenge in medical imaging, where\nmodels trained on single sources often fail under real-world distribution\nshifts. We propose KG-DG, a neuro-symbolic framework for diabetic retinopathy\n(DR) classification that integrates vision transformers with expert-guided\nsymbolic reasoning to enable robust generalization across unseen domains. Our\napproach leverages clinical lesion ontologies through structured, rule-based\nfeatures and retinal vessel segmentation, fusing them with deep visual\nrepresentations via a confidence-weighted integration strategy. The framework\naddresses both single-domain generalization (SDG) and multi-domain\ngeneralization (MDG) by minimizing the KL divergence between domain embeddings,\nthereby enforcing alignment of high-level clinical semantics. Extensive\nexperiments across four public datasets (APTOS, EyePACS, Messidor-1,\nMessidor-2) demonstrate significant improvements: up to a 5.2% accuracy gain in\ncross-domain settings and a 6% improvement over baseline ViT models. Notably,\nour symbolic-only model achieves a 63.67% average accuracy in MDG, while the\ncomplete neuro-symbolic integration achieves the highest accuracy compared to\nexisting published baselines and benchmarks in challenging SDG scenarios.\nAblation studies reveal that lesion-based features (84.65% accuracy)\nsubstantially outperform purely neural approaches, confirming that symbolic\ncomponents act as effective regularizers beyond merely enhancing\ninterpretability. Our findings establish neuro-symbolic integration as a\npromising paradigm for building clinically robust, and domain-invariant medical\nAI systems.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02918v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02918v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.445,
      "distributed_training_score": 0.373,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a neuro-symbolic framework for domain generalization in diabetic retinopathy, integrating vision transformers with symbolic reasoning and clinical knowledge. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical tasks or chain-of-thought reasoning. Thus, there is no connection to the specified topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02924",
      "title": "Simulacra Naturae: Generative Ecosystem driven by Agent-Based\n  Simulations and Brain Organoid Collective Intelligence",
      "authors": [
        "Nefeli Manoudaki",
        "Mert Toka",
        "Iason Paterakis",
        "Diarmid Flatley"
      ],
      "categories": [
        "cs.MM (Multimedia)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Simulacra Naturae is a data-driven media installation that explores\ncollective care through the entanglement of biological computation, material\necologies, and generative systems. The work translates pre-recorded neural\nactivity from brain organoids, lab-grown three-dimensional clusters of neurons,\ninto a multi-sensory environment composed of generative visuals, spatial audio,\nliving plants, and fabricated clay artifacts. These biosignals, streamed\nthrough a real-time system, modulate emergent agent behaviors inspired by\nnatural systems such as termite colonies and slime molds. Rather than using\nbiosignals as direct control inputs, Simulacra Naturae treats organoid activity\nas a co-creative force, allowing neural rhythms to guide the growth, form, and\natmosphere of a generative ecosystem. The installation features computationally\nfabricated clay prints embedded with solenoids, adding physical sound\nresonances to the generative surround composition. The spatial environment,\nfilled with live tropical plants and a floor-level projection layer featuring\nreal-time generative AI visuals, invites participants into a sensory field\nshaped by nonhuman cognition. By grounding abstract data in living materials\nand embodied experience, Simulacra Naturae reimagines visualization as a\npractice of care, one that decentralizes human agency and opens new spaces for\nethics, empathy, and ecological attunement within hybrid computational systems.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02924v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02924v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.285,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.317,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02928",
      "title": "A Data-Driven RetinaNet Model for Small Object Detection in Aerial\n  Images",
      "authors": [
        "Zhicheng Tang",
        "Jinwen Tang",
        "Yi Shang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In the realm of aerial imaging, the ability to detect small objects is\npivotal for a myriad of applications, encompassing environmental surveillance,\nurban design, and crisis management. Leveraging RetinaNet, this work unveils\nDDR-Net: a data-driven, deep-learning model devised to enhance the detection of\ndiminutive objects. DDR-Net introduces novel, data-driven techniques to\nautonomously ascertain optimal feature maps and anchor estimations, cultivating\na tailored and proficient training process while maintaining precision.\nAdditionally, this paper presents an innovative sampling technique to bolster\nmodel efficacy under limited data training constraints. The model's enhanced\ndetection capabilities support critical applications including wildlife and\nhabitat monitoring, traffic flow optimization, and public safety improvements\nthrough accurate identification of small objects like vehicles and pedestrians.\nDDR-Net significantly reduces the cost and time required for data collection\nand training, offering efficient performance even with limited data. Empirical\nassessments over assorted aerial avian imagery datasets demonstrate that\nDDR-Net markedly surpasses RetinaNet and alternative contemporary models. These\ninnovations advance current aerial image analysis technologies and promise\nwide-ranging impacts across multiple sectors including agriculture, security,\nand archaeology.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02928v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02928v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.398,
      "datasets_score": 0.39,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02930",
      "title": "VendiRL: A Framework for Self-Supervised Reinforcement Learning of\n  Diversely Diverse Skills",
      "authors": [
        "Erik M. Lintunen"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "In self-supervised reinforcement learning (RL), one of the key challenges is\nlearning a diverse set of skills to prepare agents for unknown future tasks.\nDespite impressive advances, scalability and evaluation remain prevalent\nissues. Regarding scalability, the search for meaningful skills can be obscured\nby high-dimensional feature spaces, where relevant features may vary across\ndownstream task domains. For evaluating skill diversity, defining what\nconstitutes \"diversity\" typically requires a hard commitment to a specific\nnotion of what it means for skills to be diverse, potentially leading to\ninconsistencies in how skill diversity is understood, making results across\ndifferent approaches hard to compare, and leaving many forms of diversity\nunexplored. To address these issues, we adopt a measure of sample diversity\nthat translates ideas from ecology to machine learning -- the Vendi Score --\nallowing the user to specify and evaluate any desired form of diversity. We\ndemonstrate how this metric facilitates skill evaluation and introduce VendiRL,\na unified framework for learning diversely diverse sets of skills. Given\ndistinct similarity functions, VendiRL motivates distinct forms of diversity,\nwhich could support skill-diversity pretraining in new and richly interactive\nenvironments where optimising for various forms of diversity may be desirable.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02930v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02930v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.455,
      "weak_supervision_score": 0.429,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.391,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on self-supervised reinforcement learning for skill diversity using the Vendi Score, without any involvement of human feedback, human-ranked data, or a separate reward model trained on human preferences. Thus, it does not align with RLHF.",
      "weak_supervision_justification": "The paper employs self-supervised RL, which indirectly reduces reliance on hand-labeled data by using intrinsic diversity measures like the Vendi Score. However, it does not explicitly involve programmatically generating noisy labels from high-level sources, making it only loosely connected to weak supervision.",
      "diffusion_reasoning_justification": "The paper discusses reinforcement learning frameworks for skill diversity and does not involve diffusion models, iterative refinement for logical reasoning, or multi-step chains of thought. There is no component related to diffusion-based processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02949",
      "title": "ProMQA-Assembly: Multimodal Procedural QA Dataset on Assembly",
      "authors": [
        "Kimihiro Hasegawa",
        "Wiradee Imrattanatrai",
        "Masaki Asada",
        "Susan Holm",
        "Yuran Wang",
        "Vincent Zhou",
        "Ken Fukuda",
        "Teruko Mitamura"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Assistants on assembly tasks have a large potential to benefit humans from\neveryday tasks to industrial settings. However, no testbeds support\napplication-oriented system evaluation in a practical setting, especially in\nassembly. To foster the development, we propose a new multimodal QA dataset on\nassembly activities. Our dataset, ProMQA-Assembly, consists of 391 QA pairs\nthat require the multimodal understanding of human-activity recordings and\ntheir instruction manuals in an online-style manner. In the development, we\nadopt a semi-automated QA annotation approach, where LLMs generate candidates\nand humans verify them, as a cost-effective method, and further improve it by\nintegrating fine-grained action labels to diversify question types.\nFurthermore, we create instruction task graphs for the target tasks of\nassembling toy vehicles. These newly created task graphs are used in our\nbenchmarking experiment, as well as to facilitate the human verification\nprocess in the QA annotation. Utilizing our dataset, we benchmark models,\nincluding competitive proprietary multimodal models. Our results suggest great\nroom for improvement for the current models. We believe our new evaluation\ndataset can contribute to the further development of procedural-activity\nassistants.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02949v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02949v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.294,
      "datasets_score": 0.434,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction and development of a new multimodal QA dataset, ProMQA-Assembly, specifically for assembly tasks in AI applications. It covers dataset creation through a semi-automated annotation process, benchmarking of models on the dataset, and analysis of its features and differences from existing datasets, directly aligning with research on creating, analyzing, benchmarking, and evaluating datasets for machine learning and AI.",
      "llm_score_status": "completed",
      "summary": "The paper introduces ProMQA-Assembly, a new multimodal question-answering dataset comprising 391 QA pairs focused on assembly tasks, utilizing videos from Assembly101 and instruction manuals to evaluate systems' abilities in online-style procedural understanding. Employing a semi-automated annotation process with LLMs for question generation, human verification, and integration of fine-grained action labels and task graphs, the authors benchmark various models, revealing that current multimodal models perform inadequately, highlighting significant opportunities for advancement in procedural activity assistants.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting and extending the existing ProMQA framework to a new domain of assembly tasks with enhanced annotation techniques like fine-grained action labels and task graphs, though it builds on prior work rather than introducing a completely novel problem.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of multimodal QA for procedural tasks, potentially influencing developments in AI assistants for assembly and related applications, but its scope is somewhat limited to specific domains like manufacturing and DIY.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a valuable new dataset and benchmarking insights that advance research in multimodal procedural understanding, making it essential for specialists in AI for assembly tasks.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/592dde543f213859c8790d883c95e763ffaacaeb",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 3,
      "average_h_index": 1.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Kimihiro Hasegawa",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/151073899"
        },
        {
          "name": "Wiradee Imrattanatrai",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/22796675"
        },
        {
          "name": "Masaki Asada",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2304563312"
        },
        {
          "name": "Susan Holm",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/30470090"
        },
        {
          "name": "Yuran Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2328263736"
        },
        {
          "name": "Vincent Zhou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378844305"
        },
        {
          "name": "Ken Fukuda",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2304566093"
        },
        {
          "name": "Teruko Mitamura",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2289840558"
        }
      ]
    },
    {
      "id": "2509.02952",
      "title": "STAR: A Fast and Robust Rigid Registration Framework for Serial\n  Histopathological Images",
      "authors": [
        "Zeyu Liu",
        "Shengwei Ding"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Registration of serial whole-slide histopathological images (WSIs) is\ncritical for enabling direct comparison across diverse stains and for preparing\npaired datasets in artificial intelligence (AI) workflows such as virtual\nstaining and biomarker prediction. While existing methods often rely on complex\ndeformable or deep learning approaches that are computationally intensive and\ndifficult to reproduce, lightweight rigid frameworks-sufficient for many\nconsecutive-section scenarios-remain underdeveloped. We introduce STAR (Serial\nTissue Alignment for Rigid registration), a fast and robust open-source\nframework for multi-WSI alignment. STAR integrates stain-conditioned\npreprocessing with a hierarchical coarse-to-fine correlation strategy, adaptive\nkernel scaling, and built-in quality control, achieving reliable rigid\nregistration across heterogeneous tissue types and staining protocols,\nincluding hematoxylin-eosin (H&E), special histochemical stains (e.g., PAS,\nPASM, Masson's), and immunohistochemical (IHC) markers (e.g., CD31, KI67).\nEvaluated on the ANHIR 2019 and ACROBAT 2022 datasets spanning multiple organs\nand scanning conditions, STAR consistently produced stable alignments within\nminutes per slide, demonstrating robustness to cross-stain variability and\npartial tissue overlap. Beyond benchmarks, we present case studies on H&E-IHC\nalignment, construction of multi-IHC panels, and typical failure modes,\nunderscoring both utility and limitations. Released as an open and lightweight\ntool, STAR provides a reproducible baseline that lowers the barrier for\nclinical adoption and enables large-scale paired data preparation for\nnext-generation computational pathology.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02952v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02952v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.29,
      "weak_supervision_score": 0.296,
      "diffusion_reasoning_score": 0.285,
      "distributed_training_score": 0.32,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02957",
      "title": "Ensemble YOLO Framework for Multi-Domain Mitotic Figure Detection in\n  Histopathology Images",
      "authors": [
        "Navya Sri Kelam",
        "Akash Parekh",
        "Saikiran Bonthu",
        "Nitin Singhal"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The reliable identification of mitotic figures in whole-slide\nhistopathological images remains difficult, owing to their low prevalence,\nsubstantial morphological heterogeneity, and the inconsistencies introduced by\ntissue processing and staining procedures. The MIDOG competition series\nprovides standardized benchmarks for evaluating detection approaches across\ndiverse domains, thus motivating the development of generalizable deep learning\nmodels. In this work, we investigate the performance of two modern one-stage\ndetectors, YOLOv5 and YOLOv8, trained on MIDOG++, CMC, and CCMCT datasets. To\nenhance robustness, training incorporated stain-invariant color perturbations\nand texture-preserving augmentations. Ininternal validation, YOLOv5 achieved\nhigher precision (84.3%), while YOLOv8 offered improved recall (82.6%),\nreflecting architectural trade-offs between anchor-based and anchor-free\ndetections. To capitalize on their complementary strengths, weemployed an\nensemble of the two models, which improved sensitivity (85.3%) while\nmaintaining competitive precision, yielding the best F1 score of 83.1%. On the\npreliminary MIDOG 2025 test leaderboard, our ensemble ranked 5th with an F1\nscore of 79.2%, precision of 73.6%, and recall of 85.8%, confirming that the\nproposed strategy generalizes effectively across unseen test data. These\nfindings highlight the effectiveness of combining anchor-based and anchor-free\nobject detectors to advance automated mitosis detection in digital pathology.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02957v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02957v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.25,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.357,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02958",
      "title": "Lattice Annotated Temporal (LAT) Logic for Non-Markovian Reasoning",
      "authors": [
        "Kaustuv Mukherji",
        "Jaikrishna Manojkumar Patil",
        "Dyuman Aditya",
        "Paulo Shakarian",
        "Devendra Parkar",
        "Lahari Pokala",
        "Clark Dorman",
        "Gerardo I. Simari"
      ],
      "categories": [
        "cs.LO (Logic in Computer Science)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.PL (Programming Languages)"
      ],
      "abstract": "We introduce Lattice Annotated Temporal (LAT) Logic, an extension of\nGeneralized Annotated Logic Programs (GAPs) that incorporates temporal\nreasoning and supports open-world semantics through the use of a lower lattice\nstructure. This logic combines an efficient deduction process with temporal\nlogic programming to support non-Markovian relationships and open-world\nreasoning capabilities. The open-world aspect, a by-product of the use of the\nlower-lattice annotation structure, allows for efficient grounding through a\nSkolemization process, even in domains with infinite or highly diverse\nconstants.\n  We provide a suite of theoretical results that bound the computational\ncomplexity of the grounding process, in addition to showing that many of the\nresults on GAPs (using an upper lattice) still hold with the lower lattice and\ntemporal extensions (though different proof techniques are required). Our\nopen-source implementation, PyReason, features modular design, machine-level\noptimizations, and direct integration with reinforcement learning environments.\nEmpirical evaluations across multi-agent simulations and knowledge graph tasks\ndemonstrate up to three orders of magnitude speedup and up to five orders of\nmagnitude memory reduction while maintaining or improving task performance.\nAdditionally, we evaluate LAT Logic's value in reinforcement learning\nenvironments as a non-Markovian simulator, achieving up to three orders of\nmagnitude faster simulation with improved agent performance, including a 26%\nincrease in win rate due to capturing richer temporal dependencies. These\nresults highlight LAT Logic's potential as a unified, extensible framework for\nopen-world temporal reasoning in dynamic and uncertain environments. Our\nimplementation is available at: pyreason.syracuse.edu.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02958v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02958v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.288,
      "datasets_score": 0.253,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Lattice Annotated Temporal (LAT) Logic, which extends logic programs with temporal reasoning and lattice structures for efficient non-Markovian and open-world reasoning. It involves fixpoint operators and deductive processes, but these do not adapt or relate to the iterative refinement process of diffusion models for logical tasks. There is no mention of diffusion-based methods, Chain-of-Thought as a holistic entity, or multi-step refinement in the context of diffusion, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02962",
      "title": "Resilient Multimodal Industrial Surface Defect Detection with Uncertain\n  Sensors Availability",
      "authors": [
        "Shuai Jiang",
        "Yunfeng Ma",
        "Jingyu Zhou",
        "Yuan Bian",
        "Yaonan Wang",
        "Min Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal industrial surface defect detection (MISDD) aims to identify and\nlocate defect in industrial products by fusing RGB and 3D modalities. This\narticle focuses on modality-missing problems caused by uncertain sensors\navailability in MISDD. In this context, the fusion of multiple modalities\nencounters several troubles, including learning mode transformation and\ninformation vacancy. To this end, we first propose cross-modal prompt learning,\nwhich includes: i) the cross-modal consistency prompt serves the establishment\nof information consistency of dual visual modalities; ii) the modality-specific\nprompt is inserted to adapt different input patterns; iii) the missing-aware\nprompt is attached to compensate for the information vacancy caused by dynamic\nmodalities-missing. In addition, we propose symmetric contrastive learning,\nwhich utilizes text modality as a bridge for fusion of dual vision modalities.\nSpecifically, a paired antithetical text prompt is designed to generate binary\ntext semantics, and triple-modal contrastive pre-training is offered to\naccomplish multimodal learning. Experiment results show that our proposed\nmethod achieves 73.83% I-AUROC and 93.05% P-AUROC with a total missing rate 0.7\nfor RGB and 3D modalities (exceeding state-of-the-art methods 3.84% and 5.58%\nrespectively), and outperforms existing approaches to varying degrees under\ndifferent missing types and rates. The source code will be available at\nhttps://github.com/SvyJ/MISDD-MM.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02962v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02962v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.421,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.369,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on unsupervised industrial surface defect detection due to scarce negative samples, which indirectly relates to weak supervision by not relying on hand-labeled data. However, it does not explicitly involve programmatically generating labels from noisy sources, making the connection indirect rather than central.",
      "diffusion_reasoning_justification": "The paper's main contributions involve cross-modal prompt learning and symmetric contrastive learning for handling missing modalities, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. Thus, it does not align with diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02964",
      "title": "EdgeAttNet: Towards Barb-Aware Filament Segmentation",
      "authors": [
        "Victor Solomon",
        "Piet Martens",
        "Jingyu Liu",
        "Rafal Angryk"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "astro-ph.SR (Solar and Stellar Astrophysics)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Accurate segmentation of solar filaments in H-alpha observations is critical\nfor determining filament chirality, a key factor in the behavior of Coronal\nMass Ejections (CMEs). However, existing methods often fail to capture\nfine-scale filament structures, particularly barbs, due to a limited ability to\nmodel long-range dependencies and spatial detail.\n  We propose EdgeAttNet, a segmentation architecture built on a U-Net backbone\nby introducing a novel, learnable edge map derived directly from the input\nimage. This edge map is incorporated into the model by linearly transforming\nthe attention Key and Query matrices with the edge information, thereby guiding\nthe self-attention mechanism at the network's bottleneck to more effectively\ncapture filament boundaries and barbs. By explicitly integrating this\nstructural prior into the attention computations, EdgeAttNet enhances spatial\nsensitivity and segmentation accuracy while reducing the number of trainable\nparameters.\n  Trained end-to-end, EdgeAttNet outperforms U-Net and other U-Net-based\ntransformer baselines on the MAGFILO dataset. It achieves higher segmentation\naccuracy and significantly better recognition of filament barbs, with faster\ninference performance suitable for practical deployment.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02964v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02964v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.272,
      "weak_supervision_score": 0.283,
      "diffusion_reasoning_score": 0.314,
      "distributed_training_score": 0.283,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02966",
      "title": "KEPT: Knowledge-Enhanced Prediction of Trajectories from Consecutive\n  Driving Frames with Vision-Language Models",
      "authors": [
        "Yujin Wang",
        "Tianyi Wang",
        "Quanfeng Liu",
        "Wenxian Fan",
        "Junfeng Jiao",
        "Christian Claudel",
        "Yunbing Yan",
        "Bingzhao Gao",
        "Jianqiang Wang",
        "Hong Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Accurate short-horizon trajectory prediction is pivotal for safe and reliable\nautonomous driving, yet existing vision-language models (VLMs) often fail to\neffectively ground their reasoning in scene dynamics and domain knowledge. To\naddress this challenge, this paper introduces KEPT, a knowledge-enhanced VLM\nframework that predicts ego trajectories directly from consecutive front-view\ndriving frames. KEPT couples a temporal frequency-spatial fusion (TFSF) video\nencoder, trained via self-supervised learning with hard-negative mining, with a\nscalable k-means + HNSW retrieval stack that supplies scene-aligned exemplars.\nRetrieved priors are embedded into chain-of-thought (CoT) prompts with explicit\nplanning constraints, while a triple-stage fine-tuning schedule incrementally\naligns the language head to metric spatial cues, physically feasible motion,\nand temporally conditioned front-view planning. Evaluated on nuScenes dataset,\nKEPT achieves state-of-the-art performance across open-loop protocols: under\nNoAvg, it achieves 0.70m average L2 with a 0.21\\% collision rate; under TemAvg\nwith lightweight ego status, it attains 0.31m average L2 and a 0.07\\% collision\nrate. Ablation studies show that all three fine-tuning stages contribute\ncomplementary benefits, and that using Top-2 retrieved exemplars yields the\nbest accuracy-safety trade-off. The k-means-clustered HNSW index delivers\nsub-millisecond retrieval latency, supporting practical deployment. These\nresults indicate that retrieval-augmented, CoT-guided VLMs offer a promising,\ndata-efficient pathway toward interpretable and trustworthy autonomous driving.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02966v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02966v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.309,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces KEPT, a framework for trajectory prediction using vision-language models, incorporating components like a video encoder, retrieval stack, chain-of-thought prompts, and fine-tuning. However, it does not mention or utilize diffusion models for iterative refinement or multi-step logical reasoning. The chain-of-thought process is used for prompting but lacks any adaptation of diffusion processes, making the paper unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02967",
      "title": "AR-KAN: Autoregressive-Weight-Enhanced Kolmogorov-Arnold Network for\n  Time Series Forecasting",
      "authors": [
        "Chen Zeng",
        "Tiehang Xu",
        "Qiao Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "Traditional neural networks struggle to capture the spectral structure of\ncomplex signals. Fourier neural networks (FNNs) attempt to address this by\nembedding Fourier series components, yet many real-world signals are\nalmost-periodic with non-commensurate frequencies, posing additional\nchallenges. Building on prior work showing that ARIMA outperforms large\nlanguage models (LLMs) for forecasting, we extend the comparison to neural\npredictors and find ARIMA still superior. We therefore propose the\nAutoregressive-Weight-Enhanced Kolmogorov-Arnold Network (AR-KAN), which\nintegrates a pre-trained AR module for temporal memory with a KAN for nonlinear\nrepresentation. The AR module preserves essential temporal features while\nreducing redundancy. Experiments demonstrate that AR-KAN matches ARIMA on\nalmost-periodic functions and achieves the best results on $72\\%$ of Rdatasets\nseries, with a clear advantage on data with periodic structure. These results\nhighlight AR-KAN as a robust and effective framework for time series\nforecasting.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02967v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02967v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.287,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.307,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02969",
      "title": "VQualA 2025 Challenge on Engagement Prediction for Short Videos: Methods\n  and Results",
      "authors": [
        "Dasong Li",
        "Sizhuo Ma",
        "Hang Hua",
        "Wenjie Li",
        "Jian Wang",
        "Chris Wei Zhou",
        "Fengbin Guan",
        "Xin Li",
        "Zihao Yu",
        "Yiting Lu",
        "Ru-Ling Liao",
        "Yan Ye",
        "Zhibo Chen",
        "Wei Sun",
        "Linhan Cao",
        "Yuqin Cao",
        "Weixia Zhang",
        "Wen Wen",
        "Kaiwei Zhang",
        "Zijian Chen",
        "Fangfang Lu",
        "Xiongkuo Min",
        "Guangtao Zhai",
        "Erjia Xiao",
        "Lingfeng Zhang",
        "Zhenjie Su",
        "Hao Cheng",
        "Yu Liu",
        "Renjing Xu",
        "Long Chen",
        "Xiaoshuai Hao",
        "Zhenpeng Zeng",
        "Jianqin Wu",
        "Xuxu Wang",
        "Qian Yu",
        "Bo Hu",
        "Weiwei Wang",
        "Pinxin Liu",
        "Yunlong Tang",
        "Luchuan Song",
        "Jinxi He",
        "Jiaru Wu",
        "Hanjia Lyu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)",
        "cs.SI (Social and Information Networks)"
      ],
      "abstract": "This paper presents an overview of the VQualA 2025 Challenge on Engagement\nPrediction for Short Videos, held in conjunction with ICCV 2025. The challenge\nfocuses on understanding and modeling the popularity of user-generated content\n(UGC) short videos on social media platforms. To support this goal, the\nchallenge uses a new short-form UGC dataset featuring engagement metrics\nderived from real-world user interactions. This objective of the Challenge is\nto promote robust modeling strategies that capture the complex factors\ninfluencing user engagement. Participants explored a variety of multi-modal\nfeatures, including visual content, audio, and metadata provided by creators.\nThe challenge attracted 97 participants and received 15 valid test submissions,\ncontributing significantly to progress in short-form UGC video engagement\nprediction.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02969v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02969v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.296,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02973",
      "title": "InstaDA: Augmenting Instance Segmentation Data with Dual-Agent System",
      "authors": [
        "Xianbao Hou",
        "Yonghao He",
        "Zeyd Boukhers",
        "John See",
        "Hu Su",
        "Wei Sui",
        "Cong Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Acquiring high-quality instance segmentation data is challenging due to the\nlabor-intensive nature of the annotation process and significant class\nimbalances within datasets. Recent studies have utilized the integration of\nCopy-Paste and diffusion models to create more diverse datasets. However, these\nstudies often lack deep collaboration between large language models (LLMs) and\ndiffusion models, and underutilize the rich information within the existing\ntraining data. To address these limitations, we propose InstaDA, a novel,\ntraining-free Dual-Agent system designed to augment instance segmentation\ndatasets. First, we introduce a Text-Agent (T-Agent) that enhances data\ndiversity through collaboration between LLMs and diffusion models. This agent\nfeatures a novel Prompt Rethink mechanism, which iteratively refines prompts\nbased on the generated images. This process not only fosters collaboration but\nalso increases image utilization and optimizes the prompts themselves.\nAdditionally, we present an Image-Agent (I-Agent) aimed at enriching the\noverall data distribution. This agent augments the training set by generating\nnew instances conditioned on the training images. To ensure practicality and\nefficiency, both agents operate as independent and automated workflows,\nenhancing usability. Experiments conducted on the LVIS 1.0 validation set\nindicate that InstaDA achieves significant improvements, with an increase of\n+4.0 in box average precision (AP) and +3.3 in mask AP compared to the\nbaseline. Furthermore, it outperforms the leading model, DiverGen, by +0.3 in\nbox AP and +0.1 in mask AP, with a notable +0.7 gain in box AP on common\ncategories and mask AP gains of +0.2 on common categories and +0.5 on frequent\ncategories.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02973v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02973v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.428,
      "diffusion_reasoning_score": 0.455,
      "distributed_training_score": 0.385,
      "datasets_score": 0.408,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves generating synthetic data and labels using diffusion models and LLMs for instance segmentation, which aligns with weak supervision by programmatically creating labels from automated sources rather than manual annotation. However, it focuses more on data augmentation than directly training models with noisy labels, making it moderately relevant.",
      "diffusion_reasoning_justification": "The paper uses diffusion models for image generation and data augmentation, but it does not involve adapting diffusion for multi-step logical reasoning or treating a Chain-of-Thought as an entity. The iterative Prompt Rethink mechanism is for refining prompts in image generation, not for complex logical tasks, so it does not qualify.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is a method to augment and create more diverse datasets for instance segmentation, including techniques for generation, filtration, and evaluation on datasets like LVIS. This directly aligns with research on dataset curation, creation, and benchmarking for AI applications.",
      "llm_score_status": "completed",
      "summary": "InstaDA introduces a Dual-Agent system to augment instance segmentation datasets by addressing challenges like annotation labor and class imbalances, utilizing a Text-Agent (T-Agent) that employs large language models and diffusion models with a Prompt Rethink mechanism for generating diverse images, and an Image-Agent (I-Agent) that creates new instances based on existing training images to enrich data distribution. The methodology includes stages for instance generation, segmentation using tools like BiRefNet and SAM, filtration with a CLIP dual-similarity metric, and augmentation via Copy-Paste, resulting in significant performance improvements on the LVIS 1.0 dataset, with +4.0 box AP and +3.3 mask AP over the baseline, and superior results compared to DiverGen.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing ideas, such as LLMs and diffusion models, through the novel Dual-Agent system and Prompt Rethink mechanism, offering a notable improvement in data augmentation for instance segmentation without introducing an entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of computer vision data augmentation due to its demonstrated improvements in handling class imbalances and data scarcity, though its influence may be limited to specific applications in instance segmentation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution with innovative mechanisms and empirical results that advance data augmentation techniques, making it essential for researchers focused on computer vision to be aware of its insights.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5150531d21784b4a951dc163942ec5a9fe05855c",
      "total_authors": 7,
      "authors_found": 6,
      "highest_h_index": 2,
      "average_h_index": 0.8333333333333334,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Xianbao Hou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379569887"
        },
        {
          "name": "Yonghao He",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2336250303"
        },
        {
          "name": "Zeyd Boukhers",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2316963850"
        },
        {
          "name": "John See",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376267717"
        },
        {
          "name": "Hu Su",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Wei Sui",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2243336441"
        },
        {
          "name": "Cong Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2336256424"
        }
      ]
    },
    {
      "id": "2509.02982",
      "title": "StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with\n  Lightweight Safety Rails",
      "authors": [
        "Hritik Arasu",
        "Faisal R Jahangiri"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.NE (Neural and Evolutionary Computing)"
      ],
      "abstract": "Sleep staging models often degrade when deployed on patients with unseen\nphysiology or recording conditions. We propose a streaming, source-free\ntest-time adaptation (TTA) recipe that combines entropy minimization (Tent)\nwith Batch-Norm statistic refresh and two safety rails: an entropy gate to\npause adaptation on uncertain windows and an EMA-based reset to reel back\ndrift. On Sleep-EDF Expanded, using single-lead EEG (Fpz-Cz, 100 Hz, 30s\nepochs; R&K to AASM mapping), we show consistent gains over a frozen baseline\nat seconds-level latency and minimal memory, reporting per-stage metrics and\nCohen's k. The method is model-agnostic, requires no source data or patient\ncalibration, and is practical for on-device or bedside use.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02982v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02982v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.359,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02983",
      "title": "DUViN: Diffusion-Based Underwater Visual Navigation via\n  Knowledge-Transferred Depth Features",
      "authors": [
        "Jinghe Yang",
        "Minh-Quan Le",
        "Mingming Gong",
        "Ye Pu"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Autonomous underwater navigation remains a challenging problem due to limited\nsensing capabilities and the difficulty of constructing accurate maps in\nunderwater environments. In this paper, we propose a Diffusion-based Underwater\nVisual Navigation policy via knowledge-transferred depth features, named DUViN,\nwhich enables vision-based end-to-end 4-DoF motion control for underwater\nvehicles in unknown environments. DUViN guides the vehicle to avoid obstacles\nand maintain a safe and perception awareness altitude relative to the terrain\nwithout relying on pre-built maps. To address the difficulty of collecting\nlarge-scale underwater navigation datasets, we propose a method that ensures\nrobust generalization under domain shifts from in-air to underwater\nenvironments by leveraging depth features and introducing a novel model\ntransfer strategy. Specifically, our training framework consists of two phases:\nwe first train the diffusion-based visual navigation policy on in-air datasets\nusing a pre-trained depth feature extractor. Secondly, we retrain the extractor\non an underwater depth estimation task and integrate the adapted extractor into\nthe trained navigation policy from the first step. Experiments in both\nsimulated and real-world underwater environments demonstrate the effectiveness\nand generalization of our approach. The experimental videos are available at\nhttps://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02983v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02983v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.458,
      "distributed_training_score": 0.354,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a diffusion model to generate velocity commands for underwater navigation based on depth features, involving iterative refinement processes typical of diffusion models. However, it applies this to visual navigation and motion control rather than multi-step logical reasoning or Chain-of-Thought tasks. There is no clear component for solving complex logical problems, as the diffusion model is primarily used for generating actions in a robotic context, not holistic reasoning paths. Thus, it is only tangentially related due to the use of diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02993",
      "title": "SPENet: Self-guided Prototype Enhancement Network for Few-shot Medical\n  Image Segmentation",
      "authors": [
        "Chao Fan",
        "Xibin Jia",
        "Anqi Xiao",
        "Hongyuan Yu",
        "Zhenghan Yang",
        "Dawei Yang",
        "Hui Xu",
        "Yan Huang",
        "Liang Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Few-Shot Medical Image Segmentation (FSMIS) aims to segment novel classes of\nmedical objects using only a few labeled images. Prototype-based methods have\nmade significant progress in addressing FSMIS. However, they typically generate\na single global prototype for the support image to match with the query image,\noverlooking intra-class variations. To address this issue, we propose a\nSelf-guided Prototype Enhancement Network (SPENet). Specifically, we introduce\na Multi-level Prototype Generation (MPG) module, which enables\nmulti-granularity measurement between the support and query images by\nsimultaneously generating a global prototype and an adaptive number of local\nprototypes. Additionally, we observe that not all local prototypes in the\nsupport image are beneficial for matching, especially when there are\nsubstantial discrepancies between the support and query images. To alleviate\nthis issue, we propose a Query-guided Local Prototype Enhancement (QLPE)\nmodule, which adaptively refines support prototypes by incorporating guidance\nfrom the query image, thus mitigating the negative effects of such\ndiscrepancies. Extensive experiments on three public medical datasets\ndemonstrate that SPENet outperforms existing state-of-the-art methods,\nachieving superior performance.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.02993v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02993v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.283,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.309,
      "distributed_training_score": 0.324,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03002",
      "title": "SOPSeg: Prompt-based Small Object Instance Segmentation in Remote\n  Sensing Imagery",
      "authors": [
        "Chenhao Wang",
        "Yingrui Ji",
        "Yu Meng",
        "Yunjian Zhang",
        "Yao Zhu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Extracting small objects from remote sensing imagery plays a vital role in\nvarious applications, including urban planning, environmental monitoring, and\ndisaster management. While current research primarily focuses on small object\ndetection, instance segmentation for small objects remains underexplored, with\nno dedicated datasets available. This gap stems from the technical challenges\nand high costs of pixel-level annotation for small objects. While the Segment\nAnything Model (SAM) demonstrates impressive zero-shot generalization, its\nperformance on small-object segmentation deteriorates significantly, largely\ndue to the coarse 1/16 feature resolution that causes severe loss of fine\nspatial details. To this end, we propose SOPSeg, a prompt-based framework\nspecifically designed for small object segmentation in remote sensing imagery.\nIt incorporates a region-adaptive magnification strategy to preserve\nfine-grained details, and employs a customized decoder that integrates edge\nprediction and progressive refinement for accurate boundary delineation.\nMoreover, we introduce a novel prompting mechanism tailored to the oriented\nbounding boxes widely adopted in remote sensing applications. SOPSeg\noutperforms existing methods in small object segmentation and facilitates\nefficient dataset construction for remote sensing tasks. We further construct a\ncomprehensive small object instance segmentation dataset based on SODA-A, and\nwill release both the model and dataset to support future research.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03002v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03002v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.315,
      "distributed_training_score": 0.308,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03006",
      "title": "Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack\n  Network Using CNNs and Transformers",
      "authors": [
        "Tzuhsuan Huang",
        "Cheng Yu Yeo",
        "Tsai-Ling Huang",
        "Hong-Han Shuai",
        "Wen-Huang Cheng",
        "Jun-Cheng Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent studies on deep watermarking have predominantly focused on\nin-processing watermarking, which integrates the watermarking process into\nimage generation. However, post-processing watermarking, which embeds\nwatermarks after image generation, offers more flexibility. It can be applied\nto outputs from any generative model (e.g. GANs, diffusion models) without\nneeding access to the model's internal structure. It also allows users to embed\nunique watermarks into individual images. Therefore, this study focuses on\npost-processing watermarking and enhances its robustness by incorporating an\nensemble attack network during training. We construct various versions of\nattack networks using CNN and Transformer in both spatial and frequency domains\nto investigate how each combination influences the robustness of the\nwatermarking model. Our results demonstrate that combining a CNN-based attack\nnetwork in the spatial domain with a Transformer-based attack network in the\nfrequency domain yields the highest robustness in watermarking models.\nExtensive evaluation on the WAVES benchmark, using average bit accuracy as the\nmetric, demonstrates that our ensemble attack network significantly enhances\nthe robustness of baseline watermarking methods under various stress tests. In\nparticular, for the Regeneration Attack defined in WAVES, our method improves\nStegaStamp by 18.743%. The code is released\nat:https://github.com/aiiu-lab/DeepRobustWatermark.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03006v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03006v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.351,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03011",
      "title": "Lesion-Aware Visual-Language Fusion for Automated Image Captioning of\n  Ulcerative Colitis Endoscopic Examinations",
      "authors": [
        "Alexis Ivan Lopez Escamilla",
        "Gilberto Ochoa",
        "Sharib Al"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present a lesion-aware image captioning framework for ulcerative colitis\n(UC). The model integrates ResNet embeddings, Grad-CAM heatmaps, and\nCBAM-enhanced attention with a T5 decoder. Clinical metadata (MES score 0-3,\nvascular pattern, bleeding, erythema, friability, ulceration) is injected as\nnatural-language prompts to guide caption generation. The system produces\nstructured, interpretable descriptions aligned with clinical practice and\nprovides MES classification and lesion tags. Compared with baselines, our\napproach improves caption quality and MES classification accuracy, supporting\nreliable endoscopic reporting.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03011v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03011v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.272,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a lesion-aware image captioning framework for ulcerative colitis using ResNet, Grad-CAM, CBAM, and a T5 decoder, focusing on integrating visual features with clinical metadata for accurate medical descriptions. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03012",
      "title": "Uncertainty-aware Test-Time Training (UT$^3$) for Efficient On-the-fly\n  Domain Adaptive Dense Regression",
      "authors": [
        "Uddeshya Upadhyay"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep neural networks (DNNs) are increasingly being used in autonomous\nsystems. However, DNNs do not generalize well to domain shift. Adapting to a\ncontinuously evolving environment is a safety-critical challenge inevitably\nfaced by all autonomous systems deployed to the real world. Recent work on\ntest-time training proposes methods that adapt to a new test distribution on\nthe fly by optimizing the DNN model for each test input using self-supervision.\nHowever, these techniques result in a sharp increase in inference time as\nmultiple forward and backward passes are required for a single test sample (for\ntest-time training) before finally making the prediction based on the\nfine-tuned features. This is undesirable for real-world robotics applications\nwhere these models may be deployed to resource constraint hardware with strong\nlatency requirements. In this work, we propose a new framework (called UT$^3$)\nthat leverages test-time training for improved performance in the presence of\ncontinuous domain shift while also decreasing the inference time, making it\nsuitable for real-world applications. Our method proposes an uncertainty-aware\nself-supervision task for efficient test-time training that leverages the\nquantified uncertainty to selectively apply the training leading to sharp\nimprovements in the inference time while performing comparably to standard\ntest-time training protocol. Our proposed protocol offers a continuous setting\nto identify the selected keyframes, allowing the end-user to control how often\nto apply test-time training. We demonstrate the efficacy of our method on a\ndense regression task - monocular depth estimation.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03012v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03012v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.431,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper uses self-supervision for test-time training, which involves generating labels from the data itself, aligning somewhat with weak supervision concepts. However, it focuses on on-the-fly adaptation rather than initial training with noisy or programmatically generated labels, making the connection indirect.",
      "diffusion_reasoning_justification": "The paper discusses uncertainty-aware test-time training for domain adaptation in neural networks, with no mention of diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes. It is solely focused on regression tasks like depth estimation.",
      "distributed_training_justification": "The paper addresses efficiency in test-time training for resource-constrained hardware but does not involve distributed training, parallel computing, or partitioning data/computation across multiple nodes. It is centered on single-model adaptation techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03024",
      "title": "Efficient Privacy-Preserving Recommendation on Sparse Data using Fully\n  Homomorphic Encryption",
      "authors": [
        "Moontaha Nishat Chowdhury",
        "André Bauer",
        "Minxuan Zhou"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In today's data-driven world, recommendation systems personalize user\nexperiences across industries but rely on sensitive data, raising privacy\nconcerns. Fully homomorphic encryption (FHE) can secure these systems, but a\nsignificant challenge in applying FHE to recommendation systems is efficiently\nhandling the inherently large and sparse user-item rating matrices. FHE\noperations are computationally intensive, and naively processing various sparse\nmatrices in recommendation systems would be prohibitively expensive.\nAdditionally, the communication overhead between parties remains a critical\nconcern in encrypted domains. We propose a novel approach combining Compressed\nSparse Row (CSR) representation with FHE-based matrix factorization that\nefficiently handles matrix sparsity in the encrypted domain while minimizing\ncommunication costs. Our experimental results demonstrate high recommendation\naccuracy with encrypted data while achieving the lowest communication costs,\neffectively preserving user privacy.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03024v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03024v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.29,
      "distributed_training_score": 0.334,
      "datasets_score": 0.259,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03025",
      "title": "Unveiling the Response of Large Vision-Language Models to Visually\n  Absent Tokens",
      "authors": [
        "Sohee Kim",
        "Soohyun Ryu",
        "Joonhyung Park",
        "Eunho Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) generate contextually relevant responses\nby jointly interpreting visual and textual inputs. However, our finding reveals\nthey often mistakenly perceive text inputs lacking visual evidence as being\npart of the image, leading to erroneous responses. In light of this finding, we\nprobe whether LVLMs possess an internal capability to determine if textual\nconcepts are grounded in the image, and discover a specific subset of\nFeed-Forward Network (FFN) neurons, termed Visual Absence-aware (VA) neurons,\nthat consistently signal the visual absence through a distinctive activation\npattern. Leveraging these patterns, we develop a detection module that\nsystematically classifies whether an input token is visually grounded. Guided\nby its prediction, we propose a method to refine the outputs by reinterpreting\nquestion prompts or replacing the detected absent tokens during generation.\nExtensive experiments show that our method effectively mitigates the models'\ntendency to falsely presume the visual presence of text input and its\ngenerality across various LVLMs.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03025v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03025v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.45,
      "distributed_training_score": 0.317,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on identifying and utilizing specific neurons in Large Vision-Language Models (LVLMs) to detect visually absent tokens and refine outputs, which involves neural activation analysis and response correction in vision-language tasks. It does not involve diffusion models, iterative refinement processes for logical reasoning, or treating Chain-of-Thought as a holistic entity for multi-step correction. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03032",
      "title": "Background Matters Too: A Language-Enhanced Adversarial Framework for\n  Person Re-Identification",
      "authors": [
        "Kaicong Huang",
        "Talha Azfar",
        "Jack M. Reilly",
        "Thomas Guggisberg",
        "Ruimin Ke"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Person re-identification faces two core challenges: precisely locating the\nforeground target while suppressing background noise and extracting\nfine-grained features from the target region. Numerous visual-only approaches\naddress these issues by partitioning an image and applying attention modules,\nyet they rely on costly manual annotations and struggle with complex\nocclusions. Recent multimodal methods, motivated by CLIP, introduce semantic\ncues to guide visual understanding. However, they focus solely on foreground\ninformation, but overlook the potential value of background cues. Inspired by\nhuman perception, we argue that background semantics are as important as the\nforeground semantics in ReID, as humans tend to eliminate background\ndistractions while focusing on target appearance. Therefore, this paper\nproposes an end-to-end framework that jointly models foreground and background\ninformation within a dual-branch cross-modal feature extraction pipeline. To\nhelp the network distinguish between the two domains, we propose an\nintra-semantic alignment and inter-semantic adversarial learning strategy.\nSpecifically, we align visual and textual features that share the same\nsemantics across domains, while simultaneously penalizing similarity between\nforeground and background features to enhance the network's discriminative\npower. This strategy drives the model to actively suppress noisy background\nregions and enhance attention toward identity-relevant foreground cues.\nComprehensive experiments on two holistic and two occluded ReID benchmarks\ndemonstrate the effectiveness and generality of the proposed method, with\nresults that match or surpass those of current state-of-the-art approaches.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03032v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03032v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.335,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03036",
      "title": "Knowledge Integration for Physics-informed Symbolic Regression Using\n  Pre-trained Large Language Models",
      "authors": [
        "Bilge Taskin",
        "Wenxiong Xie",
        "Teddy Lazebnik"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)",
        "cs.SC (Symbolic Computation)"
      ],
      "abstract": "Symbolic regression (SR) has emerged as a powerful tool for automated\nscientific discovery, enabling the derivation of governing equations from\nexperimental data. A growing body of work illustrates the promise of\nintegrating domain knowledge into the SR to improve the discovered equation's\ngenerality and usefulness. Physics-informed SR (PiSR) addresses this by\nincorporating domain knowledge, but current methods often require specialized\nformulations and manual feature engineering, limiting their adaptability only\nto domain experts. In this study, we leverage pre-trained Large Language Models\n(LLMs) to facilitate knowledge integration in PiSR. By harnessing the\ncontextual understanding of LLMs trained on vast scientific literature, we aim\nto automate the incorporation of domain knowledge, reducing the need for manual\nintervention and making the process more accessible to a broader range of\nscientific problems. Namely, the LLM is integrated into the SR's loss function,\nadding a term of the LLM's evaluation of the SR's produced equation. We\nextensively evaluate our method using three SR algorithms (DEAP, gplearn, and\nPySR) and three pre-trained LLMs (Falcon, Mistral, and LLama 2) across three\nphysical dynamics (dropping ball, simple harmonic motion, and electromagnetic\nwave). The results demonstrate that LLM integration consistently improves the\nreconstruction of physical dynamics from data, enhancing the robustness of SR\nmodels to noise and complexity. We further explore the impact of prompt\nengineering, finding that more informative prompts significantly improve\nperformance.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03036v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03036v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.346,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on integrating pre-trained Large Language Models into symbolic regression for physics-informed tasks, without any mention of human feedback, reward models, or reinforcement learning techniques. There is no alignment process involving human preferences or fine-tuning via RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes. It uses LLMs solely for evaluating equations in symbolic regression, without any adaptation of diffusion mechanisms for reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03041",
      "title": "MedLiteNet: Lightweight Hybrid Medical Image Segmentation Model",
      "authors": [
        "Pengyang Yu",
        "Haoquan Wang",
        "Gerard Marks",
        "Tahar Kechadi",
        "Laurence T. Yang",
        "Sahraoui Dhelim",
        "Nyothiri Aung"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Accurate skin-lesion segmentation remains a key technical challenge for\ncomputer-aided diagnosis of skin cancer. Convolutional neural networks, while\neffective, are constrained by limited receptive fields and thus struggle to\nmodel long-range dependencies. Vision Transformers capture global context, yet\ntheir quadratic complexity and large parameter budgets hinder use on the\nsmall-sample medical datasets common in dermatology. We introduce the\nMedLiteNet, a lightweight CNN Transformer hybrid tailored for dermoscopic\nsegmentation that achieves high precision through hierarchical feature\nextraction and multi-scale context aggregation. The encoder stacks depth-wise\nMobile Inverted Bottleneck blocks to curb computation, inserts a\nbottleneck-level cross-scale token-mixing unit to exchange information between\nresolutions, and embeds a boundary-aware self-attention module to sharpen\nlesion contours.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03041v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03041v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.275,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.358,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03044",
      "title": "DCDB: Dynamic Conditional Dual Diffusion Bridge for Ill-posed\n  Multi-Tasks",
      "authors": [
        "Chengjie Huang",
        "Jiafeng Yan",
        "Jing Li",
        "Lu Bai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Conditional diffusion models have made impressive progress in the field of\nimage processing, but the characteristics of constructing data distribution\npathways make it difficult to exploit the intrinsic correlation between tasks\nin multi-task scenarios, which is even worse in ill-posed tasks with a lack of\ntraining data. In addition, traditional static condition control makes it\ndifficult for networks to learn in multi-task scenarios with its dynamically\nevolving characteristics. To address these challenges, we propose a dynamic\nconditional double diffusion bridge training paradigm to build a general\nframework for ill-posed multi-tasks. Firstly, this paradigm decouples the\ndiffusion and condition generation processes, avoiding the dependence of the\ndiffusion model on supervised data in ill-posed tasks. Secondly, generated by\nthe same noise schedule, dynamic conditions are used to gradually adjust their\nstatistical characteristics, naturally embed time-related information, and\nreduce the difficulty of network learning. We analyze the learning objectives\nof the network under different conditional forms in the single-step denoising\nprocess and compare the changes in its attention weights in the network,\ndemonstrating the superiority of our dynamic conditions. Taking dehazing and\nvisible-infrared fusion as typical ill-posed multi-task scenarios, we achieve\nthe best performance in multiple indicators on public datasets. The code has\nbeen publicly released at: https://anonymous.4open.science/r/DCDB-D3C2.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03044v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03044v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.555,
      "distributed_training_score": 0.432,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on diffusion models for image processing tasks, such as dehazing and visible-infrared fusion, emphasizing iterative denoising for multi-task scenarios. However, it does not involve adapting diffusion for complex logical reasoning, multi-step Chain-of-Thought processes, or holistic correction of reasoning paths. There is no component for logical tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper proposes a training paradigm for diffusion models in ill-posed multi-tasks but does not address distributed training, parallel computing, or multi-node machine learning. It lacks any discussion on partitioning data, model architecture, or computation across processors or nodes to accelerate training, focusing solely on model design for image processing.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03047",
      "title": "FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale\n  Training of LLMs",
      "authors": [
        "Haijun Zhang",
        "Jinxiang Wang",
        "Zhenhua Yu",
        "Yanyong Zhang",
        "Xuejie Ji",
        "Kaining Mao",
        "Jun Zhang",
        "Yaqing Zhang",
        "Ting Wu",
        "Fei Jie",
        "Xiemin Huang",
        "Zhifang Cai",
        "Junhua Cheng",
        "Shuwei Wang",
        "Wei Li",
        "Xiaoming Bao",
        "Hua Xu",
        "Shixiong Zhao",
        "Jun Li",
        "Hongwei Sun",
        "Ziyang Zhang",
        "Yi Xiong",
        "Chunsheng Li"
      ],
      "categories": [
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) have made a profound impact across various\nfields due to their advanced capabilities. However, training these models at\nunprecedented scales requires extensive AI accelerator clusters and\nsophisticated parallelism strategies, which pose significant challenges in\nmaintaining system reliability over prolonged training periods. A major concern\nis the substantial loss of training time caused by inevitable hardware and\nsoftware failures. To address these challenges, we present FlashRecovery, a\nfast and low-cost failure recovery system comprising three core modules: (1)\nActive and real-time failure detection. This module performs continuous\ntraining state monitoring, enabling immediate identification of hardware and\nsoftware failures within seconds, thus ensuring rapid incident response; (2)\nScale-independent task restart. By employing different recovery strategies for\nnormal and faulty nodes, combined with an optimized communication group\nreconstruction protocol, our approach ensures that the recovery time remains\nnearly constant, regardless of cluster scale; (3) Checkpoint-free recovery\nwithin one step. Our novel recovery mechanism enables single-step restoration,\ncompletely eliminating dependence on traditional checkpointing methods and\ntheir associated overhead. Collectively, these innovations enable FlashRecovery\nto achieve optimal Recovery Time Objective (RTO) and Recovery Point Objective\n(RPO), substantially improving the reliability and efficiency of long-duration\nLLM training. Experimental results demonstrate that FlashRecovery system can\nachieve training restoration on training cluster with 4, 800 devices in 150\nseconds. We also verify that the time required for failure recovery is nearly\nconsistent for different scales of training tasks.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03047v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03047v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.433,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.572,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on failure recovery mechanisms for large-scale LLM training, including failure detection and checkpoint-free recovery, with no mention of human feedback, reward models, or reinforcement learning techniques for aligning models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a system for efficient failure recovery in distributed training of LLMs, directly addressing challenges in parallel computing, multi-node setups, and strategies like data parallelism and model parallelism to enhance reliability and scalability.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "FlashRecovery is a system designed to enhance the reliability of large-scale language model (LLM) training by addressing hardware and software failures through three key modules: real-time failure detection, scale-independent task restarts, and checkpoint-free recovery within a single step. By eliminating traditional checkpointing overhead and ensuring rapid recovery times—such as restoring training on 4,800 devices in 150 seconds—the system achieves optimal Recovery Time Objective (RTO) and Recovery Point Objective (RPO), making LLM training more efficient and scalable across various cluster sizes.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly innovative checkpoint-free recovery mechanism and scale-independent restart strategy, which significantly advances fault tolerance in large-scale LLM training by eliminating traditional overheads and ensuring rapid, consistent recovery. This represents a substantial leap beyond existing methods that rely on periodic checkpointing.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a broad range of future research in distributed computing and AI, as well as commercial applications, by improving the efficiency and reliability of LLM training on large clusters. Its innovations could lead to widespread adoption in handling failures, reducing downtime in real-world scenarios.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a high-quality, practical contribution to fault recovery in LLM training, making it essential for researchers and practitioners in distributed systems and AI to understand these advancements. While innovative, it is not transformative enough to be classified as a must-read for all in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6d0efedc2299eb961ff252b91d0f8dbfe826771a",
      "total_authors": 23,
      "authors_found": 22,
      "highest_h_index": 2,
      "average_h_index": 0.45454545454545453,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Haijun Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378865534"
        },
        {
          "name": "Jinxiang Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2282346580"
        },
        {
          "name": "Zhenhua Yu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2286537687"
        },
        {
          "name": "Yanyong Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2286252814"
        },
        {
          "name": "Xuejie Ji",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Kaining Mao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378800749"
        },
        {
          "name": "Jun Zhang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2244413092"
        },
        {
          "name": "Yaqing Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378901150"
        },
        {
          "name": "Ting Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379514992"
        },
        {
          "name": "Fei Jie",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2282307275"
        },
        {
          "name": "Xiemin Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378875587"
        },
        {
          "name": "Zhifang Cai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381254937"
        },
        {
          "name": "Junhua Cheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378896433"
        },
        {
          "name": "Shuwei Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2265456862"
        },
        {
          "name": "Wei Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2301587657"
        },
        {
          "name": "Xiaoming Bao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378813188"
        },
        {
          "name": "Hua Xu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2225998352"
        },
        {
          "name": "Shi-Mei Zhao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2316521825"
        },
        {
          "name": "Jun Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378879600"
        },
        {
          "name": "Hongwei Sun",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2303463677"
        },
        {
          "name": "Ziyang Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2346430340"
        },
        {
          "name": "Yi Xiong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376149193"
        },
        {
          "name": "Chunsheng Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378865303"
        }
      ]
    },
    {
      "id": "2509.03054",
      "title": "Binary Quantization For LLMs Through Dynamic Grouping",
      "authors": [
        "Xinzhe Zheng",
        "Zhen-Qun Yang",
        "Haoran Xie",
        "S. Joe Qin",
        "Arlene Chen",
        "Fangzhen Lin"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of Natural Language Processing (NLP) tasks, but require\nsubstantial memory and computational resources. Binary quantization, which\ncompresses model weights from 16-bit Brain Float to 1-bit representations in\n{-1, 1}, offers significant reductions in storage and inference costs. However,\nsuch aggressive quantization often leads to notable performance degradation\ncompared to more conservative 4-bit quantization methods. In this research, we\npropose a novel optimization objective tailored for binary quantization, along\nwith three algorithms designed to realize it effectively. Our method enhances\nblocked quantization by dynamically identifying optimal unstructured\nsub-matrices through adaptive grouping strategies. Experimental results\ndemonstrate that our approach achieves an average bit length of just 1.007\nbits, while maintaining high model quality. Specifically, our quantized LLaMA\n3.2 3B model attains a perplexity of 8.23, remarkably close to the original\n7.81, and surpasses previous SOTA BiLLM with a perplexity of only 123.90.\nFurthermore, our method is competitive with SOTA 4-bit approaches such as GPTQ\nin both performance and efficiency. The compression process is highly\nefficient, requiring only 14 seconds to quantize the full LLaMA 3.2 3B weights\non a single CPU core, with the entire process completing in under 100 minutes\nand exhibiting embarrassingly parallel properties.\n  Code - https://github.com/johnnyzheng0636/WGM_bi_quan",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03054v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03054v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.407,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on binary quantization for LLMs, emphasizing compression techniques and efficient processing with \"embarrassingly parallel properties.\" While this hints at parallel computing for quantization, it does not address distributed training, such as partitioning data or computation across nodes for model training. Thus, the relevance is indirect and not central to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03059",
      "title": "Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers",
      "authors": [
        "Xingyue Huang",
        "Rishabh",
        "Gregor Franke",
        "Ziyi Yang",
        "Jiamu Bai",
        "Weijie Bai",
        "Jinhe Bi",
        "Zifeng Ding",
        "Yiqun Duan",
        "Chengyu Fan",
        "Wendong Fan",
        "Xin Gao",
        "Ruohao Guo",
        "Yuan He",
        "Zhuangzhuang He",
        "Xianglong Hu",
        "Neil Johnson",
        "Bowen Li",
        "Fangru Lin",
        "Siyu Lin",
        "Tong Liu",
        "Yunpu Ma",
        "Hao Shen",
        "Hao Sun",
        "Beibei Wang",
        "Fangyijie Wang",
        "Hao Wang",
        "Haoran Wang",
        "Yang Wang",
        "Yifeng Wang",
        "Zhaowei Wang",
        "Ziyang Wang",
        "Yifan Wu",
        "Zikai Xiao",
        "Chengxing Xie",
        "Fan Yang",
        "Junxiao Yang",
        "Qianshuo Ye",
        "Ziyu Ye",
        "Guangtao Zeng",
        "Yuwen Ebony Zhang",
        "Zeyu Zhang",
        "Zihao Zhu",
        "Bernard Ghanem",
        "Philip Torr",
        "Guohao Li"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have shown that their\nreasoning capabilities can be significantly improved through Reinforcement\nLearning with Verifiable Reward (RLVR), particularly in domains like\nmathematics and programming, where ground-truth correctness can be\nautomatically evaluated. However, extending this success to other\nreasoning-intensive domains remains challenging due to the scarcity of\nhigh-quality, verifiable datasets and the high cost of human supervision. In\nthis work, we introduce the Loong Project: an open-source framework for\nscalable synthetic data generation and verification across a diverse range of\nreasoning-intensive domains. The framework consists of two key components: (1)\nLoongBench, a curated seed dataset containing 8,729 human-vetted examples\nacross 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired\nwith executable code and rich metadata; and (2) LoongEnv, a modular synthetic\ndata generation environment that supports multiple prompting strategies to\nproduce new question-answer-code triples. Together, these components form an\nagent-environment loop that enables reinforcement learning, where an LLM-based\nagent is rewarded for generating Chain-of-Thought (CoT) solutions that align\nwith code-executed answers. Empirically, we benchmark LoongBench on a broad\nsuite of both open-source and proprietary LLMs to evaluate domain coverage and\nreveal performance bottlenecks. In addition, we conduct a comprehensive\nanalysis of synthetic data generated by LoongEnv, examining correctness,\ndifficulty, and diversity. Code and documentation are available at\nhttps://github.com/camel-ai/loong.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03059v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03059v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.438,
      "diffusion_reasoning_score": 0.54,
      "distributed_training_score": 0.39,
      "datasets_score": 0.405,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on Reinforcement Learning with Verifiable Reward (RLVR), which uses automated verification through code execution, not human feedback or a reward model trained on human-ranked data. Since RLHF specifically requires human involvement, this paper does not align with that definition.",
      "weak_supervision_justification": "The paper involves programmatically generating synthetic data and labels using LLMs and automated verification, which resembles weak supervision by relying on high-level sources rather than perfect hand-labeled data. However, it starts with a human-vetted seed dataset, making it only partially aligned and not a pure example of weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper does not mention or utilize diffusion models or iterative refinement processes for reasoning. It focuses on Chain-of-Thought generation with LLMs and reinforcement learning via verifiers, with no components involving multi-step logical reasoning through diffusion mechanisms.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contributions include introducing LoongBench, a new curated dataset with 8,729 examples across 12 domains, along with methodologies for its creation, benchmarking on various LLMs, and analysis of synthetic data generation. This directly aligns with research on creating, analyzing, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The Loong Project introduces an open-source framework to enhance the reasoning capabilities of Large Language Models (LLMs) across diverse domains by generating and verifying synthetic data, addressing the scarcity of high-quality datasets in areas beyond mathematics and programming. It comprises LoongBench, a curated seed dataset of 8,729 human-vetted examples across 12 reasoning-intensive domains, each with executable code and metadata, and LoongEnv, a modular environment for synthetic data generation that supports multiple prompting strategies to create question-answer-code triples, enabling an agent-environment loop for reinforcement learning with verifiable rewards. The paper benchmarks LoongBench on various LLMs to evaluate performance and analyzes the synthetic data for correctness, difficulty, and diversity, demonstrating its potential for scalable improvement of LLM reasoning.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by extending Reinforcement Learning with Verifiable Reward to new domains through a scalable synthetic data framework, combining existing ideas in a clever way to address dataset scarcity. However, it builds on established concepts like RL and synthetic data generation rather than introducing a entirely new problem or technique.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research by providing an open-source framework for generating verifiable synthetic data in underrepresented reasoning domains, which could lead to broader advancements in LLM capabilities and applications. As an open resource, it is likely to be adopted and built upon in both academic and commercial AI development.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by offering a practical framework for enhancing LLM reasoning across diverse domains, making it essential for researchers focused on synthetic data and reinforcement learning. While not groundbreaking enough to be a must-read, its tools and insights are highly relevant for advancing AI capabilities.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a12a067120f2a6a8fde119d636e5952c35c2ca94",
      "total_authors": 46,
      "authors_found": 40,
      "highest_h_index": 11,
      "average_h_index": 1.3,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Xingyue Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2370946595"
        },
        {
          "name": "Rishabh",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2370937216"
        },
        {
          "name": "Gregor Franke",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378847112"
        },
        {
          "name": "Ziyi Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378908680"
        },
        {
          "name": "Jiamu Bai",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Weijie Bai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378801026"
        },
        {
          "name": "Jinhe Bi",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2267727908"
        },
        {
          "name": "Zifeng Ding",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2046761003"
        },
        {
          "name": "Yiqun Duan",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Chengyu Fan",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Wendong Fan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2365355340"
        },
        {
          "name": "Xin Gao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378880320"
        },
        {
          "name": "Ruohao Guo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380497124"
        },
        {
          "name": "Yuan He",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2315916031"
        },
        {
          "name": "Zhuangzhuang He",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378898660"
        },
        {
          "name": "Xianglong Hu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363678366"
        },
        {
          "name": "Neil Johnson",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379548193"
        },
        {
          "name": "Bowen Li",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2237381947"
        },
        {
          "name": "Fangru Lin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363878201"
        },
        {
          "name": "Siyu Lin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378902585"
        },
        {
          "name": "Tonggan Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2283190752"
        },
        {
          "name": "Yunpu Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375151391"
        },
        {
          "name": "Hao Shen",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Hao Sun",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2274762125"
        },
        {
          "name": "Beibei Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2283878075"
        },
        {
          "name": "Fang-Fang Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2308560991"
        },
        {
          "name": "Hao Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378903601"
        },
        {
          "name": "Haoran Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2303894333"
        },
        {
          "name": "Yang Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2155654840"
        },
        {
          "name": "Yifeng Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2364620180"
        },
        {
          "name": "Zhaowei Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2308538216"
        },
        {
          "name": "Ziyang Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2366271635"
        },
        {
          "name": "Yifan Wu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2352395775"
        },
        {
          "name": "Zikai Xiao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Chengxing Xie",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2283436996"
        },
        {
          "name": "Fan Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2362966860"
        },
        {
          "name": "Junxiao Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378895796"
        },
        {
          "name": "Qianshuo Ye",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2365042914"
        },
        {
          "name": "Ziyu Ye",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Guang Zeng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2261430298"
        },
        {
          "name": "Yuwen Ebony Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378900512"
        },
        {
          "name": "Zeyu Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2365346105"
        },
        {
          "name": "Zihao Zhu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2305799942"
        },
        {
          "name": "Bernard Ghanem",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2274278297"
        },
        {
          "name": "Philip Torr",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2325901137"
        },
        {
          "name": "Guohao Li",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2325917687"
        }
      ]
    },
    {
      "id": "2509.03061",
      "title": "Isolated Bangla Handwritten Character Classification using Transfer\n  Learning",
      "authors": [
        "Abdul Karim",
        "S M Rafiuddin",
        "Jahidul Islam Razin",
        "Tahira Alam"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Bangla language consists of fifty distinct characters and many compound\ncharacters. Several notable studies have been performed to recognize Bangla\ncharacters, both handwritten and optical. Our approach uses transfer learning\nto classify the basic, distinct, as well as compound Bangla handwritten\ncharacters while avoiding the vanishing gradient problem. Deep Neural Network\ntechniques such as 3D Convolutional Neural Network (3DCNN), Residual Neural\nNetwork (ResNet), and MobileNet are applied to generate an end-to-end\nclassification of all possible standard formations of handwritten characters in\nthe Bangla language. The Bangla Lekha Isolated dataset, which contains 166,105\nBangla character image samples categorized into 84 distinct classes, is used\nfor this classification model. The model achieved 99.82% accuracy on training\ndata and 99.46% accuracy on test data. Comparisons with various\nstate-of-the-art benchmarks of Bangla handwritten character classification show\nthat the proposed model achieves better accuracy in classifying the data.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03061v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03061v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.332,
      "distributed_training_score": 0.337,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03062",
      "title": "High Cursive Complex Character Recognition using GAN External Classifier",
      "authors": [
        "S M Rafiuddin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Handwritten characters can be trickier to classify due to their complex and\ncursive nature compared to simple and non-cursive characters. We present an\nexternal classifier along with a Generative Adversarial Network that can\nclassify highly cursive and complex characters. The generator network produces\nfake handwritten character images, which are then used to augment the training\ndata after adding adversarially perturbed noise and achieving a confidence\nscore above a threshold with the discriminator network. The results show that\nthe accuracy of convolutional neural networks decreases as character complexity\nincreases, but our proposed model, ADA-GAN, remains more robust and effective\nfor both cursive and complex characters.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03062v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03062v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.311,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03066",
      "title": "S2M2ECG: Spatio-temporal bi-directional State Space Model Enabled\n  Multi-branch Mamba for ECG",
      "authors": [
        "Huaicheng Zhang",
        "Ruoxin Wang",
        "Chenlian Zhou",
        "Jiguang Shi",
        "Yue Ge",
        "Zhoutong Li",
        "Sheng Chang",
        "Hao Wang",
        "Jin He",
        "Qijun Huang"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "As one of the most effective methods for cardiovascular disease (CVD)\ndiagnosis, multi-lead Electrocardiogram (ECG) signals present a characteristic\nmulti-sensor information fusion challenge that has been continuously researched\nin deep learning domains. Despite the numerous algorithms proposed with\ndifferent DL architectures, maintaining a balance among performance,\ncomputational complexity, and multi-source ECG feature fusion remains\nchallenging. Recently, state space models (SSMs), particularly Mamba, have\ndemonstrated remarkable effectiveness across various fields. Their inherent\ndesign for high-efficiency computation and linear complexity makes them\nparticularly suitable for low-dimensional data like ECGs. This work proposes\nS2M2ECG, an SSM architecture featuring three-level fusion mechanisms: (1)\nSpatio-temporal bi-directional SSMs with segment tokenization for low-level\nsignal fusion, (2) Intra-lead temporal information fusion with bi-directional\nscanning to enhance recognition accuracy in both forward and backward\ndirections, (3) Cross-lead feature interaction modules for spatial information\nfusion. To fully leverage the ECG-specific multi-lead mechanisms inherent in\nECG signals, a multi-branch design and lead fusion modules are incorporated,\nenabling individual analysis of each lead while ensuring seamless integration\nwith others. Experimental results reveal that S2M2ECG achieves superior\nperformance in the rhythmic, morphological, and clinical scenarios. Moreover,\nits lightweight architecture ensures it has nearly the fewest parameters among\nexisting models, making it highly suitable for efficient inference and\nconvenient deployment. Collectively, S2M2ECG offers a promising alternative\nthat strikes an excellent balance among performance, computational complexity,\nand ECG-specific characteristics, paving the way for high-performance,\nlightweight computations in CVD diagnosis.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03066v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03066v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.354,
      "datasets_score": 0.299,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03093",
      "title": "Are We SOLID Yet? An Empirical Study on Prompting LLMs to Detect Design\n  Principle Violations",
      "authors": [
        "Fatih Pehlivan",
        "Arçin Ülkü Ergüzen",
        "Sahand Moslemi Yengejeh",
        "Mayasah Lami",
        "Anil Koyuncu"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Traditional static analysis methods struggle to detect semantic design flaws,\nsuch as violations of the SOLID principles, which require a strong\nunderstanding of object-oriented design patterns and principles. Existing\nsolutions typically focus on individual SOLID principles or specific\nprogramming languages, leaving a gap in the ability to detect violations across\nall five principles in multi-language codebases. This paper presents a new\napproach: a methodology that leverages tailored prompt engineering to assess\nLLMs on their ability to detect SOLID violations across multiple languages. We\npresent a benchmark of four leading LLMs-CodeLlama, DeepSeekCoder, QwenCoder,\nand GPT-4o Mini-on their ability to detect violations of all five SOLID\nprinciples. For this evaluation, we construct a new benchmark dataset of 240\nmanually validated code examples. Using this dataset, we test four distinct\nprompt strategies inspired by established zero-shot, few-shot, and\nchain-of-thought techniques to systematically measure their impact on detection\naccuracy. Our emerging results reveal a stark hierarchy among models, with\nGPT-4o Mini decisively outperforming others, yet even struggles with\nchallenging principles like DIP. Crucially, we show that prompt strategy has a\ndramatic impact, but no single strategy is universally best; for instance, a\ndeliberative ENSEMBLE prompt excels at OCP detection while a hint-based EXAMPLE\nprompt is superior for DIP violations. Across all experiments, detection\naccuracy is heavily influenced by language characteristics and degrades sharply\nwith increasing code complexity. These initial findings demonstrate that\neffective, AI-driven design analysis requires not a single best model, but a\ntailored approach that matches the right model and prompt to the specific\ndesign context, highlighting the potential of LLMs to support maintainability\nthrough AI-assisted code analysis.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03093v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03093v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.413,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.34,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating pre-trained LLMs using prompt engineering for detecting SOLID principle violations, without any mention of training models with human feedback, reward models, or reinforcement learning techniques. It does not involve aligning models with human preferences through RLHF.",
      "weak_supervision_justification": "The paper creates and uses a manually validated benchmark dataset of 240 code examples, relying on precise human labeling for evaluation, rather than employing weak supervision methods like programmatically generating noisy or imprecise labels for model training.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03095",
      "title": "TRELLIS-Enhanced Surface Features for Comprehensive Intracranial\n  Aneurysm Analysis",
      "authors": [
        "Clément Hervé",
        "Paul Garnier",
        "Jonathan Viquerat",
        "Elie Hachem"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Intracranial aneurysms pose a significant clinical risk yet are difficult to\ndetect, delineate and model due to limited annotated 3D data. We propose a\ncross-domain feature-transfer approach that leverages the latent geometric\nembeddings learned by TRELLIS, a generative model trained on large-scale\nnon-medical 3D datasets, to augment neural networks for aneurysm analysis. By\nreplacing conventional point normals or mesh descriptors with TRELLIS surface\nfeatures, we systematically enhance three downstream tasks: (i) classifying\naneurysms versus healthy vessels in the Intra3D dataset, (ii) segmenting\naneurysm and vessel regions on 3D meshes, and (iii) predicting time-evolving\nblood-flow fields using a graph neural network on the AnXplore dataset. Our\nexperiments show that the inclusion of these features yields strong gains in\naccuracy, F1-score and segmentation quality over state-of-the-art baselines,\nand reduces simulation error by 15\\%. These results illustrate the broader\npotential of transferring 3D representations from general-purpose generative\nmodels to specialized medical tasks.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03095v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03095v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.329,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.371,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03108",
      "title": "Backdoor Poisoning Attack Against Face Spoofing Attack Detection Methods",
      "authors": [
        "Shota Iwamatsu",
        "Koichi Ito",
        "Takafumi Aoki"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Face recognition systems are robust against environmental changes and noise,\nand thus may be vulnerable to illegal authentication attempts using user face\nphotos, such as spoofing attacks. To prevent such spoofing attacks, it is\ncrucial to discriminate whether the input image is a live user image or a\nspoofed image prior to the face recognition process. Most existing spoofing\nattack detection methods utilize deep learning, which necessitates a\nsubstantial amount of training data. Consequently, if malicious data is\ninjected into a portion of the training dataset, a specific spoofing attack may\nbe erroneously classified as live, leading to false positives. In this paper,\nwe propose a novel backdoor poisoning attack method to demonstrate the latent\nthreat of backdoor poisoning within face anti-spoofing detection. The proposed\nmethod enables certain spoofing attacks to bypass detection by embedding\nfeatures extracted from the spoofing attack's face image into a live face image\nwithout inducing any perceptible visual alterations. Through experiments\nconducted on public datasets, we demonstrate that the proposed method\nconstitutes a realistic threat to existing spoofing attack detection systems.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03108v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03108v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.298,
      "distributed_training_score": 0.302,
      "datasets_score": 0.277,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03112",
      "title": "Information transmission: Inferring change area from change moment in\n  time series remote sensing images",
      "authors": [
        "Jialu Li",
        "Chen Wu",
        "Meiqi Hu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Time series change detection is a critical task for exploring ecosystem\ndynamics using time series remote sensing images, because it can simultaneously\nindicate where and when change occur. While deep learning has shown excellent\nperformance in this domain, it continues to approach change area detection and\nchange moment identification as distinct tasks. Given that change area can be\ninferred from change moment, we propose a time series change detection network,\nnamed CAIM-Net (Change Area Inference from Moment Network), to ensure\nconsistency between change area and change moment results. CAIM-Net infers\nchange area from change moment based on the intrinsic relationship between time\nseries analysis and spatial change detection. The CAIM-Net comprises three key\nsteps: Difference Extraction and Enhancement, Coarse Change Moment Extraction,\nand Fine Change Moment Extraction and Change Area Inference. In the Difference\nExtraction and Enhancement, a lightweight encoder with batch dimension stacking\nis designed to rapidly extract difference features. Subsequently, boundary\nenhancement convolution is applied to amplify these difference features. In the\nCoarse Change Moment Extraction, the enhanced difference features from the\nfirst step are used to spatiotemporal correlation analysis, and then two\ndistinct methods are employed to determine coarse change moments. In the Fine\nChange Moment Extraction and Change Area Inference, a multiscale temporal Class\nActivation Mapping (CAM) module first increases the weight of the\nchange-occurring moment from coarse change moments. Then the weighted change\nmoment is used to infer change area based on the fact that pixels with the\nchange moment must have undergone a change.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03112v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03112v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.276,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.292,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03113",
      "title": "Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection",
      "authors": [
        "Shan Wang",
        "Maying Shen",
        "Nadine Chang",
        "Chuong Nguyen",
        "Hongdong Li",
        "Jose M. Alvarez"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Hallucinations in multimodal large language model are caused by the\ntext-visual bias and the co-occurrence bias. The former reflects an\nover-reliance on text information in the decision-making process, while the\nlatter arises from the statistical object-pairing patterns abstracted from the\ntraining data. Existing mitigation methods heuristically address these biases\nwithout understanding the fluctuating bias level across the instances. We first\npropose estimating the influence of respective token types (visual, prompt, and\nprevious outputs) using a gradient-based self-reflection method. The estimated\ntoken influence further enables the detection of object-related visual tokens\nand their integration into an influence-aware contrastive decoding framework to\nmitigate both types of biases simultaneously. Our method operates without the\nneed for additional resources, such as costly fine-tuning, extra models, or\ndata statistics. Extensive experiments show it effectively reduces\nhallucinations, achieving up to a 92% accuracy increase on LLaVA-QA90.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03113v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03113v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.438,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.485,
      "distributed_training_score": 0.336,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a gradient-based method for mitigating hallucinations in multimodal models, focusing on bias estimation and contrastive decoding. It does not involve human feedback, reward models, or any form of reinforcement learning for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses hallucination mitigation in MLLMs using gradient-based self-reflection and contrastive decoding, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning for Chain-of-Thought.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03114",
      "title": "Towards Realistic Hand-Object Interaction with Gravity-Field Based\n  Diffusion Bridge",
      "authors": [
        "Miao Xu",
        "Xiangyu Zhu",
        "Xusheng Liang",
        "Zidu Wang",
        "Jinlin Wu",
        "Zhen Lei"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Existing reconstruction or hand-object pose estimation methods are capable of\nproducing coarse interaction states. However, due to the complex and diverse\ngeometry of both human hands and objects, these approaches often suffer from\ninterpenetration or leave noticeable gaps in regions that are supposed to be in\ncontact. Moreover, the surface of a real human hand undergoes non-negligible\ndeformations during interaction, which are difficult to capture and represent\nwith previous methods. To tackle these challenges, we formulate hand-object\ninteraction as an attraction-driven process and propose a Gravity-Field Based\nDiffusion Bridge (GravityDB) to simulate interactions between a deformable hand\nsurface and rigid objects. Our approach effectively resolves the aforementioned\nissues by generating physically plausible interactions that are free of\ninterpenetration, ensure stable grasping, and capture realistic hand\ndeformations. Furthermore, we incorporate semantic information from textual\ndescriptions to guide the construction of the gravitational field, enabling\nmore semantically meaningful interaction regions. Extensive qualitative and\nquantitative experiments on multiple datasets demonstrate the effectiveness of\nour method.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03114v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03114v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.365,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.321,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a diffusion model (GravityDB) for simulating physical hand-object interactions and refining 3D shapes through iterative processes, such as integrating stochastic differential equations for convergence and perturbations. However, it focuses on generative and physical simulation tasks in computer vision, not on adapting diffusion for complex logical tasks, multi-step reasoning, or holistic correction of a 'Chain-of-Thought' entity as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03118",
      "title": "A Hierarchical Deep Reinforcement Learning Framework for Traffic Signal\n  Control with Predictable Cycle Planning",
      "authors": [
        "Hankang Gu",
        "Yuli Zhang",
        "Chengming Wang",
        "Ruiyuan Jiang",
        "Ziheng Qiao",
        "Pengfei Fan",
        "Dongyao Jia"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Deep reinforcement learning (DRL) has become a popular approach in traffic\nsignal control (TSC) due to its ability to learn adaptive policies from complex\ntraffic environments. Within DRL-based TSC methods, two primary control\nparadigms are ``choose phase\" and ``switch\" strategies. Although the agent in\nthe choose phase paradigm selects the next active phase adaptively, this\nparadigm may result in unexpected phase sequences for drivers, disrupting their\nanticipation and potentially compromising safety at intersections. Meanwhile,\nthe switch paradigm allows the agent to decide whether to switch to the next\npredefined phase or extend the current phase. While this structure maintains a\nmore predictable order, it can lead to unfair and inefficient phase\nallocations, as certain movements may be extended disproportionately while\nothers are neglected. In this paper, we propose a DRL model, named Deep\nHierarchical Cycle Planner (DHCP), to allocate the traffic signal cycle\nduration hierarchically. A high-level agent first determines the split of the\ntotal cycle time between the North-South (NS) and East-West (EW) directions\nbased on the overall traffic state. Then, a low-level agent further divides the\nallocated duration within each major direction between straight and left-turn\nmovements, enabling more flexible durations for the two movements. We test our\nmodel on both real and synthetic road networks, along with multiple sets of\nreal and synthetic traffic flows. Empirical results show our model achieves the\nbest performance over all datasets against baselines.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03118v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03118v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.391,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes a hierarchical deep reinforcement learning framework for traffic signal control, focusing on learning adaptive policies from traffic environments using algorithms like DDPG. It does not involve human feedback, such as training a reward model on human-ranked data or fine-tuning based on human preferences. The core contribution is about optimizing traffic flow through DRL interactions with simulated or real traffic states, which is unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03122",
      "title": "From Evaluation to Defense: Constructing Persistent Edit-Based\n  Fingerprints for Large Language Models",
      "authors": [
        "Yue Li",
        "Xin Yi",
        "Dongsheng Shi",
        "Yongyi Cui",
        "Gerard de Melo",
        "Xiaoling Wang",
        "Linlin Wang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The intellectual property (IP) protection of Large Language Models (LLMs) is\nincreasingly critical. Injecting specialized fingerprints into LLMs through\ninstruction tuning is a common IP protection technique. However, this may\nsignificantly degrade model performance, requires substantial computational\nresources, and exhibits poor persistence under model modifications. We argue\nthat knowledge editing offers a lightweight alternative that is more suitable\nfor fingerprint injection. Accordingly, we apply knowledge editing to\nfingerprint injection for the first time and demonstrate its strong capability.\nDespite using scrambled text as fingerprints to prevent them from being\noverwritten during fine-tuning, degradation still occurs under large-scale\nfine-tuning. To address this, we propose Fingerprint Subspace-aware Fine-Tuning\n(FSFT), which reduces fingerprint degradation by constraining the update of the\nfingerprint subspace. The performance of FSFT exceeds fine-tuning by 10% even\nin the worst-case scenario. Additionally, we observe that the\nfingerprint-injected models struggle to distinguish between fingerprints and\nsimilar texts due to the high similarity of their features. This finding\nunderscores the urgent need for more robust and fine-grained fingerprinting\ninjection methods for LLMs.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03122v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03122v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.374,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on injecting fingerprints into LLMs using knowledge editing and fine-tuning for intellectual property protection, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses fingerprint injection and persistence in LLMs through knowledge editing, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03136",
      "title": "Adaptive KV-Cache Compression without Manually Setting Budget",
      "authors": [
        "Chenxia Tang",
        "Jianchun Liu",
        "Hongli Xu",
        "Liusheng Huang"
      ],
      "categories": [
        "cs.DB (Databases)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03136v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03136v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.387,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03137",
      "title": "A Neural Network Approach to Multi-radionuclide TDCR Beta Spectroscopy",
      "authors": [
        "Li Yi",
        "Qian Yang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "nucl-ex (Nuclear Experiment)"
      ],
      "abstract": "Liquid scintillation triple-to-doubly coincident ratio (TDCR) spectroscopy is\nwidely adopted as a standard method for radionuclide quantification because of\nits inherent advantages such as high precision, self-calibrating capability,\nand independence from radioactive reference sources. However, multiradionuclide\nanalysis via TDCR faces the challenges of limited automation and reliance on\nmixture-specific standards, which may not be easily available. Here, we present\nan Artificial Intelligence (AI) framework that combines numerical spectral\nsimulation and deep learning for standard-free automated analysis. $\\beta$\nspectra for model training were generated using Geant4 simulations coupled with\nstatistically modeled detector response sampling. A tailored neural network\narchitecture, trained on this dataset covering various nuclei mix ratio and\nquenching scenarios, enables autonomous resolution of individual radionuclide\nactivities and detecting efficiency through end-to-end learning paradigms. The\nmodel delivers consistent high accuracy across tasks: activity proportions\n(mean absolute error = 0.009), detection efficiencies (mean absolute error =\n0.002), and spectral reconstruction (Structural Similarity Index = 0.9998),\nvalidating its physical plausibility for quenched $\\beta$ spectroscopy. This\nAI-driven methodology exhibits significant potential for automated\nsafety-compliant multiradionuclide analysis with robust generalization,\nreal-time processing capabilities, and engineering feasibility, particularly in\nscenarios where reference materials are unavailable or rapid field analysis is\nrequired.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03137v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03137v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.402,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on developing an AI framework for multi-radionuclide TDCR beta spectroscopy using neural networks, including dataset generation via simulations and model training for spectral analysis. However, it does not address distributed training, parallel computing, or multi-node machine learning techniques. There is no discussion of partitioning data, model architecture, or computation across multiple processors or nodes to accelerate training, making the paper's contributions unrelated to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03140",
      "title": "Decentralised self-organisation of pivoting cube ensembles using\n  geometric deep learning",
      "authors": [
        "Nadezhda Dobreva",
        "Emmanuel Blazquez",
        "Jai Grover",
        "Dario Izzo",
        "Yuzhen Qin",
        "Dominik Dold"
      ],
      "categories": [
        "cs.NE (Neural and Evolutionary Computing)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "We present a decentralized model for autonomous reconfiguration of\nhomogeneous pivoting cube modular robots in two dimensions. Each cube in the\nensemble is controlled by a neural network that only gains information from\nother cubes in its local neighborhood, trained using reinforcement learning.\nFurthermore, using geometric deep learning, we include the grid symmetries of\nthe cube ensemble in the neural network architecture. We find that even the\nmost localized versions succeed in reconfiguring to the target shape, although\nreconfiguration happens faster the more information about the whole ensemble is\navailable to individual cubes. Near-optimal reconfiguration is achieved with\nonly nearest neighbor interactions by using multiple information passing\nbetween cubes, allowing them to accumulate more global information about the\nensemble. Compared to standard neural network architectures, using geometric\ndeep learning approaches provided only minor benefits. Overall, we successfully\ndemonstrate mostly local control of a modular self-assembling system, which is\ntransferable to other space-relevant systems with different action spaces, such\nas sliding cube modular robots and CubeSat swarms.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03140v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03140v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.404,
      "datasets_score": 0.273,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper describes training neural networks for decentralized control of modular robots using reinforcement learning (specifically PPO), but it does not focus on distributed training techniques such as parallel computing across multiple nodes or partitioning data/computation. While the robot ensemble operates in a distributed manner, the training process itself appears to be handled in a standard, non-distributed way, making the connection indirect.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03141",
      "title": "Temporally-Aware Diffusion Model for Brain Progression Modelling with\n  Bidirectional Temporal Regularisation",
      "authors": [
        "Mattia Litrico",
        "Francesco Guarnera",
        "Mario Valerio Giuffrida",
        "Daniele Ravì",
        "Sebastiano Battiato"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Generating realistic MRIs to accurately predict future changes in the\nstructure of brain is an invaluable tool for clinicians in assessing clinical\noutcomes and analysing the disease progression at the patient level. However,\ncurrent existing methods present some limitations: (i) some approaches fail to\nexplicitly capture the relationship between structural changes and time\nintervals, especially when trained on age-imbalanced datasets; (ii) others rely\nonly on scan interpolation, which lack clinical utility, as they generate\nintermediate images between timepoints rather than future pathological\nprogression; and (iii) most approaches rely on 2D slice-based architectures,\nthereby disregarding full 3D anatomical context, which is essential for\naccurate longitudinal predictions. We propose a 3D Temporally-Aware Diffusion\nModel (TADM-3D), which accurately predicts brain progression on MRI volumes. To\nbetter model the relationship between time interval and brain changes, TADM-3D\nuses a pre-trained Brain-Age Estimator (BAE) that guides the diffusion model in\nthe generation of MRIs that accurately reflect the expected age difference\nbetween baseline and generated follow-up scans. Additionally, to further\nimprove the temporal awareness of TADM-3D, we propose the Back-In-Time\nRegularisation (BITR), by training TADM-3D to predict bidirectionally from the\nbaseline to follow-up (forward), as well as from the follow-up to baseline\n(backward). Although predicting past scans has limited clinical applications,\nthis regularisation helps the model generate temporally more accurate scans. We\ntrain and evaluate TADM-3D on the OASIS-3 dataset, and we validate the\ngeneralisation performance on an external test set from the NACC dataset. The\ncode will be available upon acceptance.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03141v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03141v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.317,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.549,
      "distributed_training_score": 0.331,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a diffusion model for generating MRI scans to predict brain progression, focusing on image synthesis and temporal regularization. It uses iterative refinement for image generation but does not adapt this process to solve complex logical tasks, such as multi-step reasoning or treating a 'Chain-of-Thought' as an entity. There is no component for logical reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03154",
      "title": "Preserving instance continuity and length in segmentation through\n  connectivity-aware loss computation",
      "authors": [
        "Karol Szustakowski",
        "Luk Frank",
        "Julia Esser",
        "Jan Gründemann",
        "Marie Piraud"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In many biomedical segmentation tasks, the preservation of elongated\nstructure continuity and length is more important than voxel-wise accuracy. We\npropose two novel loss functions, Negative Centerline Loss and Simplified\nTopology Loss, that, applied to Convolutional Neural Networks (CNNs), help\npreserve connectivity of output instances. Moreover, we discuss characteristics\nof experiment design, such as downscaling and spacing correction, that help\nobtain continuous segmentation masks. We evaluate our approach on a 3D\nlight-sheet fluorescence microscopy dataset of axon initial segments (AIS), a\ntask prone to discontinuity due to signal dropout. Compared to standard CNNs\nand existing topology-aware losses, our methods reduce the number of\nsegmentation discontinuities per instance, particularly in regions with missing\ninput signal, resulting in improved instance length calculation in downstream\napplications. Our findings demonstrate that structural priors embedded in the\nloss design can significantly enhance the reliability of segmentation for\nbiological applications.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03154v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03154v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.321,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03161",
      "title": "Domain Adaptation of LLMs for Process Data",
      "authors": [
        "Rafael Seidi Oyamada",
        "Jari Peeperkorn",
        "Jochen De Weerdt",
        "Johannes De Smedt"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In recent years, Large Language Models (LLMs) have emerged as a prominent\narea of interest across various research domains, including Process Mining\n(PM). Current applications in PM have predominantly centered on prompt\nengineering strategies or the transformation of event logs into narrative-style\ndatasets, thereby exploiting the semantic capabilities of LLMs to address\ndiverse tasks. In contrast, this study investigates the direct adaptation of\npretrained LLMs to process data without natural language reformulation,\nmotivated by the fact that these models excel in generating sequences of\ntokens, similar to the objective in PM. More specifically, we focus on\nparameter-efficient fine-tuning techniques to mitigate the computational\noverhead typically associated with such models. Our experimental setup focuses\non Predictive Process Monitoring (PPM), and considers both single- and\nmulti-task predictions. The results demonstrate a potential improvement in\npredictive performance over state-of-the-art recurrent neural network (RNN)\napproaches and recent narrative-style-based solutions, particularly in the\nmulti-task setting. Additionally, our fine-tuned models exhibit faster\nconvergence and require significantly less hyperparameter optimization.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03161v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03161v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.431,
      "distributed_training_score": 0.37,
      "datasets_score": 0.387,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on parameter-efficient fine-tuning of LLMs for Predictive Process Monitoring using supervised methods on process data, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper uses real-world event logs with presumably accurate labels for fine-tuning LLMs, without any indication of programmatically generated, noisy, or imprecise labels characteristic of weak supervision.",
      "diffusion_reasoning_justification": "The paper adapts LLMs for sequence prediction in process mining through fine-tuning, and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03169",
      "title": "Rashomon in the Streets: Explanation Ambiguity in Scene Understanding",
      "authors": [
        "Helge Spieker",
        "Jørn Eirik Betten",
        "Arnaud Gotlieb",
        "Nadjib Lazaar",
        "Nassim Belmecheri"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Explainable AI (XAI) is essential for validating and trusting models in\nsafety-critical applications like autonomous driving. However, the reliability\nof XAI is challenged by the Rashomon effect, where multiple, equally accurate\nmodels can offer divergent explanations for the same prediction. This paper\nprovides the first empirical quantification of this effect for the task of\naction prediction in real-world driving scenes. Using Qualitative Explainable\nGraphs (QXGs) as a symbolic scene representation, we train Rashomon sets of two\ndistinct model classes: interpretable, pair-based gradient boosting models and\ncomplex, graph-based Graph Neural Networks (GNNs). Using feature attribution\nmethods, we measure the agreement of explanations both within and between these\nclasses. Our results reveal significant explanation disagreement. Our findings\nsuggest that explanation ambiguity is an inherent property of the problem, not\njust a modeling artifact.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03169v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03169v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.478,
      "distributed_training_score": 0.318,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines the Rashomon effect in Explainable AI for scene understanding in autonomous driving, using models like gradient boosting and Graph Neural Networks. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03170",
      "title": "Count2Density: Crowd Density Estimation without Location-level\n  Annotations",
      "authors": [
        "Mattia Litrico",
        "Feng Chen",
        "Michael Pound",
        "Sotirios A Tsaftaris",
        "Sebastiano Battiato",
        "Mario Valerio Giuffrida"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Crowd density estimation is a well-known computer vision task aimed at\nestimating the density distribution of people in an image. The main challenge\nin this domain is the reliance on fine-grained location-level annotations,\n(i.e. points placed on top of each individual) to train deep networks.\nCollecting such detailed annotations is both tedious, time-consuming, and poses\na significant barrier to scalability for real-world applications. To alleviate\nthis burden, we present Count2Density: a novel pipeline designed to predict\nmeaningful density maps containing quantitative spatial information using only\ncount-level annotations (i.e., total number of people) during training. To\nachieve this, Count2Density generates pseudo-density maps leveraging past\npredictions stored in a Historical Map Bank, thereby reducing confirmation\nbias. This bank is initialised using an unsupervised saliency estimator to\nprovide an initial spatial prior and is iteratively updated with an EMA of\npredicted density maps. These pseudo-density maps are obtained by sampling\nlocations from estimated crowd areas using a hypergeometric distribution, with\nthe number of samplings determined by the count-level annotations. To further\nenhance the spatial awareness of the model, we add a self-supervised\ncontrastive spatial regulariser to encourage similar feature representations\nwithin crowded regions while maximising dissimilarity with background regions.\nExperimental results demonstrate that our approach significantly outperforms\ncross-domain adaptation methods and achieves better results than recent\nstate-of-the-art approaches in semi-supervised settings across several\ndatasets. Additional analyses validate the effectiveness of each individual\ncomponent of our pipeline, confirming the ability of Count2Density to\neffectively retrieve spatial information from count-level annotations and\nenabling accurate subregion counting.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03170v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03170v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.354,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, Count2Density, aligns closely with weak supervision as it trains a model using count-level annotations—high-level, imprecise sources—to programmatically generate pseudo-density maps via a Historical Map Bank and sampling techniques. This approach avoids the need for finely hand-labeled data, directly embodying weak supervision by deriving training labels from noisy or high-level inputs while achieving effective density estimation.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Count2Density, a novel pipeline for crowd density estimation that leverages only count-level annotations to generate accurate density maps, addressing the limitations of methods requiring detailed location-level annotations. The methodology involves creating pseudo-density maps using a Historical Map Bank initialized with unsupervised saliency estimates and updated via exponential moving averages, combined with sampling based on attention maps and a self-supervised contrastive spatial regularizer to enhance spatial awareness. Key findings demonstrate that Count2Density outperforms state-of-the-art cross-domain and semi-supervised approaches on multiple datasets, significantly reducing errors and enabling subregion counting while validating the effectiveness of its components.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing techniques like a Historical Map Bank and contrastive learning to innovatively derive density maps from count-level annotations, offering a notable improvement over traditional methods without introducing an entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of crowd density estimation and computer vision, as it reduces annotation costs and improves scalability for real-world applications, though its influence may remain confined to specific domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, practical contribution by advancing annotation-efficient techniques in crowd density estimation, making it valuable for researchers in computer vision and machine learning focused on real-world challenges.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3f4d076c2db973eb9ea8a693e828baf022cc463e",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 49,
      "average_h_index": 17.666666666666668,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Mattia Litrico",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378844150"
        },
        {
          "name": "Feng Chen",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2258683216"
        },
        {
          "name": "Michael Pound",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378841988"
        },
        {
          "name": "S. Tsaftaris",
          "h_index": 49,
          "profile_url": "https://www.semanticscholar.org/author/1919157"
        },
        {
          "name": "S. Battiato",
          "h_index": 41,
          "profile_url": "https://www.semanticscholar.org/author/1742452"
        },
        {
          "name": "M. Giuffrida",
          "h_index": 14,
          "profile_url": "https://www.semanticscholar.org/author/3393888"
        }
      ]
    },
    {
      "id": "2509.03173",
      "title": "Deep Self-knowledge Distillation: A hierarchical supervised learning for\n  coronary artery segmentation",
      "authors": [
        "Mingfeng Lin"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Coronary artery disease is a leading cause of mortality, underscoring the\ncritical importance of precise diagnosis through X-ray angiography. Manual\ncoronary artery segmentation from these images is time-consuming and\ninefficient, prompting the development of automated models. However, existing\nmethods, whether rule-based or deep learning models, struggle with issues like\npoor performance and limited generalizability. Moreover, current knowledge\ndistillation methods applied in this field have not fully exploited the\nhierarchical knowledge of the model, leading to certain information waste and\ninsufficient enhancement of the model's performance capabilities for\nsegmentation tasks. To address these issues, this paper introduces Deep\nSelf-knowledge Distillation, a novel approach for coronary artery segmentation\nthat leverages hierarchical outputs for supervision. By combining Deep\nDistribution Loss and Pixel-wise Self-knowledge Distillation Loss, our method\nenhances the student model's segmentation performance through a hierarchical\nlearning strategy, effectively transferring knowledge from the teacher model.\nOur method combines a loosely constrained probabilistic distribution vector\nwith tightly constrained pixel-wise supervision, providing dual regularization\nfor the segmentation model while also enhancing its generalization and\nrobustness. Extensive experiments on XCAD and DCA1 datasets demonstrate that\nour approach outperforms the dice coefficient, accuracy, sensitivity and IoU\ncompared to other models in comparative evaluations.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03173v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03173v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.38,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03179",
      "title": "AutoDetect: Designing an Autoencoder-based Detection Method for\n  Poisoning Attacks on Object Detection Applications in the Military Domain",
      "authors": [
        "Alma M. Liezenga",
        "Stefan Wijnja",
        "Puck de Haan",
        "Niels W. T. Brink",
        "Jip J. van Stijn",
        "Yori Kamphuis",
        "Klamer Schutte"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Poisoning attacks pose an increasing threat to the security and robustness of\nArtificial Intelligence systems in the military domain. The widespread use of\nopen-source datasets and pretrained models exacerbates this risk. Despite the\nseverity of this threat, there is limited research on the application and\ndetection of poisoning attacks on object detection systems. This is especially\nproblematic in the military domain, where attacks can have grave consequences.\nIn this work, we both investigate the effect of poisoning attacks on military\nobject detectors in practice, and the best approach to detect these attacks. To\nsupport this research, we create a small, custom dataset featuring military\nvehicles: MilCivVeh. We explore the vulnerability of military object detectors\nfor poisoning attacks by implementing a modified version of the BadDet attack:\na patch-based poisoning attack. We then assess its impact, finding that while a\npositive attack success rate is achievable, it requires a substantial portion\nof the data to be poisoned -- raising questions about its practical\napplicability. To address the detection challenge, we test both specialized\npoisoning detection methods and anomaly detection methods from the visual\nindustrial inspection domain. Since our research shows that both classes of\nmethods are lacking, we introduce our own patch detection method: AutoDetect, a\nsimple, fast, and lightweight autoencoder-based method. Our method shows\npromising results in separating clean from poisoned samples using the\nreconstruction error of image slices, outperforming existing methods, while\nbeing less time- and memory-intensive. We urge that the availability of large,\nrepresentative datasets in the military domain is a prerequisite to further\nevaluate risks of poisoning attacks and opportunities patch detection.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03179v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03179v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.326,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03185",
      "title": "PPORLD-EDNetLDCT: A Proximal Policy Optimization-Based Reinforcement\n  Learning Framework for Adaptive Low-Dose CT Denoising",
      "authors": [
        "Debopom Sutradhar",
        "Ripon Kumar Debnath",
        "Mohaimenul Azam Khan Raiaan",
        "Yan Zhang",
        "Reem E. Mohamed",
        "Sami Azam"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Low-dose computed tomography (LDCT) is critical for minimizing radiation\nexposure, but it often leads to increased noise and reduced image quality.\nTraditional denoising methods, such as iterative optimization or supervised\nlearning, often fail to preserve image quality. To address these challenges, we\nintroduce PPORLD-EDNetLDCT, a reinforcement learning-based (RL) approach with\nEncoder-Decoder for LDCT. Our method utilizes a dynamic RL-based approach in\nwhich an advanced posterior policy optimization (PPO) algorithm is used to\noptimize denoising policies in real time, based on image quality feedback,\ntrained via a custom gym environment. The experimental results on the low dose\nCT image and projection dataset demonstrate that the proposed PPORLD-EDNetLDCT\nmodel outperforms traditional denoising techniques and other DL-based methods,\nachieving a peak signal-to-noise ratio of 41.87, a structural similarity index\nmeasure of 0.9814 and a root mean squared error of 0.00236. Moreover, in\nNIH-AAPM-Mayo Clinic Low Dose CT Challenge dataset our method achieved a PSNR\nof 41.52, SSIM of 0.9723 and RMSE of 0.0051. Furthermore, we validated the\nquality of denoising using a classification task in the COVID-19 LDCT dataset,\nwhere the images processed by our method improved the classification accuracy\nto 94%, achieving 4% higher accuracy compared to denoising without RL-based\ndenoising.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03185v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03185v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.419,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.356,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a reinforcement learning framework using Proximal Policy Optimization (PPO) for low-dose CT denoising, where rewards are based on automated metrics like PSNR and SSIM from a custom Gym environment. RLHF specifically requires human feedback to train a reward model and fine-tune the AI, such as using human-ranked data for preference alignment. This paper does not involve any human feedback, relying instead on algorithmic rewards, so it does not align with RLHF concepts.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03188",
      "title": "Prompt-Guided Patch UNet-VAE with Adversarial Supervision for Adrenal\n  Gland Segmentation in Computed Tomography Medical Images",
      "authors": [
        "Hania Ghouse",
        "Muzammil Behzad"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Segmentation of small and irregularly shaped abdominal organs, such as the\nadrenal glands in CT imaging, remains a persistent challenge due to severe\nclass imbalance, poor spatial context, and limited annotated data. In this\nwork, we propose a unified framework that combines variational reconstruction,\nsupervised segmentation, and adversarial patch-based feedback to address these\nlimitations in a principled and scalable manner. Our architecture is built upon\na VAE-UNet backbone that jointly reconstructs input patches and generates\nvoxel-level segmentation masks, allowing the model to learn disentangled\nrepresentations of anatomical structure and appearance. We introduce a\npatch-based training pipeline that selectively injects synthetic patches\ngenerated from the learned latent space, and systematically study the effects\nof varying synthetic-to-real patch ratios during training. To further enhance\noutput fidelity, the framework incorporates perceptual reconstruction loss\nusing VGG features, as well as a PatchGAN-style discriminator for adversarial\nsupervision over spatial realism. Comprehensive experiments on the BTCV dataset\ndemonstrate that our approach improves segmentation accuracy, particularly in\nboundary-sensitive regions, while maintaining strong reconstruction quality.\nOur findings highlight the effectiveness of hybrid generative-discriminative\ntraining regimes for small-organ segmentation and provide new insights into\nbalancing realism, diversity, and anatomical consistency in data-scarce\nscenarios.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03188v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03188v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.323,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03206",
      "title": "Autonomous Learning From Success and Failure: Goal-Conditioned\n  Supervised Learning with Negative Feedback",
      "authors": [
        "Zeqiang Zhang",
        "Fabian Wurzberger",
        "Gerrit Schmid",
        "Sebastian Gottwald",
        "Daniel A. Braun"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reinforcement learning faces significant challenges when applied to tasks\ncharacterized by sparse reward structures. Although imitation learning, within\nthe domain of supervised learning, offers faster convergence, it relies heavily\non human-generated demonstrations. Recently, Goal-Conditioned Supervised\nLearning (GCSL) has emerged as a potential solution by enabling self-imitation\nlearning for autonomous systems. By strategically relabelling goals, agents can\nderive policy insights from their own experiences. Despite the successes of\nthis framework, it presents two notable limitations: (1) Learning exclusively\nfrom self-generated experiences can exacerbate the agents' inherent biases; (2)\nThe relabelling strategy allows agents to focus solely on successful outcomes,\nprecluding them from learning from their mistakes. To address these issues, we\npropose a novel model that integrates contrastive learning principles into the\nGCSL framework to learn from both success and failure. Through empirical\nevaluations, we demonstrate that our algorithm overcomes limitations imposed by\nagents' initial biases and thereby enables more exploratory behavior. This\nfacilitates the identification and adoption of effective policies, leading to\nsuperior performance across a variety of challenging environments.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03206v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03206v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.514,
      "weak_supervision_score": 0.476,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.36,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces GCSL-NF, which relies on autonomous learning from the agent's own experiences and contrastive learning, without any involvement of human feedback, preferences, or ranked data. RLHF specifically requires human-generated data to train a reward model, which is absent here.",
      "weak_supervision_justification": "The paper uses programmatic relabelling of goals from the agent's experiences to generate training labels for both successes and failures, aligning with weak supervision's use of noisy or imprecise sources for labels. However, it is primarily focused on RL contexts rather than general weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces GCSL-NF, a novel extension to Goal-Conditioned Supervised Learning (GCSL), which addresses the limitations of traditional GCSL by incorporating learning from both successful and failed experiences through the integration of contrastive learning principles. The methodology involves collecting trajectories, relabelling achieved states as goals for positive samples, and using a learned distance function to identify negative samples from failures, thereby promoting exploration and overcoming agent biases; empirical evaluations demonstrate that GCSL-NF outperforms existing methods in challenging environments, leading to more effective policies.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by integrating contrastive learning into GCSL to learn from failures, significantly advancing the state-of-the-art in reinforcement learning for sparse reward tasks. This addresses key limitations like bias reinforcement and lack of exploration, making it a substantial innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of goal-conditioned reinforcement learning, as it enhances agent exploration and performance in sparse reward environments. However, its influence may be confined to specific AI applications rather than broadly transformative.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a high-quality contribution by innovating on existing RL methods to improve learning efficiency, making it valuable for researchers focused on reinforcement learning. While not essential for all, it provides insights that could inspire further advancements in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/49f6b72525f8a0ec70b7c114e6ecfe616e315229",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 7,
      "average_h_index": 2.2,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Zeqiang Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378949110"
        },
        {
          "name": "Fabian Wurzberger",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378842205"
        },
        {
          "name": "Gerrit Schmid",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/1703432418"
        },
        {
          "name": "Sebastian Gottwald",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/1823247"
        },
        {
          "name": "Daniel A. Braun",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2240073295"
        }
      ]
    },
    {
      "id": "2509.03211",
      "title": "Efficient Active Training for Deep LiDAR Odometry",
      "authors": [
        "Beibei Zhou",
        "Zhiyuan Zhang",
        "Zhenbo Song",
        "Jianhui Guo",
        "Hui Kong"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Robust and efficient deep LiDAR odometry models are crucial for accurate\nlocalization and 3D reconstruction, but typically require extensive and diverse\ntraining data to adapt to diverse environments, leading to inefficiencies. To\ntackle this, we introduce an active training framework designed to selectively\nextract training data from diverse environments, thereby reducing the training\nload and enhancing model generalization. Our framework is based on two key\nstrategies: Initial Training Set Selection (ITSS) and Active Incremental\nSelection (AIS). ITSS begins by breaking down motion sequences from general\nweather into nodes and edges for detailed trajectory analysis, prioritizing\ndiverse sequences to form a rich initial training dataset for training the base\nmodel. For complex sequences that are difficult to analyze, especially under\nchallenging snowy weather conditions, AIS uses scene reconstruction and\nprediction inconsistency to iteratively select training samples, refining the\nmodel to handle a wide range of real-world scenarios. Experiments across\ndatasets and weather conditions validate our approach's effectiveness. Notably,\nour method matches the performance of full-dataset training with just 52\\% of\nthe sequence volume, demonstrating the training efficiency and robustness of\nour active training paradigm. By optimizing the training process, our approach\nsets the stage for more agile and reliable LiDAR odometry systems, capable of\nnavigating diverse environmental conditions with greater precision.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03211v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03211v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.427,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on active training for selecting data subsets in deep LiDAR odometry, emphasizing data selection strategies like ITSS and AIS to improve efficiency. It does not involve programmatically generating labels from high-level, noisy, or imprecise sources, nor does it rely on weak supervision techniques; instead, it assumes standard labeled data for training.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper addresses efficient training through selective data sampling to reduce training load, but it does not discuss distributed training, parallel computing, or partitioning data/computation across multiple nodes or processors. The methods described, such as ITSS and AIS, are centered on data selection rather than distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03212",
      "title": "AIVA: An AI-based Virtual Companion for Emotion-aware Interaction",
      "authors": [
        "Chenxi Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have significantly improved\nnatural language understanding and generation, enhancing Human-Computer\nInteraction (HCI). However, LLMs are limited to unimodal text processing and\nlack the ability to interpret emotional cues from non-verbal signals, hindering\nmore immersive and empathetic interactions. This work explores integrating\nmultimodal sentiment perception into LLMs to create emotion-aware agents. We\npropose \\ours, an AI-based virtual companion that captures multimodal sentiment\ncues, enabling emotionally aligned and animated HCI. \\ours introduces a\nMultimodal Sentiment Perception Network (MSPN) using a cross-modal fusion\ntransformer and supervised contrastive learning to provide emotional cues.\nAdditionally, we develop an emotion-aware prompt engineering strategy for\ngenerating empathetic responses and integrate a Text-to-Speech (TTS) system and\nanimated avatar module for expressive interactions. \\ours provides a framework\nfor emotion-aware agents with applications in companion robotics, social care,\nmental health, and human-centered AI.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03212v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03212v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.469,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.333,
      "datasets_score": 0.388,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is developing an emotion-aware virtual companion by integrating multimodal sentiment perception with LLMs, using techniques like prompt engineering and sentiment networks. It does not involve reinforcement learning, human feedback for training a reward model, or fine-tuning based on human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on multimodal sentiment analysis and emotion-aware interactions with LLMs, including components like cross-modal fusion and prompt engineering, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03214",
      "title": "RTGMFF: Enhanced fMRI-based Brain Disorder Diagnosis via ROI-driven Text\n  Generation and Multimodal Feature Fusion",
      "authors": [
        "Junhao Jia",
        "Yifei Sun",
        "Yunyou Liu",
        "Cheng Yang",
        "Changmiao Wang",
        "Feiwei Qin",
        "Yong Peng",
        "Wenwen Min"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Functional magnetic resonance imaging (fMRI) is a powerful tool for probing\nbrain function, yet reliable clinical diagnosis is hampered by low\nsignal-to-noise ratios, inter-subject variability, and the limited frequency\nawareness of prevailing CNN- and Transformer-based models. Moreover, most fMRI\ndatasets lack textual annotations that could contextualize regional activation\nand connectivity patterns. We introduce RTGMFF, a framework that unifies\nautomatic ROI-level text generation with multimodal feature fusion for\nbrain-disorder diagnosis. RTGMFF consists of three components: (i) ROI-driven\nfMRI text generation deterministically condenses each subject's activation,\nconnectivity, age, and sex into reproducible text tokens; (ii) Hybrid\nfrequency-spatial encoder fuses a hierarchical wavelet-mamba branch with a\ncross-scale Transformer encoder to capture frequency-domain structure alongside\nlong-range spatial dependencies; and (iii) Adaptive semantic alignment module\nembeds the ROI token sequence and visual features in a shared space, using a\nregularized cosine-similarity loss to narrow the modality gap. Extensive\nexperiments on the ADHD-200 and ABIDE benchmarks show that RTGMFF surpasses\ncurrent methods in diagnostic accuracy, achieving notable gains in sensitivity,\nspecificity, and area under the ROC curve. Code is available at\nhttps://github.com/BeistMedAI/RTGMFF.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03214v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03214v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.487,
      "distributed_training_score": 0.371,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on fMRI-based brain disorder diagnosis using ROI-driven text generation, feature fusion, and encoders like wavelet-mamba and Transformer, with no involvement of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not utilize diffusion models or iterative refinement for multi-step logical reasoning; it instead emphasizes fMRI analysis, text generation, and multimodal feature fusion for diagnostic tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03219",
      "title": "Uncertainty-driven Adaptive Exploration",
      "authors": [
        "Leonidas Bakopoulos",
        "Georgios Chalkiadakis"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Adaptive exploration methods propose ways to learn complex policies via\nalternating between exploration and exploitation. An important question for\nsuch methods is to determine the appropriate moment to switch between\nexploration and exploitation and vice versa. This is critical in domains that\nrequire the learning of long and complex sequences of actions. In this work, we\npresent a generic adaptive exploration framework that employs uncertainty to\naddress this important issue in a principled manner. Our framework includes\nprevious adaptive exploration approaches as special cases. Moreover, we can\nincorporate in our framework any uncertainty-measuring mechanism of choice, for\ninstance mechanisms used in intrinsic motivation or epistemic uncertainty-based\nexploration methods. We experimentally demonstrate that our framework gives\nrise to adaptive exploration strategies that outperform standard ones across\nseveral MuJoCo environments.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03219v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03219v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.307,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on uncertainty-driven adaptive exploration in deep reinforcement learning for robotics tasks, using mechanisms to switch between exploration and exploitation based on uncertainty. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. There is no mention of treating a 'Chain-of-Thought' as an entity or any diffusion-based components, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03221",
      "title": "LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer\n  Features for Robust Organoid Segmentation and Tracking",
      "authors": [
        "Jing Zhang",
        "Siying Tao",
        "Jiao Li",
        "Tianhe Wang",
        "Junchen Wu",
        "Ruqian Hao",
        "Xiaohui Du",
        "Ruirong Tan",
        "Rui Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Organoids replicate organ structure and function, playing a crucial role in\nfields such as tumor treatment and drug screening. Their shape and size can\nindicate their developmental status, but traditional fluorescence labeling\nmethods risk compromising their structure. Therefore, this paper proposes an\nautomated, non-destructive approach to organoid segmentation and tracking. We\nintroduced the LGBP-OrgaNet, a deep learning-based system proficient in\naccurately segmenting, tracking, and quantifying organoids. The model leverages\ncomplementary information extracted from CNN and Transformer modules and\nintroduces the innovative feature fusion module, Learnable Gaussian Band Pass\nFusion, to merge data from two branches. Additionally, in the decoder, the\nmodel proposes a Bidirectional Cross Fusion Block to fuse multi-scale features,\nand finally completes the decoding through progressive concatenation and\nupsampling. SROrga demonstrates satisfactory segmentation accuracy and\nrobustness on organoids segmentation datasets, providing a potent tool for\norganoid research.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03221v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03221v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.361,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03240",
      "title": "Evaluation of Stress Detection as Time Series Events -- A Novel\n  Window-Based F1-Metric",
      "authors": [
        "Harald Vilhelm Skat-Rørdam",
        "Sneha Das",
        "Kathrine Sofie Rasmussen",
        "Nicole Nadine Lønfeldt",
        "Line Clemmensen"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ME (Methodology)"
      ],
      "abstract": "Accurate evaluation of event detection in time series is essential for\napplications such as stress monitoring with wearable devices, where ground\ntruth is typically annotated as single-point events, even though the underlying\nphenomena are gradual and temporally diffused. Standard metrics like F1 and\npoint-adjusted F1 (F1$_{pa}$) often misrepresent model performance in such\nreal-world, imbalanced datasets. We introduce a window-based F1 metric (F1$_w$)\nthat incorporates temporal tolerance, enabling a more robust assessment of\nevent detection when exact alignment is unrealistic. Empirical analysis in\nthree physiological datasets, two in-the-wild (ADARP, Wrist Angel) and one\nexperimental (ROAD), indicates that F1$_w$ reveals meaningful model performance\npatterns invisible to conventional metrics, while its window size can be\nadapted to domain knowledge to avoid overestimation. We show that the choice of\nevaluation metric strongly influences the interpretation of model performance:\nusing predictions from TimesFM, only our temporally tolerant metrics reveal\nstatistically significant improvements over random and null baselines in the\ntwo in-the-wild use cases. This work addresses key gaps in time series\nevaluation and provides practical guidance for healthcare applications where\nrequirements for temporal precision vary by context.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03240v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03240v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.251,
      "distributed_training_score": 0.294,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03244",
      "title": "FoMEMO: Towards Foundation Models for Expensive Multi-objective\n  Optimization",
      "authors": [
        "Yiming Yao",
        "Fei Liu",
        "Liang Zhao",
        "Xi Lin",
        "Qingfu Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Expensive multi-objective optimization is a prevalent and crucial concern in\nmany real-world scenarios, where sample-efficiency is vital due to the limited\nevaluations to recover the true Pareto front for decision making. Existing\nworks either involve rebuilding Gaussian process surrogates from scratch for\neach objective in each new problem encountered, or rely on extensive past\ndomain experiments for pre-training deep learning models, making them hard to\ngeneralize and impractical to cope with various emerging applications in the\nreal world. To address this issue, we propose a new paradigm named FoMEMO\n(Foundation Models for Expensive Multi-objective Optimization), which enables\nthe establishment of a foundation model conditioned on any domain trajectory\nand user preference, and facilitates fast in-context optimization based on the\npredicted preference-wise aggregation posteriors. Rather than accessing\nextensive domain experiments in the real world, we demonstrate that\npre-training the foundation model with a diverse set of hundreds of millions of\nsynthetic data can lead to superior adaptability to unknown problems, without\nnecessitating any subsequent model training or updates in the optimization\nprocess. We evaluate our method across a variety of synthetic benchmarks and\nreal-word applications, and demonstrate its superior generality and competitive\nperformance compared to existing methods.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03244v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03244v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.396,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces FoMEMO, a foundation model for expensive multi-objective optimization, pre-trained on synthetic data and conditioned on domain trajectories and user preferences for in-context optimization. While it mentions user preferences, there is no use of human-ranked data to train a reward model or apply reinforcement learning for fine-tuning, which are core elements of RLHF. The method relies on synthetic data and predictive modeling, not human feedback or RL processes, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03249",
      "title": "Structure Transfer: an Inference-Based Calculus for the Transformation\n  of Representations",
      "authors": [
        "Daniel Raggi",
        "Gem Stapleton",
        "Mateja Jamnik",
        "Aaron Stockdill",
        "Grecia Garcia Garcia",
        "Peter C-H. Cheng"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "Representation choice is of fundamental importance to our ability to\ncommunicate and reason effectively. A major unsolved problem, addressed in this\npaper, is how to devise representational-system (RS) agnostic techniques that\ndrive representation transformation and choice. We present a novel calculus,\ncalled structure transfer, that enables representation transformation across\ndiverse RSs. Specifically, given a source representation drawn from a source\nRS, the rules of structure transfer allow us to generate a target\nrepresentation for a target RS. The generality of structure transfer comes in\npart from its ability to ensure that the source representation and the\ngenerated target representation satisfy any specified relation (such as\nsemantic equivalence). This is done by exploiting schemas, which encode\nknowledge about RSs. Specifically, schemas can express preservation of\ninformation across relations between any pair of RSs, and this knowledge is\nused by structure transfer to derive a structure for the target representation\nwhich ensures that the desired relation holds. We formalise this using\nRepresentational Systems Theory, building on the key concept of a construction\nspace. The abstract nature of construction spaces grants them the generality to\nmodel RSs of diverse kinds, including formal languages, geometric figures and\ndiagrams, as well as informal notations. Consequently, structure transfer is a\nsystem-agnostic calculus that can be used to identify alternative\nrepresentations in a wide range of practical settings.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03249v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03249v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.284,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.265,
      "datasets_score": 0.258,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a calculus for transforming representations between representational systems using schemas and invariants, focusing on symbolic and logical transformations. It does not involve diffusion models, iterative refinement processes for logical tasks, or treating a chain-of-thought as a holistic entity for correction. There is no mention of multi-step reasoning via diffusion mechanisms, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03260",
      "title": "HyPV-LEAD: Proactive Early-Warning of Cryptocurrency Anomalies through\n  Data-Driven Structural-Temporal Modeling",
      "authors": [
        "Minjung Park",
        "Gyuyeon Na",
        "Soyoun Kim",
        "Sunyoung Moon",
        "HyeonJeong Cha",
        "Sangmi Chai"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Abnormal cryptocurrency transactions - such as mixing services, fraudulent\ntransfers, and pump-and-dump operations -- pose escalating risks to financial\nintegrity but remain notoriously difficult to detect due to class imbalance,\ntemporal volatility, and complex network dependencies. Existing approaches are\npredominantly model-centric and post hoc, flagging anomalies only after they\noccur and thus offering limited preventive value. This paper introduces\nHyPV-LEAD (Hyperbolic Peak-Valley Lead-time Enabled Anomaly Detection), a\ndata-driven early-warning framework that explicitly incorporates lead time into\nanomaly detection. Unlike prior methods, HyPV-LEAD integrates three\ninnovations: (1) window-horizon modeling to guarantee actionable lead-time\nalerts, (2) Peak-Valley (PV) sampling to mitigate class imbalance while\npreserving temporal continuity, and (3) hyperbolic embedding to capture the\nhierarchical and scale-free properties of blockchain transaction networks.\nEmpirical evaluation on large-scale Bitcoin transaction data demonstrates that\nHyPV-LEAD consistently outperforms state-of-the-art baselines, achieving a\nPR-AUC of 0.9624 with significant gains in precision and recall. Ablation\nstudies further confirm that each component - PV sampling, hyperbolic\nembedding, and structural-temporal modeling - provides complementary benefits,\nwith the full framework delivering the highest performance. By shifting anomaly\ndetection from reactive classification to proactive early-warning, HyPV-LEAD\nestablishes a robust foundation for real-time risk management, anti-money\nlaundering (AML) compliance, and financial security in dynamic blockchain\nenvironments.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03260v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03260v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.314,
      "distributed_training_score": 0.31,
      "datasets_score": 0.276,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03262",
      "title": "PI3DETR: Parametric Instance Detection of 3D Point Cloud Edges with a\n  Geometry-Aware 3DETR",
      "authors": [
        "Fabio F. Oberweger",
        "Michael Schwingshackl",
        "Vanessa Staderini"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present PI3DETR, an end-to-end framework that directly predicts 3D\nparametric curve instances from raw point clouds, avoiding the intermediate\nrepresentations and multi-stage processing common in prior work. Extending\n3DETR, our model introduces a geometry-aware matching strategy and specialized\nloss functions that enable unified detection of differently parameterized curve\ntypes, including cubic B\\'ezier curves, line segments, circles, and arcs, in a\nsingle forward pass. Optional post-processing steps further refine predictions\nwithout adding complexity. This streamlined design improves robustness to noise\nand varying sampling densities, addressing critical challenges in real world\nLiDAR and 3D sensing scenarios. PI3DETR sets a new state-of-the-art on the ABC\ndataset and generalizes effectively to real sensor data, offering a simple yet\npowerful solution for 3D edge and curve estimation.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03262v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03262v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.271,
      "weak_supervision_score": 0.277,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.315,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03263",
      "title": "Estudio de la eficiencia en la escalabilidad de GPUs para el\n  entrenamiento de Inteligencia Artificial",
      "authors": [
        "David Cortes",
        "Carlos Juiz",
        "Belen Bermejo"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.PF (Performance)"
      ],
      "abstract": "Training large-scale deep learning models has become a key challenge for the\nscientific community and industry. While the massive use of GPUs can\nsignificantly speed up training times, this approach has a negative impact on\nefficiency. In this article, we present a detailed analysis of the times\nreported by MLPerf Training v4.1 on four workloads: BERT, Llama2 LoRA,\nRetinaNet, and Stable Diffusion, showing that there are configurations that\noptimise the relationship between performance, GPU usage, and efficiency. The\nresults point to a break-even point that allows training times to be reduced\nwhile maximising efficiency.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03263v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03263v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.297,
      "distributed_training_score": 0.546,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution involves analyzing the scalability and efficiency of GPU-based training for AI models, focusing on distributed training scenarios across multiple GPUs. It examines workloads like BERT and Stable Diffusion using MLPerf benchmarks, addressing issues such as communication overhead and synchronization in multi-node setups, which directly align with distributed training techniques, parallel computing, and optimizing computation partitioning for accelerated model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper examines the efficiency and scalability of GPUs in training large-scale AI models, using data from MLPerf Training v4.1 on workloads such as BERT, Llama2 LoRA, RetinaNet, and Stable Diffusion, to identify optimal configurations that balance reduced training times with maximized GPU efficiency and resource utilization. The authors analyze reported training times to reveal a break-even point where performance gains are achieved without proportional efficiency losses, potentially lowering operational costs and environmental impact, and discuss factors influencing hardware performance to inform decision-making in AI infrastructure planning.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by analyzing existing MLPerf benchmark data to identify optimal GPU configurations for efficiency, offering a clever combination of established techniques rather than introducing a entirely new problem or architecture. However, it does not significantly advance the state-of-the-art beyond incremental insights into scalability trade-offs.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI hardware performance and efficiency, as it provides practical guidance for optimizing GPU usage in training models. While it has potential applications in reducing costs and environmental impact, its influence is primarily confined to specific areas like machine learning infrastructure rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers high-quality analysis and practical insights for researchers and practitioners focused on AI training efficiency, making it a valuable contribution for those involved in hardware optimization. However, it is not essential for a general audience, as its scope is somewhat niche and incremental.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2b8cf794d9bb9c26fcf1bb80845ea6b6f93023f5",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 1,
      "average_h_index": 0.6666666666666666,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "D. Cortés",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/133821999"
        },
        {
          "name": "Carlos Juiz",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2073438723"
        },
        {
          "name": "B. Bermejo",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2069478184"
        }
      ]
    },
    {
      "id": "2509.03267",
      "title": "SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D\n  Diffusion Model",
      "authors": [
        "Hongxu Yang",
        "Edina Timko",
        "Levente Lippenszky",
        "Vanda Czipczer",
        "Lehel Ferenczi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Synthetic tumors in medical images offer controllable characteristics that\nfacilitate the training of machine learning models, leading to an improved\nsegmentation performance. However, the existing methods of tumor synthesis\nyield suboptimal performances when tumor occupies a large spatial volume, such\nas breast tumor segmentation in MRI with a large field-of-view (FOV), while\ncommonly used tumor generation methods are based on small patches. In this\npaper, we propose a 3D medical diffusion model, called SynBT, to generate\nhigh-quality breast tumor (BT) in contrast-enhanced MRI images. The proposed\nmodel consists of a patch-to-volume autoencoder, which is able to compress the\nhigh-resolution MRIs into compact latent space, while preserving the resolution\nof volumes with large FOV. Using the obtained latent space feature vector, a\nmask-conditioned diffusion model is used to synthesize breast tumors within\nselected regions of breast tissue, resulting in realistic tumor appearances. We\nevaluated the proposed method for a tumor segmentation task, which demonstrated\nthe proposed high-quality tumor synthesis method can facilitate the common\nsegmentation models with performance improvement of 2-3% Dice Score on a large\npublic dataset, and therefore provides benefits for tumor segmentation in MRI\nimages.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03267v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03267v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.261,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.335,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using a diffusion model for generating synthetic breast tumors in MRI images, which involves iterative refinement for image synthesis. However, it does not adapt diffusion for multi-step logical reasoning or Chain-of-Thought processes; instead, it applies diffusion to visual data generation tasks. Thus, there is a minor connection through the use of diffusion models, but no clear component for logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03277",
      "title": "PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly\n  Detection",
      "authors": [
        "Qihang Zhou",
        "Shibo He",
        "Jiangtao Yan",
        "Wenchao Meng",
        "Jiming Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we aim to transfer CLIP's robust 2D generalization\ncapabilities to identify 3D anomalies across unseen objects of highly diverse\nclass semantics. To this end, we propose a unified framework to comprehensively\ndetect and segment 3D anomalies by leveraging both point- and pixel-level\ninformation. We first design PointAD, which leverages point-pixel\ncorrespondence to represent 3D anomalies through their associated rendering\npixel representations. This approach is referred to as implicit 3D\nrepresentation, as it focuses solely on rendering pixel anomalies but neglects\nthe inherent spatial relationships within point clouds. Then, we propose\nPointAD+ to further broaden the interpretation of 3D anomalies by introducing\nexplicit 3D representation, emphasizing spatial abnormality to uncover abnormal\nspatial relationships. Hence, we propose G-aggregation to involve geometry\ninformation to enable the aggregated point representations spatially aware. To\nsimultaneously capture rendering and spatial abnormality, PointAD+ proposes\nhierarchical representation learning, incorporating implicit and explicit\nanomaly semantics into hierarchical text prompts: rendering prompts for the\nrendering layer and geometry prompts for the geometry layer. A cross-hierarchy\ncontrastive alignment is further introduced to promote the interaction between\nthe rendering and geometry layers, facilitating mutual anomaly learning.\nFinally, PointAD+ integrates anomaly semantics from both layers to capture the\ngeneralized anomaly semantics. During the test, PointAD+ can integrate RGB\ninformation in a plug-and-play manner and further improve its detection\nperformance. Extensive experiments demonstrate the superiority of PointAD+ in\nZS 3D anomaly detection across unseen objects with highly diverse class\nsemantics, achieving a holistic understanding of abnormality.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03277v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03277v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.376,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03286",
      "title": "Accountability Framework for Healthcare AI Systems: Towards Joint\n  Accountability in Decision Making",
      "authors": [
        "Prachi Bagave",
        "Marcus Westberg",
        "Marijn Janssen",
        "Aaron Yi Ding"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "AI is transforming the healthcare domain and is increasingly helping\npractitioners to make health-related decisions. Therefore, accountability\nbecomes a crucial concern for critical AI-driven decisions. Although regulatory\nbodies, such as the EU commission, provide guidelines, they are highlevel and\nfocus on the ''what'' that should be done and less on the ''how'', creating a\nknowledge gap for actors. Through an extensive analysis, we found that the term\naccountability is perceived and dealt with in many different ways, depending on\nthe actor's expertise and domain of work. With increasing concerns about AI\naccountability issues and the ambiguity around this term, this paper bridges\nthe gap between the ''what'' and ''how'' of AI accountability, specifically for\nAI systems in healthcare. We do this by analysing the concept of\naccountability, formulating an accountability framework, and providing a\nthree-tier structure for handling various accountability mechanisms. Our\naccountability framework positions the regulations of healthcare AI systems and\nthe mechanisms adopted by the actors under a consistent accountability regime.\nMoreover, the three-tier structure guides the actors of the healthcare AI\nsystem to categorise the mechanisms based on their conduct. Through our\nframework, we advocate that decision-making in healthcare AI holds shared\ndependencies, where accountability should be dealt with jointly and should\nfoster collaborations. We highlight the role of explainability in instigating\ncommunication and information sharing between the actors to further facilitate\nthe collaborative process.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03286v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03286v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.427,
      "weak_supervision_score": 0.329,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.31,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is an accountability framework for AI in healthcare, focusing on concepts like joint accountability, explainability, and regulatory compliance. It does not discuss reinforcement learning, human feedback for training models, reward models, or fine-tuning AI systems based on human preferences. Therefore, it has no connection to Reinforcement Learning from Human Feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03294",
      "title": "A Comprehensive Guide to Differential Privacy: From Theory to User\n  Expectations",
      "authors": [
        "Napsu Karmitsa",
        "Antti Airola",
        "Tapio Pahikkala",
        "Tinja Pitkämäki"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The increasing availability of personal data has enabled significant advances\nin fields such as machine learning, healthcare, and cybersecurity. However,\nthis data abundance also raises serious privacy concerns, especially in light\nof powerful re-identification attacks and growing legal and ethical demands for\nresponsible data use. Differential privacy (DP) has emerged as a principled,\nmathematically grounded framework for mitigating these risks. This review\nprovides a comprehensive survey of DP, covering its theoretical foundations,\npractical mechanisms, and real-world applications. It explores key algorithmic\ntools and domain-specific challenges - particularly in privacy-preserving\nmachine learning and synthetic data generation. The report also highlights\nusability issues and the need for improved communication and transparency in DP\nsystems. Overall, the goal is to support informed adoption of DP by researchers\nand practitioners navigating the evolving landscape of data privacy.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03294v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03294v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.368,
      "datasets_score": 0.405,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper provides a survey of differential privacy, with a section on privacy-preserving synthetic data generation, which involves creating and evaluating synthetic datasets for machine learning applications. However, this is not the primary focus of the paper, as it primarily covers theoretical and practical aspects of differential privacy rather than in-depth research on dataset creation, analysis, benchmarking, or evaluation. Thus, datasets are mentioned tangentially but not as a core contribution.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03303",
      "title": "Automatic Differentiation of Agent-Based Models",
      "authors": [
        "Arnau Quera-Bofarull",
        "Nicholas Bishop",
        "Joel Dyer",
        "Daniel Jarne Ornia",
        "Anisoara Calinescu",
        "Doyne Farmer",
        "Michael Wooldridge"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Agent-based models (ABMs) simulate complex systems by capturing the bottom-up\ninteractions of individual agents comprising the system. Many complex systems\nof interest, such as epidemics or financial markets, involve thousands or even\nmillions of agents. Consequently, ABMs often become computationally demanding\nand rely on the calibration of numerous free parameters, which has\nsignificantly hindered their widespread adoption. In this paper, we demonstrate\nthat automatic differentiation (AD) techniques can effectively alleviate these\ncomputational burdens. By applying AD to ABMs, the gradients of the simulator\nbecome readily available, greatly facilitating essential tasks such as\ncalibration and sensitivity analysis. Specifically, we show how AD enables\nvariational inference (VI) techniques for efficient parameter calibration. Our\nexperiments demonstrate substantial performance improvements and computational\nsavings using VI on three prominent ABMs: Axtell's model of firms; Sugarscape;\nand the SIR epidemiological model. Our approach thus significantly enhances the\npracticality and scalability of ABMs for studying complex systems.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03303v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03303v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.433,
      "distributed_training_score": 0.345,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on automatic differentiation techniques for agent-based models to enhance computational efficiency in simulations, calibration, and sensitivity analysis. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks. There is no mention of treating a Chain-of-Thought as a single entity or holistic correction, making the paper unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03310",
      "title": "app.build: A Production Framework for Scaling Agentic Prompt-to-App\n  Generation with Environment Scaffolding",
      "authors": [
        "Evgenii Kniazev",
        "Arseny Kravchenko",
        "Igor Rekun",
        "James Broadhead",
        "Nikita Shamgunov",
        "Pranav Sah",
        "Pratik Nichite",
        "Ivan Yamshchikov"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "We present app.build (https://github.com/appdotbuild/agent/), an open-source\nframework that improves LLM-based application generation through systematic\nvalidation and structured environments. Our approach combines multi-layered\nvalidation pipelines, stack-specific orchestration, and model-agnostic\narchitecture, implemented across three reference stacks. Through evaluation on\n30 generation tasks, we demonstrate that comprehensive validation achieves\n73.3% viability rate with 30% reaching perfect quality scores, while\nopen-weights models achieve 80.8% of closed-model performance when provided\nstructured environments. The open-source framework has been adopted by the\ncommunity, with over 3,000 applications generated to date. This work\ndemonstrates that scaling reliable AI agents requires scaling environments, not\njust models -- providing empirical insights and complete reference\nimplementations for production-oriented agent systems.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03310v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03310v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.348,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on environment scaffolding for LLM-based code generation, emphasizing validation and structured feedback to improve reliability, rather than training models with programmatically generated labels. While the multi-layered validation and repair loops involve programmatic feedback, this is for iterative generation and error correction, not for creating training data from noisy sources as in weak supervision. Thus, there is a loose connection through automated feedback mechanisms, but the paper's core contribution does not align directly with weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03321",
      "title": "Empowering Lightweight MLLMs with Reasoning via Long CoT SFT",
      "authors": [
        "Linyu Ou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "While Reinforcement Learning with Verifiable Rewards has enhanced the\nreasoning of large-scale language models (LLMs), its efficacy for lightweight\nmultimodal language models (MLLMs) with fewer than seven billion parameters\nremains underexplored. This paper investigates the role of long\nChain-of-Thought (long CoT) data in enhancing the reasoning abilities of such\nMLLMs. Our findings demonstrate that Supervised Fine-Tuning (SFT) with long CoT\ndata significantly improves MLLM reasoning. Furthermore, we observe that after\nthis initial SFT phase, MLLMs can achieve additional performance gains through\na subsequent RL stage. We conclude that a SFT stage with long CoT data is a\ncritical prerequisite for developing the reasoning capabilities of lightweight\nMLLMs.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03321v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03321v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.434,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.544,
      "distributed_training_score": 0.374,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions a Reinforcement Learning (RL) stage after Supervised Fine-Tuning (SFT) to enhance reasoning in lightweight MLLMs, referencing RL with verifiable rewards. However, it does not specify that the RL involves a reward model trained on human-ranked data or aligns with human preferences, which is central to RLHF. Thus, while RL is discussed, it is not explicitly RLHF, making the paper only tangentially relevant.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using long Chain-of-Thought (CoT) data in Supervised Fine-Tuning (SFT) and subsequent RL to improve reasoning in MLLMs, with no mention of diffusion models, iterative refinement processes, or treating reasoning paths as entities for multi-step correction. Therefore, it does not relate to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03323",
      "title": "Heatmap Guided Query Transformers for Robust Astrocyte Detection across\n  Immunostains and Resolutions",
      "authors": [
        "Xizhe Zhang",
        "Jiayang Zhu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Astrocytes are critical glial cells whose altered morphology and density are\nhallmarks of many neurological disorders. However, their intricate branching\nand stain dependent variability make automated detection of histological images\na highly challenging task. To address these challenges, we propose a hybrid CNN\nTransformer detector that combines local feature extraction with global\ncontextual reasoning. A heatmap guided query mechanism generates spatially\ngrounded anchors for small and faint astrocytes, while a lightweight\nTransformer module improves discrimination in dense clusters. Evaluated on\nALDH1L1 and GFAP stained astrocyte datasets, the model consistently\noutperformed Faster R-CNN, YOLOv11 and DETR, achieving higher sensitivity with\nfewer false positives, as confirmed by FROC analysis. These results highlight\nthe potential of hybrid CNN Transformer architectures for robust astrocyte\ndetection and provide a foundation for advanced computational pathology tools.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03323v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03323v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.369,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03324",
      "title": "InfraDiffusion: zero-shot depth map restoration with diffusion models\n  and prompted segmentation from sparse infrastructure point clouds",
      "authors": [
        "Yixiong Jing",
        "Cheng Zhang",
        "Haibing Wu",
        "Guangming Wang",
        "Olaf Wysocki",
        "Brian Sheil"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Point clouds are widely used for infrastructure monitoring by providing\ngeometric information, where segmentation is required for downstream tasks such\nas defect detection. Existing research has automated semantic segmentation of\nstructural components, while brick-level segmentation (identifying defects such\nas spalling and mortar loss) has been primarily conducted from RGB images.\nHowever, acquiring high-resolution images is impractical in low-light\nenvironments like masonry tunnels. Point clouds, though robust to dim lighting,\nare typically unstructured, sparse, and noisy, limiting fine-grained\nsegmentation. We present InfraDiffusion, a zero-shot framework that projects\nmasonry point clouds into depth maps using virtual cameras and restores them by\nadapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific\ntraining, InfraDiffusion enhances visual clarity and geometric consistency of\ndepth maps. Experiments on masonry bridge and tunnel point cloud datasets show\nsignificant improvements in brick-level segmentation using the Segment Anything\nModel (SAM), underscoring its potential for automated inspection of masonry\nassets. Our code and data is available at\nhttps://github.com/Jingyixiong/InfraDiffusion-official-implement.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03324v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03324v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.345,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for depth map restoration from point clouds, specifically adapting the Denoising Diffusion Null-space Model (DDNM) for image enhancement in infrastructure monitoring. While it employs the iterative refinement process of diffusion models, this is applied to visual and geometric data processing, not to multi-step logical reasoning or treating a 'Chain-of-Thought' as a holistic entity for complex tasks. There is no component involving logical reasoning, making the paper unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03340",
      "title": "Equivariant Flow Matching for Symmetry-Breaking Bifurcation Problems",
      "authors": [
        "Fleur Hendriks",
        "Ondřej Rokoš",
        "Martin Doškář",
        "Marc G. D. Geers",
        "Vlado Menkovski"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)"
      ],
      "abstract": "Bifurcation phenomena in nonlinear dynamical systems often lead to multiple\ncoexisting stable solutions, particularly in the presence of symmetry breaking.\nDeterministic machine learning models struggle to capture this multiplicity,\naveraging over solutions and failing to represent lower-symmetry outcomes. In\nthis work, we propose a generative framework based on flow matching to model\nthe full probability distribution over bifurcation outcomes. Our method enables\ndirect sampling of multiple valid solutions while preserving system symmetries\nthrough equivariant modeling. We introduce a symmetric matching strategy that\naligns predicted and target outputs under group actions, allowing accurate\nlearning in equivariant settings. We validate our approach on a range of\nsystems, from toy models to complex physical problems such as buckling beams\nand the Allen-Cahn equation. Our results demonstrate that flow matching\nsignificantly outperforms non-probabilistic and variational methods in\ncapturing multimodal distributions and symmetry-breaking bifurcations, offering\na principled and scalable solution for modeling multistability in\nhigh-dimensional systems.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03340v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03340v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.368,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs flow matching, an iterative method similar to diffusion models, to generate distributions for bifurcation problems in dynamical systems. However, it focuses on physical simulations and symmetry-breaking in scientific contexts, not on adapting iterative refinement for complex logical tasks or multi-step reasoning like Chain-of-Thought. The shared iterative aspect provides a loose connection, but the paper lacks any component for logical reasoning, making it only tangentially relevant.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03341",
      "title": "On the MIA Vulnerability Gap Between Private GANs and Diffusion Models",
      "authors": [
        "Ilana Sebag",
        "Jean-Yves Franceschi",
        "Alain Rakotomamonjy",
        "Alexandre Allauzen",
        "Jamal Atif"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generative Adversarial Networks (GANs) and diffusion models have emerged as\nleading approaches for high-quality image synthesis. While both can be trained\nunder differential privacy (DP) to protect sensitive data, their sensitivity to\nmembership inference attacks (MIAs), a key threat to data confidentiality,\nremains poorly understood. In this work, we present the first unified\ntheoretical and empirical analysis of the privacy risks faced by differentially\nprivate generative models. We begin by showing, through a stability-based\nanalysis, that GANs exhibit fundamentally lower sensitivity to data\nperturbations than diffusion models, suggesting a structural advantage in\nresisting MIAs. We then validate this insight with a comprehensive empirical\nstudy using a standardized MIA pipeline to evaluate privacy leakage across\ndatasets and privacy budgets. Our results consistently reveal a marked privacy\nrobustness gap in favor of GANs, even in strong DP regimes, highlighting that\nmodel type alone can critically shape privacy leakage.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03341v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03341v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.479,
      "distributed_training_score": 0.354,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a theoretical and empirical analysis of privacy risks, specifically membership inference attacks, in differentially private GANs and diffusion models for image synthesis. It examines the stability and vulnerability of diffusion models in generative tasks but does not involve adapting the iterative refinement process of diffusion models for multi-step logical reasoning, chain-of-thought processing, or solving complex logical tasks. Since the paper lacks any component related to reasoning or logical inference, it does not align with the topic of Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03345",
      "title": "Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive\n  and Abductive Reasoning",
      "authors": [
        "Yunxin Sun",
        "Abulhair Saparov"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Reasoning is a core capability in artificial intelligence systems, for which\nlarge language models (LLMs) have recently shown remarkable progress. However,\nmost work focuses exclusively on deductive reasoning, which is problematic\nsince other types of reasoning are also essential in solving real-world\nproblems, and they are less explored. This work focuses on evaluating LLMs'\ninductive and abductive reasoning capabilities. We introduce a programmable and\nsynthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example\nconsists of an incomplete world model and a set of observations. The task for\nthe intelligent agent is to produce hypotheses to explain observations under\nthe incomplete world model to solve each reasoning example. We propose a new\nmetric to evaluate the quality of hypotheses based on Occam's Razor. We\nevaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs\ncan perform inductive and abductive reasoning in simple scenarios, but struggle\nwith complex world models and producing high-quality hypotheses, even with\npopular reasoning-enhancing techniques such as in-context learning and RLVR.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03345v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03345v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.538,
      "distributed_training_score": 0.329,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates RLVR (Reinforcement Learning with Verifiable Reward) as a technique for improving reasoning in LLMs, but it does not specify that this involves human feedback, such as training a reward model on human-ranked data. Since RLHF requires human feedback for alignment, the paper's focus on inductive and abductive reasoning benchmarks does not meet this criterion.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not mention or utilize any diffusion-based models, iterative refinement processes, or multi-step logical reasoning paths characteristic of diffusion-based reasoning. Its main contribution is a benchmark for inductive and abductive reasoning in LLMs, with no reference to diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03351",
      "title": "epiGPTope: A machine learning-based epitope generator and classifier",
      "authors": [
        "Natalia Flechas Manrique",
        "Alberto Martínez",
        "Elena López-Martínez",
        "Luc Andrea",
        "Román Orus",
        "Aitor Manteca",
        "Aitziber L. Cortajarena",
        "Llorenç Espinosa-Portalés"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Epitopes are short antigenic peptide sequences which are recognized by\nantibodies or immune cell receptors. These are central to the development of\nimmunotherapies, vaccines, and diagnostics. However, the rational design of\nsynthetic epitope libraries is challenging due to the large combinatorial\nsequence space, $20^n$ combinations for linear epitopes of n amino acids,\nmaking screening and testing unfeasible, even with high throughput experimental\ntechniques. In this study, we present a large language model, epiGPTope,\npre-trained on protein data and specifically fine-tuned on linear epitopes,\nwhich for the first time can directly generate novel epitope-like sequences,\nwhich are found to possess statistical properties analogous to the ones of\nknown epitopes. This generative approach can be used to prepare libraries of\nepitope candidate sequences. We further train statistical classifiers to\npredict whether an epitope sequence is of bacterial or viral origin, thus\nnarrowing the candidate library and increasing the likelihood of identifying\nspecific epitopes. We propose that such combination of generative and\npredictive models can be of assistance in epitope discovery. The approach uses\nonly primary amino acid sequences of linear epitopes, bypassing the need for a\ngeometric framework or hand-crafted features of the sequences. By developing a\nmethod to create biologically feasible sequences, we anticipate faster and more\ncost-effective generation and screening of synthetic epitopes, with relevant\napplications in the development of new biotechnologies.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03351v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03351v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.275,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.292,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03376",
      "title": "Transformer-Guided Content-Adaptive Graph Learning for Hyperspectral\n  Unmixing",
      "authors": [
        "Hui Chen",
        "Liangyu Liu",
        "Xianchao Xiu",
        "Wanquan Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Hyperspectral unmixing (HU) targets to decompose each mixed pixel in remote\nsensing images into a set of endmembers and their corresponding abundances.\nDespite significant progress in this field using deep learning, most methods\nfail to simultaneously characterize global dependencies and local consistency,\nmaking it difficult to preserve both long-range interactions and boundary\ndetails. This letter proposes a novel transformer-guided content-adaptive graph\nunmixing framework (T-CAGU), which overcomes these challenges by employing a\ntransformer to capture global dependencies and introducing a content-adaptive\ngraph neural network to enhance local relationships. Unlike previous work,\nT-CAGU integrates multiple propagation orders to dynamically learn the graph\nstructure, ensuring robustness against noise. Furthermore, T-CAGU leverages a\ngraph residual mechanism to preserve global information and stabilize training.\nExperimental results demonstrate its superiority over the state-of-the-art\nmethods. Our code is available at https://github.com/xianchaoxiu/T-CAGU.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03376v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03376v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.328,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03379",
      "title": "TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers",
      "authors": [
        "Guoxin Wang",
        "Qingyuan Wang",
        "Binhua Huang",
        "Shaowu Chen",
        "Deepu John"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Vision Transformers (ViTs) achieve strong performance in image classification\nbut incur high computational costs from processing all image tokens. To reduce\ninference costs in large ViTs without compromising accuracy, we propose\nTinyDrop, a training-free token dropping framework guided by a lightweight\nvision model. The guidance model estimates the importance of tokens while\nperforming inference, thereby selectively discarding low-importance tokens if\nlarge vit models need to perform attention calculations. The framework operates\nplug-and-play, requires no architectural modifications, and is compatible with\ndiverse ViT architectures. Evaluations on standard image classification\nbenchmarks demonstrate that our framework reduces FLOPs by up to 80% for ViTs\nwith minimal accuracy degradation, highlighting its generalization capability\nand practical utility for efficient ViT-based classification.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03379v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03379v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.409,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a training-free framework for reducing inference costs in Vision Transformers by selectively dropping tokens using a lightweight guidance model. It focuses on inference efficiency and computational optimization during model deployment, with no discussion of distributed training techniques, parallel computing across multiple nodes, or strategies for accelerating model training through data/model partitioning. Therefore, it does not relate to the topic of distributed training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03380",
      "title": "Situating AI Agents in their World: Aspective Agentic AI for Dynamic\n  Partially Observable Information Systems",
      "authors": [
        "Peter J. Bentley",
        "Soo Ling Lim",
        "Fuyuki Ishikawa"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Agentic LLM AI agents are often little more than autonomous chatbots: actors\nfollowing scripts, often controlled by an unreliable director. This work\nintroduces a bottom-up framework that situates AI agents in their environment,\nwith all behaviors triggered by changes in their environments. It introduces\nthe notion of aspects, similar to the idea of umwelt, where sets of agents\nperceive their environment differently to each other, enabling clearer control\nof information. We provide an illustrative implementation and show that\ncompared to a typical architecture, which leaks up to 83% of the time,\naspective agentic AI enables zero information leakage. We anticipate that this\nconcept of specialist agents working efficiently in their own information\nniches can provide improvements to both security and efficiency.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03380v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03380v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.271,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a framework for AI agents that focuses on environmental awareness, aspects for information control, and reducing leakage in dynamic systems. It does not involve human feedback, reward models, or reinforcement learning for aligning AI with human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03383",
      "title": "ANNIE: Be Careful of Your Robots",
      "authors": [
        "Yiyang Huang",
        "Zixuan Wang",
        "Zishen Wan",
        "Yapeng Tian",
        "Haobo Xu",
        "Yinhe Han",
        "Yiming Gan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "The integration of vision-language-action (VLA) models into embodied AI (EAI)\nrobots is rapidly advancing their ability to perform complex, long-horizon\ntasks in humancentric environments. However, EAI systems introduce critical\nsecurity risks: a compromised VLA model can directly translate adversarial\nperturbations on sensory input into unsafe physical actions. Traditional safety\ndefinitions and methodologies from the machine learning community are no longer\nsufficient. EAI systems raise new questions, such as what constitutes safety,\nhow to measure it, and how to design effective attack and defense mechanisms in\nphysically grounded, interactive settings. In this work, we present the first\nsystematic study of adversarial safety attacks on embodied AI systems, grounded\nin ISO standards for human-robot interactions. We (1) formalize a principled\ntaxonomy of safety violations (critical, dangerous, risky) based on physical\nconstraints such as separation distance, velocity, and collision boundaries;\n(2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with\n2,400 video-action sequences for evaluating embodied safety; and (3)\nANNIE-Attack, a task-aware adversarial framework with an attack leader model\nthat decomposes long-horizon goals into frame-level perturbations. Our\nevaluation across representative EAI models shows attack success rates\nexceeding 50% across all safety categories. We further demonstrate sparse and\nadaptive attack strategies and validate the real-world impact through physical\nrobot experiments. These results expose a previously underexplored but highly\nconsequential attack surface in embodied AI systems, highlighting the urgent\nneed for security-driven defenses in the physical AI era. Code is available at\nhttps://github.com/RLCLab/Annie.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03383v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03383v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.324,
      "datasets_score": 0.378,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on adversarial safety attacks, benchmarks, and safety definitions for embodied AI systems, emphasizing vision-language-action models and physical robot interactions. It does not involve training AI models using human feedback, reward models, or reinforcement learning for alignment with human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03385",
      "title": "Human Preference-Aligned Concept Customization Benchmark via Decomposed\n  Evaluation",
      "authors": [
        "Reina Ishikawa",
        "Ryo Fujii",
        "Hideo Saito",
        "Ryo Hachiuma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Evaluating concept customization is challenging, as it requires a\ncomprehensive assessment of fidelity to generative prompts and concept images.\nMoreover, evaluating multiple concepts is considerably more difficult than\nevaluating a single concept, as it demands detailed assessment not only for\neach individual concept but also for the interactions among concepts. While\nhumans can intuitively assess generated images, existing metrics often provide\neither overly narrow or overly generalized evaluations, resulting in\nmisalignment with human preference. To address this, we propose Decomposed GPT\nScore (D-GPTScore), a novel human-aligned evaluation method that decomposes\nevaluation criteria into finer aspects and incorporates aspect-wise assessments\nusing Multimodal Large Language Model (MLLM). Additionally, we release Human\nPreference-Aligned Concept Customization Benchmark (CC-AlignBench), a benchmark\ndataset containing both single- and multi-concept tasks, enabling stage-wise\nevaluation across a wide difficulty range -- from individual actions to\nmulti-person interactions. Our method significantly outperforms existing\napproaches on this benchmark, exhibiting higher correlation with human\npreferences. This work establishes a new standard for evaluating concept\ncustomization and highlights key challenges for future research. The benchmark\nand associated materials are available at\nhttps://github.com/ReinaIshikawa/D-GPTScore.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03385v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03385v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.498,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.366,
      "datasets_score": 0.471,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on evaluating concept customization in image generation using a new metric and benchmark, but it does not involve training AI models with human feedback via a reward model and reinforcement learning. There is no mention of RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses diffusion models for text-to-image generation and concept customization, but it does not adapt the diffusion process for multi-step logical reasoning or treat reasoning paths as entities for iterative refinement. The work is centered on image evaluation, not reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes creating and introducing the CC-AlignBench dataset for evaluating concept customization, along with detailed descriptions of its curation, benchmarking, and analysis for AI applications, directly aligning with research on datasets.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenges in evaluating concept customization for text-to-image generation by introducing D-GPTScore, a novel metric that uses Multimodal Large Language Models to decompose evaluation criteria into finer aspects for more accurate, human-aligned assessments. They also release CC-AlignBench, a comprehensive benchmark dataset with single and multi-concept tasks of varying complexity, demonstrating that their method achieves higher correlation with human preferences compared to existing approaches, thereby establishing a new standard for evaluation in this field.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting Multimodal Large Language Models for decomposed evaluation of concept customization, combining existing ideas like LLM-as-a-Judge in a new way to better align with human preferences.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in computer vision subfields by providing a standardized benchmark and evaluation method for concept customization, potentially leading to more human-aligned advancements in text-to-image generation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a valuable contribution with a new benchmark and metric that enhances evaluation practices in AI-generated images, making it essential for researchers focused on computer vision and human preference alignment.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0d5f4214d0b05562d8e72b3edd3d479f43d642f1",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 12,
      "average_h_index": 7.25,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Reina Ishikawa",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2059604554"
        },
        {
          "name": "Ryo Fujii",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/151184222"
        },
        {
          "name": "Hideo Saito",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2278219725"
        },
        {
          "name": "Ryo Hachiuma",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/10681518"
        }
      ]
    },
    {
      "id": "2509.03403",
      "title": "Beyond Correctness: Harmonizing Process and Outcome Rewards through RL\n  Training",
      "authors": [
        "Chenlu Ye",
        "Zhou Yu",
        "Ziji Zhang",
        "Hao Chen",
        "Narayanan Sadagopan",
        "Jing Huang",
        "Tong Zhang",
        "Anurag Beniwal"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has emerged to be a\npredominant paradigm for mathematical reasoning tasks, offering stable\nimprovements in reasoning ability. However, Outcome Reward Models (ORMs) in\nRLVR are too coarse-grained to distinguish flawed reasoning within correct\nanswers or valid reasoning within incorrect answers. This lack of granularity\nintroduces noisy and misleading gradients significantly and hinders further\nprogress in reasoning process quality. While Process Reward Models (PRMs) offer\nfine-grained guidance for intermediate steps, they frequently suffer from\ninaccuracies and are susceptible to reward hacking.\n  To resolve this dilemma, we introduce PRocess cOnsistency Filter (PROF), an\neffective data process curation method that harmonizes noisy, fine-grained\nprocess rewards with accurate, coarse-grained outcome rewards. Rather than\nnaively blending PRM and ORM in the objective function\n(arXiv:archive/2506.18896), PROF leverages their complementary strengths\nthrough consistency-driven sample selection. Our approach retains correct\nresponses with higher averaged process values and incorrect responses with\nlower averaged process values, while maintaining positive/negative training\nsample balance. Extensive experiments demonstrate that our method not only\nconsistently improves the final accuracy over $4\\%$ compared to the blending\napproaches, but also strengthens the quality of intermediate reasoning steps.\nCodes and training recipes are available at https://github.com/Chenluye99/PROF.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03403v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03403v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.487,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.349,
      "datasets_score": 0.268,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper employs reinforcement learning with reward models (PRMs and ORMs) to improve reasoning tasks, which shares similarities with RLHF in using rewards for fine-tuning, but it relies on verifiable and model-based rewards rather than explicitly using human-ranked data for training the reward model.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning techniques for harmonizing rewards in reasoning tasks and does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03408",
      "title": "Scalable and Loosely-Coupled Multimodal Deep Learning for Breast Cancer\n  Subtyping",
      "authors": [
        "Mohammed Amer",
        "Mohamed A. Suliman",
        "Tu Bui",
        "Nuria Garcia",
        "Serban Georgescu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Healthcare applications are inherently multimodal, benefiting greatly from\nthe integration of diverse data sources. However, the modalities available in\nclinical settings can vary across different locations and patients. A key area\nthat stands to gain from multimodal integration is breast cancer molecular\nsubtyping, an important clinical task that can facilitate personalized\ntreatment and improve patient prognosis. In this work, we propose a scalable\nand loosely-coupled multimodal framework that seamlessly integrates data from\nvarious modalities, including copy number variation (CNV), clinical records,\nand histopathology images, to enhance breast cancer subtyping. While our\nprimary focus is on breast cancer, our framework is designed to easily\naccommodate additional modalities, offering the flexibility to scale up or down\nwith minimal overhead without requiring re-training of existing modalities,\nmaking it applicable to other types of cancers as well. We introduce a\ndual-based representation for whole slide images (WSIs), combining traditional\nimage-based and graph-based WSI representations. This novel dual approach\nresults in significant performance improvements. Moreover, we present a new\nmultimodal fusion strategy, demonstrating its ability to enhance performance\nacross a range of multimodal conditions. Our comprehensive results show that\nintegrating our dual-based WSI representation with CNV and clinical health\nrecords, along with our pipeline and fusion strategy, outperforms\nstate-of-the-art methods in breast cancer subtyping.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03408v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03408v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.278,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.37,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03409",
      "title": "Multi-level SSL Feature Gating for Audio Deepfake Detection",
      "authors": [
        "Hoan My Tran",
        "Damien Lolive",
        "Aghilas Sini",
        "Arnaud Delhay",
        "Pierre-François Marteau",
        "David Guennec"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Recent advancements in generative AI, particularly in speech synthesis, have\nenabled the generation of highly natural-sounding synthetic speech that closely\nmimics human voices. While these innovations hold promise for applications like\nassistive technologies, they also pose significant risks, including misuse for\nfraudulent activities, identity theft, and security threats. Current research\non spoofing detection countermeasures remains limited by generalization to\nunseen deepfake attacks and languages. To address this, we propose a gating\nmechanism extracting relevant feature from the speech foundation XLS-R model as\na front-end feature extractor. For downstream back-end classifier, we employ\nMulti-kernel gated Convolution (MultiConv) to capture both local and global\nspeech artifacts. Additionally, we introduce Centered Kernel Alignment (CKA) as\na similarity metric to enforce diversity in learned features across different\nMultiConv layers. By integrating CKA with our gating mechanism, we hypothesize\nthat each component helps improving the learning of distinct synthetic speech\npatterns. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance on in-domain benchmarks while generalizing\nrobustly to out-of-domain datasets, including multilingual speech samples. This\nunderscores its potential as a versatile solution for detecting evolving speech\ndeepfake threats.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03409v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03409v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.366,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on audio deepfake detection using SSL features, gating mechanisms, and convolutional layers to extract and classify speech patterns. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03421",
      "title": "Generalist versus Specialist Vision Foundation Models for Ocular Disease\n  and Oculomics",
      "authors": [
        "Yukun Zhou",
        "Paul Nderitu",
        "Jocelyn Hui Lin Goh",
        "Justin Engelmann",
        "Siegfried K. Wagner",
        "Anran Ran",
        "Hongyang Jiang",
        "Lie Ju",
        "Ke Zou",
        "Sahana Srinivasan",
        "Hyunmin Kim",
        "Takahiro Ninomiya",
        "Zheyuan Wang",
        "Gabriel Dawei Yang",
        "Eden Ruffell",
        "Dominic Williamson",
        "Rui Santos",
        "Gabor Mark Somfai",
        "Carol Y. Cheung",
        "Tien Yin Wong",
        "Daniel C. Alexander",
        "Yih Chung Tham",
        "Pearse A. Keane"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Medical foundation models, pre-trained with large-scale clinical data,\ndemonstrate strong performance in diverse clinically relevant applications.\nRETFound, trained on nearly one million retinal images, exemplifies this\napproach in applications with retinal images. However, the emergence of\nincreasingly powerful and multifold larger generalist foundation models such as\nDINOv2 and DINOv3 raises the question of whether domain-specific pre-training\nremains essential, and if so, what gap persists. To investigate this, we\nsystematically evaluated the adaptability of DINOv2 and DINOv3 in retinal image\napplications, compared to two specialist RETFound models, RETFound-MAE and\nRETFound-DINOv2. We assessed performance on ocular disease detection and\nsystemic disease prediction using two adaptation strategies: fine-tuning and\nlinear probing. Data efficiency and adaptation efficiency were further analysed\nto characterise trade-offs between predictive performance and computational\ncost. Our results show that although scaling generalist models yields strong\nadaptability across diverse tasks, RETFound-DINOv2 consistently outperforms\nthese generalist foundation models in ocular-disease detection and oculomics\ntasks, demonstrating stronger generalisability and data efficiency. These\nfindings suggest that specialist retinal foundation models remain the most\neffective choice for clinical applications, while the narrowing gap with\ngeneralist foundation models suggests that continued data and model scaling can\ndeliver domain-relevant gains and position them as strong foundations for\nfuture medical foundation models.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03421v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03421v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.365,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03426",
      "title": "Time-Scaling State-Space Models for Dense Video Captioning",
      "authors": [
        "AJ Piergiovanni",
        "Ganesh Satish Mallya",
        "Dahun Kim",
        "Anelia Angelova"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Dense video captioning is a challenging video understanding task which aims\nto simultaneously segment the video into a sequence of meaningful consecutive\nevents and to generate detailed captions to accurately describe each event.\nExisting methods often encounter difficulties when working with the long videos\nassociated with dense video captioning, due to the computational complexity and\nmemory limitations. Furthermore, traditional approaches require the entire\nvideo as input, in order to produce an answer, which precludes online\nprocessing of the video. We address these challenges by time-scaling\nState-Space Models (SSMs) to even longer sequences than before. Our approach,\nState-Space Models with Transfer State, combines both the long-sequence and\nrecurrent properties of SSMs and addresses the main limitation of SSMs which\nare otherwise not able to sustain their state for very long contexts,\neffectively scaling SSMs further in time. The proposed model is particularly\nsuitable for generating captions on-the-fly, in an online or streaming manner,\nwithout having to wait for the full video to be processed, which is more\nbeneficial in practice. When applied to dense video captioning, our approach\nscales well with video lengths and uses 7x fewer FLOPs.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03426v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03426v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.298,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.386,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on State-Space Models (SSMs) for efficient processing of long video sequences in dense video captioning, emphasizing time-scaling and state propagation. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03430",
      "title": "EclipseTouch: Touch Segmentation on Ad Hoc Surfaces using Worn Infrared\n  Shadow Casting",
      "authors": [
        "Vimal Mollyn",
        "Nathan DeVrio",
        "Chris Harrison"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.GR (Graphics)",
        "cs.RO (Robotics)"
      ],
      "abstract": "The ability to detect touch events on uninstrumented, everyday surfaces has\nbeen a long-standing goal for mixed reality systems. Prior work has shown that\nvirtual interfaces bound to physical surfaces offer performance and ergonomic\nbenefits over tapping at interfaces floating in the air. A wide variety of\napproaches have been previously developed, to which we contribute a new\nheadset-integrated technique called \\systemname. We use a combination of a\ncomputer-triggered camera and one or more infrared emitters to create\nstructured shadows, from which we can accurately estimate hover distance (mean\nerror of 6.9~mm) and touch contact (98.0\\% accuracy). We discuss how our\ntechnique works across a range of conditions, including surface material,\ninteraction orientation, and environmental lighting.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03430v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03430v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.278,
      "weak_supervision_score": 0.292,
      "diffusion_reasoning_score": 0.251,
      "distributed_training_score": 0.26,
      "datasets_score": 0.221,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03433",
      "title": "Decoding Visual Neural Representations by Multimodal with Dynamic\n  Balancing",
      "authors": [
        "Kaili sun",
        "Xingyu Miao",
        "Bing Zhai",
        "Haoran Duan",
        "Yang Long"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this work, we propose an innovative framework that integrates EEG, image,\nand text data, aiming to decode visual neural representations from low\nsignal-to-noise ratio EEG signals. Specifically, we introduce text modality to\nenhance the semantic correspondence between EEG signals and visual content.\nWith the explicit semantic labels provided by text, image and EEG features of\nthe same category can be more closely aligned with the corresponding text\nrepresentations in a shared multimodal space. To fully utilize pre-trained\nvisual and textual representations, we propose an adapter module that\nalleviates the instability of high-dimensional representation while\nfacilitating the alignment and fusion of cross-modal features. Additionally, to\nalleviate the imbalance in multimodal feature contributions introduced by the\ntextual representations, we propose a Modal Consistency Dynamic Balance (MCDB)\nstrategy that dynamically adjusts the contribution weights of each modality. We\nfurther propose a stochastic perturbation regularization (SPR) term to enhance\nthe generalization ability of semantic perturbation-based models by introducing\ndynamic Gaussian noise in the modality optimization process. The evaluation\nresults on the ThingsEEG dataset show that our method surpasses previous\nstate-of-the-art methods in both Top-1 and Top-5 accuracy metrics, improving by\n2.0\\% and 4.7\\% respectively.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03433v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03433v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.343,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a framework for decoding visual neural representations using EEG, image, and text data, with techniques like multimodal alignment, dynamic balancing, and stochastic perturbation regularization. It does not involve diffusion models, iterative refinement for logical tasks, or treating a Chain-of-Thought as an entity for multi-step reasoning. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03451",
      "title": "SmartPoser: Arm Pose Estimation with a Smartphone and Smartwatch Using\n  UWB and IMU Data",
      "authors": [
        "Nathan DeVrio",
        "Vimal Mollyn",
        "Chris Harrison"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.GR (Graphics)",
        "cs.RO (Robotics)"
      ],
      "abstract": "The ability to track a user's arm pose could be valuable in a wide range of\napplications, including fitness, rehabilitation, augmented reality input, life\nlogging, and context-aware assistants. Unfortunately, this capability is not\nreadily available to consumers. Systems either require cameras, which carry\nprivacy issues, or utilize multiple worn IMUs or markers. In this work, we\ndescribe how an off-the-shelf smartphone and smartwatch can work together to\naccurately estimate arm pose. Moving beyond prior work, we take advantage of\nmore recent ultra-wideband (UWB) functionality on these devices to capture\nabsolute distance between the two devices. This measurement is the perfect\ncomplement to inertial data, which is relative and suffers from drift. We\nquantify the performance of our software-only approach using off-the-shelf\ndevices, showing it can estimate the wrist and elbow joints with a \\hl{median\npositional error of 11.0~cm}, without the user having to provide training data.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03451v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03451v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.288,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.25,
      "distributed_training_score": 0.276,
      "datasets_score": 0.244,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03462",
      "title": "sam-llm: interpretable lane change trajectoryprediction via parametric\n  finetuning",
      "authors": [
        "Zhuo Cao",
        "Yunxiao Shi",
        "Min Xu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "This work introduces SAM-LLM, a novel hybrid architecture that bridges the\ngap between the contextual reasoning of Large Language Models (LLMs) and the\nphysical precision of kinematic lane change models for autonomous driving. The\nsystem is designed for interpretable lane change trajectory prediction by\nfinetuning an LLM to output the core physical parameters of a trajectory model\ninstead of raw coordinates. For lane-keeping scenarios, the model predicts\ndiscrete coordinates, but for lane change maneuvers, it generates the\nparameters for an enhanced Sinusoidal Acceleration Model (SAM), including\nlateral displacement, maneuver duration, initial lateral velocity, and\nlongitudinal velocity change. This parametric approach yields a complete,\ncontinuous, and physically plausible trajectory model that is inherently\ninterpretable and computationally efficient, achieving an 80% reduction in\noutput size compared to coordinate-based methods. The SAM-LLM achieves a\nstate-of-the-art overall intention prediction accuracy of 98.73%, demonstrating\nperformance equivalent to traditional LLM predictors while offering significant\nadvantages in explainability and resource efficiency.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03462v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03462v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.449,
      "distributed_training_score": 0.35,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on fine-tuning an LLM for lane change trajectory prediction using parametric outputs, but it does not mention or involve reinforcement learning, human feedback, reward models, or alignment with human preferences. The fine-tuning described appears to be standard, likely supervised, without any indication of RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes LLMs and mentions Chain-of-Thought reasoning for predictions, but it does not incorporate diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. The approach is centered on parametric fine-tuning for trajectory prediction, not holistic correction of reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03465",
      "title": "Joint Training of Image Generator and Detector for Road Defect Detection",
      "authors": [
        "Kuan-Chuan Peng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Road defect detection is important for road authorities to reduce the vehicle\ndamage caused by road defects. Considering the practical scenarios where the\ndefect detectors are typically deployed on edge devices with limited memory and\ncomputational resource, we aim at performing road defect detection without\nusing ensemble-based methods or test-time augmentation (TTA). To this end, we\npropose to Jointly Train the image Generator and Detector for road defect\ndetection (dubbed as JTGD). We design the dual discriminators for the\ngenerative model to enforce both the synthesized defect patches and overall\nimages to look plausible. The synthesized image quality is improved by our\nproposed CLIP-based Fr\\'echet Inception Distance loss. The generative model in\nJTGD is trained jointly with the detector to encourage the generative model to\nsynthesize harder examples for the detector. Since harder synthesized images of\nbetter quality caused by the aforesaid design are used in the data\naugmentation, JTGD outperforms the state-of-the-art method in the RDD2022 road\ndefect detection benchmark across various countries under the condition of no\nensemble and TTA. JTGD only uses less than 20% of the number of parameters\ncompared with the competing baseline, which makes it more suitable for\ndeployment on edge devices in practice.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03465v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03465v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.394,
      "distributed_training_score": 0.418,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is the development of a joint training method for a generative model and a detector specifically for road defect detection, emphasizing techniques like dual discriminators and FID loss to improve image synthesis and detection performance. It does not address distributed training, parallel computing, multi-node machine learning, or any strategies for partitioning data/computation across multiple processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03467",
      "title": "Continuous Saudi Sign Language Recognition: A Vision Transformer\n  Approach",
      "authors": [
        "Soukeina Elhassen",
        "Lama Al Khuzayem",
        "Areej Alhothali",
        "Ohoud Alzamzami",
        "Nahed Alowaidi"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Sign language (SL) is an essential communication form for hearing-impaired\nand deaf people, enabling engagement within the broader society. Despite its\nsignificance, limited public awareness of SL often leads to inequitable access\nto educational and professional opportunities, thereby contributing to social\nexclusion, particularly in Saudi Arabia, where over 84,000 individuals depend\non Saudi Sign Language (SSL) as their primary form of communication. Although\ncertain technological approaches have helped to improve communication for\nindividuals with hearing impairments, there continues to be an urgent\nrequirement for more precise and dependable translation techniques, especially\nfor Arabic sign language variants like SSL. Most state-of-the-art solutions\nhave primarily focused on non-Arabic sign languages, resulting in a\nconsiderable absence of resources dedicated to Arabic sign language,\nspecifically SSL. The complexity of the Arabic language and the prevalence of\nisolated sign language datasets that concentrate on individual words instead of\ncontinuous speech contribute to this issue. To address this gap, our research\nrepresents an important step in developing SSL resources. To address this, we\nintroduce the first continuous Saudi Sign Language dataset called KAU-CSSL,\nfocusing on complete sentences to facilitate further research and enable\nsophisticated recognition systems for SSL recognition and translation.\nAdditionally, we propose a transformer-based model, utilizing a pretrained\nResNet-18 for spatial feature extraction and a Transformer Encoder with\nBidirectional LSTM for temporal dependencies, achieving 99.02\\% accuracy at\nsigner dependent mode and 77.71\\% accuracy at signer independent mode. This\ndevelopment leads the way to not only improving communication tools for the SSL\ncommunity but also making a substantial contribution to the wider field of sign\nlanguage.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03467v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03467v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.332,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03472",
      "title": "DPQuant: Efficient and Differentially-Private Model Training via Dynamic\n  Quantization Scheduling",
      "authors": [
        "Yubo Gao",
        "Renbo Tu",
        "Gennady Pekhimenko",
        "Nandita Vijaykumar"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "Differentially-Private SGD (DP-SGD) is a powerful technique to protect user\nprivacy when using sensitive data to train neural networks. During training,\nconverting model weights and activations into low-precision formats, i.e.,\nquantization, can drastically reduce training times, energy consumption, and\ncost, and is thus a widely used technique. In this work, we demonstrate that\nquantization causes significantly higher accuracy degradation in DP-SGD\ncompared to regular SGD. We observe that this is caused by noise injection in\nDP-SGD, which amplifies quantization variance, leading to disproportionately\nlarge accuracy degradation. To address this challenge, we present QPQuant, a\ndynamic quantization framework that adaptively selects a changing subset of\nlayers to quantize at each epoch. Our method combines two key ideas that\neffectively reduce quantization variance: (i) probabilistic sampling of the\nlayers that rotates which layers are quantized every epoch, and (ii) loss-aware\nlayer prioritization, which uses a differentially private loss sensitivity\nestimator to identify layers that can be quantized with minimal impact on model\nquality. This estimator consumes a negligible fraction of the overall privacy\nbudget, preserving DP guarantees. Empirical evaluations on ResNet18, ResNet50,\nand DenseNet121 across a range of datasets demonstrate that DPQuant\nconsistently outperforms static quantization baselines, achieving near\nPareto-optimal accuracy-compute trade-offs and up to 2.21x theoretical\nthroughput improvements on low-precision hardware, with less than 2% drop in\nvalidation accuracy.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03472v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03472v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.484,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on dynamic quantization for differentially private SGD (DP-SGD) to improve efficiency, which could indirectly benefit distributed training scenarios like federated learning by reducing resource demands on edge devices. However, it does not primarily address distributed training concepts such as data partitioning, model parallelism, or multi-node computation; instead, it emphasizes quantization techniques for single-device optimization.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03477",
      "title": "Robult: Leveraging Redundancy and Modality Specific Features for Robust\n  Multimodal Learning",
      "authors": [
        "Duy A. Nguyen",
        "Abhi Kamboj",
        "Minh N. Do"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Addressing missing modalities and limited labeled data is crucial for\nadvancing robust multimodal learning. We propose Robult, a scalable framework\ndesigned to mitigate these challenges by preserving modality-specific\ninformation and leveraging redundancy through a novel information-theoretic\napproach. Robult optimizes two core objectives: (1) a soft Positive-Unlabeled\n(PU) contrastive loss that maximizes task-relevant feature alignment while\neffectively utilizing limited labeled data in semi-supervised settings, and (2)\na latent reconstruction loss that ensures unique modality-specific information\nis retained. These strategies, embedded within a modular design, enhance\nperformance across various downstream tasks and ensure resilience to incomplete\nmodalities during inference. Experimental results across diverse datasets\nvalidate that Robult achieves superior performance over existing approaches in\nboth semi-supervised learning and missing modality contexts. Furthermore, its\nlightweight design promotes scalability and seamless integration with existing\narchitectures, making it suitable for real-world multimodal applications.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03477v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03477v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.453,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.387,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on multimodal learning with techniques like contrastive loss and information decomposition for handling missing modalities and limited labels, without any mention of human feedback, reward models, or reinforcement learning for alignment.",
      "weak_supervision_justification": "The paper employs semi-supervised learning with a soft PU contrastive loss to utilize limited labeled data and potentially noisy pseudo-labels, which aligns with weak supervision's use of imperfect or programmatically generated labels, though it does not primarily focus on large-scale label generation from high-level sources.",
      "diffusion_reasoning_justification": "The paper discusses information-theoretic approaches for multimodal learning and modality alignment, with no reference to diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Robult, a scalable framework for robust multimodal learning that tackles the challenges of missing modalities and limited labeled data by employing an information-theoretic approach based on Partial Information Decomposition. It optimizes a soft Positive-Unlabeled contrastive loss for aligning task-relevant features in semi-supervised settings and a latent reconstruction loss to retain modality-specific information, resulting in superior performance across diverse datasets, enhanced resilience to incomplete modalities, and improved scalability for real-world applications.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing techniques like contrastive learning and reconstruction with an information-theoretic framework to jointly address missing modalities and semi-supervised learning, though it builds on established concepts rather than introducing an entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in multimodal learning subfields due to its practical approach for real-world scenarios like autonomous vehicles and medical diagnostics, but its influence may be limited to specific applications rather than broadly transformative.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution by integrating solutions for key challenges in multimodal learning, making it important for researchers in AI and computer vision to be aware of its methods and findings.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6cfbea483bcd82100865d2680b54a63012071b7f",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 1,
      "average_h_index": 0.3333333333333333,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Duy A. Nguyen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2357824721"
        },
        {
          "name": "Abhi Kamboj",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2293318296"
        },
        {
          "name": "Minh N. Do",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2356402315"
        }
      ]
    },
    {
      "id": "2509.03487",
      "title": "SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation\n  Models",
      "authors": [
        "Jigang Fan",
        "Zhenghong Zhou",
        "Ruofan Jin",
        "Le Cong",
        "Mengdi Wang",
        "Zaixi Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Proteins play crucial roles in almost all biological processes. The\nadvancement of deep learning has greatly accelerated the development of protein\nfoundation models, leading to significant successes in protein understanding\nand design. However, the lack of systematic red-teaming for these models has\nraised serious concerns about their potential misuse, such as generating\nproteins with biological safety risks. This paper introduces SafeProtein, the\nfirst red-teaming framework designed for protein foundation models to the best\nof our knowledge. SafeProtein combines multimodal prompt engineering and\nheuristic beam search to systematically design red-teaming methods and conduct\ntests on protein foundation models. We also curated SafeProtein-Bench, which\nincludes a manually constructed red-teaming benchmark dataset and a\ncomprehensive evaluation protocol. SafeProtein achieved continuous jailbreaks\non state-of-the-art protein foundation models (up to 70% attack success rate\nfor ESM3), revealing potential biological safety risks in current protein\nfoundation models and providing insights for the development of robust security\nprotection technologies for frontier models. The codes will be made publicly\navailable at https://github.com/jigang-fan/SafeProtein.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03487v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03487v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.418,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.374,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces SafeProtein, a framework for red-teaming protein foundation models to assess biosafety risks through prompt engineering and benchmarking. It does not involve reinforcement learning, human feedback, reward models, or fine-tuning AI models based on human preferences. RLHF specifically requires training on human-ranked data for alignment, which is not addressed in the paper.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03493",
      "title": "On Entropy Control in LLM-RL Algorithms",
      "authors": [
        "Han Shen"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "For RL algorithms, appropriate entropy control is crucial to their\neffectiveness. To control the policy entropy, a commonly used method is entropy\nregularization, which is adopted in various popular RL algorithms including\nPPO, SAC and A3C. Although entropy regularization proves effective in robotic\nand games RL conventionally, studies found that it gives weak to no gains in\nLLM-RL training. In this work, we study the issues of entropy bonus in LLM-RL\nsetting. Specifically, we first argue that the conventional entropy\nregularization suffers from the LLM's extremely large response space and the\nsparsity of the optimal outputs. As a remedy, we propose AEnt, an entropy\ncontrol method that utilizes a new clamped entropy bonus with an automatically\nadjusted coefficient. The clamped entropy is evaluated with the re-normalized\npolicy defined on certain smaller token space, which encourages exploration\nwithin a more compact response set. In addition, the algorithm automatically\nadjusts entropy coefficient according to the clamped entropy value, effectively\ncontrolling the entropy-induced bias while leveraging the entropy's benefits.\nAEnt is tested in math-reasoning tasks under different base models and\ndatasets, and it is observed that AEnt outperforms the baselines consistently\nacross multiple benchmarks.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03493v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03493v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.472,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.346,
      "datasets_score": 0.266,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on entropy control in general RL algorithms for LLMs, such as PPO and A3C, and proposes improvements like AEnt for tasks like math-reasoning. It does not mention training a reward model using human-ranked data or aligning models with human preferences, which are core to RLHF. Therefore, it does not qualify as RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses RL techniques for LLMs, specifically entropy regularization, and evaluates it on tasks like math-reasoning. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as a holistic Chain-of-Thought entity, which are essential for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03494",
      "title": "Parameter-Efficient Adaptation of mPLUG-Owl2 via Pixel-Level Visual\n  Prompts for NR-IQA",
      "authors": [
        "Yahya Benmahane",
        "Mohammed El Hassouni"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we propose a novel parameter-efficient adaptation method for\nNo- Reference Image Quality Assessment (NR-IQA) using visual prompts optimized\nin pixel-space. Unlike full fine-tuning of Multimodal Large Language Models\n(MLLMs), our approach trains only 600K parameters at most (< 0.01% of the base\nmodel), while keeping the underlying model fully frozen. During inference,\nthese visual prompts are combined with images via addition and processed by\nmPLUG-Owl2 with the textual query \"Rate the technical quality of the image.\"\nEvaluations across distortion types (synthetic, realistic, AI-generated) on\nKADID- 10k, KonIQ-10k, and AGIQA-3k demonstrate competitive performance against\nfull finetuned methods and specialized NR-IQA models, achieving 0.93 SRCC on\nKADID-10k. To our knowledge, this is the first work to leverage pixel-space\nvisual prompts for NR-IQA, enabling efficient MLLM adaptation for low-level\nvision tasks. The source code is publicly available at https: // github. com/\nyahya-ben/ mplug2-vp-for-nriqa.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03494v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03494v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.32,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a parameter-efficient adaptation method for No-Reference Image Quality Assessment (NR-IQA) using visual prompts on mPLUG-Owl2, focusing on training a small set of parameters without altering the base model. It does not involve reinforcement learning, human feedback for training a reward model, or fine-tuning via RL to align with human preferences. Instead, it relies on standard dataset-based adaptation, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03498",
      "title": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and\n  Generation",
      "authors": [
        "Han Li",
        "Xinyu Peng",
        "Yaoming Wang",
        "Zelin Peng",
        "Xin Chen",
        "Rongxiang Weng",
        "Jingang Wang",
        "Xunliang Cai",
        "Wenrui Dai",
        "Hongkai Xiong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03498v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03498v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.464,
      "distributed_training_score": 0.351,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses OneCAT as an autoregressive model that reduces decoding steps compared to diffusion-based methods, positioning itself as an alternative for multimodal tasks. However, it does not adapt or use the iterative refinement process of diffusion for logical reasoning or Chain-of-Thought tasks. The reference to diffusion is merely for contrast, making the paper only tangentially related to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03499",
      "title": "DeepSea MOT: A benchmark dataset for multi-object tracking on deep-sea\n  video",
      "authors": [
        "Kevin Barnard",
        "Elaine Liu",
        "Kristine Walz",
        "Brian Schlining",
        "Nancy Jacobsen Stout",
        "Lonny Lundsten"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Benchmarking multi-object tracking and object detection model performance is\nan essential step in machine learning model development, as it allows\nresearchers to evaluate model detection and tracker performance on\nhuman-generated 'test' data, facilitating consistent comparisons between models\nand trackers and aiding performance optimization. In this study, a novel\nbenchmark video dataset was developed and used to assess the performance of\nseveral Monterey Bay Aquarium Research Institute object detection models and a\nFathomNet single-class object detection model together with several trackers.\nThe dataset consists of four video sequences representing midwater and benthic\ndeep-sea habitats. Performance was evaluated using Higher Order Tracking\nAccuracy, a metric that balances detection, localization, and association\naccuracy. To the best of our knowledge, this is the first publicly available\nbenchmark for multi-object tracking in deep-sea video footage. We provide the\nbenchmark data, a clearly documented workflow for generating additional\nbenchmark videos, as well as example Python notebooks for computing metrics.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03499v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03499v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.262,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.305,
      "distributed_training_score": 0.335,
      "datasets_score": 0.411,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and introduction of a novel benchmark dataset for multi-object tracking in deep-sea video footage, which directly aligns with research on datasets for machine learning and AI. It covers dataset creation (e.g., developing four video sequences from midwater and benthic habitats), curation methodologies (e.g., selecting representative footage and generating ground truth annotations), benchmark evaluation (e.g., using Higher Order Tracking Accuracy to assess detection and tracking performance), and analysis tools (e.g., providing Python notebooks and workflows). This makes it a strong fit for the topic.",
      "llm_score_status": "completed",
      "summary": "The paper introduces DeepSea MOT, a novel benchmark dataset comprising four video sequences from midwater and benthic deep-sea habitats, aimed at evaluating multi-object tracking (MOT) performance for object detection and association in challenging underwater environments. It assesses several Monterey Bay Aquarium Research Institute object detection models and trackers using the Higher Order Tracking Accuracy metric, highlighting unique deep-sea challenges like low visibility and high biodiversity, and provides the dataset, workflow for creating additional benchmarks, and example notebooks for metrics computation, marking the first publicly available MOT benchmark for deep-sea footage.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark dataset specifically for multi-object tracking in deep-sea video, addressing a previously uncovered domain and significantly advancing the state-of-the-art in underwater computer vision by filling a gap in existing benchmarks.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like marine biology and underwater computer vision, as it provides a standardized evaluation framework for MOT in deep-sea environments, though its influence may be limited to specialized applications rather than broad commercial or research areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper represents a strong and valuable contribution by establishing the first benchmark for deep-sea multi-object tracking, making it essential for researchers in aquatic AI and marine data analysis to be aware of this resource.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/bfeb7596b942b546774ab6da15b0c758543ad237",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 20,
      "average_h_index": 5.5,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "K. Barnard",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/104362802"
        },
        {
          "name": "Elaine Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378798940"
        },
        {
          "name": "Kristine Walz",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378845728"
        },
        {
          "name": "B. Schlining",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/13801389"
        },
        {
          "name": "Nancy Jacobsen Stout",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378841966"
        },
        {
          "name": "L. Lundsten",
          "h_index": 20,
          "profile_url": "https://www.semanticscholar.org/author/6629827"
        }
      ]
    },
    {
      "id": "2509.03500",
      "title": "Real-Time Instrument Planning and Perception for Novel Measurements of\n  Dynamic Phenomena",
      "authors": [
        "Itai Zilberstein",
        "Alberto Candela",
        "Steve Chien"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Advancements in onboard computing mean remote sensing agents can employ\nstate-of-the-art computer vision and machine learning at the edge. These\ncapabilities can be leveraged to unlock new rare, transient, and pinpoint\nmeasurements of dynamic science phenomena. In this paper, we present an\nautomated workflow that synthesizes the detection of these dynamic events in\nlook-ahead satellite imagery with autonomous trajectory planning for a\nfollow-up high-resolution sensor to obtain pinpoint measurements. We apply this\nworkflow to the use case of observing volcanic plumes. We analyze\nclassification approaches including traditional machine learning algorithms and\nconvolutional neural networks. We present several trajectory planning\nalgorithms that track the morphological features of a plume and integrate these\nalgorithms with the classifiers. We show through simulation an order of\nmagnitude increase in the utility return of the high-resolution instrument\ncompared to baselines while maintaining efficient runtimes.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03500v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03500v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.357,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03501",
      "title": "Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning\n  via Synthetic Instruction Data",
      "authors": [
        "Honglu Zhou",
        "Xiangyu Peng",
        "Shrikant Kendre",
        "Michael S. Ryoo",
        "Silvio Savarese",
        "Caiming Xiong",
        "Juan Carlos Niebles"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Next-generation AI companions must go beyond general video understanding to\nresolve spatial and temporal references in dynamic, real-world environments.\nExisting Video Large Language Models (Video LLMs), while capable of\ncoarse-level comprehension, struggle with fine-grained, spatiotemporal\nreasoning, especially when user queries rely on time-based event references for\ntemporal anchoring, or gestural cues for spatial anchoring to clarify object\nreferences and positions. To bridge this critical gap, we introduce Strefer, a\nsynthetic instruction data generation framework designed to equip Video LLMs\nwith spatiotemporal referring and reasoning capabilities. Strefer produces\ndiverse instruction-tuning data using a data engine that pseudo-annotates\ntemporally dense, fine-grained video metadata, capturing rich spatial and\ntemporal information in a structured manner, including subjects, objects, their\nlocations as masklets, and their action descriptions and timelines. Our\napproach enhances the ability of Video LLMs to interpret spatial and temporal\nreferences, fostering more versatile, space-time-aware reasoning essential for\nreal-world AI companions. Without using proprietary models, costly human\nannotation, or the need to annotate large volumes of new videos, experimental\nevaluations show that models trained with data produced by Strefer outperform\nbaselines on tasks requiring spatial and temporal disambiguation. Additionally,\nthese models exhibit enhanced space-time-aware reasoning, establishing a new\nfoundation for perceptually grounded, instruction-tuned Video LLMs.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03501v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03501v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.448,
      "diffusion_reasoning_score": 0.468,
      "distributed_training_score": 0.375,
      "datasets_score": 0.39,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on generating synthetic instruction data for training Video LLMs using automated processes, without any mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "The paper's core contribution involves programmatically generating large quantities of training labels through a data engine that pseudo-annotates videos using pre-trained models, aligning directly with weak supervision by relying on noisy or imprecise automated sources rather than hand-labeled data.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion; it centers on synthetic data generation for spatiotemporal tasks in Video LLMs, with no components related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces Strefer, a synthetic instruction data generation framework designed to enhance Video Large Language Models (Video LLMs) with advanced spatiotemporal referring and reasoning capabilities, addressing limitations in handling fine-grained spatial and temporal references in videos. The methodology involves using a modular data engine with pre-trained models to pseudo-annotate videos with detailed metadata, such as object masks and timelines, and generate instruction-response pairs for training; key findings demonstrate that models trained on this data outperform baselines in tasks requiring spatial and temporal disambiguation, leading to more robust space-time-aware reasoning.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing AI models to create a new synthetic data framework for spatiotemporal tasks in Video LLMs, offering a notable improvement over current methods without introducing an entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in video AI and human-computer interaction by providing a scalable approach for improving Video LLMs, though its applicability may remain confined to specific subfields rather than broadly transformative applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to enhancing Video LLMs for real-world scenarios, making it a significant read for researchers in computer vision and AI, though it is not essential for those outside these areas.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/50099ab770e1ce468ff47211bb23a8d6ce9b7d45",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 61,
      "average_h_index": 16.571428571428573,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Honglu Zhou",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2316451558"
        },
        {
          "name": "Xiangyu Peng",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2324805044"
        },
        {
          "name": "S. Kendre",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/18115832"
        },
        {
          "name": "Michael S. Ryoo",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2316325210"
        },
        {
          "name": "Silvio Savarese",
          "h_index": 20,
          "profile_url": "https://www.semanticscholar.org/author/2238207181"
        },
        {
          "name": "Caiming Xiong",
          "h_index": 18,
          "profile_url": "https://www.semanticscholar.org/author/2256976968"
        },
        {
          "name": "Juan Carlos Niebles",
          "h_index": 61,
          "profile_url": "https://www.semanticscholar.org/author/9200530"
        }
      ]
    },
    {
      "id": "2509.03503",
      "title": "Warming Up for Zeroth-Order Federated Pre-Training with Low Resource\n  Clients",
      "authors": [
        "Gwen Legate",
        "Irina Rish",
        "Eugene Belilovsky"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Federated learning enables collaborative model training across numerous edge\ndevices without requiring participants to share data; however, memory and\ncommunication constraints on these edge devices may preclude their\nparticipation in training. We consider a setting in which a subset of edge\ndevices are below a critical memory or communication threshold required to\nconduct model updates. Under typical federated optimization algorithms, these\ndevices are excluded from training which renders their data inaccessible and\nincreases system induced bias. We are inspired by MeZO, a zeroth-order method\nused for memory-efficient fine-tuning. The increased variance inherent to\nzeroth-order gradient approximations has relegated previous zeroth-order\noptimizers exclusively to the domain of fine tuning; a limitation we seek to\ncorrect. We devise a federated, memory-efficient zeroth-order optimizer,\nZOWarmUp that permits zeroth-order training from a random initialization.\nZOWarmUp leverages differing client capabilities and careful variance reduction\ntechniques to facilitate participation of under-represented, low-resource\nclients in model training. Like other federated zeroth-order methods, ZOWarmUp\neliminates the need for edge devices to transmit their full gradients to the\nserver and instead relies on only a small set of random seeds, rendering the\nup-link communication cost negligible. We present experiments using various\ndatasets and model architectures to show that ZOWarmUp is a robust algorithm\nthat can can be applied under a wide variety of circumstances. For systems with\na high proportion of edge devices that would otherwise be excluded from\ntraining, this algorithm provides access to a greater volume and diversity of\ndata, thus improving training outcomes.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03503v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03503v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.287,
      "distributed_training_score": 0.483,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution, ZOWarmUp, is a federated learning algorithm that enhances distributed training by enabling low-resource clients to participate in model training across multiple nodes. It addresses resource heterogeneity, optimizes communication and computation partitioning, and aligns directly with distributed training concepts like parallel computing and multi-node machine learning, as seen in FL frameworks such as FedAvg.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces ZOWarmUp, a novel two-step federated learning algorithm designed to include low-resource clients in pre-training by leveraging zeroth-order optimization, addressing memory and communication constraints that typically exclude such devices. The methodology involves high-resource clients initially training the model to provide a warm start, followed by low-resource clients participating with variance-reduced zeroth-order updates, which minimize communication costs; experiments across various datasets and models demonstrate that ZOWarmUp improves data diversity, reduces system-induced bias, and outperforms existing zeroth-order methods in federated pre-training scenarios.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by extending zeroth-order methods to federated pre-training and introducing a two-step warm-up technique, which is a clever combination of existing ideas rather than a completely new problem or architecture. While it advances the state-of-the-art in handling low-resource clients, it builds on prior zeroth-order optimizers like MeZO.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in federated learning, particularly in scenarios with heterogeneous devices, by enabling broader participation and improving data diversity. However, its applicability is mainly within specific subfields of machine learning and AI, limiting its wider commercial or general impact.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to federated learning by addressing resource heterogeneity in a practical way, making it essential for researchers focused on edge devices and optimization techniques. While not groundbreaking for all audiences, it provides high-quality insights that could inspire further developments in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a6055eaa9eb7910e17c84ef148510a5ae02b2bce",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 11,
      "average_h_index": 6.666666666666667,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "G. Legate",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/91036637"
        },
        {
          "name": "Irina Rish",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2239232896"
        },
        {
          "name": "Eugene Belilovsky",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2064781956"
        }
      ]
    },
    {
      "id": "2509.03505",
      "title": "LimiX: Unleashing Structured-Data Modeling Capability for Generalist\n  Intelligence",
      "authors": [
        "Xingxuan Zhang",
        "Gang Ren",
        "Han Yu",
        "Hao Yuan",
        "Hui Wang",
        "Jiansheng Li",
        "Jiayun Wu",
        "Lang Mo",
        "Li Mao",
        "Mingchao Hao",
        "Ningbo Dai",
        "Renzhe Xu",
        "Shuyang Li",
        "Tianyang Zhang",
        "Yue He",
        "Yuanrui Wang",
        "Yunjia Zhang",
        "Zijing Xu",
        "Dongzhe Li",
        "Fang Gao",
        "Hao Zou",
        "Jiandong Liu",
        "Jiashuo Liu",
        "Jiawei Xu",
        "Kaijie Cheng",
        "Kehan Li",
        "Linjun Zhou",
        "Qing Li",
        "Shaohua Fan",
        "Xiaoyu Lin",
        "Xinyan Han",
        "Xuanyue Li",
        "Yan Lu",
        "Yuan Xue",
        "Yuanyuan Jiang",
        "Zimu Wang",
        "Zhenlei Wang",
        "Peng Cui"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "We argue that progress toward general intelligence requires complementary\nfoundation models grounded in language, the physical world, and structured\ndata. This report presents LimiX, the first installment of our large\nstructured-data models (LDMs). LimiX treats structured data as a joint\ndistribution over variables and missingness, thus capable of addressing a wide\nrange of tabular tasks through query-based conditional prediction via a single\nmodel. LimiX is pretrained using masked joint-distribution modeling with an\nepisodic, context-conditional objective, where the model predicts for query\nsubsets conditioned on dataset-specific contexts, supporting rapid,\ntraining-free adaptation at inference. We evaluate LimiX across 10 large\nstructured-data benchmarks with broad regimes of sample size, feature\ndimensionality, class number, categorical-to-numerical feature ratio,\nmissingness, and sample-to-feature ratios. With a single model and a unified\ninterface, LimiX consistently surpasses strong baselines including\ngradient-boosting trees, deep tabular networks, recent tabular foundation\nmodels, and automated ensembles, as shown in Figure 1 and Figure 2. The\nsuperiority holds across a wide range of tasks, such as classification,\nregression, missing value imputation, and data generation, often by substantial\nmargins, while avoiding task-specific architectures or bespoke training per\ntask. All LimiX models are publicly accessible under Apache 2.0.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03505v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03505v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.449,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.422,
      "datasets_score": 0.418,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on developing a structured-data model for tabular tasks and does not involve human feedback, reward models, or reinforcement learning techniques for alignment.",
      "weak_supervision_justification": "The paper uses programmatically generated data from hierarchical structural causal models for pretraining, which aligns with weak supervision by relying on synthetic, noisy sources rather than hand-labeled data, though this is not the primary focus.",
      "diffusion_reasoning_justification": "The paper does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning; it centers on masked modeling and conditional prediction for structured data.",
      "distributed_training_justification": "The paper discusses pretraining objectives for the model but does not address distributed training, parallel computing, or strategies for partitioning data across multiple nodes.",
      "datasets_justification": "The paper involves creating a synthesized pretraining corpus from structural causal models and evaluates the model on 10 benchmarks, contributing to dataset benchmarking and analysis in structured data, though the main focus is on the model itself.",
      "llm_score_status": "completed",
      "summary": "The paper introduces LimiX, a large structured-data model designed to advance general intelligence by handling a wide range of tabular tasks such as classification, regression, missing value imputation, and data generation through a unified, query-based conditional prediction approach. It employs a pretrained methodology using masked joint-distribution modeling with an episodic, context-conditional objective on synthesized data from hierarchical structural causal models, allowing for rapid adaptation without fine-tuning; evaluations on 10 benchmarks demonstrate that LimiX outperforms strong baselines like gradient-boosting trees and other tabular models across various tasks and data regimes.",
      "novelty_score": "High",
      "novelty_justification": "LimiX introduces a truly new unified model for structured data that addresses multiple tasks via a single pretrained framework, significantly advancing beyond existing methods by enabling training-free adaptation and broad generality. This represents a substantial leap in state-of-the-art for tabular data modeling, as it combines innovative architecture and objectives not previously achieved at this scale.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research in AI and commercial applications by providing a versatile foundation model for structured data, potentially streamlining data-driven decision-making in fields like finance and healthcare. Its superior performance and public availability under an open license make it likely to be adopted and built upon extensively.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality and significant contribution to structured-data modeling that advances the field of generalist intelligence, making it valuable for researchers and practitioners in AI and machine learning. While exceptional, it may not be essential for all audiences outside of tabular data specialists, but it offers insights that warrant attention.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b214eb3f0b23783779be142f27647d964bea9b31",
      "total_authors": 38,
      "authors_found": 37,
      "highest_h_index": 13,
      "average_h_index": 1.2972972972972974,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Xingxuan Zhang",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/51258901"
        },
        {
          "name": "Gang Ren",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378801363"
        },
        {
          "name": "Han Yu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379189609"
        },
        {
          "name": "Hao Yuan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381242599"
        },
        {
          "name": "Hui Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378902345"
        },
        {
          "name": "Jiansheng Li",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2283791721"
        },
        {
          "name": "Jiayun Wu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2265797598"
        },
        {
          "name": "Lang Mo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378843189"
        },
        {
          "name": "Li Mao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378803661"
        },
        {
          "name": "Mingchao Hao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378787223"
        },
        {
          "name": "Ningbo Dai",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378842132"
        },
        {
          "name": "Renzhe Xu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Shuyang Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378880628"
        },
        {
          "name": "Tianyang Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379041235"
        },
        {
          "name": "Yue He",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2318674204"
        },
        {
          "name": "Yuanrui Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379035290"
        },
        {
          "name": "Yunjia Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2334736458"
        },
        {
          "name": "Zijing Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379158608"
        },
        {
          "name": "Dongzhe Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378876423"
        },
        {
          "name": "Fang-Fang Gao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2279864810"
        },
        {
          "name": "Hao Zou",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2265753601"
        },
        {
          "name": "Jiandong Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378873530"
        },
        {
          "name": "Jiashuo Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374293389"
        },
        {
          "name": "Jiawei Xu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376792134"
        },
        {
          "name": "K. Cheng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2087269666"
        },
        {
          "name": "Kehan Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2378911582"
        },
        {
          "name": "Linjun Zhou",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/48207021"
        },
        {
          "name": "Qing Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379642574"
        },
        {
          "name": "Shaohua Fan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2371007523"
        },
        {
          "name": "Xiaoyu Lin",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2292083040"
        },
        {
          "name": "Xinyan Han",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378794450"
        },
        {
          "name": "Xuanyue Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378906023"
        },
        {
          "name": "Yan Lu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2252724781"
        },
        {
          "name": "Yuan Xue",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2114922854"
        },
        {
          "name": "Y. Jiang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2222172512"
        },
        {
          "name": "Zimu Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2326073877"
        },
        {
          "name": "Zhenlei Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378897918"
        },
        {
          "name": "Peng Cui",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2318046189"
        }
      ]
    },
    {
      "id": "2509.03510",
      "title": "A comprehensive Persian offline handwritten database for investigating\n  the effects of heritability and family relationships on handwriting",
      "authors": [
        "Abbas Zohrevand",
        "Javad Sadri",
        "Zahra Imani"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper introduces a comprehensive database for research and investigation\non the effects of inheritance on handwriting. A database has been created that\ncan be used to answer questions such as: Is there a genetic component to\nhandwriting? Is handwriting inherited? Do family relationships affect\nhandwriting? Varieties of samples of handwritten components such as: digits,\nletters, shapes and free paragraphs of 210 families including (grandparents,\nparents, uncles, aunts, siblings, cousins, nephews and nieces) have been\ncollected using specially designed forms, and family relationships of all\nwriters are captured. To the best of our knowledge, no such database is\npresently available. Based on comparisons and investigation of features of\nhandwritings of family members, similarities among their features and writing\nstyles are detected. Our database is freely available to the pattern\nrecognition community and hope it will pave the way for investigations on the\neffects of inheritance and family relationships on handwritings.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03510v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03510v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.216,
      "weak_supervision_score": 0.228,
      "diffusion_reasoning_score": 0.241,
      "distributed_training_score": 0.216,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03515",
      "title": "Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling?\n  A Validation Study with Naturalistic Trajectories",
      "authors": [
        "Yanlin Zhang",
        "Sungyong Chung",
        "Nachuan Li",
        "Dana Monzer",
        "Hani S. Mahmassani",
        "Samer H. Hamdar",
        "Alireza Talebpour"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)",
        "stat.AP (Applications)"
      ],
      "abstract": "The Waymo Open Motion Dataset (WOMD) has become a popular resource for\ndata-driven modeling of autonomous vehicles (AVs) behavior. However, its\nvalidity for behavioral analysis remains uncertain due to proprietary\npost-processing, the absence of error quantification, and the segmentation of\ntrajectories into 20-second clips. This study examines whether WOMD accurately\ncaptures the dynamics and interactions observed in real-world AV operations.\nLeveraging an independently collected naturalistic dataset from Level 4 AV\noperations in Phoenix, Arizona (PHX), we perform comparative analyses across\nthree representative urban driving scenarios: discharging at signalized\nintersections, car-following, and lane-changing behaviors. For the discharging\nanalysis, headways are manually extracted from aerial video to ensure\nnegligible measurement error. For the car-following and lane-changing cases, we\napply the Simulation-Extrapolation (SIMEX) method to account for empirically\nestimated error in the PHX data and use Dynamic Time Warping (DTW) distances to\nquantify behavioral differences. Results across all scenarios consistently show\nthat behavior in PHX falls outside the behavioral envelope of WOMD. Notably,\nWOMD underrepresents short headways and abrupt decelerations. These findings\nsuggest that behavioral models calibrated solely on WOMD may systematically\nunderestimate the variability, risk, and complexity of naturalistic driving.\nCaution is therefore warranted when using WOMD for behavior modeling without\nproper validation against independently collected data.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03515v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03515v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.315,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03516",
      "title": "Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,\n  but Not Direct the Play?",
      "authors": [
        "Ouxiang Li",
        "Yuan Wang",
        "Xinting Hu",
        "Huijuan Huang",
        "Rui Chen",
        "Jiarong Ou",
        "Xin Tao",
        "Pengfei Wan",
        "Fuli Feng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-image (T2I) generation aims to synthesize images from textual\nprompts, which jointly specify what must be shown and imply what can be\ninferred, thereby corresponding to two core capabilities: composition and\nreasoning. However, with the emerging advances of T2I models in reasoning\nbeyond composition, existing benchmarks reveal clear limitations in providing\ncomprehensive evaluations across and within these capabilities. Meanwhile,\nthese advances also enable models to handle more complex prompts, whereas\ncurrent benchmarks remain limited to low scene density and simplified\none-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a\ncomprehensive and complex benchmark that evaluates both composition and\nreasoning capabilities of T2I models. To ensure comprehensiveness, we structure\ncomposition around scene graph elements (instance, attribute, and relation) and\nreasoning around the philosophical framework of inference (deductive,\ninductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To\nincrease complexity, driven by the inherent complexities of real-world\nscenarios, we curate each prompt with high compositional density for\ncomposition and multi-step inference for reasoning. We also pair each prompt\nwith a checklist that specifies individual yes/no questions to assess each\nintended element independently to facilitate fine-grained and reliable\nevaluation. In statistics, our benchmark comprises 1,080 challenging prompts\nand around 13,500 checklist questions. Experiments across 27 current T2I models\nreveal that their composition capability still remains limited in complex\nhigh-density scenarios, while the reasoning capability lags even further behind\nas a critical bottleneck, with all models struggling to infer implicit elements\nfrom prompts. Our project page: https://t2i-corebench.github.io/.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03516v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03516v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.544,
      "distributed_training_score": 0.326,
      "datasets_score": 0.381,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of a benchmark (T2I-CoReBench) to evaluate composition and reasoning capabilities in text-to-image (T2I) models. While it discusses reasoning in T2I models, it does not specifically address or adapt the iterative refinement process of diffusion models for solving complex logical tasks, such as treating a 'Chain-of-Thought' as a single entity for multi-step correction. The paper evaluates general T2I models without a clear focus on diffusion-based reasoning, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03581",
      "title": "Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM\n  Agents",
      "authors": [
        "Davide Paglieri",
        "Bartłomiej Cupiał",
        "Jonathan Cook",
        "Ulyana Piterbarg",
        "Jens Tuyls",
        "Edward Grefenstette",
        "Jakob Nicolaus Foerster",
        "Jack Parker-Holder",
        "Tim Rocktäschel"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Training large language models (LLMs) to reason via reinforcement learning\n(RL) significantly improves their problem-solving capabilities. In agentic\nsettings, existing methods like ReAct prompt LLMs to explicitly plan before\nevery action; however, we demonstrate that always planning is computationally\nexpensive and degrades performance on long-horizon tasks, while never planning\nfurther limits performance. To address this, we introduce a conceptual\nframework formalizing dynamic planning for LLM agents, enabling them to\nflexibly decide when to allocate test-time compute for planning. We propose a\nsimple two-stage training pipeline: (1) supervised fine-tuning on diverse\nsynthetic data to prime models for dynamic planning, and (2) RL to refine this\ncapability in long-horizon environments. Experiments on the Crafter environment\nshow that dynamic planning agents trained with this approach are more\nsample-efficient and consistently achieve more complex objectives.\nAdditionally, we demonstrate that these agents can be effectively steered by\nhuman-written plans, surpassing their independent capabilities. To our\nknowledge, this work is the first to explore training LLM agents for dynamic\ntest-time compute allocation in sequential decision-making tasks, paving the\nway for more efficient, adaptive, and controllable agentic systems.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03581v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03581v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.468,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.497,
      "distributed_training_score": 0.406,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning (RL) for training LLM agents in environments, but it does not involve human feedback, such as training a reward model on human-ranked data. Instead, it relies on supervised fine-tuning with synthetic data followed by RL in simulated environments, making it distinct from RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses chain-of-thought reasoning and planning for LLMs but does not adapt or use diffusion models for iterative refinement or multi-step logical reasoning. It focuses on RL and dynamic planning strategies, with no mention of diffusion processes.",
      "distributed_training_justification": "The paper addresses test-time compute allocation for planning in LLM agents, not distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes. There is no discussion of accelerating model training through distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03594",
      "title": "The Optimiser Hidden in Plain Sight: Training with the Loss Landscape's\n  Induced Metric",
      "authors": [
        "Thomas R. Harvey"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "math.OC (Optimization and Control)"
      ],
      "abstract": "We present a class of novel optimisers for training neural networks that\nmakes use of the Riemannian metric naturally induced when the loss landscape is\nembedded in higher-dimensional space. This is the same metric that underlies\ncommon visualisations of loss landscapes. By taking this geometric perspective\nliterally and using the induced metric, we develop a new optimiser and compare\nit to existing methods, namely: SGD, Adam, AdamW, and Muon, across a range of\ntasks and architectures. Empirically, we conclude that this new class of\noptimisers is highly effective in low dimensional examples, and provides slight\nimprovement over state-of-the-art methods for training neural networks. These\nnew optimisers have theoretically desirable properties. In particular, the\neffective learning rate is automatically decreased in regions of high curvature\nacting as a smoothed out form of gradient clipping. Similarly, one variant of\nthese optimisers can also be viewed as inducing an effective scheduled learning\nrate and decoupled weight decay is the natural choice from our geometric\nperspective. The basic method can be used to modify any existing\npreconditioning method. The new optimiser has a computational complexity\ncomparable to that of Adam.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03594v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03594v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.406,
      "datasets_score": 0.288,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is the development of new optimizers based on the Riemannian metric of the loss landscape for neural network training, focusing on single-device optimization techniques. It does not discuss distributed training, parallel computing, multi-node setups, or strategies for partitioning data, models, or computation across multiple processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03609",
      "title": "Towards Efficient General Feature Prediction in Masked Skeleton Modeling",
      "authors": [
        "Shengkai Sun",
        "Zefan Zhang",
        "Jianfeng Dong",
        "Zhiyong Cheng",
        "Xiaojun Chang",
        "Meng Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in the masked autoencoder (MAE) paradigm have significantly\npropelled self-supervised skeleton-based action recognition. However, most\nexisting approaches limit reconstruction targets to raw joint coordinates or\ntheir simple variants, resulting in computational redundancy and limited\nsemantic representation. To address this, we propose a novel General Feature\nPrediction framework (GFP) for efficient mask skeleton modeling. Our key\ninnovation is replacing conventional low-level reconstruction with high-level\nfeature prediction that spans from local motion patterns to global semantic\nrepresentations. Specifically, we introduce a collaborative learning framework\nwhere a lightweight target generation network dynamically produces diversified\nsupervision signals across spatial-temporal hierarchies, avoiding reliance on\npre-computed offline features. The framework incorporates constrained\noptimization to ensure feature diversity while preventing model collapse.\nExperiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits\nof our approach: Computational efficiency (with 6.2$\\times$ faster training\nthan standard masked skeleton modeling methods) and superior representation\nquality, achieving state-of-the-art performance in various downstream tasks.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03609v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03609v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.376,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03614",
      "title": "Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG\n  2025 Challenge",
      "authors": [
        "Seungho Choe",
        "Xiaoli Qin",
        "Abubakr Shafique",
        "Amanda Dy",
        "Susan Done",
        "Dimitrios Androutsos",
        "April Khademi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Counting mitotic figures is time-intensive for pathologists and leads to\ninter-observer variability. Artificial intelligence (AI) promises a solution by\nautomatically detecting mitotic figures while maintaining decision consistency.\nHowever, AI tools are susceptible to domain shift, where a significant drop in\nperformance can occur due to differences in the training and testing sets,\nincluding morphological diversity between organs, species, and variations in\nstaining protocols. Furthermore, the number of mitoses is much less than the\ncount of normal nuclei, which introduces severely imbalanced data for the\ndetection task. In this work, we formulate mitosis detection as a pixel-level\nsegmentation and propose a teacher-student model that simultaneously addresses\nmitosis detection (Track 1) and atypical mitosis classification (Track 2). Our\nmethod is based on a UNet segmentation backbone that integrates domain\ngeneralization modules, namely contrastive representation learning and\ndomain-adversarial training. A teacher-student strategy is employed to generate\npixel-level pseudo-masks not only for annotated mitoses and hard negatives but\nalso for normal nuclei, thereby enhancing feature discrimination and improving\nrobustness against domain shift. For the classification task, we introduce a\nmulti-scale CNN classifier that leverages feature maps from the segmentation\nmodel within a multi-task learning paradigm. On the preliminary test set, the\nalgorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of\n0.8414 in Track 2, demonstrating the effectiveness of integrating\nsegmentation-based detection and classification into a unified framework for\nrobust mitosis analysis.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03614v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03614v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.274,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.381,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03615",
      "title": "E-ARMOR: Edge case Assessment and Review of Multilingual Optical\n  Character Recognition",
      "authors": [
        "Aryan Gupta",
        "Anupam Purwar"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Optical Character Recognition (OCR) in multilingual, noisy, and diverse\nreal-world images remains a significant challenge for optical character\nrecognition systems. With the rise of Large Vision-Language Models (LVLMs),\nthere is growing interest in their ability to generalize and reason beyond\nfixed OCR pipelines. In this work, we introduce Sprinklr-Edge-OCR, a novel OCR\nsystem built specifically optimized for edge deployment in resource-constrained\nenvironments. We present a large-scale comparative evaluation of five\nstate-of-the-art LVLMs (InternVL, Qwen, GOT OCR, LLaMA, MiniCPM) and two\ntraditional OCR systems (Sprinklr-Edge-OCR, SuryaOCR) on a proprietary, doubly\nhand annotated dataset of multilingual (54 languages) images. Our benchmark\ncovers a broad range of metrics including accuracy, semantic consistency,\nlanguage coverage, computational efficiency (latency, memory, GPU usage), and\ndeployment cost. To better reflect real-world applicability, we also conducted\nedge case deployment analysis, evaluating model performance on CPU only\nenvironments. Among the results, Qwen achieved the highest precision (0.54),\nwhile Sprinklr-Edge-OCR delivered the best overall F1 score (0.46) and\noutperformed others in efficiency, processing images 35 faster (0.17 seconds\nper image on average) and at less than 0.01 of the cost (0.006 USD per 1,000\nimages) compared to LVLM. Our findings demonstrate that the most optimal OCR\nsystems for edge deployment are the traditional ones even in the era of LLMs\ndue to their low compute requirements, low latency, and very high\naffordability.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03615v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03615v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.403,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is the introduction and evaluation of Sprinklr-Edge-OCR for edge deployment in OCR tasks, comparing it with other systems on metrics like accuracy, efficiency, and cost. It discusses inference optimization (e.g., TensorRT acceleration) and deployment on resource-constrained devices, but does not address distributed training, parallel computing for model training, or strategies for partitioning data/computation across multiple nodes. Therefore, it has no direct relevance to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03616",
      "title": "Multi Attribute Bias Mitigation via Representation Learning",
      "authors": [
        "Rajeev Ranjan Dwivedi",
        "Ankur Kumar",
        "Vinod K Kurmi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Real world images frequently exhibit multiple overlapping biases, including\ntextures, watermarks, gendered makeup, scene object pairings, etc. These biases\ncollectively impair the performance of modern vision models, undermining both\ntheir robustness and fairness. Addressing these biases individually proves\ninadequate, as mitigating one bias often permits or intensifies others. We\ntackle this multi bias problem with Generalized Multi Bias Mitigation (GMBM), a\nlean two stage framework that needs group labels only while training and\nminimizes bias at test time. First, Adaptive Bias Integrated Learning (ABIL)\ndeliberately identifies the influence of known shortcuts by training encoders\nfor each attribute and integrating them with the main backbone, compelling the\nclassifier to explicitly recognize these biases. Then Gradient Suppression Fine\nTuning prunes those very bias directions from the backbone's gradients, leaving\na single compact network that ignores all the shortcuts it just learned to\nrecognize. Moreover we find that existing bias metrics break under subgroup\nimbalance and train test distribution shifts, so we introduce Scaled Bias\nAmplification (SBA): a test time measure that disentangles model induced bias\namplification from distributional differences. We validate GMBM on FB CMNIST,\nCelebA, and COCO, where we boost worst group accuracy, halve multi attribute\nbias amplification, and set a new low in SBA even as bias complexity and\ndistribution shifts intensify, making GMBM the first practical, end to end\nmultibias solution for visual recognition. Project page:\nhttp://visdomlab.github.io/GMBM/",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03616v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03616v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.392,
      "datasets_score": 0.391,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper uses group labels during training for bias mitigation, which could be considered a form of weak supervision as they are potentially noisy or high-level annotations. However, the main contribution is focused on multi-bias mitigation techniques rather than programmatically generating or relying on weak labels as a primary method.",
      "diffusion_reasoning_justification": "The paper's main contribution involves representation learning and gradient suppression for bias mitigation in vision models, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It is entirely focused on computer vision tasks, not reasoning or diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03623",
      "title": "Revealing Fine Structure in Protoplanetary Disks with Physics\n  Constrained Neural Fields",
      "authors": [
        "Aviad Levis",
        "Nhan Luong",
        "Richard Teague",
        "Katherine. L. Bouman",
        "Marcelo Barraza-Alfaro",
        "Kevin Flaherty"
      ],
      "categories": [
        "astro-ph.EP (Earth and Planetary Astrophysics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Protoplanetary disks are the birthplaces of planets, and resolving their\nthree-dimensional structure is key to understanding disk evolution. The\nunprecedented resolution of ALMA demands modeling approaches that capture\nfeatures beyond the reach of traditional methods. We introduce a computational\nframework that integrates physics-constrained neural fields with differentiable\nrendering and present RadJAX, a GPU-accelerated, fully differentiable line\nradiative transfer solver achieving up to 10,000x speedups over conventional\nray tracers, enabling previously intractable, high-dimensional neural\nreconstructions. Applied to ALMA CO observations of HD 163296, this framework\nrecovers the vertical morphology of the CO-rich layer, revealing a pronounced\nnarrowing and flattening of the emission surface beyond 400 au - a feature\nmissed by existing approaches. Our work establish a new paradigm for extracting\ncomplex disk structure and advancing our understanding of protoplanetary\nevolution.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03623v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03623v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.283,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.36,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03626",
      "title": "Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with\n  KG-SMILE",
      "authors": [
        "Zahra Zehtabi Sabeti Moghaddam",
        "Zeinab Dehghani",
        "Maneeha Rani",
        "Koorosh Aslansefat",
        "Bhupesh Kumar Mishra",
        "Rameez Raja Kureshi",
        "Dhavalkumar Thakker"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generative AI, such as Large Language Models (LLMs), has achieved impressive\nprogress but still produces hallucinations and unverifiable claims, limiting\nreliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves\naccuracy by grounding outputs in external knowledge, especially in domains like\nhealthcare, where precision is vital. However, RAG remains opaque and\nessentially a black box, heavily dependent on data quality. We developed a\nmethod-agnostic, perturbation-based framework that provides token and\ncomponent-level interoperability for Graph RAG using SMILE and named it as\nKnowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing\nsimilarities, and training weighted linear surrogates, KG-SMILE identifies the\ngraph entities and relations most influential to generated outputs, thereby\nmaking RAG more transparent. We evaluate KG-SMILE using comprehensive\nattribution metrics, including fidelity, faithfulness, consistency, stability,\nand accuracy. Our findings show that KG-SMILE produces stable, human-aligned\nexplanations, demonstrating its capacity to balance model effectiveness with\ninterpretability and thereby fostering greater transparency and trust in\nmachine learning technologies.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03626v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03626v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.303,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on developing an explainable framework for Knowledge Graph Retrieval-Augmented Generation (KG-RAG) using perturbations to enhance transparency in AI outputs. It does not involve training models with human feedback, reward models, or reinforcement learning techniques, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a perturbation-based method for explainability in KG-RAG, involving similarities and surrogates, but it does not adapt diffusion processes for multi-step logical reasoning or treat chains-of-thought as entities for iterative refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03631",
      "title": "Lightweight image segmentation for echocardiography",
      "authors": [
        "Anders Kjelsrud",
        "Lasse Løvstakken",
        "Erik Smistad",
        "Håvard Dalen",
        "Gilles Van De Vyver"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate segmentation of the left ventricle in echocardiography can enable\nfully automatic extraction of clinical measurements such as volumes and\nejection fraction. While models configured by nnU-Net perform well, they are\nlarge and slow, thus limiting real-time use. We identified the most effective\ncomponents of nnU-Net for cardiac segmentation through an ablation study,\nincrementally evaluating data augmentation schemes, architectural\nmodifications, loss functions, and post-processing techniques. Our analysis\nrevealed that simple affine augmentations and deep supervision drive\nperformance, while complex augmentations and large model capacity offer\ndiminishing returns. Based on these insights, we developed a lightweight U-Net\n(2M vs 33M parameters) that achieves statistically equivalent performance to\nnnU-Net on CAMUS (N=500) with Dice scores of 0.93/0.85/0.89 vs 0.93/0.86/0.89\nfor LV/MYO/LA ($p>0.05$), while being 16 times smaller and 4 times faster\n(1.35ms vs 5.40ms per frame) than the default nnU-Net configuration.\nCross-dataset evaluation on an internal dataset (N=311) confirms comparable\ngeneralization.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03631v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03631v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.243,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.286,
      "distributed_training_score": 0.29,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03633",
      "title": "treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point\n  Clouds",
      "authors": [
        "Josafat-Mattias Burmeister",
        "Andreas Tockner",
        "Stefan Reder",
        "Markus Engel",
        "Rico Richter",
        "Jan-Peter Mund",
        "Jürgen Döllner"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Close-range laser scanning provides detailed 3D captures of forest stands but\nrequires efficient software for processing 3D point cloud data and extracting\nindividual trees. Although recent studies have introduced deep learning methods\nfor tree instance segmentation, these approaches require large annotated\ndatasets and substantial computational resources. As a resource-efficient\nalternative, we present a revised version of the treeX algorithm, an\nunsupervised method that combines clustering-based stem detection with region\ngrowing for crown delineation. While the original treeX algorithm was developed\nfor personal laser scanning (PLS) data, we provide two parameter presets, one\nfor ground-based laser scanning (stationary terrestrial - TLS and PLS), and one\nfor UAV-borne laser scanning (ULS). We evaluated the method on six public\ndatasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham\nWoods) and compared it to six open-source methods (original treeX, treeiso,\nRayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original\ntreeX algorithm, our revision reduces runtime and improves accuracy, with\ninstance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data.\nFor ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original\nalgorithm fails to segment any correct instances. For TLS and PLS data, our\nalgorithm achieves accuracy similar to recent open-source methods, including\ndeep learning. Given its algorithmic design, we see two main applications for\nour method: (1) as a resource-efficient alternative to deep learning approaches\nin scenarios where the data characteristics align with the method design\n(sufficient stem visibility and point density), and (2) for the semi-automatic\ngeneration of labels for deep learning models. To enable broader adoption, we\nprovide an open-source Python implementation in the pointtree package.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03633v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03633v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.283,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.281,
      "distributed_training_score": 0.339,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03635",
      "title": "Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene\n  Understanding",
      "authors": [
        "Hongpei Zheng",
        "Lintao Xiang",
        "Qijun Yang",
        "Qian Lin",
        "Hujun Yin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The rapid development of Large Multimodal Models (LMMs) has led to remarkable\nprogress in 2D visual understanding; however, extending these capabilities to\n3D scene understanding remains a significant challenge. Existing approaches\npredominantly rely on text-only supervision, which fails to provide the\ngeometric constraints required for learning robust 3D spatial representations.\nIn this paper, we introduce Reg3D, a novel Reconstructive Geometry Instruction\nTuning framework that addresses this limitation by incorporating geometry-aware\nsupervision directly into the training process. Our key insight is that\neffective 3D understanding necessitates reconstructing underlying geometric\nstructures rather than merely describing them. Unlike existing methods that\ninject 3D information solely at the input level, Reg3D adopts a\ndual-supervision paradigm that leverages 3D geometric information both as input\nand as explicit learning targets. Specifically, we design complementary\nobject-level and frame-level reconstruction tasks within a dual-encoder\narchitecture, enforcing geometric consistency to encourage the development of\nspatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap,\nScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performance\nimprovements, establishing a new training paradigm for spatially aware\nmultimodal models.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03635v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03635v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.341,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces Reg3D, a framework for 3D scene understanding using reconstructive geometry tasks and instruction tuning, focusing on spatial reasoning and geometric reconstruction. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03636",
      "title": "CausalARC: Abstract Reasoning with Causal World Models",
      "authors": [
        "Jacqueline Maasch",
        "John Kalantari",
        "Kia Khezeli"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Reasoning requires adaptation to novel problem settings under limited data\nand distribution shift. This work introduces CausalARC: an experimental testbed\nfor AI reasoning in low-data and out-of-distribution regimes, modeled after the\nAbstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is\nsampled from a fully specified causal world model, formally expressed as a\nstructural causal model. Principled data augmentations provide observational,\ninterventional, and counterfactual feedback about the world model in the form\nof few-shot, in-context learning demonstrations. As a proof-of-concept, we\nillustrate the use of CausalARC for four language model evaluation settings:\n(1) abstract reasoning with test-time training, (2) counterfactual reasoning\nwith in-context learning, (3) program synthesis, and (4) causal discovery with\nlogical reasoning.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03636v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03636v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.522,
      "distributed_training_score": 0.348,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces CausalARC, a testbed for evaluating causal reasoning in AI using structural causal models, with no mention of human feedback, reward models, or reinforcement learning techniques for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on causal reasoning and test-time training with language models, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03643",
      "title": "CEHR-XGPT: A Scalable Multi-Task Foundation Model for Electronic Health\n  Records",
      "authors": [
        "Chao Pang",
        "Jiheum Park",
        "Xinzhuo Jiang",
        "Nishanth Parameshwar Pavinkurve",
        "Krishna S. Kalluri",
        "Shalmali Joshi",
        "Noémie Elhadad",
        "Karthik Natarajan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Electronic Health Records (EHRs) provide a rich, longitudinal view of patient\nhealth and hold significant potential for advancing clinical decision support,\nrisk prediction, and data-driven healthcare research. However, most artificial\nintelligence (AI) models for EHRs are designed for narrow, single-purpose\ntasks, limiting their generalizability and utility in real-world settings.\nHere, we present CEHR-XGPT, a general-purpose foundation model for EHR data\nthat unifies three essential capabilities - feature representation, zero-shot\nprediction, and synthetic data generation - within a single architecture. To\nsupport temporal reasoning over clinical sequences, CEHR-XGPT incorporates a\nnovel time-token-based learning framework that explicitly encodes patients'\ndynamic timelines into the model structure. CEHR-XGPT demonstrates strong\nperformance across all three tasks and generalizes effectively to external\ndatasets through vocabulary expansion and fine-tuning. Its versatility enables\nrapid model development, cohort discovery, and patient outcome forecasting\nwithout the need for task-specific retraining.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03643v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03643v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.383,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03644",
      "title": "Towards a Neurosymbolic Reasoning System Grounded in Schematic\n  Representations",
      "authors": [
        "François Olivier",
        "Zied Bouraoui"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Despite significant progress in natural language understanding, Large\nLanguage Models (LLMs) remain error-prone when performing logical reasoning,\noften lacking the robust mental representations that enable human-like\ncomprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that\ngrounds understanding and logical reasoning in schematic representations based\non image schemas-recurring patterns derived from sensorimotor experience that\nstructure human cognition. Our system operationalizes the spatial foundations\nof these cognitive structures using declarative spatial reasoning within Answer\nSet Programming. Through evaluation on logical deduction problems, we\ndemonstrate that LLMs can be guided to interpret scenarios through embodied\ncognitive structures, that these structures can be formalized as executable\nprograms, and that the resulting representations support effective logical\nreasoning with enhanced interpretability. While our current implementation\nfocuses on spatial primitives, it establishes the computational foundation for\nincorporating more complex and dynamic representations.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03644v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03644v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.548,
      "distributed_training_score": 0.288,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a neurosymbolic system, Embodied-LM, that integrates LLMs with declarative spatial reasoning and Answer Set Programming to enhance logical reasoning through image schemas. It does not mention or utilize diffusion models, iterative refinement processes, or any mechanism for holistically correcting a Chain-of-Thought, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03646",
      "title": "Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning",
      "authors": [
        "Haozhe Wang",
        "Qixin Xu",
        "Che Liu",
        "Junhong Wu",
        "Fangzhen Lin",
        "Wenhu Chen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Reinforcement Learning (RL) has proven highly effective at enhancing the\ncomplex reasoning abilities of Large Language Models (LLMs), yet underlying\nmechanisms driving this success remain largely opaque. Our analysis reveals\nthat puzzling phenomena like ``aha moments\", ``length-scaling'' and entropy\ndynamics are not disparate occurrences but hallmarks of an emergent reasoning\nhierarchy, akin to the separation of high-level strategic planning from\nlow-level procedural execution in human cognition. We uncover a compelling\ntwo-phase dynamic: initially, a model is constrained by procedural correctness\nand must improve its low-level skills. The learning bottleneck then decisively\nshifts, with performance gains being driven by the exploration and mastery of\nhigh-level strategic planning. This insight exposes a core inefficiency in\nprevailing RL algorithms like GRPO, which apply optimization pressure\nagnostically and dilute the learning signal across all tokens. To address this,\nwe propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that\nconcentrates optimization efforts on high-impact planning tokens. HICRA\nsignificantly outperforms strong baselines, demonstrating that focusing on this\nstrategic bottleneck is key to unlocking advanced reasoning. Furthermore, we\nvalidate semantic entropy as a superior compass for measuring strategic\nexploration over misleading metrics such as token-level entropy.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03646v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03646v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.515,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.575,
      "distributed_training_score": 0.387,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses Reinforcement Learning (RL) for enhancing LLMs, citing works like ouyang2022training that involve RLHF, but it focuses on general RL dynamics and proposes HICRA without explicitly detailing human feedback, reward models, or human-ranked data. Thus, it is related but not central to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper centers on RL and emergent hierarchical reasoning in LLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. It does not adapt or use diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03647",
      "title": "Breaking the Mirror: Activation-Based Mitigation of Self-Preference in\n  LLM Evaluators",
      "authors": [
        "Dani Roytburg",
        "Matthew Bozoukov",
        "Matthew Nguyen",
        "Jou Barzdukas",
        "Simon Fu",
        "Narmeen Oozeer"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large language models (LLMs) increasingly serve as automated evaluators, yet\nthey suffer from \"self-preference bias\": a tendency to favor their own outputs\nover those of other models. This bias undermines fairness and reliability in\nevaluation pipelines, particularly for tasks like preference tuning and model\nrouting. We investigate whether lightweight steering vectors can mitigate this\nproblem at inference time without retraining. We introduce a curated dataset\nthat distinguishes self-preference bias into justified examples of\nself-preference and unjustified examples of self-preference, and we construct\nsteering vectors using two methods: Contrastive Activation Addition (CAA) and\nan optimization-based approach. Our results show that steering vectors can\nreduce unjustified self-preference bias by up to 97\\%, substantially\noutperforming prompting and direct preference optimization baselines. Yet\nsteering vectors are unstable on legitimate self-preference and unbiased\nagreement, implying self-preference spans multiple or nonlinear directions.\nThis underscores both their promise and limits as safeguards for LLM-as-judges\nand motivates more robust interventions.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03647v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03647v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.504,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.362,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses self-preference bias in LLM evaluators and mentions preference tuning as a risk area, with a baseline comparison to Direct Preference Optimization (DPO), which is related to RLHF. However, the main contribution focuses on inference-time bias mitigation using steering vectors, not on training reward models or using human feedback for RL-based fine-tuning.",
      "weak_supervision_justification": "The paper curates a dataset using ensemble \"gold\" judges from diverse models to generate labels for distinguishing self-preference types, which involves programmatically derived labels that could be noisy. However, the primary focus is on mitigating bias with steering vectors, not on training models using weak supervision as a core technique.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical reasoning, or any multi-step reasoning processes. It focuses on steering vectors for bias mitigation in LLM evaluators, with no mention of diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03649",
      "title": "An Empirical Evaluation of Factors Affecting SHAP Explanation of Time\n  Series Classification",
      "authors": [
        "Davide Italo Serramazza",
        "Nikos Papadeas",
        "Zahraa Abdallah",
        "Georgiana Ifrim"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Explainable AI (XAI) has become an increasingly important topic for\nunderstanding and attributing the predictions made by complex Time Series\nClassification (TSC) models. Among attribution methods, SHapley Additive\nexPlanations (SHAP) is widely regarded as an excellent attribution method; but\nits computational complexity, which scales exponentially with the number of\nfeatures, limits its practicality for long time series. To address this, recent\nstudies have shown that aggregating features via segmentation, to compute a\nsingle attribution value for a group of consecutive time points, drastically\nreduces SHAP running time. However, the choice of the optimal segmentation\nstrategy remains an open question. In this work, we investigated eight\ndifferent Time Series Segmentation algorithms to understand how segment\ncompositions affect the explanation quality. We evaluate these approaches using\ntwo established XAI evaluation methodologies: InterpretTime and AUC Difference.\nThrough experiments on both Multivariate (MTS) and Univariate Time Series\n(UTS), we find that the number of segments has a greater impact on explanation\nquality than the specific segmentation method. Notably, equal-length\nsegmentation consistently outperforms most of the custom time series\nsegmentation algorithms. Furthermore, we introduce a novel attribution\nnormalisation technique that weights segments by their length and we show that\nit consistently improves attribution quality.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03649v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03649v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.297,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03652",
      "title": "Nonnegative matrix factorization and the principle of the common cause",
      "authors": [
        "E. Khalafyan",
        "A. E. Allahverdyan",
        "A. Hovhannisyan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Nonnegative matrix factorization (NMF) is a known unsupervised data-reduction\nmethod. The principle of the common cause (PCC) is a basic methodological\napproach in probabilistic causality, which seeks an independent mixture model\nfor the joint probability of two dependent random variables. It turns out that\nthese two concepts are closely related. This relationship is explored\nreciprocally for several datasets of gray-scale images, which are conveniently\nmapped into probability models. On one hand, PCC provides a predictability tool\nthat leads to a robust estimation of the effective rank of NMF. Unlike other\nestimates (e.g., those based on the Bayesian Information Criteria), our\nestimate of the rank is stable against weak noise. We show that NMF implemented\naround this rank produces features (basis images) that are also stable against\nnoise and against seeds of local optimization, thereby effectively resolving\nthe NMF nonidentifiability problem. On the other hand, NMF provides an\ninteresting possibility of implementing PCC in an approximate way, where larger\nand positively correlated joint probabilities tend to be explained better via\nthe independent mixture model. We work out a clustering method, where data\npoints with the same common cause are grouped into the same cluster. We also\nshow how NMF can be employed for data denoising.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03652v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03652v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.302,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03658",
      "title": "Efficient Virtuoso: A Latent Diffusion Transformer Model for\n  Goal-Conditioned Trajectory Planning",
      "authors": [
        "Antonio Guillen-Perez"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The ability to generate a diverse and plausible distribution of future\ntrajectories is a critical capability for autonomous vehicle planning systems.\nWhile recent generative models have shown promise, achieving high fidelity,\ncomputational efficiency, and precise control remains a significant challenge.\nIn this paper, we present the Efficient Virtuoso, a conditional latent\ndiffusion model for goal-conditioned trajectory planning. Our approach\nintroduces a novel two-stage normalization pipeline that first scales\ntrajectories to preserve their geometric aspect ratio and then normalizes the\nresulting PCA latent space to ensure a stable training target. The denoising\nprocess is performed efficiently in this low-dimensional latent space by a\nsimple MLP denoiser, which is conditioned on a rich scene context fused by a\npowerful Transformer-based StateEncoder. We demonstrate that our method\nachieves state-of-the-art performance on the Waymo Open Motion Dataset,\nachieving a minimum Average Displacement Error (minADE) of 0.25. Furthermore,\nthrough a rigorous ablation study on goal representation, we provide a key\ninsight: while a single endpoint goal can resolve strategic ambiguity, a\nricher, multi-step sparse route is essential for enabling the precise,\nhigh-fidelity tactical execution that mirrors nuanced human driving behavior.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03658v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03658v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.546,
      "distributed_training_score": 0.379,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a diffusion model for iterative refinement in trajectory planning, which involves denoising processes similar to those in diffusion-based methods. However, it applies this to generating physical trajectories for autonomous vehicles, not to solving complex logical tasks or refining a 'Chain-of-Thought' for reasoning. Thus, while there is a shared mechanism of iterative refinement, the paper lacks a focus on multi-step logical reasoning, making it only tangentially relevant.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03677",
      "title": "Insights from Gradient Dynamics: Gradient Autoscaled Normalization",
      "authors": [
        "Vincent-Daniel Yun"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.IT (Information Theory)",
        "math.IT (Information Theory)"
      ],
      "abstract": "Gradient dynamics play a central role in determining the stability and\ngeneralization of deep neural networks. In this work, we provide an empirical\nanalysis of how variance and standard deviation of gradients evolve during\ntraining, showing consistent changes across layers and at the global scale in\nconvolutional networks. Motivated by these observations, we propose a\nhyperparameter-free gradient normalization method that aligns gradient scaling\nwith their natural evolution. This approach prevents unintended amplification,\nstabilizes optimization, and preserves convergence guarantees. Experiments on\nthe challenging CIFAR-100 benchmark with ResNet-20, ResNet-56, and VGG-16-BN\ndemonstrate that our method maintains or improves test accuracy even under\nstrong generalization. Beyond practical performance, our study highlights the\nimportance of directly tracking gradient dynamics, aiming to bridge the gap\nbetween theoretical expectations and empirical behaviors, and to provide\ninsights for future optimization research.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03677v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03677v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.381,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03680",
      "title": "LuxDiT: Lighting Estimation with Video Diffusion Transformer",
      "authors": [
        "Ruofan Liang",
        "Kai He",
        "Zan Gojcic",
        "Igor Gilitschenski",
        "Sanja Fidler",
        "Nandita Vijaykumar",
        "Zian Wang"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Estimating scene lighting from a single image or video remains a longstanding\nchallenge in computer vision and graphics. Learning-based approaches are\nconstrained by the scarcity of ground-truth HDR environment maps, which are\nexpensive to capture and limited in diversity. While recent generative models\noffer strong priors for image synthesis, lighting estimation remains difficult\ndue to its reliance on indirect visual cues, the need to infer global\n(non-local) context, and the recovery of high-dynamic-range outputs. We propose\nLuxDiT, a novel data-driven approach that fine-tunes a video diffusion\ntransformer to generate HDR environment maps conditioned on visual input.\nTrained on a large synthetic dataset with diverse lighting conditions, our\nmodel learns to infer illumination from indirect visual cues and generalizes\neffectively to real-world scenes. To improve semantic alignment between the\ninput and the predicted environment map, we introduce a low-rank adaptation\nfinetuning strategy using a collected dataset of HDR panoramas. Our method\nproduces accurate lighting predictions with realistic angular high-frequency\ndetails, outperforming existing state-of-the-art techniques in both\nquantitative and qualitative evaluations.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03680v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03680v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.499,
      "distributed_training_score": 0.365,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using a diffusion transformer for generating HDR environment maps in lighting estimation, which involves iterative refinement for image synthesis. However, it does not adapt the diffusion process for solving complex logical tasks or multi-step reasoning, such as treating a 'Chain-of-Thought' as an entity. The work is centered on perceptual and generative tasks in computer vision, without any clear component for logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03695",
      "title": "Hierarchical Federated Foundation Models over Wireless Networks for\n  Multi-Modal Multi-Task Intelligence: Integration of Edge Learning with\n  D2D/P2P-Enabled Fog Learning Architectures",
      "authors": [
        "Payam Abdisarabshali",
        "Fardis Nadimi",
        "Kasra Borazjani",
        "Naji Khosravan",
        "Minghui Liwang",
        "Wei Ni",
        "Dusit Niyato",
        "Michael Langberg",
        "Seyyedali Hosseinalipour"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rise of foundation models (FMs) has reshaped the landscape of machine\nlearning. As these models continued to grow, leveraging geo-distributed data\nfrom wireless devices has become increasingly critical, giving rise to\nfederated foundation models (FFMs). More recently, FMs have evolved into\nmulti-modal multi-task (M3T) FMs (e.g., GPT-4) capable of processing diverse\nmodalities across multiple tasks, which motivates a new underexplored paradigm:\nM3T FFMs. In this paper, we unveil an unexplored variation of M3T FFMs by\nproposing hierarchical federated foundation models (HF-FMs), which in turn\nexpose two overlooked heterogeneity dimensions to fog/edge networks that have a\ndirect impact on these emerging models: (i) heterogeneity in collected\nmodalities and (ii) heterogeneity in executed tasks across fog/edge nodes.\nHF-FMs strategically align the modular structure of M3T FMs, comprising\nmodality encoders, prompts, mixture-of-experts (MoEs), adapters, and task\nheads, with the hierarchical nature of fog/edge infrastructures. Moreover,\nHF-FMs enable the optional usage of device-to-device (D2D) communications,\nenabling horizontal module relaying and localized cooperative training among\nnodes when feasible. Through delving into the architectural design of HF-FMs,\nwe highlight their unique capabilities along with a series of tailored future\nresearch directions. Finally, to demonstrate their potential, we prototype\nHF-FMs in a wireless network setting and release the open-source code for the\ndevelopment of HF-FMs with the goal of fostering exploration in this untapped\nfield (GitHub: https://github.com/payamsiabd/M3T-FFM).",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03695v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03695v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.447,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.47,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on hierarchical federated foundation models for distributed training in wireless networks, emphasizing multi-modal and multi-task learning without any mention of human feedback, reward models, or reinforcement learning techniques. There is no component involving human preferences or alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses distributed training of multi-modal foundation models but does not involve diffusion models, iterative refinement for logical reasoning, or multi-step chain-of-thought processes. It centers on federated learning architectures, not reasoning mechanisms based on diffusion.",
      "distributed_training_justification": "The paper's main contribution is the proposal of hierarchical federated foundation models, which directly addresses distributed training by partitioning model modules across fog/edge nodes, incorporating D2D communications, and enabling scalable multi-node learning in wireless networks. This aligns closely with concepts of parallel computing and distributed ML.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Hierarchical Federated Foundation Models (HF-FMs), a novel paradigm that integrates multi-modal multi-task foundation models with hierarchical fog/edge networks to address heterogeneities in data modalities and tasks across wireless devices. It outlines the architectural design, including module-wise circulation, D2D-enabled cooperation, and emerging capabilities like asymmetric aggregation, while providing an open-source prototype to encourage further research in scalable, distributed AI systems.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new paradigm, HF-FMs, which advances the state-of-the-art by integrating modular multi-modal multi-task models with hierarchical fog/edge architectures and D2D communications, addressing previously unexplored heterogeneities.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and commercial applications in wireless AI, such as autonomous systems, by enabling scalable and privacy-preserving distributed learning, as evidenced by the released open-source code.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This high-quality vision paper presents a significant and innovative contribution to distributed AI in wireless networks, making it valuable for researchers in machine learning and edge computing, though it is more conceptual than empirically validated.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/bd32f2d22b5ca7cde71af3cfe30d75df2824e895",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 20,
      "average_h_index": 6.444444444444445,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Payam Abdisarabshali",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2187874432"
        },
        {
          "name": "Fardis Nadimi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2362092383"
        },
        {
          "name": "Kasra Borazjani",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2278438924"
        },
        {
          "name": "Naji Khosravan",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/3451652"
        },
        {
          "name": "Minghui Liwang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2319532814"
        },
        {
          "name": "Wei Ni",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2242936355"
        },
        {
          "name": "Dusit Niyato",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/2340230621"
        },
        {
          "name": "Michael Langberg",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2309476970"
        },
        {
          "name": "Seyyedali Hosseinalipour",
          "h_index": 20,
          "profile_url": "https://www.semanticscholar.org/author/2911998"
        }
      ]
    },
    {
      "id": "2509.03704",
      "title": "QuantV2X: A Fully Quantized Multi-Agent System for Cooperative\n  Perception",
      "authors": [
        "Seth Z. Zhao",
        "Huizhi Zhang",
        "Zhaowei Li",
        "Juntong Peng",
        "Anthony Chui",
        "Zewei Zhou",
        "Zonglin Meng",
        "Hao Xiang",
        "Zhiyu Huang",
        "Fujia Wang",
        "Ran Tian",
        "Chenfeng Xu",
        "Bolei Zhou",
        "Jiaqi Ma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Cooperative perception through Vehicle-to-Everything (V2X) communication\noffers significant potential for enhancing vehicle perception by mitigating\nocclusions and expanding the field of view. However, past research has\npredominantly focused on improving accuracy metrics without addressing the\ncrucial system-level considerations of efficiency, latency, and real-world\ndeployability. Noticeably, most existing systems rely on full-precision models,\nwhich incur high computational and transmission costs, making them impractical\nfor real-time operation in resource-constrained environments. In this paper, we\nintroduce \\textbf{QuantV2X}, the first fully quantized multi-agent system\ndesigned specifically for efficient and scalable deployment of multi-modal,\nmulti-agent V2X cooperative perception. QuantV2X introduces a unified\nend-to-end quantization strategy across both neural network models and\ntransmitted message representations that simultaneously reduces computational\nload and transmission bandwidth. Remarkably, despite operating under low-bit\nconstraints, QuantV2X achieves accuracy comparable to full-precision systems.\nMore importantly, when evaluated under deployment-oriented metrics, QuantV2X\nreduces system-level latency by 3.2$\\times$ and achieves a +9.5 improvement in\nmAP30 over full-precision baselines. Furthermore, QuantV2X scales more\neffectively, enabling larger and more capable models to fit within strict\nmemory budgets. These results highlight the viability of a fully quantized\nmulti-agent intermediate fusion system for real-world deployment. The system\nwill be publicly released to promote research in this field:\nhttps://github.com/ucla-mobility/QuantV2X.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03704v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03704v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.445,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper introduces QuantV2X, a quantized multi-agent system for cooperative perception in V2X environments, focusing on inference efficiency, latency reduction, and communication optimization. While it involves multi-agent interactions that could imply a distributed setup, the main contribution centers on deployment and real-time perception, not on distributed training techniques, parallel computing for accelerating model training, or partitioning data/computation across nodes for training purposes. Thus, there is only a loose connection to distributed training concepts.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03709",
      "title": "From Federated Learning to X-Learning: Breaking the Barriers of\n  Decentrality Through Random Walks",
      "authors": [
        "Allan Salihovic",
        "Payam Abdisarabshali",
        "Michael Langberg",
        "Seyyedali Hosseinalipour"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We provide our perspective on X-Learning (XL), a novel distributed learning\narchitecture that generalizes and extends the concept of decentralization. Our\ngoal is to present a vision for XL, introducing its unexplored design\nconsiderations and degrees of freedom. To this end, we shed light on the\nintuitive yet non-trivial connections between XL, graph theory, and Markov\nchains. We also present a series of open research directions to stimulate\nfurther research.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03709v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03709v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.413,
      "weak_supervision_score": 0.412,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.487,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on distributed learning architectures like X-Learning, which involve random walks for model training across nodes, without any mention of human feedback, reward models, or reinforcement learning techniques for aligning AI with human preferences.",
      "weak_supervision_justification": "The paper discusses extending federated and fog learning through X-Learning for distributed ML, emphasizing model transfers and decentralization, but it does not address training models with programmatically generated labels, noisy sources, or any form of weak supervision.",
      "diffusion_reasoning_justification": "The paper uses random walks, which are related to Markov chains and can involve diffusion-like processes on graphs, for model traversal in X-Learning; however, it does not involve multi-step logical reasoning, iterative refinement of a chain-of-thought, or adaptation of diffusion models for complex tasks.",
      "distributed_training_justification": "The paper's main contribution is the introduction of X-Learning, a flexible distributed learning architecture that extends federated and fog learning by using random walks for multi-node model training, directly addressing distributed training, parallel computing, and strategies for partitioning computation across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces X-Learning (XL), a novel distributed learning architecture that extends federated learning (FedL) and fog learning (FogL) by employing random walks to enable flexible multi-hop model transfers between devices, treating ML models as autonomous agents. The authors explore connections between XL, graph theory, and Markov chains, highlighting its generalized notion of decentralization, potential reductions to existing methods, and open research directions to advance distributed machine learning in large-scale networks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by extending federated learning with random walks for flexible model transfers, offering a clever combination of existing distributed ML concepts and graph theory. However, it primarily builds on established ideas rather than introducing a completely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work could influence research in distributed ML for large-scale networks by providing a flexible architecture that addresses decentralization challenges, likely leading to citations and developments within its subfield. Nonetheless, its conceptual nature without empirical validation limits broader immediate applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable and innovative perspective on distributed learning architectures, making it essential for researchers in machine learning and AI to understand emerging trends in decentralization. While not groundbreaking enough for \"Must Read,\" its ideas on X-Learning provide strong insights worth exploring.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/373dfadc1a5a10b4e8c0ab7a158595fc318f1107",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 20,
      "average_h_index": 6.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Allan Salihovic",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378953982"
        },
        {
          "name": "Payam Abdisarabshali",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2187874432"
        },
        {
          "name": "Michael Langberg",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2309476970"
        },
        {
          "name": "Seyyedali Hosseinalipour",
          "h_index": 20,
          "profile_url": "https://www.semanticscholar.org/author/2911998"
        }
      ]
    },
    {
      "id": "2509.03725",
      "title": "MLSD: A Novel Few-Shot Learning Approach to Enhance Cross-Target and\n  Cross-Domain Stance Detection",
      "authors": [
        "Parush Gera",
        "Tempestt Neal"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We present the novel approach for stance detection across domains and\ntargets, Metric Learning-Based Few-Shot Learning for Cross-Target and\nCross-Domain Stance Detection (MLSD). MLSD utilizes metric learning with\ntriplet loss to capture semantic similarities and differences between stance\ntargets, enhancing domain adaptation. By constructing a discriminative\nembedding space, MLSD allows a cross-target or cross-domain stance detection\nmodel to acquire useful examples from new target domains. We evaluate MLSD in\nmultiple cross-target and cross-domain scenarios across two datasets, showing\nstatistically significant improvement in stance detection performance across\nsix widely used stance detection models.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03725v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03725v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.377,
      "datasets_score": 0.408,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces a few-shot learning approach for stance detection, which uses a small number of labeled examples to fine-tune models, potentially reducing the need for extensive labeled data. However, it does not focus on programmatically generating noisy or imprecise labels, which is central to weak supervision, making the connection indirect.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper evaluates its method on existing datasets like SemEval 2016 for cross-target and cross-domain scenarios, involving benchmarking with state-of-the-art models. However, it does not primarily contribute to dataset creation, analysis, curation, or evaluation, as its main focus is on the MLSD methodology rather than datasets themselves.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03728",
      "title": "PersonaTeaming: Exploring How Introducing Personas Can Improve Automated\n  AI Red-Teaming",
      "authors": [
        "Wesley Hanwen Deng",
        "Sunnie S. Y. Kim",
        "Akshita Jha",
        "Ken Holstein",
        "Motahhare Eslami",
        "Lauren Wilcox",
        "Leon A Gatys"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Recent developments in AI governance and safety research have called for\nred-teaming methods that can effectively surface potential risks posed by AI\nmodels. Many of these calls have emphasized how the identities and backgrounds\nof red-teamers can shape their red-teaming strategies, and thus the kinds of\nrisks they are likely to uncover. While automated red-teaming approaches\npromise to complement human red-teaming by enabling larger-scale exploration of\nmodel behavior, current approaches do not consider the role of identity. As an\ninitial step towards incorporating people's background and identities in\nautomated red-teaming, we develop and evaluate a novel method, PersonaTeaming,\nthat introduces personas in the adversarial prompt generation process to\nexplore a wider spectrum of adversarial strategies. In particular, we first\nintroduce a methodology for mutating prompts based on either \"red-teaming\nexpert\" personas or \"regular AI user\" personas. We then develop a dynamic\npersona-generating algorithm that automatically generates various persona types\nadaptive to different seed prompts. In addition, we develop a set of new\nmetrics to explicitly measure the \"mutation distance\" to complement existing\ndiversity measurements of adversarial prompts. Our experiments show promising\nimprovements (up to 144.1%) in the attack success rates of adversarial prompts\nthrough persona mutation, while maintaining prompt diversity, compared to\nRainbowPlus, a state-of-the-art automated red-teaming method. We discuss the\nstrengths and limitations of different persona types and mutation methods,\nshedding light on future opportunities to explore complementarities between\nautomated and human red-teaming approaches.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03728v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03728v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.445,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.342,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a method for enhancing automated AI red-teaming by incorporating personas into prompt generation, focusing on adversarial strategies and diversity in testing AI models. It does not involve reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences, which are core to RLHF. Thus, there is no connection to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03729",
      "title": "Transfer Learning-Based CNN Models for Plant Species Identification\n  Using Leaf Venation Patterns",
      "authors": [
        "Bandita Bharadwaj",
        "Ankur Mishra",
        "Saurav Bharadwaj"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This study evaluates the efficacy of three deep learning architectures:\nResNet50, MobileNetV2, and EfficientNetB0 for automated plant species\nclassification based on leaf venation patterns, a critical morphological\nfeature with high taxonomic relevance. Using the Swedish Leaf Dataset\ncomprising images from 15 distinct species (75 images per species, totalling\n1,125 images), the models were demonstrated using standard performance metrics\nduring training and testing phases. ResNet50 achieved a training accuracy of\n94.11% but exhibited overfitting, reflected by a reduced testing accuracy of\n88.45% and an F1 score of 87.82%. MobileNetV2 demonstrated better\ngeneralization capabilities, attaining a testing accuracy of 93.34% and an F1\nscore of 93.23%, indicating its suitability for lightweight, real-time\napplications. EfficientNetB0 outperformed both models, achieving a testing\naccuracy of 94.67% with precision, recall, and F1 scores exceeding 94.6%,\nhighlighting its robustness in venation-based classification. The findings\nunderscore the potential of deep learning, particularly EfficientNetB0, in\ndeveloping scalable and accurate tools for automated plant taxonomy using\nvenation traits.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03729v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03729v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.249,
      "weak_supervision_score": 0.291,
      "diffusion_reasoning_score": 0.268,
      "distributed_training_score": 0.316,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03730",
      "title": "The Personality Illusion: Revealing Dissociation Between Self-Reports &\n  Behavior in LLMs",
      "authors": [
        "Pengrui Han",
        "Rafal Kocielnik",
        "Peiyang Song",
        "Ramit Debnath",
        "Dean Mobbs",
        "Anima Anandkumar",
        "R. Michael Alvarez"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CY (Computers and Society)",
        "cs.LG (Machine Learning)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Personality traits have long been studied as predictors of human behavior.\nRecent advances in Large Language Models (LLMs) suggest similar patterns may\nemerge in artificial systems, with advanced LLMs displaying consistent\nbehavioral tendencies resembling human traits like agreeableness and\nself-regulation. Understanding these patterns is crucial, yet prior work\nprimarily relied on simplified self-reports and heuristic prompting, with\nlittle behavioral validation. In this study, we systematically characterize LLM\npersonality across three dimensions: (1) the dynamic emergence and evolution of\ntrait profiles throughout training stages; (2) the predictive validity of\nself-reported traits in behavioral tasks; and (3) the impact of targeted\ninterventions, such as persona injection, on both self-reports and behavior.\nOur findings reveal that instructional alignment (e.g., RLHF, instruction\ntuning) significantly stabilizes trait expression and strengthens trait\ncorrelations in ways that mirror human data. However, these self-reported\ntraits do not reliably predict behavior, and observed associations often\ndiverge from human patterns. While persona injection successfully steers\nself-reports in the intended direction, it exerts little or inconsistent effect\non actual behavior. By distinguishing surface-level trait expression from\nbehavioral consistency, our findings challenge assumptions about LLM\npersonality and underscore the need for deeper evaluation in alignment and\ninterpretability.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03730v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03730v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.495,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.327,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses instructional alignment, explicitly including RLHF as an example (e.g., in stabilizing personality traits in LLMs), and examines its effects on trait expression. However, RLHF is not the primary focus; it is one of several methods mentioned in the broader context of LLM training and alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on personality traits, self-reports, and behavioral validation in LLMs, with no mention of diffusion models, iterative refinement for logical reasoning, or multi-step reasoning processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper investigates the personality traits in Large Language Models (LLMs) by examining their emergence across training stages, the predictive validity of self-reported traits on behavioral tasks, and the effects of interventions like persona injection, revealing a significant dissociation between self-reports and actual behavior. Using adapted psychological questionnaires and behavioral probes, the authors find that instructional alignment stabilizes traits but does not ensure they predict behavior accurately, and while persona injection alters self-reports, it has minimal impact on behavior, challenging the assumption of genuine personality in LLMs and highlighting the need for deeper alignment methods.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel investigation into the dissociation between self-reported traits and actual behavior in LLMs, significantly advancing the state-of-the-art by challenging existing assumptions about AI personality and integrating psychological methods in a new way.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in AI alignment, interpretability, and ethical deployment within its subfield, as it provides critical insights into LLM limitations that could guide future studies and applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable and timely insights into the limitations of LLM personality traits, making it essential for researchers in AI and psychology to understand the gaps in current models and their implications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e2a4115bc5ce722ce42925470baa09149c35ec81",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 16,
      "average_h_index": 4.428571428571429,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "P. Han",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2125166410"
        },
        {
          "name": "Rafal Kocielnik",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2352256312"
        },
        {
          "name": "Peiyang Song",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2297671110"
        },
        {
          "name": "Ramit Debnath",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2262393353"
        },
        {
          "name": "Dean Mobbs",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378956195"
        },
        {
          "name": "Anima Anandkumar",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/2257161858"
        },
        {
          "name": "R. M. Alvarez",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2257247189"
        }
      ]
    },
    {
      "id": "2509.03733",
      "title": "Differentiable Entropy Regularization for Geometry and Neural Networks",
      "authors": [
        "Ibne Farabi Shihab",
        "Sanjeda Akter",
        "Anuj Sharma"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce a differentiable estimator of range-partition entropy, a recent\nconcept from computational geometry that enables algorithms to adapt to the\n\"sortedness\" of their input. While range-partition entropy provides strong\nguarantees in algorithm design, it has not yet been made accessible to deep\nlearning. In this work, we (i) propose the first differentiable approximation\nof range-partition entropy, enabling its use as a trainable loss or\nregularizer; (ii) design EntropyNet, a neural module that restructures data\ninto low-entropy forms to accelerate downstream instance-optimal algorithms;\nand (iii) extend this principle beyond geometry by applying entropy\nregularization directly to Transformer attention. Across tasks, we demonstrate\nthat differentiable entropy improves efficiency without degrading correctness:\nin geometry, our method achieves up to $4.1\\times$ runtime speedups with\nnegligible error ($<0.2%$); in deep learning, it induces structured attention\npatterns that yield 6% higher accuracy at 80% sparsity compared to L1\nbaselines. Our theoretical analysis provides approximation bounds for the\nestimator, and extensive ablations validate design choices. These results\nsuggest that entropy-bounded computation is not only theoretically elegant but\nalso a practical mechanism for adaptive learning, efficiency, and structured\nrepresentation.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03733v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03733v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.374,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves introducing a differentiable estimator for range-partition entropy to enhance efficiency in geometric algorithms and neural networks, such as EntropyNet and Transformer attention regularization. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, which are core to diffusion-based reasoning. Thus, there is no overlap with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03736",
      "title": "Are LLM Agents Behaviorally Coherent? Latent Profiles for Social\n  Simulation",
      "authors": [
        "James Mooney",
        "Josef Woldense",
        "Zheng Robert Jia",
        "Shirley Anugrah Hayati",
        "My Ha Nguyen",
        "Vipul Raheja",
        "Dongyeop Kang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The impressive capabilities of Large Language Models (LLMs) have fueled the\nnotion that synthetic agents can serve as substitutes for real participants in\nhuman-subject research. In an effort to evaluate the merits of this claim,\nsocial science researchers have largely focused on whether LLM-generated survey\ndata corresponds to that of a human counterpart whom the LLM is prompted to\nrepresent. In contrast, we address a more fundamental question: Do agents\nmaintain internal consistency, retaining similar behaviors when examined under\ndifferent experimental settings? To this end, we develop a study designed to\n(a) reveal the agent's internal state and (b) examine agent behavior in a basic\ndialogue setting. This design enables us to explore a set of behavioral\nhypotheses to assess whether an agent's conversation behavior is consistent\nwith what we would expect from their revealed internal state. Our findings on\nthese hypotheses show significant internal inconsistencies in LLMs across model\nfamilies and at differing model sizes. Most importantly, we find that, although\nagents may generate responses matching those of their human counterparts, they\nfail to be internally consistent, representing a critical gap in their\ncapabilities to accurately substitute for real participants in human-subject\nresearch. Our simulation code and data are publicly accessible.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03736v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03736v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.462,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.299,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating the behavioral consistency of LLM agents in social simulations through prompting and dialogue experiments, without any discussion of training AI models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines LLM agents' internal consistency in conversations and social simulations but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning adaptations from diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03737",
      "title": "LayoutGKN: Graph Similarity Learning of Floor Plans",
      "authors": [
        "Casper van Engelenburg",
        "Jan van Gemert",
        "Seyran Khademi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Floor plans depict building layouts and are often represented as graphs to\ncapture the underlying spatial relationships. Comparison of these graphs is\ncritical for applications like search, clustering, and data visualization. The\nmost successful methods to compare graphs \\ie, graph matching networks, rely on\ncostly intermediate cross-graph node-level interactions, therefore being slow\nin inference time. We introduce \\textbf{LayoutGKN}, a more efficient approach\nthat postpones the cross-graph node-level interactions to the end of the joint\nembedding architecture. We do so by using a differentiable graph kernel as a\ndistance function on the final learned node-level embeddings. We show that\nLayoutGKN computes similarity comparably or better than graph matching networks\nwhile significantly increasing the speed.\n\\href{https://github.com/caspervanengelenburg/LayoutGKN}{Code and data} are\nopen.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03737v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03737v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.283,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.311,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03738",
      "title": "Sparse Autoencoder Neural Operators: Model Recovery in Function Spaces",
      "authors": [
        "Bahareh Tolooshams",
        "Ailsa Shen",
        "Anima Anandkumar"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "eess.SP (Signal Processing)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "We frame the problem of unifying representations in neural models as one of\nsparse model recovery and introduce a framework that extends sparse\nautoencoders (SAEs) to lifted spaces and infinite-dimensional function spaces,\nenabling mechanistic interpretability of large neural operators (NO). While the\nPlatonic Representation Hypothesis suggests that neural networks converge to\nsimilar representations across architectures, the representational properties\nof neural operators remain underexplored despite their growing importance in\nscientific computing. We compare the inference and training dynamics of SAEs,\nlifted-SAE, and SAE neural operators. We highlight how lifting and operator\nmodules introduce beneficial inductive biases, enabling faster recovery,\nimproved recovery of smooth concepts, and robust inference across varying\nresolutions, a property unique to neural operators.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03738v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03738v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.37,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses sparse autoencoders and neural operators for model recovery in function spaces, focusing on representational properties and inductive biases in neural networks. It does not involve diffusion models, iterative refinement processes, multi-step logical reasoning, or any adaptation of diffusion for tasks like Chain-of-Thought processing. Thus, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03740",
      "title": "Singular Value Few-shot Adaptation of Vision-Language Models",
      "authors": [
        "Taha Koleilat",
        "Hassan Rivaz",
        "Yiming Xiao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Vision-language models (VLMs) like CLIP have shown impressive zero-shot and\nfew-shot learning capabilities across diverse applications. However, adapting\nthese models to new fine-grained domains remains difficult due to reliance on\nprompt engineering and the high cost of full model fine-tuning. Existing\nadaptation approaches rely on augmented components, such as prompt tokens and\nadapter modules, which could limit adaptation quality, destabilize the model,\nand compromise the rich knowledge learned during pretraining. In this work, we\npresent CLIP-SVD, a novel multi-modal and parameter-efficient adaptation\ntechnique that leverages Singular Value Decomposition (SVD) to modify the\ninternal parameter space of CLIP without injecting additional modules.\nSpecifically, we fine-tune only the singular values of the CLIP parameter\nmatrices to rescale the basis vectors for domain adaptation while retaining the\npretrained model. This design enables enhanced adaptation performance using\nonly 0.04% of the model's total parameters and better preservation of its\ngeneralization ability. CLIP-SVD achieves state-of-the-art classification\nresults on 11 natural and 10 biomedical datasets, outperforming previous\nmethods in both accuracy and generalization under few-shot settings.\nAdditionally, we leverage a natural language-based approach to analyze the\neffectiveness and dynamics of the CLIP adaptation to allow interpretability of\nCLIP-SVD. The code is publicly available at\nhttps://github.com/HealthX-Lab/CLIP-SVD.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03740v2",
      "pdf_url": "http://arxiv.org/pdf/2509.03740v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.388,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.374,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using Singular Value Decomposition (SVD) for parameter-efficient adaptation of vision-language models like CLIP, specifically for few-shot learning in classification tasks. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning as described in the topic. There is no mention of adapting diffusion mechanisms for reasoning tasks, making the paper entirely unrelated.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03741",
      "title": "Designing Gaze Analytics for ELA Instruction: A User-Centered Dashboard\n  with Conversational AI Support",
      "authors": [
        "Eduardo Davalos",
        "Yike Zhang",
        "Shruti Jain",
        "Namrata Srivastava",
        "Trieu Truong",
        "Nafees-ul Haque",
        "Tristan Van",
        "Jorge Salas",
        "Sara McFadden",
        "Sun-Joo Cho",
        "Gautam Biswas",
        "Amanda Goodwin"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Eye-tracking offers rich insights into student cognition and engagement, but\nremains underutilized in classroom-facing educational technology due to\nchallenges in data interpretation and accessibility. In this paper, we present\nthe iterative design and evaluation of a gaze-based learning analytics\ndashboard for English Language Arts (ELA), developed through five studies\ninvolving teachers and students. Guided by user-centered design and data\nstorytelling principles, we explored how gaze data can support reflection,\nformative assessment, and instructional decision-making. Our findings\ndemonstrate that gaze analytics can be approachable and pedagogically valuable\nwhen supported by familiar visualizations, layered explanations, and narrative\nscaffolds. We further show how a conversational agent, powered by a large\nlanguage model (LLM), can lower cognitive barriers to interpreting gaze data by\nenabling natural language interactions with multimodal learning analytics. We\nconclude with design implications for future EdTech systems that aim to\nintegrate novel data modalities in classroom contexts.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03741v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03741v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.298,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03749",
      "title": "Mapping on a Budget: Optimizing Spatial Data Collection for ML",
      "authors": [
        "Livia Betti",
        "Farooq Sanni",
        "Gnouyaro Sogoyou",
        "Togbe Agbagla",
        "Cullen Molitor",
        "Tamma Carleton",
        "Esther Rolf"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In applications across agriculture, ecology, and human development, machine\nlearning with satellite imagery (SatML) is limited by the sparsity of labeled\ntraining data. While satellite data cover the globe, labeled training datasets\nfor SatML are often small, spatially clustered, and collected for other\npurposes (e.g., administrative surveys or field measurements). Despite the\npervasiveness of this issue in practice, past SatML research has largely\nfocused on new model architectures and training algorithms to handle scarce\ntraining data, rather than modeling data conditions directly. This leaves\nscientists and policymakers who wish to use SatML for large-scale monitoring\nuncertain about whether and how to collect additional data to maximize\nperformance. Here, we present the first problem formulation for the\noptimization of spatial training data in the presence of heterogeneous data\ncollection costs and realistic budget constraints, as well as novel methods for\naddressing this problem. In experiments simulating different problem settings\nacross three continents and four tasks, our strategies reveal substantial gains\nfrom sample optimization. Further experiments delineate settings for which\noptimized sampling is particularly effective. The problem formulation and\nmethods we introduce are designed to generalize across application domains for\nSatML; we put special emphasis on a specific problem setting where our\ncoauthors can immediately use our findings to augment clustered agricultural\nsurveys for SatML monitoring in Togo.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03749v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03749v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.442,
      "diffusion_reasoning_score": 0.333,
      "distributed_training_score": 0.401,
      "datasets_score": 0.429,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on optimizing the spatial collection and distribution of labeled training data for SatML, emphasizing ground-referenced data acquisition under budget constraints. It does not involve programmatically generating noisy or imprecise labels, which is the core of weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is about strategies for collecting and optimizing spatial training datasets, not about parallel computing, partitioning data across nodes, or accelerating model training through distributed systems.",
      "datasets_justification": "The paper directly addresses the creation, analysis, and evaluation of datasets for ML by introducing methods to optimize spatial data collection, analyzing dataset biases, and benchmarking sampling strategies in simulations, which aligns with research on dataset curation and performance improvement.",
      "llm_score_status": "completed",
      "summary": "This paper addresses the limitations of machine learning with satellite imagery (SatML) due to sparse and spatially clustered training data by introducing a novel problem formulation for optimizing spatial data collection under heterogeneous costs and budget constraints. The authors develop methods to select optimal sampling strategies and evaluate them through simulations across three continents and four tasks, demonstrating substantial performance gains over baselines and highlighting settings where optimized sampling is most effective, with a focus on practical applications like augmenting agricultural surveys in Togo.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new problem formulation and methods for optimizing spatial training data collection in SatML, significantly advancing the field by shifting focus from model architectures to data acquisition strategies, which has not been addressed in prior research.",
      "impact_score": "High",
      "impact_justification": "The work could broadly influence SatML applications in policy, agriculture, and ecology by providing practical tools for efficient data collection, potentially leading to improved model performance and more effective large-scale monitoring in data-poor regions.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution with practical implications for SatML practitioners, offering essential guidance on data collection that advances the field, though it may not be critical for those outside specific subfields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ff5c91ed8c48adf00bb307d741bc2c6a9452b55c",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Livia Betti",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378950247"
        },
        {
          "name": "Farooq Sanni",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378950294"
        },
        {
          "name": "Gnouyaro Sogoyou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378950471"
        },
        {
          "name": "Togbe Agbagla",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378950231"
        },
        {
          "name": "Cullen Molitor",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354373012"
        },
        {
          "name": "Tamma Carleton",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2319491777"
        },
        {
          "name": "Esther Rolf",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2354472923"
        }
      ]
    },
    {
      "id": "2509.03754",
      "title": "STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight\n  Plant Disease Classification",
      "authors": [
        "Zongsen Qiu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Responding to rising global food security needs, precision agriculture and\ndeep learning-based plant disease diagnosis have become crucial. Yet, deploying\nhigh-precision models on edge devices is challenging. Most lightweight networks\nuse attention mechanisms designed for generic object recognition, which poorly\ncapture subtle pathological features like irregular lesion shapes and complex\ntextures. To overcome this, we propose a twofold solution: first, using a\ntraining-free neural architecture search method (DeepMAD) to create an\nefficient network backbone for edge devices; second, introducing the\nShape-Texture Attention Module (STAM). STAM splits attention into two branches\n-- one using deformable convolutions (DCNv4) for shape awareness and the other\nusing a Gabor filter bank for texture awareness. On the public CCMT plant\ndisease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs)\nreached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm\nSTAM significantly improves performance over baseline and standard attention\nmodels. Integrating domain knowledge via decoupled attention thus presents a\npromising path for edge-deployed precision agriculture AI. The source code is\navailable at https://github.com/RzMY/STA-Net.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03754v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03754v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.277,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.336,
      "distributed_training_score": 0.362,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03757",
      "title": "ARDO: A Weak Formulation Deep Neural Network Method for Elliptic and\n  Parabolic PDEs Based on Random Differences of Test Functions",
      "authors": [
        "Wei Cai",
        "Andrew Qing He"
      ],
      "categories": [
        "math.NA (Numerical Analysis)",
        "cs.AI (Artificial Intelligence)",
        "cs.NA (Numerical Analysis)"
      ],
      "abstract": "We propose ARDO method for solving PDEs and PDE-related problems with deep\nlearning techniques. This method uses a weak adversarial formulation but\ntransfers the random difference operator onto the test function. The main\nadvantage of this framework is that it is fully derivative-free with respect to\nthe solution neural network. This framework is particularly suitable for\nFokker-Planck type second-order elliptic and parabolic PDEs.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03757v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03757v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.376,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a deep learning method for solving elliptic and parabolic PDEs, specifically using a derivative-free approach for Fokker-Planck equations, which are related to diffusion processes in physics. However, it does not involve adapting diffusion models for iterative refinement in logical tasks, multi-step reasoning, or Chain-of-Thought processes. The topic requires explicit use of diffusion for reasoning, which is absent here, making the paper unrelated.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03768",
      "title": "RAGuard: A Novel Approach for in-context Safe Retrieval Augmented\n  Generation for LLMs",
      "authors": [
        "Connor Walker",
        "Koorosh Aslansefat",
        "Mohammad Naveed Akram",
        "Yiannis Papadopoulos"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet\nconventional Large Language Models (LLMs) often fail when confronted with\nhighly specialised or unexpected scenarios. We introduce RAGuard, an enhanced\nRetrieval-Augmented Generation (RAG) framework that explicitly integrates\nsafety-critical documents alongside technical manuals.By issuing parallel\nqueries to two indices and allocating separate retrieval budgets for knowledge\nand safety, RAGuard guarantees both technical depth and safety coverage. We\nfurther develop a SafetyClamp extension that fetches a larger candidate pool,\n\"hard-clamping\" exact slot guarantees to safety. We evaluate across sparse\n(BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms,\nmeasuring Technical Recall@K and Safety Recall@K. Both proposed extensions of\nRAG show an increase in Safety Recall@K from almost 0\\% in RAG to more than\n50\\% in RAGuard, while maintaining Technical Recall above 60\\%. These results\ndemonstrate that RAGuard and SafetyClamp have the potential to establish a new\nstandard for integrating safety assurance into LLM-powered decision support in\ncritical maintenance contexts.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03768v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03768v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.469,
      "weak_supervision_score": 0.433,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.354,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on an enhanced RAG framework for retrieval and generation in LLMs, emphasizing safety and technical accuracy in OSW maintenance. It does not involve training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper introduces RAGuard as a retrieval mechanism and does not address machine learning training, label generation from noisy sources, or any form of supervision. It is centered on retrieval strategies rather than data labeling or model training approaches.",
      "diffusion_reasoning_justification": "The paper describes a RAG framework with parallel queries and safety extensions, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning as defined. The focus is on retrieval and generation, not holistic chain-of-thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03771",
      "title": "Learning an Adversarial World Model for Automated Curriculum Generation\n  in MARL",
      "authors": [
        "Brennen Hill"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "World models that infer and predict environmental dynamics are foundational\nto embodied intelligence. However, their potential is often limited by the\nfinite complexity and implicit biases of hand-crafted training environments. To\ndevelop truly generalizable and robust agents, we need environments that scale\nin complexity alongside the agents learning within them. In this work, we\nreframe the challenge of environment generation as the problem of learning a\ngoal-conditioned, generative world model. We propose a system where a\ngenerative **Attacker** agent learns an implicit world model to synthesize\nincreasingly difficult challenges for a team of cooperative **Defender**\nagents. The Attacker's objective is not passive prediction, but active,\ngoal-driven interaction: it models and generates world states (i.e.,\nconfigurations of enemy units) specifically to exploit the Defenders'\nweaknesses. Concurrently, the embodied Defender team learns a cooperative\npolicy to overcome these generated worlds. This co-evolutionary dynamic creates\na self-scaling curriculum where the world model continuously adapts to\nchallenge the decision-making policy of the agents, providing an effectively\ninfinite stream of novel and relevant training scenarios. We demonstrate that\nthis framework leads to the emergence of complex behaviors, such as the world\nmodel learning to generate flanking and shielding formations, and the defenders\nlearning coordinated focus-fire and spreading tactics. Our findings position\nadversarial co-evolution as a powerful method for learning instrumental world\nmodels that drive agents toward greater strategic depth and robustness.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03771v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03771v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.397,
      "distributed_training_score": 0.357,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves an adversarial, multi-agent reinforcement learning framework where agents co-evolve through automated curriculum generation, without any involvement of human feedback. RLHF specifically requires training a reward model using human-ranked data to align AI models with human preferences, which is not present in this work. Thus, there is no connection to RLHF concepts.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03775",
      "title": "ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast,\n  Memory-Efficient Reconstruction",
      "authors": [
        "Sankeerth Durvasula",
        "Sharanshangar Muhunthan",
        "Zain Moustafa",
        "Richard Chen",
        "Ruofan Liang",
        "Yushi Guan",
        "Nilesh Ahuja",
        "Nilesh Jain",
        "Selvakumar Panneer",
        "Nandita Vijaykumar"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world\nscenes with high quality and real-time rendering. Typically, a higher quality\nrepresentation can be achieved by using a large number of 3D Gaussians.\nHowever, using large 3D Gaussian counts significantly increases the GPU device\nmemory for storing model parameters. A large model thus requires powerful GPUs\nwith high memory capacities for training and has slower training/rendering\nlatencies due to the inefficiencies of memory access and data movement. In this\nwork, we introduce ContraGS, a method to enable training directly on compressed\n3DGS representations without reducing the Gaussian Counts, and thus with a\nlittle loss in model quality. ContraGS leverages codebooks to compactly store a\nset of Gaussian parameter vectors throughout the training process, thereby\nsignificantly reducing memory consumption. While codebooks have been\ndemonstrated to be highly effective at compressing fully trained 3DGS models,\ndirectly training using codebook representations is an unsolved challenge.\nContraGS solves the problem of learning non-differentiable parameters in\ncodebook-compressed representations by posing parameter estimation as a\nBayesian inference problem. To this end, ContraGS provides a framework that\neffectively uses MCMC sampling to sample over a posterior distribution of these\ncompressed representations. With ContraGS, we demonstrate that ContraGS\nsignificantly reduces the peak memory during training (on average 3.49X) and\naccelerated training and rendering (1.36X and 1.88X on average, respectively),\nwhile retraining close to state-of-art quality.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.03775v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03775v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.252,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.388,
      "datasets_score": 0.262,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04515",
      "title": "Mitigation of Gender and Ethnicity Bias in AI-Generated Stories through\n  Model Explanations",
      "authors": [
        "Martha O. Dimgba",
        "Sharon Oba",
        "Ameeta Agrawal",
        "Philippe J. Giabbanelli"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Language models have been shown to propagate social bias through their\noutput, particularly in the representation of gender and ethnicity. This paper\ninvestigates gender and ethnicity biases in AI-generated occupational stories.\nRepresentation biases are measured before and after applying our proposed\nmitigation strategy, Bias Analysis and Mitigation through Explanation (BAME),\nrevealing improvements in demographic representation ranging from 2% to 20%.\nBAME leverages model-generated explanations to inform targeted prompt\nengineering, effectively reducing biases without modifying model parameters. By\nanalyzing stories generated across 25 occupational groups, three large language\nmodels (Claude 3.5 Sonnet, Llama 3.1 70B Instruct, and GPT-4 Turbo), and\nmultiple demographic dimensions, we identify persistent patterns of\noverrepresentation and underrepresentation linked to training data stereotypes.\nOur findings demonstrate that guiding models with their own internal reasoning\nmechanisms can significantly enhance demographic parity, thereby contributing\nto the development of more transparent generative AI systems.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.04515v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04515v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.415,
      "distributed_training_score": 0.295,
      "datasets_score": 0.395,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on mitigating gender and ethnicity biases in AI-generated stories using a method called BAME, which involves model explanations and prompt engineering. It does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning through diffusion. The work is based on standard language models like Claude and GPT, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05352",
      "title": "Unsupervised Instance Segmentation with Superpixels",
      "authors": [
        "Cuong Manh Hoang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Instance segmentation is essential for numerous computer vision applications,\nincluding robotics, human-computer interaction, and autonomous driving.\nCurrently, popular models bring impressive performance in instance segmentation\nby training with a large number of human annotations, which are costly to\ncollect. For this reason, we present a new framework that efficiently and\neffectively segments objects without the need for human annotations. Firstly, a\nMultiCut algorithm is applied to self-supervised features for coarse mask\nsegmentation. Then, a mask filter is employed to obtain high-quality coarse\nmasks. To train the segmentation network, we compute a novel superpixel-guided\nmask loss, comprising hard loss and soft loss, with high-quality coarse masks\nand superpixels segmented from low-level image features. Lastly, a\nself-training process with a new adaptive loss is proposed to improve the\nquality of predicted masks. We conduct experiments on public datasets in\ninstance segmentation and object detection to demonstrate the effectiveness of\nthe proposed framework. The results show that the proposed framework\noutperforms previous state-of-the-art methods.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.05352v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05352v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.281,
      "weak_supervision_score": 0.438,
      "diffusion_reasoning_score": 0.309,
      "distributed_training_score": 0.313,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves training a segmentation network using programmatically generated labels from sources like coarse masks derived from self-supervised features and superpixels from low-level image features, which are noisy and imprecise compared to hand-labeled data. This directly aligns with weak supervision, as it relies on high-level, automatically generated supervision rather than perfect annotations, enabling effective model training without human involvement.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces a novel unsupervised framework for instance segmentation that eliminates the need for human annotations, addressing limitations in existing methods like TokenCut and CutLER. It utilizes a MultiCut algorithm on self-supervised features to generate coarse masks, refines them with a mask filter, and trains a segmentation network using a new superpixel-guided mask loss that combines hard and soft losses based on superpixels and image features, followed by self-training with an adaptive loss to enhance mask quality. Experimental results on public datasets demonstrate that this approach outperforms previous state-of-the-art methods in unsupervised instance segmentation.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework with innovative elements like the superpixel-guided mask loss and adaptive self-training, significantly advancing unsupervised instance segmentation by addressing key limitations of prior works.",
      "impact_score": "High",
      "impact_justification": "This work could influence a wide range of computer vision applications, such as robotics and autonomous driving, by providing an effective unsupervised method that reduces annotation costs and improves model performance.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "As a high-quality paper that presents a significant advancement in unsupervised instance segmentation with demonstrated superior results, it is valuable for researchers in computer vision to understand its contributions and methodologies.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/03f0f3e5f4e78172ef59d04a4ae40d9fa8b8b316",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Cuong Manh Hoang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379393996"
        }
      ]
    },
    {
      "id": "2509.05356",
      "title": "Spiking Neural Networks for Continuous Control via End-to-End\n  Model-Based Learning",
      "authors": [
        "Justus Huebotter",
        "Pablo Lanillos",
        "Marcel van Gerven",
        "Serge Thill"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Despite recent progress in training spiking neural networks (SNNs) for\nclassification, their application to continuous motor control remains limited.\nHere, we demonstrate that fully spiking architectures can be trained end-to-end\nto control robotic arms with multiple degrees of freedom in continuous\nenvironments. Our predictive-control framework combines Leaky\nIntegrate-and-Fire dynamics with surrogate gradients, jointly optimizing a\nforward model for dynamics prediction and a policy network for goal-directed\naction. We evaluate this approach on both a planar 2D reaching task and a\nsimulated 6-DOF Franka Emika Panda robot. Results show that SNNs can achieve\nstable training and accurate torque control, establishing their viability for\nhigh-dimensional motor tasks. An extensive ablation study highlights the role\nof initialization, learnable time constants, and regularization in shaping\ntraining dynamics. We conclude that while stable and effective control can be\nachieved, recurrent spiking networks remain highly sensitive to hyperparameter\nsettings, underscoring the importance of principled design choices.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.05356v2",
      "pdf_url": "http://arxiv.org/pdf/2509.05356v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.404,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on training spiking neural networks for robotic control using surrogate gradients and model-based learning, with no mention of human feedback, reward models based on human preferences, or fine-tuning via human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses SNNs for continuous motor control and predictive modeling, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "The paper discusses end-to-end training of SNNs for control tasks but does not cover distributed training, parallel computing, multi-node setups, or strategies for partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05359",
      "title": "An Empirical Analysis of Discrete Unit Representations in Speech\n  Language Modeling Pre-training",
      "authors": [
        "Yanis Labrak",
        "Richard Dufour",
        "Mickaël Rouvier"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "This paper investigates discrete unit representations in Speech Language\nModels (SLMs), focusing on optimizing speech modeling during continual\npre-training. In this paper, we systematically examine how model architecture,\ndata representation, and training robustness influence the pre-training stage\nin which we adapt existing pre-trained language models to the speech modality.\nOur experiments highlight the role of speech encoders and clustering\ngranularity across different model scales, showing how optimal discretization\nstrategies vary with model capacity. By examining cluster distribution and\nphonemic alignments, we investigate the effective use of discrete vocabulary,\nuncovering both linguistic and paralinguistic patterns. Additionally, we\nexplore the impact of clustering data selection on model robustness,\nhighlighting the importance of domain matching between discretization training\nand target applications.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.05359v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05359v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.394,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on empirical analysis of discrete unit representations in speech language modeling, including pre-training strategies, model architectures, and speech encoders. It does not involve diffusion models, iterative refinement processes, Chain-of-Thought reasoning, or any adaptation for solving logical tasks. Thus, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05361",
      "title": "Governing AI R&D: A Legal Framework for Constraining Dangerous AI",
      "authors": [
        "Alex Mark",
        "Aaron Scher"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As AI advances, governing its development may become paramount to public\nsafety. Lawmakers may seek to restrict the development and release of AI models\nor of AI research itself. These governance actions could trigger legal\nchallenges that invalidate the actions, so lawmakers should consider these\nchallenges ahead of time. We investigate three classes of potential litigation\nrisk for AI regulation in the U.S.: the First Amendment, administrative law,\nand the Fourteenth Amendment. We discuss existing precedent that is likely to\napply to AI, which legal challenges are likely to arise, and how lawmakers\nmight preemptively address them. Effective AI regulation is possible, but it\nrequires careful implementation to avoid these legal challenges.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.05361v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05361v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.332,
      "diffusion_reasoning_score": 0.333,
      "distributed_training_score": 0.306,
      "datasets_score": 0.376,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on legal frameworks and potential litigation risks for governing AI development, such as First Amendment and administrative law issues. It does not discuss or contribute to reinforcement learning techniques, including those involving human feedback for aligning AI models. Therefore, there is no connection to the topic of Reinforcement Learning from Human Feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.06992",
      "title": "FedAPT: Federated Adversarial Prompt Tuning for Vision-Language Models",
      "authors": [
        "Kun Zhai",
        "Siheng Chen",
        "Xingjun Ma",
        "Yu-Gang Jiang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Federated Prompt Tuning (FPT) is an efficient method for cross-client\ncollaborative fine-tuning of large Vision-Language Models (VLMs). However,\nmodels tuned using FPT are vulnerable to adversarial attacks, leading to\nmisclassification in downstream tasks. In this work, we introduce Federated\nAdversarial Prompt Tuning (\\textbf{FedAPT}), a novel method designed to enhance\nthe adversarial robustness of FPT. We identify a key issue in FedAPT under\nnon-independent and identically distributed (non-IID) settings: a \\textit{class\ninformation gap} between clients and the global model. Clients rely solely on\nlimited local label information to generate adversarial samples for training,\nwhile the global model must defend against adversarial attacks from global\nlabels. To address this issue, we propose a \\textbf{class-aware prompt\ngenerator} that generates visual prompts from text prompts. This generator is\nguided by a \\emph{Global Label Embedding} (serving as a ``beacon\") which\nencodes cross-client label information to create more globally-aligned visual\nprompts. Additionally, we propose a \\textbf{cross-layer generator sharing}\nstrategy to enhance prompt coupling across different layers of the model,\nfurther boosting adversarial robustness. Extensive experiments on multiple\nimage classification datasets demonstrate the superiority of FedAPT in\nimproving adversarial robustness, outperforming existing methods by a large\nmargin. FedAPT also exhibits exceptional generalization in cross-domain and\ncross-dataset scenarios, indicating its effectiveness in real-world\napplications.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.06992v1",
      "pdf_url": "http://arxiv.org/pdf/2509.06992v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.414,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on federated adversarial prompt tuning for vision-language models, emphasizing adversarial robustness and federated learning techniques. It does not involve human feedback, reward models, or reinforcement learning for aligning AI with human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution involves federated learning (FL), a distributed training paradigm, where multiple clients collaboratively train a model without sharing data. It addresses challenges like non-IID data distribution and introduces techniques for efficient cross-client optimization, directly aligning with distributed training, parallel computing, and multi-node machine learning.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces FedAPT, a novel federated adversarial prompt tuning method for vision-language models, aimed at enhancing adversarial robustness in non-IID settings by addressing the class information gap through a class-aware prompt generator guided by global label embeddings and a cross-layer generator sharing strategy. Extensive experiments on multiple image classification datasets demonstrate that FedAPT significantly outperforms existing methods in adversarial robustness, maintains higher clean accuracy, and exhibits strong generalization in cross-domain and cross-dataset scenarios.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique, FedAPT, which advances the state-of-the-art by addressing the class information gap in federated adversarial training through innovative components like the class-aware prompt generator and global label embedding. This represents a significant advancement in enhancing robustness for vision-language models in distributed environments.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of federated learning and adversarial robustness for vision-language models, given its demonstrated superior performance and practical relevance to privacy-sensitive applications. However, its influence may be limited to specific domains rather than broadly across all AI research.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a high-quality, innovative contribution to adversarial robustness in federated settings, making it valuable for researchers in computer vision and federated learning. While not essential for all readers, it offers strong insights and experimental evidence that warrant attention in relevant fields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/29dd4c0acda9e9a2794e23b98929ee3f1409d8b3",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 9,
      "average_h_index": 5.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Kun Zhai",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2297189371"
        },
        {
          "name": "Siheng Chen",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2333394822"
        },
        {
          "name": "Xingjun Ma",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2267387052"
        },
        {
          "name": "Yu-Gang Jiang",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2259536780"
        }
      ]
    },
    {
      "id": "2509.06993",
      "title": "Geospatial Foundational Embedder: Top-1 Winning Solution on EarthVision\n  Embed2Scale Challenge (CVPR 2025)",
      "authors": [
        "Zirui Xu",
        "Raphael Tang",
        "Mike Bianco",
        "Qi Zhang",
        "Rishi Madhok",
        "Nikolaos Karianakis",
        "Fuxun Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "EarthVision Embed2Scale challenge (CVPR 2025) aims to develop foundational\ngeospatial models to embed SSL4EO-S12 hyperspectral geospatial data cubes into\nembedding vectors that faciliatetes various downstream tasks, e.g.,\nclassification, regression, etc. In this technical report, we introduce our\nproposed method for the Top-1 winning solution on the Embed2Scale Challenge.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.06993v1",
      "pdf_url": "http://arxiv.org/pdf/2509.06993v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.365,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.06994",
      "title": "VLMs-in-the-Wild: Bridging the Gap Between Academic Benchmarks and\n  Enterprise Reality",
      "authors": [
        "Srihari Bandraupalli",
        "Anupam Purwar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Open-source Vision-Language Models show immense promise for enterprise\napplications, yet a critical disconnect exists between academic evaluation and\nenterprise deployment requirements. Current benchmarks rely heavily on\nmultiple-choice questions and synthetic data, failing to capture the complexity\nof real-world business applications like social media content analysis. This\npaper introduces VLM-in-the-Wild (ViLD), a comprehensive framework to bridge\nthis gap by evaluating VLMs on operational enterprise requirements. We define\nten business-critical tasks: logo detection, OCR, object detection, human\npresence and demographic analysis, human activity and appearance analysis,\nscene detection, camera perspective and media quality assessment, dominant\ncolors, comprehensive description, and NSFW detection. To this framework, we\nbring an innovative BlockWeaver Algorithm that solves the challenging problem\nof comparing unordered, variably-grouped OCR outputs from VLMs without relying\non embeddings or LLMs, achieving remarkable speed and reliability. To\ndemonstrate efficacy of ViLD, we constructed a new benchmark dataset of 7,500\ndiverse samples, carefully stratified from a corpus of one million real-world\nimages and videos. ViLD provides actionable insights by combining semantic\nmatching (both embedding-based and LLM-as-a-judge approaches), traditional\nmetrics, and novel methods to measure the completeness and faithfulness of\ndescriptive outputs. By benchmarking leading open-source VLMs (Qwen, MIMO, and\nInternVL) against a powerful proprietary baseline as per ViLD framework, we\nprovide one of the first industry-grounded, task-driven assessment of VLMs\ncapabilities, offering actionable insights for their deployment in enterprise\nenvironments.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.06994v1",
      "pdf_url": "http://arxiv.org/pdf/2509.06994v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.415,
      "diffusion_reasoning_score": 0.433,
      "distributed_training_score": 0.369,
      "datasets_score": 0.421,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on evaluating and benchmarking Vision-Language Models (VLMs) for enterprise applications, introducing a new framework and dataset, but does not involve training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper describes creating a dataset from real-world images and videos through stratification, but it does not discuss programmatically generating labels from noisy sources or using weak supervision methods for training models.",
      "diffusion_reasoning_justification": "The paper introduces the BlockWeaver Algorithm for comparing OCR outputs, which involves speed and reliability in matching, but there is no mention of diffusion models, iterative refinement for logical reasoning, or multi-step reasoning processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes creating a new benchmark dataset of 7,500 diverse samples from real-world data, stratifying it for enterprise evaluation, and using it to benchmark VLMs, directly aligning with research on dataset creation, curation, and benchmarking for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper introduces the VLM-in-the-Wild (ViLD) framework to address the disconnect between academic benchmarks and real-world enterprise needs for Vision-Language Models (VLMs), defining ten business-critical tasks such as logo detection and OCR, and proposing the innovative BlockWeaver Algorithm for comparing unordered outputs. It creates a new benchmark dataset of 7,500 real-world samples, evaluates leading open-source VLMs against a proprietary baseline using a combination of semantic matching and traditional metrics, and provides actionable insights into their deployment suitability by highlighting gaps in current evaluation methods.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework (ViLD) and algorithm (BlockWeaver) that significantly advances VLM evaluation by focusing on enterprise realities, addressing critical gaps not covered in existing benchmarks. This represents a substantial step forward in adapting AI assessments to practical applications.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and commercial applications by providing a more relevant evaluation method for VLMs in enterprise settings, likely leading to improved model development and adoption. Its comprehensive approach and new dataset could drive changes in how AI is benchmarked and deployed in business contexts.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, significant contribution to VLM evaluation that is essential for researchers and practitioners in AI and enterprise applications, providing practical insights that advance the field. However, it may not be critical for those outside of computer vision and language processing domains.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/459cc557791f61cf696f24e80b9827756de94e1a",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 10,
      "average_h_index": 5.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Srihari Bandraupalli",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379765278"
        },
        {
          "name": "A. Purwar",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/33856997"
        }
      ]
    },
    {
      "id": "2509.06995",
      "title": "The Protocol Genome A Self Supervised Learning Framework from DICOM\n  Headers",
      "authors": [
        "Jimmy Joseph"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "In this paper, we introduce the Protocol Genome, a self-supervised learning\nsystem that learns correlations from DICOM headers and achieves AUROC 0.901 (vs\n0.847 baseline) and ECE 0.036 (vs 0.058) on fully held-out external validation.\nOur method also improves calibration and robustness across modalities (CT, MRI,\nCXR) and vendors. Clinical imaging is funneled through PACS/DICOM, where\nprocedure choices (scanner make/model, sequence, kernel, kVp, TR/TE, and slice\nthickness) have consequences for contrast, noise, and artifact. These latent\nconfounders impede the generalization of image-only networks across sites. We\nconsider structured DICOM headers as a label and learn protocol-aware but\nclinically robust image representations. Protocol Genome obtains tokenized\nembeddings of de-identified header fields and models them along with image\nfeatures using: (1) protocol-image contrastive learning, (2) masked protocol\nprediction, and (3) protocol-protocol translation. With 1.26M studies (7 health\nsystems, 31 scanners, 3 vendors; CT, MR, CR/DR), we experiment on: (A) chest CT\ntriage for PE, (B) brain MRI glioma grading, and (C) chest radiograph\ncardiomegaly detection. Relative to strong SSL baselines (SimCLR, MAE) as well\nas ImageNet transfer, Protocol Genome (+0.046: PE, +0.058: glioma, +0.041:\ncardiomegaly) is associated with higher external AUROC; 25-37% calibration\nimprovements are obtained (p < 0.01, DeLong tests). While the gains may be\ntask-dependent, they are preserved with 10-20% of labeled data. From a clinical\npoint of view, the technique reduces false positives at protocol borders and is\napplicable in a PACS (DICOM C-FIND/C-MOVE, DICOMweb QIDO/WADO). We publish a\nmodel card and deployment guide, complete with both de-identification and bias\naudits.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.06995v1",
      "pdf_url": "http://arxiv.org/pdf/2509.06995v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.382,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces a self-supervised learning framework using DICOM headers as supervisory signals to learn image representations, which involves programmatically derived labels from structured metadata. While this shares some similarities with weak supervision—such as using high-level, potentially noisy sources (e.g., headers) instead of hand-labeled data—it is primarily self-supervised and does not fully align with weak supervision's emphasis on external, imprecise label generation. Thus, it is only tangentially related.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10516",
      "title": "Privacy-Preserving Personalization in Education: A Federated Recommender\n  System for Student Performance Prediction",
      "authors": [
        "Rodrigo Tertulino"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "The increasing digitalization of education presents unprecedented\nopportunities for data-driven personalization, yet it introduces significant\nstudent data privacy challenges. Conventional recommender systems rely on\ncentralized data, a paradigm often incompatible with modern data protection\nregulations. A novel privacy-preserving recommender system is proposed and\nevaluated to address this critical issue using Federated Learning (FL). The\napproach utilizes a Deep Neural Network (DNN) with rich, engineered features\nfrom the large-scale ASSISTments educational dataset. A rigorous comparative\nanalysis of federated aggregation strategies was conducted, identifying FedProx\nas a significantly more stable and effective method for handling heterogeneous\nstudent data than the standard FedAvg baseline. The optimized federated model\nachieves a high-performance F1-Score of 76.28\\%, corresponding to 82.85\\% of\nthe performance of a powerful, centralized XGBoost model. These findings\nvalidate that a federated approach can provide highly effective content\nrecommendations without centralizing sensitive student data. Consequently, our\nwork presents a viable and robust solution to the personalization-privacy\ndilemma in modern educational platforms.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.10516v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10516v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.421,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.409,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Federated Learning for a privacy-preserving recommender system in education, using techniques like FedAvg and FedProx for model aggregation. It does not involve reinforcement learning, human feedback, reward models, or any mechanism for aligning AI with human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution centers on Federated Learning, a distributed training approach that involves training models across decentralized devices and aggregating updates, as seen in the comparison of FedAvg and FedProx strategies. This directly aligns with distributed training concepts, including parallel computing and handling heterogeneous data across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of balancing personalized education with student data privacy by proposing a federated recommender system using Deep Neural Networks (DNN) on the ASSISTments dataset, which allows model training without centralizing sensitive data. The methodology involves comparing federated aggregation strategies, FedAvg and FedProx, with results showing FedProx's superior stability and effectiveness, achieving an F1-Score of 76.28%—equivalent to 82.85% of a centralized XGBoost model's performance—thus demonstrating a viable privacy-preserving approach for educational personalization.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of Federated Learning with educational recommender systems, offering a notable improvement in privacy for a known problem, though it does not introduce an entirely new technique or architecture. This application to education is innovative in context but builds on existing FL methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like educational technology and privacy-preserving AI, given its practical implications for compliance with data regulations. However, its influence may be limited to specific applications rather than broadly transformative across all AI domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution by effectively demonstrating a privacy-preserving solution for educational personalization, making it valuable for researchers in AI and education. While not essential for all, it provides insightful advancements that warrant attention in relevant fields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/37fe4a064bb332bd69cd5cc7e144c57eb43cf04b",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 1,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Rodrigo Tertulino",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2375902426"
        }
      ]
    },
    {
      "id": "2509.10517",
      "title": "A Comparative Benchmark of Federated Learning Strategies for Mortality\n  Prediction on Heterogeneous and Imbalanced Clinical Data",
      "authors": [
        "Rodrigo Tertulino"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "Machine learning models hold significant potential for predicting in-hospital\nmortality, yet data privacy constraints and the statistical heterogeneity of\nreal-world clinical data often hamper their development. Federated Learning\n(FL) offers a privacy-preserving solution, but its performance under\nnon-Independent and Identically Distributed (non-IID) and imbalanced conditions\nrequires rigorous investigation. The study presents a comparative benchmark of\nfive federated learning strategies: FedAvg, FedProx, FedAdagrad, FedAdam, and\nFedCluster for mortality prediction. Using the large-scale MIMIC-IV dataset, we\nsimulate a realistic non-IID environment by partitioning data by clinical care\nunit. To address the inherent class imbalance of the task, the SMOTE-Tomek\ntechnique is applied to each client's local training data. Our experiments,\nconducted over 50 communication rounds, reveal that the regularization-based\nstrategy, FedProx, consistently outperformed other methods, achieving the\nhighest F1-Score of 0.8831 while maintaining stable convergence. While the\nbaseline FedAvg was the most computationally efficient, its predictive\nperformance was substantially lower. Our findings indicate that\nregularization-based FL algorithms like FedProx offer a more robust and\neffective solution for heterogeneous and imbalanced clinical prediction tasks\nthan standard or server-side adaptive aggregation methods. The work provides a\ncrucial empirical benchmark for selecting appropriate FL strategies for\nreal-world healthcare applications.",
      "published_date": "2025-09-03",
      "arxiv_url": "http://arxiv.org/abs/2509.10517v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10517v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.278,
      "distributed_training_score": 0.43,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution involves benchmarking Federated Learning (FL) strategies, which are distributed training methods that partition data across multiple clients (e.g., clinical units) to train models collaboratively without centralizing data. This directly addresses distributed training by focusing on algorithms like FedAvg and FedProx that handle data heterogeneity and parallel computation across nodes, aligning closely with topics of parallel computing and multi-node machine learning for accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper conducts a comparative benchmark of five federated learning strategies—FedAvg, FedProx, FedAdagrad, FedAdam, and FedCluster—for predicting in-hospital mortality using the MIMIC-IV dataset, which is partitioned by clinical care unit to simulate non-IID and imbalanced conditions, with SMOTE-Tomek applied to address class imbalance. The experiments over 50 communication rounds demonstrate that FedProx, a regularization-based method, achieves the highest F1-Score of 0.8831 and stable convergence, outperforming others and highlighting its robustness for heterogeneous clinical data, while providing empirical guidance for selecting FL approaches in real-world healthcare applications.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying and comparing existing FL strategies in the specific context of non-IID and imbalanced clinical data for mortality prediction, offering a clever combination for a known problem. However, it does not introduce a truly new problem, architecture, or technique, making it incremental rather than groundbreaking.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like federated learning and healthcare AI, as it provides practical benchmarks for selecting strategies in real-world clinical applications. While it has potential to influence specific areas, its broader applicability may be limited to those dealing with similar data challenges.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a strong, valuable contribution through its empirical benchmarks and insights for FL in healthcare, making it important for researchers in AI and medical data analysis. However, it is not essential for a general audience, as its focus is somewhat niche.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/86bcd82c6b462e46280190f11c7edf0d4cae86b7",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 1,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Rodrigo Tertulino",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2375902426"
        }
      ]
    }
  ],
  "total_papers": 163,
  "date": "2025-09-03"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Relevance Filter State Management
        let currentRelevanceFilters = {
            highlyRelevant: true,
            moderatelyRelevant: true,
            tangentiallyRelevant: true,
            notRelevant: true
        };
        
        let pendingRelevanceFilters = { ...currentRelevanceFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function calculateRelevanceScore(paper) {
            let score = 0;
            
            // Only consider topics that are currently selected/enabled in the topic filter
            const topicsToConsider = [];
            if (currentTopicFilters.rlhf) topicsToConsider.push('rlhf_relevance');
            if (currentTopicFilters.weakSupervision) topicsToConsider.push('weak_supervision_relevance');
            if (currentTopicFilters.diffusionReasoning) topicsToConsider.push('diffusion_reasoning_relevance');
            if (currentTopicFilters.distributedTraining) topicsToConsider.push('distributed_training_relevance');
            if (currentTopicFilters.datasets) topicsToConsider.push('datasets_relevance');
            
            // If no topics are selected, return 0
            if (topicsToConsider.length === 0) return 0;
            
            // Weighted scoring system
            const relevanceWeights = {
                'Highly Relevant': 4,
                'Moderately Relevant': 3,
                'Tangentially Relevant': 2,
                'Not Relevant': 1
            };
            
            // Sum up scores for selected topics only
            for (let topicField of topicsToConsider) {
                const relevance = paper[topicField];
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = relevance === "not_validated" ? "Not Relevant" : relevance;
                score += relevanceWeights[normalizedRelevance] || 1; // Default to 1 if unknown
            }
            
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'relevance_high':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(b) - calculateRelevanceScore(a));
                    break;
                case 'relevance_low':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(a) - calculateRelevanceScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'relevance_high': 'Relevance (Highest to Lowest)',
                'relevance_low': 'Relevance (Lowest to Highest)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
            updateNoveltyButtonText();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
            updateImpactButtonText();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for relevance checkboxes to sync between mobile and desktop
            document.getElementById('mobile-relevance-highly').addEventListener('change', () => {
                document.getElementById('desktop-relevance-highly').checked = 
                    document.getElementById('mobile-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-moderately').addEventListener('change', () => {
                document.getElementById('desktop-relevance-moderately').checked = 
                    document.getElementById('mobile-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('desktop-relevance-tangentially').checked = 
                    document.getElementById('mobile-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-not').addEventListener('change', () => {
                document.getElementById('desktop-relevance-not').checked = 
                    document.getElementById('mobile-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-highly').addEventListener('change', () => {
                document.getElementById('mobile-relevance-highly').checked = 
                    document.getElementById('desktop-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-moderately').addEventListener('change', () => {
                document.getElementById('mobile-relevance-moderately').checked = 
                    document.getElementById('desktop-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('mobile-relevance-tangentially').checked = 
                    document.getElementById('desktop-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-not').addEventListener('change', () => {
                document.getElementById('mobile-relevance-not').checked = 
                    document.getElementById('desktop-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function resetPendingScoringFilters() {
            pendingScoringFilters = { ...currentScoringFilters };
            syncPendingScoringUI();
            updateScoringButtonText();
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function resetPendingRecommendationFilters() {
            pendingRecommendationFilters = { ...currentRecommendationFilters };
            syncPendingRecommendationUI();
            updateRecommendationButtonText();
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
            updateTopicButtonText();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // RELEVANCE FILTER FUNCTIONS  
        // ============================================================================
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = pendingRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = pendingRelevanceFilters.notRelevant;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = currentRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = currentRelevanceFilters.notRelevant;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            // Read the current checkbox states from the UI (like H-Index filter does)
            const highlyRelevantChecked = document.getElementById('mobile-relevance-highly').checked;
            const moderatelyRelevantChecked = document.getElementById('mobile-relevance-moderately').checked;
            const tangentiallyRelevantChecked = document.getElementById('mobile-relevance-tangentially').checked;
            const notRelevantChecked = document.getElementById('mobile-relevance-not').checked;
            
            const selectedOptions = [];
            if (highlyRelevantChecked) selectedOptions.push("Highly Relevant");
            if (moderatelyRelevantChecked) selectedOptions.push("Moderately Relevant");
            if (tangentiallyRelevantChecked) selectedOptions.push("Tangentially Relevant");
            if (notRelevantChecked) selectedOptions.push("Not Relevant");
            
            const selectionText = selectedOptions.length === 4 ? "All Selected" : 
                                selectedOptions.length === 0 ? "None Selected" : 
                                `${selectedOptions.length} Selected`;
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            // Get values from mobile (primary source)
            const mobileHighly = document.getElementById('mobile-relevance-highly');
            const mobileModerately = document.getElementById('mobile-relevance-moderately');
            const mobileTangentially = document.getElementById('mobile-relevance-tangentially');
            const mobileNot = document.getElementById('mobile-relevance-not');
            
            // Update pending filters from mobile if available, otherwise from desktop
            pendingRelevanceFilters.highlyRelevant = mobileHighly ? mobileHighly.checked : document.getElementById('desktop-relevance-highly').checked;
            pendingRelevanceFilters.moderatelyRelevant = mobileModerately ? mobileModerately.checked : document.getElementById('desktop-relevance-moderately').checked;
            pendingRelevanceFilters.tangentiallyRelevant = mobileTangentially ? mobileTangentially.checked : document.getElementById('desktop-relevance-tangentially').checked;
            pendingRelevanceFilters.notRelevant = mobileNot ? mobileNot.checked : document.getElementById('desktop-relevance-not').checked;
        }
        
        function resetPendingRelevanceFilters() {
            pendingRelevanceFilters = { ...currentRelevanceFilters };
            syncPendingRelevanceUI();
            updateRelevanceButtonText();
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_highly', currentRelevanceFilters.highlyRelevant.toString());
            params.set('relevance_moderately', currentRelevanceFilters.moderatelyRelevant.toString());
            params.set('relevance_tangentially', currentRelevanceFilters.tangentiallyRelevant.toString());
            params.set('relevance_not', currentRelevanceFilters.notRelevant.toString());
            window.history.replaceState({}, '', `${window.location.pathname}?${params.toString()}`);
        }
        
        function loadRelevanceFiltersFromURL() {
            const highlyRelevant = getUrlParameter('relevance_highly');
            const moderatelyRelevant = getUrlParameter('relevance_moderately');
            const tangentiallyRelevant = getUrlParameter('relevance_tangentially');
            const notRelevant = getUrlParameter('relevance_not');
            
            if (highlyRelevant !== null) {
                currentRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
                pendingRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
            }
            if (moderatelyRelevant !== null) {
                currentRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
                pendingRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
            }
            if (tangentiallyRelevant !== null) {
                currentRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
                pendingRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
            }
            if (notRelevant !== null) {
                currentRelevanceFilters.notRelevant = notRelevant === 'true';
                pendingRelevanceFilters.notRelevant = notRelevant === 'true';
            }
            
            syncRelevanceUI();
        }
        
        function passesRelevanceFilter(paper) {
            // Get selected topics
            const selectedTopics = [];
            if (currentTopicFilters.rlhf) selectedTopics.push('rlhf');
            if (currentTopicFilters.weakSupervision) selectedTopics.push('weak_supervision');
            if (currentTopicFilters.diffusionReasoning) selectedTopics.push('diffusion_reasoning');
            if (currentTopicFilters.distributedTraining) selectedTopics.push('distributed_training');
            if (currentTopicFilters.datasets) selectedTopics.push('datasets');
            
            // If no topics selected, skip relevance filtering
            if (selectedTopics.length === 0) return true;
            
            // Get selected relevance levels
            const selectedRelevanceLevels = [];
            if (currentRelevanceFilters.highlyRelevant) selectedRelevanceLevels.push('Highly Relevant');
            if (currentRelevanceFilters.moderatelyRelevant) selectedRelevanceLevels.push('Moderately Relevant');
            if (currentRelevanceFilters.tangentiallyRelevant) selectedRelevanceLevels.push('Tangentially Relevant');
            if (currentRelevanceFilters.notRelevant) selectedRelevanceLevels.push('Not Relevant');
            
            // If no relevance levels selected, show 0 papers
            if (selectedRelevanceLevels.length === 0) return false;
            
            // Check each selected topic
            for (let topic of selectedTopics) {
                const relevanceField = `${topic}_relevance`;
                const paperRelevance = paper[relevanceField];
                
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = paperRelevance === "not_validated" ? "Not Relevant" : paperRelevance;
                
                // If this topic's relevance matches any selected relevance level, paper passes
                if (selectedRelevanceLevels.includes(normalizedRelevance)) {
                    return true; // At least one topic matches
                }
            }
            
            // No selected topics had matching relevance levels
            return false;
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
            
            // Update Relevance
            updateDropdownDisabledState('mobile-relevance-btn', 'mobile-relevance-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-relevance-btn', 'desktop-relevance-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            // Check if there are no papers to display
            if (filteredSortedPapers.length === 0) {
                showNoPapersMessage();
                hidePaginationSections();
                return;
            }
            
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            showPaginationSections();
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        function showNoPapersMessage() {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const noPapersHTML = '<div class="flex items-center justify-center min-h-screen"><h2 class="font-heading text-2xl text-neutral-600">No papers to show</h2></div>';
            
            if (mobileContainer) {
                mobileContainer.innerHTML = noPapersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = noPapersHTML;
            }
        }

        function hidePaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = 'none';
                }
            });
        }

        function showPaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = '';
                }
            });
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            // Find the similarity container in the currently visible layout
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            // Find the justification container in the currently visible layout
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            // Find the normalized similarity container in the currently visible layout
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // QUICK FILTER FUNCTIONS
        // ============================================================================


        function applyQuickFilter(filterType) {
            // Build URL parameters based on filter type
            const url = new URL(window.location.href);
            
            // Clear all existing parameters
            url.search = '';
            
            // Set sort to recommendation best first for all filters
            url.searchParams.set('sort', 'recommend_best');
            
            switch(filterType) {
                case 'must-read':
                    // Recommendation: Only Must Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'true');
                    url.searchParams.set('recommendation_should', 'false');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'should-read':
                    // Recommendation: Only Should Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'false');
                    url.searchParams.set('recommendation_should', 'true');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'rlhf':
                    // Topic: Only RLHF + Relevance: Exclude Not Relevant
                    url.searchParams.set('topic_rlhf', 'true');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'weak-supervision':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'true');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'diffusion-reasoning':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'true');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'distributed-training':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'true');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'datasets':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'true');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'reset':
                    // Clear all parameters, which will reset everything to defaults
                    break;
                    
                default:
                    console.warn('Unknown quick filter type:', filterType);
                    return;
            }
            
            // Navigate to new URL, which will trigger existing URL parsing logic
            window.location.href = url.toString();
        }



        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Load Relevance filters from URL
            loadRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-relevance-dropdown'), button: document.getElementById('mobile-relevance-btn') },
                { dropdown: document.getElementById('desktop-relevance-dropdown'), button: document.getElementById('desktop-relevance-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('relevance')) {
                            resetPendingRelevanceFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        } else if (dropdown.id.includes('recommendation')) {
                            resetPendingRecommendationFilters();
                        } else if (dropdown.id.includes('scoring')) {
                            resetPendingScoringFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
