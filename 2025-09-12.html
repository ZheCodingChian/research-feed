<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 12 September 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: 'rgba(107, 106, 101, 0.7)' // #6B6A65 with 70% opacity
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 12 September 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100 min-h-screen">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="mobile-quick-must-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="mobile-quick-should-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="mobile-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="mobile-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="mobile-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="mobile-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="mobile-quick-datasets" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="mobile-quick-reset" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-highly" checked>
                                            <label for="mobile-relevance-highly"></label>
                                        </div>
                                        <label for="mobile-relevance-highly" class="text-neutral-10 text-xl font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-moderately" checked>
                                            <label for="mobile-relevance-moderately"></label>
                                        </div>
                                        <label for="mobile-relevance-moderately" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-tangentially" checked>
                                            <label for="mobile-relevance-tangentially"></label>
                                        </div>
                                        <label for="mobile-relevance-tangentially" class="text-neutral-10 text-xl font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-not" checked>
                                            <label for="mobile-relevance-not"></label>
                                        </div>
                                        <label for="mobile-relevance-not" class="text-neutral-10 text-xl font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="desktop-quick-must-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="desktop-quick-should-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="desktop-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="desktop-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="desktop-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="desktop-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="desktop-quick-datasets" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="desktop-quick-reset" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-highly" checked>
                                            <label for="desktop-relevance-highly"></label>
                                        </div>
                                        <label for="desktop-relevance-highly" class="text-neutral-10 text-lg font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-moderately" checked>
                                            <label for="desktop-relevance-moderately"></label>
                                        </div>
                                        <label for="desktop-relevance-moderately" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-tangentially" checked>
                                            <label for="desktop-relevance-tangentially"></label>
                                        </div>
                                        <label for="desktop-relevance-tangentially" class="text-neutral-10 text-lg font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-not" checked>
                                            <label for="desktop-relevance-not"></label>
                                        </div>
                                        <label for="desktop-relevance-not" class="text-neutral-10 text-lg font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 12 September 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl min-h-screen">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2509.09906",
      "title": "Tackling One Health Risks: How Large Language Models are leveraged for\n  Risk Negotiation and Consensus-building",
      "authors": [
        "Alexandra Fetsch",
        "Iurii Savvateev",
        "Racem Ben Romdhane",
        "Martin Wiedmann",
        "Artemiy Dimov",
        "Maciej Durkalec",
        "Josef Teichmann",
        "Jakob Zinsstag",
        "Konstantinos Koutsoumanis",
        "Andreja Rajkovic",
        "Jason Mann",
        "Mauro Tonolla",
        "Monika Ehling-Schulz",
        "Matthias Filter",
        "Sophia Johler"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Key global challenges of our times are characterized by complex\ninterdependencies and can only be effectively addressed through an integrated,\nparticipatory effort. Conventional risk analysis frameworks often reduce\ncomplexity to ensure manageability, creating silos that hinder comprehensive\nsolutions. A fundamental shift towards holistic strategies is essential to\nenable effective negotiations between different sectors and to balance the\ncompeting interests of stakeholders. However, achieving this balance is often\nhindered by limited time, vast amounts of information, and the complexity of\nintegrating diverse perspectives. This study presents an AI-assisted\nnegotiation framework that incorporates large language models (LLMs) and\nAI-based autonomous agents into a negotiation-centered risk analysis workflow.\nThe framework enables stakeholders to simulate negotiations, systematically\nmodel dynamics, anticipate compromises, and evaluate solution impacts. By\nleveraging LLMs' semantic analysis capabilities we could mitigate information\noverload and augment decision-making process under time constraints.\nProof-of-concept implementations were conducted in two real-world scenarios:\n(i) prudent use of a biopesticide, and (ii) targeted wild animal population\ncontrol. Our work demonstrates the potential of AI-assisted negotiation to\naddress the current lack of tools for cross-sectoral engagement. Importantly,\nthe solution's open source, web based design, suits for application by a\nbroader audience with limited resources and enables users to tailor and develop\nit for their own needs.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09906v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09906v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.469,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.361,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using large language models for semantic analysis in negotiation frameworks for risk analysis, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes LLMs for semantic analysis and simulation in negotiations but does not involve diffusion models, iterative refinement for complex logical tasks, or treating a Chain-of-Thought as a single entity for multi-step correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09911",
      "title": "An Autoencoder and Vision Transformer-based Interpretability Analysis of\n  the Differences in Automated Staging of Second and Third Molars",
      "authors": [
        "Barkin Buyukcakir",
        "Jannick De Tobel",
        "Patrick Thevissen",
        "Dirk Vandermeulen",
        "Peter Claes"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The practical adoption of deep learning in high-stakes forensic applications,\nsuch as dental age estimation, is often limited by the 'black box' nature of\nthe models. This study introduces a framework designed to enhance both\nperformance and transparency in this context. We use a notable performance\ndisparity in the automated staging of mandibular second (tooth 37) and third\n(tooth 38) molars as a case study. The proposed framework, which combines a\nconvolutional autoencoder (AE) with a Vision Transformer (ViT), improves\nclassification accuracy for both teeth over a baseline ViT, increasing from\n0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond\nimproving performance, the framework provides multi-faceted diagnostic\ninsights. Analysis of the AE's latent space metrics and image reconstructions\nindicates that the remaining performance gap is data-centric, suggesting high\nintra-class morphological variability in the tooth 38 dataset is a primary\nlimiting factor. This work highlights the insufficiency of relying on a single\nmode of interpretability, such as attention maps, which can appear anatomically\nplausible yet fail to identify underlying data issues. By offering a\nmethodology that both enhances accuracy and provides evidence for why a model\nmay be uncertain, this framework serves as a more robust tool to support expert\ndecision-making in forensic age estimation.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09911v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09911v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.315,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves using an autoencoder and Vision Transformer for improving classification accuracy and interpretability in dental age estimation from images. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09915",
      "title": "The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards\n  Autonomous Science",
      "authors": [
        "Woong Shin",
        "Renan Souza",
        "Daniel Rosendo",
        "Frédéric Suter",
        "Feiyi Wang",
        "Prasanna Balaprakash",
        "Rafael Ferreira da Silva"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "Modern scientific discovery increasingly requires coordinating distributed\nfacilities and heterogeneous resources, forcing researchers to act as manual\nworkflow coordinators rather than scientists. Advances in AI leading to AI\nagents show exciting new opportunities that can accelerate scientific discovery\nby providing intelligence as a component in the ecosystem. However, it is\nunclear how this new capability would materialize and integrate in the real\nworld. To address this, we propose a conceptual framework where workflows\nevolve along two dimensions which are intelligence (from static to intelligent)\nand composition (from single to swarm) to chart an evolutionary path from\ncurrent workflow management systems to fully autonomous, distributed scientific\nlaboratories. With these trajectories in mind, we present an architectural\nblueprint that can help the community take the next steps towards harnessing\nthe opportunities in autonomous science with the potential for 100x discovery\nacceleration and transformational scientific workflows.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09915v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09915v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.384,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09918",
      "title": "WALL: A Web Application for Automated Quality Assurance using Large\n  Language Models",
      "authors": [
        "Seyed Moein Abtahi",
        "Akramul Azim"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As software projects become increasingly complex, the volume and variety of\nissues in code files have grown substantially. Addressing this challenge\nrequires efficient issue detection, resolution, and evaluation tools. This\npaper presents WALL, a web application that integrates SonarQube and large\nlanguage models (LLMs) such as GPT-3.5 Turbo and GPT-4o to automate these\ntasks. WALL comprises three modules: an issue extraction tool, code issues\nreviser, and code comparison tool. Together, they enable a seamless pipeline\nfor detecting software issues, generating automated code revisions, and\nevaluating the accuracy of revisions. Our experiments, conducted on 563 files\nwith over 7,599 issues, demonstrate WALL's effectiveness in reducing human\neffort while maintaining high-quality revisions. Results show that employing a\nhybrid approach of cost-effective and advanced LLMs can significantly lower\ncosts and improve revision rates. Future work aims to enhance WALL's\ncapabilities by integrating open-source LLMs and eliminating human\nintervention, paving the way for fully automated code quality management.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09918v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09918v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.31,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper describes a web application using pre-trained LLMs for code issue detection, revision, and evaluation, but it does not involve training machine learning models with programmatically generated labels or weak supervision techniques. There is no mention of label generation, noisy data sources, or related methodologies, making the paper unrelated to this topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09919",
      "title": "A Markovian Framing of WaveFunctionCollapse for Procedurally Generating\n  Aesthetically Complex Environments",
      "authors": [
        "Franklin Yiu",
        "Mohan Lu",
        "Nina Li",
        "Kevin Joseph",
        "Tianxu Zhang",
        "Julian Togelius",
        "Timothy Merino",
        "Sam Earle"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Procedural content generation often requires satisfying both\ndesigner-specified objectives and adjacency constraints implicitly imposed by\nthe underlying tile set. To address the challenges of jointly optimizing both\nconstraints and objectives, we reformulate WaveFunctionCollapse (WFC) as a\nMarkov Decision Process (MDP), enabling external optimization algorithms to\nfocus exclusively on objective maximization while leveraging WFC's propagation\nmechanism to enforce constraint satisfaction. We empirically compare optimizing\nthis MDP to traditional evolutionary approaches that jointly optimize global\nmetrics and local tile placement. Across multiple domains with various\ndifficulties, we find that joint optimization not only struggles as task\ncomplexity increases, but consistently underperforms relative to optimization\nover the WFC-MDP, underscoring the advantages of decoupling local constraint\nsatisfaction from global objective optimization.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09919v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09919v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.318,
      "datasets_score": 0.259,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is reformulating WaveFunctionCollapse as a Markov Decision Process for procedural content generation, focusing on optimization of objectives while enforcing adjacency constraints. It does not involve diffusion models, iterative refinement processes for logical tasks, or multi-step reasoning on a Chain-of-Thought. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09926",
      "title": "LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised\n  Learning in Open-World Scenarios",
      "authors": [
        "Zhiyuan Huang",
        "Jiahao Chen",
        "Yurou Liu",
        "Bing Su"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Long-tailed learning has garnered increasing attention due to its wide\napplicability in real-world scenarios. Among existing approaches, Long-Tailed\nSemi-Supervised Learning (LTSSL) has emerged as an effective solution by\nincorporating a large amount of unlabeled data into the imbalanced labeled\ndataset. However, most prior LTSSL methods are designed to train models from\nscratch, which often leads to issues such as overconfidence and low-quality\npseudo-labels. To address these challenges, we extend LTSSL into the foundation\nmodel fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed\nsemi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate\nthat fine-tuned foundation models can generate more reliable pseudolabels,\nthereby benefiting imbalanced learning. Furthermore, we explore a more\npractical setting by investigating semi-supervised learning under open-world\nconditions, where the unlabeled data may include out-of-distribution (OOD)\nsamples. To handle this problem, we propose LoFT-OW (LoFT under Open-World\nscenarios) to improve the discriminative ability. Experimental results on\nmultiple benchmarks demonstrate that our method achieves superior performance\ncompared to previous approaches, even when utilizing only 1\\% of the unlabeled\ndata compared with previous works.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09926v2",
      "pdf_url": "http://arxiv.org/pdf/2509.09926v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.443,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.381,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on long-tailed semi-supervised learning and parameter-efficient fine-tuning of foundation models, using pseudo-labels from unlabeled data. It does not involve human feedback, reward models, or reinforcement learning techniques for aligning AI with human preferences.",
      "weak_supervision_justification": "The paper employs pseudo-labels generated from model predictions on unlabeled data, which aligns with weak supervision by using noisy, programmatically derived labels rather than perfect hand-labeled data. However, it primarily addresses semi-supervised learning in long-tailed scenarios, not broader weak supervision techniques like heuristics or distant supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces LoFT, a framework for Long-Tailed Semi-Supervised Learning (LTSSL) that utilizes parameter-efficient fine-tuning of pre-trained transformer-based foundation models to address issues like overconfidence and low-quality pseudo-labels in imbalanced datasets. It extends this approach to open-world scenarios with LoFT-OW, which incorporates out-of-distribution (OOD) detection to filter irrelevant samples, demonstrating superior performance on benchmarks like CIFAR-LT and ImageNet127, even with only 1% of the unlabeled data used in prior works.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining parameter-efficient fine-tuning with LTSSL, applying existing techniques in a new way to enhance pseudo-label quality and handle open-world scenarios, though it does not introduce an entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in semi-supervised learning for imbalanced and open-world data within machine learning and computer vision subfields, as it improves efficiency and robustness, potentially leading to more citations and adaptations in related applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution by advancing LTSSL through innovative fine-tuning techniques and open-world extensions, making it important for researchers in imbalanced learning to be aware of, though it may not be essential for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/de62bbc287612c9b5afb2b7d8e3278c4af10bd1c",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 1,
      "average_h_index": 0.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zhiyuan Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380896235"
        },
        {
          "name": "Jiahao Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2284311358"
        },
        {
          "name": "Yurou Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2370873045"
        },
        {
          "name": "Bing Su",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2284067166"
        }
      ]
    },
    {
      "id": "2509.09935",
      "title": "SCoDA: Self-supervised Continual Domain Adaptation",
      "authors": [
        "Chirayu Agrawal",
        "Snehasis Mukherjee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a\nmodel to a target domain without access to the data of the source domain.\nPrevailing methods typically start with a source model pre-trained with full\nsupervision and distill the knowledge by aligning instance-level features.\nHowever, these approaches, relying on cosine similarity over L2-normalized\nfeature vectors, inadvertently discard crucial geometric information about the\nlatent manifold of the source model. We introduce Self-supervised Continual\nDomain Adaptation (SCoDA) to address these limitations. We make two key\ndepartures from standard practice: first, we avoid the reliance on supervised\npre-training by initializing the proposed framework with a teacher model\npre-trained entirely via self-supervision (SSL). Second, we adapt the principle\nof geometric manifold alignment to the SFDA setting. The student is trained\nwith a composite objective combining instance-level feature matching with a\nSpace Similarity Loss. To combat catastrophic forgetting, the teacher's\nparameters are updated via an Exponential Moving Average (EMA) of the student's\nparameters. Extensive experiments on benchmark datasets demonstrate that SCoDA\nsignificantly outperforms state-of-the-art SFDA methods.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09935v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09935v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.36,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper employs self-supervised learning (SSL) for pre-training, which generates labels programmatically from data (e.g., via augmentations), sharing some conceptual overlap with weak supervision. However, the main contribution focuses on Source-Free Domain Adaptation (SFDA) and geometric manifold alignment, not on weak supervision methods like noisy label generation or programmatic labeling for training. Thus, relevance is indirect.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09942",
      "title": "SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation\n  with Security-Aware Group Relative Policy Optimization",
      "authors": [
        "Lei Yu",
        "Jingyuan Zhang",
        "Xin Wang",
        "Jiajia Ma",
        "Li Yang",
        "Fengjun Zhang"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Smart contracts automate the management of high-value assets, where\nvulnerabilities can lead to catastrophic financial losses. This challenge is\namplified in Large Language Models (LLMs) by two interconnected failures: they\noperate as unauditable \"black boxes\" lacking a transparent reasoning process,\nand consequently, generate code riddled with critical security vulnerabilities.\nTo address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a\nnovel framework for secure and explainable smart contract generation. It begins\nwith Continual Pre-training (CPT) to specialize the model. We then apply Long\nChain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated\nreasoning-and-code samples to train the model to emulate human security\nanalysis. Finally, to directly mitigate vulnerabilities, we employ\nSecurity-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement\nlearning phase that refines the generation policy by optimizing a weighted\nreward signal for compilation success, security compliance, and format\ncorrectness. Evaluated against 17 baselines on a benchmark of 756 real-world\nfunctions, SmartCoder-R1 establishes a new state of the art, achieving top\nperformance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a\nSafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This\nFullRate marks a 45.79% relative improvement over the strongest baseline,\nDeepSeek-R1. Crucially, its generated reasoning also excels in human\nevaluations, achieving high-quality ratings for Functionality (82.7%), Security\n(85.3%), and Clarity (90.7%).",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09942v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09942v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.345,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for secure and explainable smart contract generation using Continual Pre-training, Long Chain-of-Thought Supervised Fine-Tuning, and Security-Aware Group Relative Policy Optimization. It involves step-by-step reasoning but does not adapt the iterative refinement process of diffusion models for logical tasks. There is no mention of diffusion-based methods, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09943",
      "title": "Segment Anything for Cell Tracking",
      "authors": [
        "Zhu Chen",
        "Mert Edgü",
        "Er Jin",
        "Johannes Stegmaier"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Tracking cells and detecting mitotic events in time-lapse microscopy image\nsequences is a crucial task in biomedical research. However, it remains highly\nchallenging due to dividing objects, low signal-tonoise ratios, indistinct\nboundaries, dense clusters, and the visually similar appearance of individual\ncells. Existing deep learning-based methods rely on manually labeled datasets\nfor training, which is both costly and time-consuming. Moreover, their\ngeneralizability to unseen datasets remains limited due to the vast diversity\nof microscopy data. To overcome these limitations, we propose a zero-shot cell\ntracking framework by integrating Segment Anything 2 (SAM2), a large foundation\nmodel designed for general image and video segmentation, into the tracking\npipeline. As a fully-unsupervised approach, our method does not depend on or\ninherit biases from any specific training dataset, allowing it to generalize\nacross diverse microscopy datasets without finetuning. Our approach achieves\ncompetitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos\nwhile eliminating the need for dataset-specific adaptation.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09943v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09943v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.235,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.353,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09946",
      "title": "Online 3D Multi-Camera Perception through Robust 2D Tracking and\n  Depth-based Late Aggregation",
      "authors": [
        "Vu-Minh Le",
        "Thao-Anh Tran",
        "Duc Huy Do",
        "Xuan Canh Do",
        "Huong Ninh",
        "Hai Tran"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision\ntask for automating large-scale surveillance. With camera calibration and depth\ninformation, the targets in the scene can be projected into 3D space, offering\nunparalleled levels of automatic perception of a 3D environment. However,\ntracking in the 3D space requires replacing all 2D tracking components from the\nground up, which may be infeasible for existing MTMC systems. In this paper, we\npresent an approach for extending any online 2D multi-camera tracking system\ninto 3D space by utilizing depth information to reconstruct a target in\npoint-cloud space, and recovering its 3D box through clustering and yaw\nrefinement following tracking. We also introduced an enhanced online data\nassociation mechanism that leverages the target's local ID consistency to\nassign global IDs across frames. The proposed framework is evaluated on the\n2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the\nleaderboard.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09946v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09946v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.331,
      "distributed_training_score": 0.355,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09952",
      "title": "Chord: Chain of Rendering Decomposition for PBR Material Estimation from\n  Generated Texture Images",
      "authors": [
        "Zhi Ying",
        "Boxiang Rong",
        "Jingyu Wang",
        "Maoyuan Xu"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Material creation and reconstruction are crucial for appearance modeling but\ntraditionally require significant time and expertise from artists. While recent\nmethods leverage visual foundation models to synthesize PBR materials from\nuser-provided inputs, they often fall short in quality, flexibility, and user\ncontrol. We propose a novel two-stage generate-and-estimate framework for PBR\nmaterial generation. In the generation stage, a fine-tuned diffusion model\nsynthesizes shaded, tileable texture images aligned with user input. In the\nestimation stage, we introduce a chained decomposition scheme that sequentially\npredicts SVBRDF channels by passing previously extracted representation as\ninput into a single-step image-conditional diffusion model. Our method is\nefficient, high quality, and enables flexible user control. We evaluate our\napproach against existing material generation and estimation methods,\ndemonstrating superior performance. Our material estimation method shows strong\nrobustness on both generated textures and in-the-wild photographs. Furthermore,\nwe highlight the flexibility of our framework across diverse applications,\nincluding text-to-material, image-to-material, structure-guided generation, and\nmaterial editing.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09952v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09952v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.291,
      "diffusion_reasoning_score": 0.48,
      "distributed_training_score": 0.297,
      "datasets_score": 0.265,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes a chained decomposition scheme using a diffusion model for sequentially predicting SVBRDF channels in material estimation, but this is focused on image generation and graphical tasks, not multi-step logical reasoning or Chain-of-Thought processes. The method employs a single-step diffusion approach for efficiency, lacking the iterative refinement for complex logical tasks as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09955",
      "title": "Adaptive Token Merging for Efficient Transformer Semantic Communication\n  at the Edge",
      "authors": [
        "Omar Erak",
        "Omar Alhussein",
        "Hatem Abou-Zeid",
        "Mehdi Bennis",
        "Sami Muhaidat"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Large-scale transformers are central to modern semantic communication, yet\ntheir high computational and communication costs hinder deployment on\nresource-constrained edge devices. This paper introduces a training-free\nframework for adaptive token merging, a novel mechanism that compresses\ntransformer representations at runtime by selectively merging semantically\nredundant tokens under per-layer similarity thresholds. Unlike prior\nfixed-ratio reduction, our approach couples merging directly to input\nredundancy, enabling data-dependent adaptation that balances efficiency and\ntask relevance without retraining. We cast the discovery of merging strategies\nas a multi-objective optimization problem and leverage Bayesian optimization to\nobtain Pareto-optimal trade-offs between accuracy, inference cost, and\ncommunication cost. On ImageNet classification, we match the accuracy of the\nunmodified transformer with 30\\% fewer floating-point operations per second and\nunder 20\\% of the original communication cost, while for visual question\nanswering our method achieves performance competitive with the full LLaVA model\nat less than one-third of the compute and one-tenth of the bandwidth. Finally,\nwe show that our adaptive merging is robust across varying channel conditions\nand provides inherent privacy benefits, substantially degrading the efficacy of\nmodel inversion attacks. Our framework provides a practical and versatile\nsolution for deploying powerful transformer models in resource-limited edge\nintelligence scenarios.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09955v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09955v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.487,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on adaptive token merging in transformers for efficient semantic communication, emphasizing runtime compression and optimization techniques like Bayesian optimization. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks such as Chain-of-Thought processing.",
      "distributed_training_justification": "The paper addresses inference efficiency on edge devices through token merging and does not discuss distributed training, parallel computing for model training, or strategies for partitioning data/computation across multiple nodes. It is training-free and centered on deployment, not training acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09958",
      "title": "Zero-Shot Referring Expression Comprehension via Visual-Language\n  True/False Verification",
      "authors": [
        "Jeffrey Liu",
        "Rongbin Hu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Referring Expression Comprehension (REC) is usually addressed with\ntask-trained grounding models. We show that a zero-shot workflow, without any\nREC-specific training, can achieve competitive or superior performance. Our\napproach reformulates REC as box-wise visual-language verification: given\nproposals from a COCO-clean generic detector (YOLO-World), a general-purpose\nVLM independently answers True/False queries for each region. This simple\nprocedure reduces cross-box interference, supports abstention and multiple\nmatches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our\nmethod not only surpasses a zero-shot GroundingDINO baseline but also exceeds\nreported results for GroundingDINO trained on REC and GroundingDINO+CRG.\nControlled studies with identical proposals confirm that verification\nsignificantly outperforms selection-based prompting, and results hold with open\nVLMs. Overall, we show that workflow design, rather than task-specific\npretraining, drives strong zero-shot REC performance.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09958v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09958v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.455,
      "distributed_training_score": 0.358,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on zero-shot Referring Expression Comprehension using off-the-shelf visual-language models and verification workflows, without any involvement of human feedback, reward models, or reinforcement learning for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs a simple verification-based workflow with visual-language models for bounding box selection, lacking any diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09960",
      "title": "Limited Reference, Reliable Generation: A Two-Component Framework for\n  Tabular Data Generation in Low-Data Regimes",
      "authors": [
        "Mingxuan Jiang",
        "Yongxin Wang",
        "Ziyue Dai",
        "Yicun Liu",
        "Hongyi Nie",
        "Sen Liu",
        "Hongfeng Chai"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Synthetic tabular data generation is increasingly essential in data\nmanagement, supporting downstream applications when real-world and high-quality\ntabular data is insufficient. Existing tabular generation approaches, such as\ngenerative adversarial networks (GANs), diffusion models, and fine-tuned Large\nLanguage Models (LLMs), typically require sufficient reference data, limiting\ntheir effectiveness in domain-specific databases with scarce records. While\nprompt-based LLMs offer flexibility without parameter tuning, they often fail\nto capture dataset-specific feature-label dependencies and generate redundant\ndata, leading to degradation in downstream task performance. To overcome these\nissues, we propose ReFine, a framework that (i) derives symbolic \"if-then\"\nrules from interpretable models and embeds them into prompts to explicitly\nguide generation toward domain-specific feature distribution, and (ii) applies\na dual-granularity filtering strategy that suppresses over-sampling patterns\nand selectively refines rare but informative samples to reduce distributional\nimbalance. Extensive experiments on various regression and classification\nbenchmarks demonstrate that ReFine consistently outperforms state-of-the-art\nmethods, achieving up to 0.44 absolute improvement in R-squared for regression\nand 10.0 percent relative improvement in F1 score for classification tasks.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09960v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09960v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.46,
      "diffusion_reasoning_score": 0.461,
      "distributed_training_score": 0.373,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a framework for synthetic tabular data generation using rule-guided prompts and filtering, without any involvement of human feedback, reward models, or reinforcement learning techniques. There is no mechanism for aligning models with human preferences.",
      "weak_supervision_justification": "The paper addresses generating synthetic data in low-data regimes, which indirectly relates to weak supervision by using limited or noisy data sources, but it does not involve programmatically generating labels for model training. Instead, it emphasizes data synthesis for downstream tasks, not the core weak supervision paradigm of label creation.",
      "diffusion_reasoning_justification": "The paper mentions diffusion models only as existing baselines for tabular data generation, but ReFine does not incorporate diffusion processes for multi-step logical reasoning or iterative refinement of a Chain-of-Thought. Its core components are rule-guided generation and filtering with LLMs.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09961",
      "title": "Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and\n  Pest Segmentation",
      "authors": [
        "Tianqi Wei",
        "Xin Yu",
        "Zhi Chen",
        "Scott Chapman",
        "Zi Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate segmentation of foliar diseases and insect damage in wheat is\ncrucial for effective crop management and disease control. However, the insect\ndamage typically occupies only a tiny fraction of annotated pixels. This\nextreme pixel-level imbalance poses a significant challenge to the segmentation\nperformance, which can result in overfitting to common classes and insufficient\nlearning of rare classes, thereby impairing overall performance. In this paper,\nwe propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to\naddress the pixel imbalance problem. Specifically, we extract rare\ninsect-damage patches from annotated training images and apply random geometric\ntransformations to simulate variations. The transformed patches are then pasted\nin appropriate regions while avoiding overlaps with lesions or existing damaged\nregions. In addition, we apply a random projection filter to the pasted\nregions, refining local features and ensuring a natural blend with the new\nbackground. Experiments show that our method substantially improves\nsegmentation performance on the insect damage class, while maintaining or even\nslightly enhancing accuracy on other categories. Our results highlight the\neffectiveness of targeted augmentation in mitigating extreme pixel imbalance,\noffering a straightforward yet effective solution for agricultural segmentation\nproblems.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09961v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09961v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.297,
      "distributed_training_score": 0.353,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09962",
      "title": "An HMM-based framework for identity-aware long-term multi-object\n  tracking from sparse and uncertain identification: use case on long-term\n  tracking in livestock",
      "authors": [
        "Anne Marthe Sophie Ngo Bibinbe",
        "Chiron Bang",
        "Patrick Gagnon",
        "Jamie Ahloy-Dallaire",
        "Eric R. Paquet"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The need for long-term multi-object tracking (MOT) is growing due to the\ndemand for analyzing individual behaviors in videos that span several minutes.\nUnfortunately, due to identity switches between objects, the tracking\nperformance of existing MOT approaches decreases over time, making them\ndifficult to apply for long-term tracking. However, in many real-world\napplications, such as in the livestock sector, it is possible to obtain\nsporadic identifications for some of the animals from sources like feeders. To\naddress the challenges of long-term MOT, we propose a new framework that\ncombines both uncertain identities and tracking using a Hidden Markov Model\n(HMM) formulation. In addition to providing real-world identities to animals,\nour HMM framework improves the F1 score of ByteTrack, a leading MOT approach\neven with re-identification, on a 10 minute pig tracking dataset with 21\nidentifications at the pen's feeding station. We also show that our approach is\nrobust to the uncertainty of identifications, with performance increasing as\nidentities are provided more frequently. The improved performance of our HMM\nframework was also validated on the MOT17 and MOT20 benchmark datasets using\nboth ByteTrack and FairMOT. The code for this new HMM framework and the new\n10-minute pig tracking video dataset are available at:\nhttps://github.com/ngobibibnbe/uncertain-identity-aware-tracking",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09962v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09962v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.329,
      "distributed_training_score": 0.293,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09969",
      "title": "Large Language Models Meet Legal Artificial Intelligence: A Survey",
      "authors": [
        "Zhitian Hou",
        "Zihan Ye",
        "Nanli Zeng",
        "Tianyong Hao",
        "Kun Zeng"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have significantly advanced the development of\nLegal Artificial Intelligence (Legal AI) in recent years, enhancing the\nefficiency and accuracy of legal tasks. To advance research and applications of\nLLM-based approaches in legal domain, this paper provides a comprehensive\nreview of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and\nalso gather 15 benchmarks and 29 datasets to evaluate different legal\ncapabilities. Additionally, we analyse the challenges and discuss future\ndirections for LLM-based approaches in the legal domain. We hope this paper\nprovides a systematic introduction for beginners and encourages future research\nin this field. Resources are available at\nhttps://github.com/ZhitianHou/LLMs4LegalAI.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09969v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09969v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.441,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.355,
      "datasets_score": 0.473,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper is a survey of Large Language Models (LLMs) in Legal Artificial Intelligence, focusing on reviews of legal LLMs, frameworks, datasets, and benchmarks. It does not mention, discuss, or involve reinforcement learning from human feedback, such as training with human-ranked data or reward models, making it unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contributions include gathering and analyzing 29 datasets and 15 benchmarks for evaluating legal capabilities, as well as providing a comprehensive review of Legal AI datasets. This directly aligns with research on creating, analyzing, benchmarking, and evaluating datasets for AI applications in the legal domain.",
      "llm_score_status": "completed",
      "summary": "This survey paper provides a comprehensive review of Large Language Models (LLMs) in Legal Artificial Intelligence (Legal AI), covering 16 legal LLMs, 47 LLM-based frameworks, 15 benchmarks, and 29 datasets, while analyzing existing datasets and presenting a detailed taxonomy of approaches. It discusses challenges faced by LLM-based methods in legal applications and proposes future directions, aiming to serve as a foundational resource for researchers by offering systematic insights and accessible resources at a provided GitHub repository.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by providing the first systematic review and taxonomy of LLMs in Legal AI, cleverly organizing existing research to address a known need for consolidation, though it does not introduce a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of Legal AI, as it offers valuable resources and analysis that can guide future research, but its influence may be limited to specialized applications rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This high-quality survey represents a strong, valuable contribution by synthesizing key developments in LLMs for Legal AI, making it essential for researchers in the field to stay informed and access its resources.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ecddffd9daeb5c2a6e083b69164e7b125246c08c",
      "total_authors": 5,
      "authors_found": 4,
      "highest_h_index": 1,
      "average_h_index": 0.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zhitian Hou",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2180477412"
        },
        {
          "name": "Zihan Ye",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Nanli Zeng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2200527167"
        },
        {
          "name": "T. Hao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2238000283"
        },
        {
          "name": "Kun Zeng",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2238004130"
        }
      ]
    },
    {
      "id": "2509.09970",
      "title": "Securing LLM-Generated Embedded Firmware through AI Agent-Driven\n  Validation and Patching",
      "authors": [
        "Seyed Moein Abtahi",
        "Akramul Azim"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) show promise in generating firmware for embedded\nsystems, but often introduce security flaws and fail to meet real-time\nperformance constraints. This paper proposes a three-phase methodology that\ncombines LLM-based firmware generation with automated security validation and\niterative refinement in a virtualized environment. Using structured prompts,\nmodels like GPT-4 generate firmware for networking and control tasks, deployed\non FreeRTOS via QEMU. These implementations are tested using fuzzing, static\nanalysis, and runtime monitoring to detect vulnerabilities such as buffer\noverflows (CWE-120), race conditions (CWE-362), and denial-of-service threats\n(CWE-400). Specialized AI agents for Threat Detection, Performance\nOptimization, and Compliance Verification collaborate to improve detection and\nremediation. Identified issues are categorized using CWE, then used to prompt\ntargeted LLM-generated patches in an iterative loop. Experiments show a 92.4\\%\nVulnerability Remediation Rate (37.3\\% improvement), 95.8\\% Threat Model\nCompliance, and 0.87 Security Coverage Index. Real-time metrics include 8.6ms\nworst-case execution time and 195{\\mu}s jitter. This process enhances firmware\nsecurity and performance while contributing an open-source dataset for future\nresearch.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09970v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09970v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.455,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.371,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a methodology for generating, validating, and patching LLM-based firmware for embedded systems using AI agents, fuzzing, and iterative refinement. It does not involve reinforcement learning from human feedback, as there is no description of training a reward model on human-ranked data or using RL to fine-tune models based on human preferences. The process relies on automated AI agents and structured prompts, without any human feedback mechanisms.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09971",
      "title": "Event Camera Guided Visual Media Restoration & 3D Reconstruction: A\n  Survey",
      "authors": [
        "Aupendu Kar",
        "Vishnu Raj",
        "Guan-Ming Su"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Event camera sensors are bio-inspired sensors which asynchronously capture\nper-pixel brightness changes and output a stream of events encoding the\npolarity, location and time of these changes. These systems are witnessing\nrapid advancements as an emerging field, driven by their low latency, reduced\npower consumption, and ultra-high capture rates. This survey explores the\nevolution of fusing event-stream captured with traditional frame-based capture,\nhighlighting how this synergy significantly benefits various video restoration\nand 3D reconstruction tasks. The paper systematically reviews major deep\nlearning contributions to image/video enhancement and restoration, focusing on\ntwo dimensions: temporal enhancement (such as frame interpolation and motion\ndeblurring) and spatial enhancement (including super-resolution, low-light and\nHDR enhancement, and artifact reduction). This paper also explores how the 3D\nreconstruction domain evolves with the advancement of event driven fusion.\nDiverse topics are covered, with in-depth discussions on recent works for\nimproving visual quality under challenging conditions. Additionally, the survey\ncompiles a comprehensive list of openly available datasets, enabling\nreproducible research and benchmarking. By consolidating recent progress and\ninsights, this survey aims to inspire further research into leveraging event\ncamera systems, especially in combination with deep learning, for advanced\nvisual media restoration and enhancement.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09971v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09971v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.316,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09972",
      "title": "Drone-Based Multispectral Imaging and Deep Learning for Timely Detection\n  of Branched Broomrape in Tomato Farms",
      "authors": [
        "Mohammadreza Narimani",
        "Alireza Pourreza",
        "Ali Moghimi",
        "Mohsen Mesgaran",
        "Parastoo Farajpoor",
        "Hamid Jafarbiglu"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "This study addresses the escalating threat of branched broomrape (Phelipanche\nramosa) to California's tomato industry, which supplies over 90 percent of U.S.\nprocessing tomatoes. The parasite's largely underground life cycle makes early\ndetection difficult, while conventional chemical controls are costly,\nenvironmentally harmful, and often ineffective. To address this, we combined\ndrone-based multispectral imagery with Long Short-Term Memory (LSTM) deep\nlearning networks, using the Synthetic Minority Over-sampling Technique (SMOTE)\nto handle class imbalance. Research was conducted on a known broomrape-infested\ntomato farm in Woodland, Yolo County, CA, across five key growth stages\ndetermined by growing degree days (GDD). Multispectral images were processed to\nisolate tomato canopy reflectance. At 897 GDD, broomrape could be detected with\n79.09 percent overall accuracy and 70.36 percent recall without integrating\nlater stages. Incorporating sequential growth stages with LSTM improved\ndetection substantially. The best-performing scenario, which integrated all\ngrowth stages with SMOTE augmentation, achieved 88.37 percent overall accuracy\nand 95.37 percent recall. These results demonstrate the strong potential of\ntemporal multispectral analysis and LSTM networks for early broomrape\ndetection. While further real-world data collection is needed for practical\ndeployment, this study shows that UAV-based multispectral sensing coupled with\ndeep learning could provide a powerful precision agriculture tool to reduce\nlosses and improve sustainability in tomato production.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09972v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09972v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.269,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.252,
      "distributed_training_score": 0.289,
      "datasets_score": 0.279,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09977",
      "title": "ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking",
      "authors": [
        "Siying Liu",
        "Zikai Wang",
        "Hanle Zheng",
        "Yifan Hu",
        "Xilin Wang",
        "Qingkai Yang",
        "Jibin Wu",
        "Hao Guo",
        "Lei Deng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "RGB-Event tracking has become a promising trend in visual object tracking to\nleverage the complementary strengths of both RGB images and dynamic spike\nevents for improved performance. However, existing artificial neural networks\n(ANNs) struggle to fully exploit the sparse and asynchronous nature of event\nstreams. Recent efforts toward hybrid architectures combining ANNs and spiking\nneural networks (SNNs) have emerged as a promising solution in RGB-Event\nperception, yet effectively fusing features across heterogeneous paradigms\nremains a challenge. In this work, we propose ISTASTrack, the first\ntransformer-based \\textbf{A}NN-\\textbf{S}NN hybrid \\textbf{Track}er equipped\nwith \\textbf{ISTA} adapters for RGB-Event tracking. The two-branch model\nemploys a vision transformer to extract spatial context from RGB inputs and a\nspiking transformer to capture spatio-temporal dynamics from event streams. To\nbridge the modality and paradigm gap between ANN and SNN features, we\nsystematically design a model-based ISTA adapter for bidirectional feature\ninteraction between the two branches, derived from sparse representation theory\nby unfolding the iterative shrinkage thresholding algorithm. Additionally, we\nincorporate a temporal downsampling attention module within the adapter to\nalign multi-step SNN features with single-step ANN features in the latent\nspace, improving temporal fusion. Experimental results on RGB-Event tracking\nbenchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that\nISTASTrack achieves state-of-the-art performance while maintaining high energy\nefficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN\ndesigns for robust visual tracking. The code is publicly available at\nhttps://github.com/lsying009/ISTASTrack.git.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09977v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09977v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.27,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.363,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09982",
      "title": "Evaluation of Black-Box XAI Approaches for Predictors of Values of\n  Boolean Formulae",
      "authors": [
        "Stav Armoni-Friedmann",
        "Hana Chockler",
        "David A. Kelly"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Evaluating explainable AI (XAI) approaches is a challenging task in general,\ndue to the subjectivity of explanations. In this paper, we focus on tabular\ndata and the specific use case of AI models predicting the values of Boolean\nfunctions. We extend the previous work in this domain by proposing a formal and\nprecise measure of importance of variables based on actual causality, and we\nevaluate state-of-the-art XAI tools against this measure. We also present a\nnovel XAI tool B-ReX, based on the existing tool ReX, and demonstrate that it\nis superior to other black-box XAI tools on a large-scale benchmark.\nSpecifically, B-ReX achieves a Jensen-Shannon divergence of 0.072 $\\pm$ 0.012\non random 10-valued Boolean formulae",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09982v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09982v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.252,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the development and evaluation of explainable AI (XAI) techniques for Boolean formulae predictors, focusing on metrics like actual causality and a new tool called B-ReX. It does not involve reinforcement learning, human feedback, reward models, or any process of aligning AI models with human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.09988",
      "title": "FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for\n  72-Hour Solar Flare Prediction",
      "authors": [
        "Yusuke Takagi",
        "Shunya Nagashima",
        "Komei Sugiura"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "astro-ph.SR (Solar and Stellar Astrophysics)"
      ],
      "abstract": "Accurate and reliable solar flare predictions are essential to mitigate\npotential impacts on critical infrastructure. However, the current performance\nof solar flare forecasting is insufficient. In this study, we address the task\nof predicting the class of the largest solar flare expected to occur within the\nnext 72 hours. Existing methods often fail to adequately address the severe\nclass imbalance across flare classes. To address this issue, we propose a solar\nflare prediction model based on multiple deep state space models. In addition,\nwe introduce the frequency & local-boundary-aware reliability loss (FLARE loss)\nto improve predictive performance and reliability under class imbalance.\nExperiments were conducted on a multi-wavelength solar image dataset covering a\nfull 11-year solar activity cycle. As a result, our method outperformed\nbaseline approaches in terms of both the Gandin-Murphy-Gerrity score and the\ntrue skill statistic, which are standard metrics in terms of the performance\nand reliability.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.09988v1",
      "pdf_url": "http://arxiv.org/pdf/2509.09988v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.313,
      "datasets_score": 0.259,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10004",
      "title": "Unsupervised Hallucination Detection by Inspecting Reasoning Processes",
      "authors": [
        "Ponhvoan Srey",
        "Xiaobao Wu",
        "Anh Tuan Luu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Unsupervised hallucination detection aims to identify hallucinated content\ngenerated by large language models (LLMs) without relying on labeled data.\nWhile unsupervised methods have gained popularity by eliminating\nlabor-intensive human annotations, they frequently rely on proxy signals\nunrelated to factual correctness. This misalignment biases detection probes\ntoward superficial or non-truth-related aspects, limiting generalizability\nacross datasets and scenarios. To overcome these limitations, we propose IRIS,\nan unsupervised hallucination detection framework, leveraging internal\nrepresentations intrinsic to factual correctness. IRIS prompts the LLM to\ncarefully verify the truthfulness of a given statement, and obtain its\ncontextualized embedding as informative features for training. Meanwhile, the\nuncertainty of each response is considered a soft pseudolabel for truthfulness.\nExperimental results demonstrate that IRIS consistently outperforms existing\nunsupervised methods. Our approach is fully unsupervised, computationally low\ncost, and works well even with few training data, making it suitable for\nreal-time detection.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10004v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10004v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.318,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's IRIS framework uses the LLM's uncertainty as soft pseudolabels to train a hallucination detection probe, aligning directly with weak supervision. This approach programmatically generates noisy or imprecise labels from model outputs, eliminating the need for hand-labeled data, which is a core principle of weak supervision.",
      "diffusion_reasoning_justification": "The paper focuses on using Chain-of-Thought prompting and uncertainty estimation for hallucination detection, but it does not involve diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction. There is no mention of adapting diffusion techniques for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces IRIS, an unsupervised framework for detecting hallucinations in large language models (LLMs) by leveraging the model's internal representations and uncertainty during reasoning processes. The methodology involves prompting the LLM to verify statement truthfulness, using its uncertainty as soft pseudolabels and contextualized embeddings to train a lightweight probe, which outperforms existing methods on datasets like True-False, HaluEval2, and HELM with improvements up to 10.2%, while being computationally efficient and requiring minimal data.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing ideas like model uncertainty and internal activations in a new way to create truth-focused pseudolabels, advancing unsupervised hallucination detection without introducing entirely novel concepts.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI and language model reliability due to its efficiency and performance gains, though its influence may remain confined to specific applications in hallucination detection.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a valuable, practical advancement in unsupervised methods for hallucination detection, making it essential for researchers focused on LLM robustness and real-time applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/720196a0eb44fa7a2f99272ab2ec69e9efcc5d93",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 27,
      "average_h_index": 14.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Ponhvoan Srey",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2310223941"
        },
        {
          "name": "Xiaobao Wu",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/13901559"
        },
        {
          "name": "A. Luu",
          "h_index": 27,
          "profile_url": "https://www.semanticscholar.org/author/1755919"
        }
      ]
    },
    {
      "id": "2509.10005",
      "title": "TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal\n  Feature Extraction and Cross-Modal Feature Fusion",
      "authors": [
        "Xiaodong Guo",
        "Tong Liu",
        "Yike Li",
        "Zi'ang Lin",
        "Zhihong Deng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "RGB-thermal (RGB-T) semantic segmentation improves the environmental\nperception of autonomous platforms in challenging conditions. Prevailing models\nemploy encoders pre-trained on RGB images to extract features from both RGB and\ninfrared inputs, and design additional modules to achieve cross-modal feature\nfusion. This results in limited thermal feature extraction and suboptimal\ncross-modal fusion, while the redundant encoders further compromises the\nmodel's real-time efficiency. To address the above issues, we propose TUNI,\nwith an RGB-T encoder consisting of multiple stacked blocks that simultaneously\nperform multi-modal feature extraction and cross-modal fusion. By leveraging\nlarge-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder\nlearns to integrate feature extraction and fusion in a unified manner. By\nslimming down the thermal branch, the encoder achieves a more compact\narchitecture. Moreover, we introduce an RGB-T local module to strengthen the\nencoder's capacity for cross-modal local feature fusion. The RGB-T local module\nemploys adaptive cosine similarity to selectively emphasize salient consistent\nand distinct local features across RGB-T modalities. Experimental results show\nthat TUNI achieves competitive performance with state-of-the-art models on FMB,\nPST900 and CART, with fewer parameters and lower computational cost. Meanwhile,\nit achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its\nreal-time capability in deployment. Codes are available at\nhttps://github.com/xiaodonguo/TUNI.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10005v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10005v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.368,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10006",
      "title": "Few-Part-Shot Font Generation",
      "authors": [
        "Masaki Akiba",
        "Shumpei Takezaki",
        "Daichi Haraguchi",
        "Seiichi Uchida"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper proposes a novel model of few-part-shot font generation, which\ndesigns an entire font based on a set of partial design elements, i.e., partial\nshapes. Unlike conventional few-shot font generation, which requires entire\ncharacter shapes for a couple of character classes, our approach only needs\npartial shapes as input. The proposed model not only improves the efficiency of\nfont creation but also provides insights into how partial design details\ninfluence the entire structure of the individual characters.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10006v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10006v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.487,
      "distributed_training_score": 0.335,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a diffusion model for generating font images from partial shapes, leveraging the model's iterative refinement process for visual content creation. However, it does not adapt diffusion for solving complex logical tasks, such as multi-step reasoning or treating a Chain-of-Thought as a holistic entity. The focus is on image generation, not logical reasoning, making it only loosely related to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10011",
      "title": "Intrinsic Dimension Estimating Autoencoder (IDEA) Using CancelOut Layer\n  and a Projected Loss",
      "authors": [
        "Antoine Oriou",
        "Philipp Krah",
        "Julian Koellermeier"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.NA (Numerical Analysis)",
        "math.NA (Numerical Analysis)"
      ],
      "abstract": "This paper introduces the Intrinsic Dimension Estimating Autoencoder (IDEA),\nwhich identifies the underlying intrinsic dimension of a wide range of datasets\nwhose samples lie on either linear or nonlinear manifolds. Beyond estimating\nthe intrinsic dimension, IDEA is also able to reconstruct the original dataset\nafter projecting it onto the corresponding latent space, which is structured\nusing re-weighted double CancelOut layers. Our key contribution is the\nintroduction of the projected reconstruction loss term, guiding the training of\nthe model by continuously assessing the reconstruction quality under the\nremoval of an additional latent dimension. We first assess the performance of\nIDEA on a series of theoretical benchmarks to validate its robustness. These\nexperiments allow us to test its reconstruction ability and compare its\nperformance with state-of-the-art intrinsic dimension estimators. The\nbenchmarks show good accuracy and high versatility of our approach.\nSubsequently, we apply our model to data generated from the numerical solution\nof a vertically resolved one-dimensional free-surface flow, following a\npointwise discretization of the vertical velocity profile in the horizontal\ndirection, vertical direction, and time. IDEA succeeds in estimating the\ndataset's intrinsic dimension and then reconstructs the original solution by\nworking directly within the projection space identified by the network.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10011v2",
      "pdf_url": "http://arxiv.org/pdf/2509.10011v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.282,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.315,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10018",
      "title": "GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation\n  Enhanced by Domain Rules and Disproof Method",
      "authors": [
        "Hailong Yang",
        "Renhuo Zhao",
        "Guanjin Wang",
        "Zhaohong Deng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "With the rapid advancement of Large Language Model (LLM), LLM-based agents\nexhibit exceptional abilities in understanding and generating natural language,\nfacilitating human-like collaboration and information transmission in LLM-based\nMulti-Agent System (MAS). High-performance LLMs are often hosted on remote\nservers in public spaces. When tasks involve privacy data, MAS cannot securely\nutilize these LLMs without implementing privacy-preserving mechanisms. To\naddress this challenge, we propose a General Anonymizing Multi-Agent system\n(GAMA), which divides the agents' workspace into private and public spaces and\nprotects privacy through the anonymizing mechanism. In the private space,\nagents handle sensitive data, while in the public space, only anonymized data\nis utilized. GAMA incorporates two key modules to mitigate semantic loss caused\nby anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and\nDisproof-based Logic Enhancement (DLE). We evaluate GAMA on two public\nquestion-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The\nresults demonstrate that GAMA has superior performance compared to the\nstate-of-the-art models. To further assess its privacy-preserving capabilities,\nwe designed two new datasets: Knowledge Privacy Preservation and Logic Privacy\nPreservation. The final results highlight GAMA's exceptional effectiveness in\nboth task processing and privacy preservation.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10018v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10018v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.336,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10021",
      "title": "Efficient and Accurate Downfacing Visual Inertial Odometry",
      "authors": [
        "Jonas Kühne",
        "Christian Vogt",
        "Michele Magno",
        "Luca Benini"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Visual Inertial Odometry (VIO) is a widely used computer vision method that\ndetermines an agent's movement through a camera and an IMU sensor. This paper\npresents an efficient and accurate VIO pipeline optimized for applications on\nmicro- and nano-UAVs. The proposed design incorporates state-of-the-art feature\ndetection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and\nquantized for emerging RISC-V-based ultra-low-power parallel systems on chips\n(SoCs). Furthermore, by employing a rigid body motion model, the pipeline\nreduces estimation errors and achieves improved accuracy in planar motion\nscenarios. The pipeline's suitability for real-time VIO is assessed on an\nultra-low-power SoC in terms of compute requirements and tracking accuracy\nafter quantization. The pipeline, including the three feature tracking methods,\nwas implemented on the SoC for real-world validation. This design bridges the\ngap between high-accuracy VIO pipelines that are traditionally run on\ncomputationally powerful systems and lightweight implementations suitable for\nmicrocontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates\nan average reduction in RMSE of up to a factor of 3.65x over the baseline\npipeline when using the ORB feature tracker. The analysis of the computational\ncomplexity of the feature trackers further shows that PX4FLOW achieves on-par\ntracking accuracy with ORB at a lower runtime for movement speeds below 24\npixels/frame.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10021v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10021v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.351,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10024",
      "title": "Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction\n  From Single Images",
      "authors": [
        "Danling Cao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recovering 3D face models from 2D in-the-wild images has gained considerable\nattention in the computer vision community due to its wide range of potential\napplications. However, the lack of ground-truth labeled datasets and the\ncomplexity of real-world environments remain significant challenges. In this\nchapter, we propose a convolutional neural network-based approach, the\nHierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face\nmodels from single in-the-wild images. Our model predicts detailed facial\ngeometry, texture, pose, and illumination parameters from a single image.\nSpecifically, we employ a pre-trained hierarchical backbone network and\nintroduce multi-level attention mechanisms at different stages of 2D face image\nfeature extraction. A semi-supervised training strategy is employed,\nincorporating 3D Morphable Model (3DMM) parameters from publicly available\ndatasets along with a differentiable renderer, enabling an end-to-end training\nprocess. Extensive experiments, including both comparative and ablation\nstudies, were conducted on two benchmark datasets, AFLW2000-3D and MICC\nFlorence, focusing on 3D face reconstruction and 3D face alignment tasks. The\neffectiveness of the proposed method was evaluated both quantitatively and\nqualitatively.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10024v3",
      "pdf_url": "http://arxiv.org/pdf/2509.10024v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.364,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10025",
      "title": "Exploring Expert Specialization through Unsupervised Training in Sparse\n  Mixture of Experts",
      "authors": [
        "Strahinja Nikolic",
        "Ilker Oguz",
        "Demetri Psaltis"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Understanding the internal organization of neural networks remains a\nfundamental challenge in deep learning interpretability. We address this\nchallenge by exploring a novel Sparse Mixture of Experts Variational\nAutoencoder (SMoE-VAE) architecture. We test our model on the QuickDraw\ndataset, comparing unsupervised expert routing against a supervised baseline\nguided by ground-truth labels. Surprisingly, we find that unsupervised routing\nconsistently achieves superior reconstruction performance. The experts learn to\nidentify meaningful sub-categorical structures that often transcend\nhuman-defined class boundaries. Through t-SNE visualizations and reconstruction\nanalysis, we investigate how MoE models uncover fundamental data structures\nthat are more aligned with the model's objective than predefined labels.\nFurthermore, our study on the impact of dataset size provides insights into the\ntrade-offs between data quantity and expert specialization, offering guidance\nfor designing efficient MoE architectures.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10025v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10025v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.419,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.391,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on unsupervised and supervised routing in a Mixture of Experts Variational Autoencoder, using ground-truth labels for the supervised baseline. It does not involve programmatically generating noisy or imprecise labels, which is central to weak supervision. Thus, there is no connection to weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper explores Mixture of Experts in Variational Autoencoders for data reconstruction and expert specialization, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It does not adapt diffusion for reasoning tasks, making it unrelated.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10026",
      "title": "LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization\n  for Real-World Multilingual VQA",
      "authors": [
        "Jing Huang",
        "Zhiya Tan",
        "Shutao Gong",
        "Fanwei Zeng",
        "Joey Tianyi Zhou",
        "Jianshu Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "As large vision language models (VLMs) advance, their capabilities in\nmultilingual visual question answering (mVQA) have significantly improved.\nChain-of-thought (CoT) reasoning has been proven to enhance interpretability\nand complex reasoning. However, most existing approaches rely primarily on\ntextual CoT and provide limited support for multilingual multimodal reasoning,\nconstraining their deployment in real-world applications. To address this gap,\nwe introduce \\textbf{LaV-CoT}, the first Language-aware Visual CoT framework\nwith Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable\nmulti-stage reasoning pipeline consisting of Text Summary with Bounding Box\n(BBox), Language Identification, Spatial Object-level Captioning, and\nStep-by-step Logical Reasoning. Following this reasoning pipeline, we design an\nautomated data curation method that generates multilingual CoT annotations\nthrough iterative generation, correction, and refinement, enabling scalable and\nhigh-quality training data. To improve reasoning and generalization, LaV-CoT\nadopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)\nwith Language-aware Group Relative Policy Optimization (GRPO), guided by\nverifiable multi-aspect rewards including language consistency, structural\naccuracy, and semantic alignment. Extensive evaluations on public datasets\nincluding MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up\nto ~9.5% accuracy improvements over open-source baselines of similar size and\neven surpasses models with 2$\\times$ larger scales by ~2.6%. Moreover, LaV-CoT\noutperforms advanced proprietary models such as GPT-4o-0513 and\nGemini-2.5-flash. We further conducted an online A/B test to validate our\nmethod on real-world data, highlighting its effectiveness for industrial\ndeployment. Our code is available at this link:\n\\href{https://github.com/HJNVR/LaV-CoT}",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10026v2",
      "pdf_url": "http://arxiv.org/pdf/2509.10026v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.496,
      "distributed_training_score": 0.375,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces LaV-CoT, which uses iterative generation, correction, and refinement for data curation and a multi-stage reasoning pipeline, but it does not involve diffusion models or adapt the iterative refinement process of diffusion for logical tasks. The iterative aspects are more aligned with general optimization techniques like GRPO and SFT, without any mention of treating Chain-of-Thought as a holistically corrected entity via diffusion-based methods. Thus, there is no clear component of diffusion-based reasoning in the paper.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10054",
      "title": "XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN\n  Rules and Multipolar Task Processing Graph",
      "authors": [
        "Hailong Yang",
        "Mingxian Gu",
        "Jianqi Wang",
        "Guanjin Wang",
        "Zhaohong Deng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) has significantly\nenhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans\nwith complex, real-world tasks. However, MAS still face challenges in effective\ntask planning when handling highly complex tasks with uncertainty, often\nresulting in misleading or incorrect outputs that hinder task execution. To\naddress this, we propose XAgents, a unified multi-agent cooperative framework\nbuilt on a multipolar task processing graph and IF-THEN rules. XAgents uses the\nmultipolar task processing graph to enable dynamic task planning and handle\ntask uncertainty. During subtask processing, it integrates domain-specific\nIF-THEN rules to constrain agent behaviors, while global rules enhance\ninter-agent collaboration. We evaluate the performance of XAgents across three\ndistinct datasets, demonstrating that it consistently surpasses\nstate-of-the-art single-agent and multi-agent approaches in both\nknowledge-typed and logic-typed question-answering tasks. The codes for XAgents\nare available at: https://github.com/AGI-FHBC/XAgents.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10054v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10054v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.36,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces XAgents, a framework for multi-agent systems using multipolar task graphs and IF-THEN rules to handle uncertainty and reduce hallucinations, but it does not involve human feedback, reward models, or reinforcement learning techniques for aligning AI with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a multipolar task processing graph inspired by biological neurons for dynamic task planning, but it does not adapt diffusion models or involve iterative refinement processes for multi-step logical reasoning as defined in diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10057",
      "title": "Reinforcement learning for spin torque oscillator tasks",
      "authors": [
        "Jakub Mojsiejuk",
        "Sławomir Ziętek",
        "Witold Skowroński"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We address the problem of automatic synchronisation of the spintronic\noscillator (STO) by means of reinforcement learning (RL). A numerical solution\nof the macrospin Landau-Lifschitz-Gilbert-Slonczewski equation is used to\nsimulate the STO and we train the two types of RL agents to synchronise with a\ntarget frequency within a fixed number of steps. We explore modifications to\nthis base task and show an improvement in both convergence and energy\nefficiency of the synchronisation that can be easily achieved in the simulated\nenvironment.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10057v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10057v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.306,
      "diffusion_reasoning_score": 0.311,
      "distributed_training_score": 0.317,
      "datasets_score": 0.226,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10058",
      "title": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings\n  for Improved Diffusion Generation",
      "authors": [
        "Sung-Lin Tsai",
        "Bo-Lun Huang",
        "Yu Ting Shen",
        "Cheng Yu Yeo",
        "Chiang Tseng",
        "Bo-Kai Ruan",
        "Wen-Sheng Lien",
        "Hong-Han Shuai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate color alignment in text-to-image (T2I) generation is critical for\napplications such as fashion, product visualization, and interior design, yet\ncurrent diffusion models struggle with nuanced and compound color terms (e.g.,\nTiffany blue, lime green, hot pink), often producing images that are misaligned\nwith human intent. Existing approaches rely on cross-attention manipulation,\nreference images, or fine-tuning but fail to systematically resolve ambiguous\ncolor descriptions. To precisely render colors under prompt ambiguity, we\npropose a training-free framework that enhances color fidelity by leveraging a\nlarge language model (LLM) to disambiguate color-related prompts and guiding\ncolor blending operations directly in the text embedding space. Our method\nfirst employs a large language model (LLM) to resolve ambiguous color terms in\nthe text prompt, and then refines the text embeddings based on the spatial\nrelationships of the resulting color terms in the CIELAB color space. Unlike\nprior methods, our approach improves color accuracy without requiring\nadditional training or external reference images. Experimental results\ndemonstrate that our framework improves color alignment without compromising\nimage quality, bridging the gap between text semantics and visual generation.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10058v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10058v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.549,
      "distributed_training_score": 0.33,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework to improve color accuracy in text-to-image diffusion models by using LLMs for prompt disambiguation and refining embeddings in color spaces. It does not adapt the diffusion process for multi-step logical reasoning or treat a chain-of-thought as an entity for holistic correction; instead, it focuses on enhancing image generation for visual fidelity, lacking any component for complex logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10059",
      "title": "Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery:\n  Benchmarking, Analysis, and Exploration",
      "authors": [
        "Yue Zhou",
        "Litong Feng",
        "Mengcheng Lan",
        "Xue Yang",
        "Qingyun Li",
        "Yiping Ke",
        "Xue Jiang",
        "Wayne Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Mathematical reasoning is critical for tasks such as precise distance and\narea computations, trajectory estimations, and spatial analysis in unmanned\naerial vehicle (UAV) based remote sensing, yet current vision-language models\n(VLMs) have not been adequately tested in this domain. To address this gap, we\nintroduce AVI-Math, the first benchmark to rigorously evaluate multimodal\nmathematical reasoning in aerial vehicle imagery, moving beyond simple counting\ntasks to include domain-specific knowledge in areas such as geometry, logic,\nand algebra. The dataset comprises 3,773 high-quality vehicle-related questions\ncaptured from UAV views, covering 6 mathematical subjects and 20 topics. The\ndata, collected at varying altitudes and from multiple UAV angles, reflects\nreal-world UAV scenarios, ensuring the diversity and complexity of the\nconstructed mathematical problems. In this paper, we benchmark 14 prominent\nVLMs through a comprehensive evaluation and demonstrate that, despite their\nsuccess on previous multimodal benchmarks, these models struggle with the\nreasoning tasks in AVI-Math. Our detailed analysis highlights significant\nlimitations in the mathematical reasoning capabilities of current VLMs and\nsuggests avenues for future research. Furthermore, we explore the use of\nChain-of-Thought prompting and fine-tuning techniques, which show promise in\naddressing the reasoning challenges in AVI-Math. Our findings not only expose\nthe limitations of VLMs in mathematical reasoning but also offer valuable\ninsights for advancing UAV-based trustworthy VLMs in real-world applications.\nThe code, and datasets will be released at\nhttps://github.com/VisionXLab/avi-math",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10059v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10059v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.451,
      "distributed_training_score": 0.327,
      "datasets_score": 0.425,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on benchmarking VLMs for mathematical reasoning in aerial imagery and explores Chain-of-Thought prompting, but it does not involve diffusion models or their iterative refinement processes for reasoning tasks. There is no mention of adapting diffusion for multi-step logical reasoning, making this topic unrelated.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction, analysis, and benchmarking of the AVI-Math dataset, which includes 3,773 questions for evaluating VLMs in mathematical reasoning. This directly aligns with creating, analyzing, and evaluating datasets for AI applications, fulfilling the topic's criteria.",
      "llm_score_status": "completed",
      "summary": "This paper introduces AVI-Math, a pioneering benchmark dataset with 3,773 high-quality questions derived from UAV imagery, designed to evaluate the multimodal mathematical reasoning capabilities of vision-language models (VLMs) in real-world aerial scenarios involving geometry, logic, algebra, and other subjects. The authors benchmark 14 prominent VLMs, revealing significant limitations in their reasoning abilities, and explore enhancements like Chain-of-Thought prompting and fine-tuning techniques to improve performance, while providing insights into future developments for trustworthy UAV-based VLMs.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark for multimodal mathematical reasoning in aerial vehicle imagery, addressing an underexplored area in VLMs and significantly advancing the state-of-the-art in evaluating AI for UAV applications.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfields of computer vision and AI for remote sensing, as it provides a specialized benchmark that could enhance the development of reliable VLMs for UAV tasks.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by highlighting VLM limitations and offering a new benchmark, making it essential for researchers in multimodal AI and UAV applications to stay informed.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/16dc70716688a33ac056ab61ce5616fd4fbef7bb",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 7,
      "average_h_index": 3.625,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Yue Zhou",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2306083274"
        },
        {
          "name": "Litong Feng",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2262404878"
        },
        {
          "name": "Mengcheng Lan",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2262217751"
        },
        {
          "name": "Xue Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380571515"
        },
        {
          "name": "Qingyun Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380467968"
        },
        {
          "name": "Yiping Ke",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2238953821"
        },
        {
          "name": "Xue Jiang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2331680587"
        },
        {
          "name": "Wayne Zhang",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2262401622"
        }
      ]
    },
    {
      "id": "2509.10063",
      "title": "TwinTac: A Wide-Range, Highly Sensitive Tactile Sensor with Real-to-Sim\n  Digital Twin Sensor Model",
      "authors": [
        "Xiyan Huang",
        "Zhe Xu",
        "Chenxi Xiao"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Robot skill acquisition processes driven by reinforcement learning often rely\non simulations to efficiently generate large-scale interaction data. However,\nthe absence of simulation models for tactile sensors has hindered the use of\ntactile sensing in such skill learning processes, limiting the development of\neffective policies driven by tactile perception. To bridge this gap, we present\nTwinTac, a system that combines the design of a physical tactile sensor with\nits digital twin model. Our hardware sensor is designed for high sensitivity\nand a wide measurement range, enabling high quality sensing data essential for\nobject interaction tasks. Building upon the hardware sensor, we develop the\ndigital twin model using a real-to-sim approach. This involves collecting\nsynchronized cross-domain data, including finite element method results and the\nphysical sensor's outputs, and then training neural networks to map simulated\ndata to real sensor responses. Through experimental evaluation, we\ncharacterized the sensitivity of the physical sensor and demonstrated the\nconsistency of the digital twin in replicating the physical sensor's output.\nFurthermore, by conducting an object classification task, we showed that\nsimulation data generated by our digital twin sensor can effectively augment\nreal-world data, leading to improved accuracy. These results highlight\nTwinTac's potential to bridge the gap in cross-domain learning tasks.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10063v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10063v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.309,
      "distributed_training_score": 0.363,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10077",
      "title": "Predictive Spike Timing Enables Distributed Shortest Path Computation in\n  Spiking Neural Networks",
      "authors": [
        "Simen Storesund",
        "Kristian Valset Aars",
        "Robin Dietrich",
        "Nicolai Waniek"
      ],
      "categories": [
        "cs.NE (Neural and Evolutionary Computing)",
        "cs.AI (Artificial Intelligence)",
        "cs.DS (Data Structures and Algorithms)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Efficient planning and sequence selection are central to intelligence, yet\ncurrent approaches remain largely incompatible with biological computation.\nClassical graph algorithms like Dijkstra's or A* require global state and\nbiologically implausible operations such as backtracing, while reinforcement\nlearning methods rely on slow gradient-based policy updates that appear\ninconsistent with rapid behavioral adaptation observed in natural systems.\n  We propose a biologically plausible algorithm for shortest-path computation\nthat operates through local spike-based message-passing with realistic\nprocessing delays. The algorithm exploits spike-timing coincidences to identify\nnodes on optimal paths: Neurons that receive inhibitory-excitatory message\npairs earlier than predicted reduce their response delays, creating a temporal\ncompression that propagates backwards from target to source. Through analytical\nproof and simulations on random spatial networks, we demonstrate that the\nalgorithm converges and discovers all shortest paths using purely timing-based\nmechanisms. By showing how short-term timing dynamics alone can compute\nshortest paths, this work provides new insights into how biological networks\nmight solve complex computational problems through purely local computation and\nrelative spike-time prediction. These findings open new directions for\nunderstanding distributed computation in biological and artificial systems,\nwith possible implications for computational neuroscience, AI, reinforcement\nlearning, and neuromorphic systems.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10077v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10077v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.317,
      "weak_supervision_score": 0.274,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.353,
      "datasets_score": 0.233,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a biologically plausible algorithm for shortest-path computation using spike-timing in neural networks, focusing on local message-passing and temporal dynamics. This does not involve diffusion-based iterative refinement processes for logical tasks or holistic correction of a chain-of-thought, as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10078",
      "title": "Established Psychometric vs. Ecologically Valid Questionnaires:\n  Rethinking Psychological Assessments in Large Language Models",
      "authors": [
        "Dongmin Choi",
        "Woojung Song",
        "Jongwook Han",
        "Eun-Ju Lee",
        "Yohan Jo"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Researchers have applied established psychometric questionnaires (e.g., BFI,\nPVQ) to measure the personality traits and values reflected in the responses of\nLarge Language Models (LLMs). However, concerns have been raised about applying\nthese human-designed questionnaires to LLMs. One such concern is their lack of\necological validity--the extent to which survey questions adequately reflect\nand resemble real-world contexts in which LLMs generate texts in response to\nuser queries. However, it remains unclear how established questionnaires and\necologically valid questionnaires differ in their outcomes, and what insights\nthese differences may provide. In this paper, we conduct a comprehensive\ncomparative analysis of the two types of questionnaires. Our analysis reveals\nthat established questionnaires (1) yield substantially different profiles of\nLLMs from ecologically valid ones, deviating from the psychological\ncharacteristics expressed in the context of user queries, (2) suffer from\ninsufficient items for stable measurement, (3) create misleading impressions\nthat LLMs possess stable constructs, and (4) yield exaggerated profiles for\npersona-prompted LLMs. Overall, our work cautions against the use of\nestablished psychological questionnaires for LLMs. Our code will be released\nupon publication.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10078v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10078v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.48,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.291,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on comparing psychometric questionnaires for assessing personality traits and values in Large Language Models (LLMs), highlighting issues like ecological validity. It does not discuss training AI models, human feedback mechanisms, reward models, or reinforcement learning techniques. Since RLHF specifically involves aligning models with human preferences through feedback-based training, the paper's main contribution has no connection to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10080",
      "title": "BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View\n  with Deformable Attention and Sparse Goal Proposals",
      "authors": [
        "Minsang Kong",
        "Myeongjun Kim",
        "Sang Gu Kang",
        "Sang Hun Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In autonomous driving, trajectory prediction is essential for ensuring safe\nand efficient navigation. To improve prediction accuracy, recent approaches\noften rely on pre-built high-definition (HD) maps or real-time local map\nconstruction modules to incorporate static environmental information. However,\npre-built HD maps are limited to specific regions and cannot adapt to transient\nchanges. In addition, local map construction modules, which recognize only\npredefined elements, may fail to capture critical scene details or introduce\nerrors that degrade prediction performance. To overcome these limitations, we\npropose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory\nprediction framework that operates directly in the bird's-eye view (BEV) space\nutilizing real-time sensor data without relying on any pre-built maps. The\nBEVTraj leverages deformable attention to efficiently extract relevant context\nfrom dense BEV features. Furthermore, we introduce a Sparse Goal Candidate\nProposal (SGCP) module, which enables full end-to-end prediction without\nrequiring any post-processing steps. Extensive experiments demonstrate that the\nBEVTraj achieves performance comparable to state-of-the-art HD map-based models\nwhile offering greater flexibility by eliminating the dependency on pre-built\nmaps. The source code is available at https://github.com/Kongminsang/bevtraj.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10080v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10080v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.298,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.321,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a framework for trajectory prediction in autonomous driving using bird's-eye view features, deformable attention, and sparse goal proposals, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning. Therefore, it does not align with diffusion-based reasoning concepts.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10093",
      "title": "Leveraging Multi-View Weak Supervision for Occlusion-Aware Multi-Human\n  Parsing",
      "authors": [
        "Laura Bragagnolo",
        "Matteo Terreran",
        "Leonardo Barcellona",
        "Stefano Ghidoni"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multi-human parsing is the task of segmenting human body parts while\nassociating each part to the person it belongs to, combining instance-level and\npart-level information for fine-grained human understanding. In this work, we\ndemonstrate that, while state-of-the-art approaches achieved notable results on\npublic datasets, they struggle considerably in segmenting people with\noverlapping bodies. From the intuition that overlapping people may appear\nseparated from a different point of view, we propose a novel training framework\nexploiting multi-view information to improve multi-human parsing models under\nocclusions. Our method integrates such knowledge during the training process,\nintroducing a novel approach based on weak supervision on human instances and a\nmulti-view consistency loss. Given the lack of suitable datasets in the\nliterature, we propose a semi-automatic annotation strategy to generate human\ninstance segmentation masks from multi-view RGB+D data and 3D human skeletons.\nThe experiments demonstrate that the approach can achieve up to a 4.20\\%\nrelative improvement on human parsing over the baseline model in occlusion\nscenarios.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10093v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10093v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.418,
      "diffusion_reasoning_score": 0.338,
      "distributed_training_score": 0.347,
      "datasets_score": 0.327,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves a training framework that explicitly uses weak supervision, as it programmatically generates human instance segmentation masks from multi-view RGB+D data and 3D skeletons, rather than relying on hand-labeled data. This aligns directly with the definition of weak supervision, where labels are derived from high-level, noisy, or imprecise sources. The framework integrates weak supervision on human instances as a core element to improve multi-human parsing under occlusions, making it a central aspect of the research.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper tackles the challenges of multi-human parsing, particularly in scenarios with occlusions, by proposing a novel training framework called MVIG-MHP that leverages multi-view information through weak supervision on human instances and a multi-view consistency loss to enhance segmentation accuracy. The authors address the lack of suitable datasets by introducing Panoptic-HuIS, a new dataset created via a semi-automatic annotation pipeline from multi-view RGB+D data and 3D skeletons, and demonstrate up to a 4.20% relative improvement in parsing performance on occlusion scenarios compared to baseline models.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining multi-view data with weak supervision and a new consistency loss to address occlusions in multi-human parsing, though it builds on existing techniques rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of computer vision for occlusion handling in human parsing, as it provides a new dataset and framework that could enhance related applications, though its influence may be limited to specific scenarios.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution with practical innovations for improving multi-human parsing under occlusions, making it valuable for researchers in computer vision to stay informed on advancements in this area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/9658c48c427273ee4ec1981106bf7fcf1c4f3e75",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 21,
      "average_h_index": 8.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Laura Bragagnolo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2317111665"
        },
        {
          "name": "Matteo Terreran",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/1992906658"
        },
        {
          "name": "Leonardo Barcellona",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/1416667270"
        },
        {
          "name": "S. Ghidoni",
          "h_index": 21,
          "profile_url": "https://www.semanticscholar.org/author/2335530"
        }
      ]
    },
    {
      "id": "2509.10096",
      "title": "HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in\n  Physical Assistance Scenario",
      "authors": [
        "Saeed Saadatnejad",
        "Reyhaneh Hosseininejad",
        "Jose Barreiros",
        "Katherine M. Tsui",
        "Alexandre Alahi"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The increasing labor shortage and aging population underline the need for\nassistive robots to support human care recipients. To enable safe and\nresponsive assistance, robots require accurate human motion prediction in\nphysical interaction scenarios. However, this remains a challenging task due to\nthe variability of assistive settings and the complexity of coupled dynamics in\nphysical interactions. In this work, we address these challenges through two\nkey contributions: (1) HHI-Assist, a dataset comprising motion capture clips of\nhuman-human interactions in assistive tasks; and (2) a conditional\nTransformer-based denoising diffusion model for predicting the poses of\ninteracting agents. Our model effectively captures the coupled dynamics between\ncaregivers and care receivers, demonstrating improvements over baselines and\nstrong generalization to unseen scenarios. By advancing interaction-aware\nmotion prediction and introducing a new dataset, our work has the potential to\nsignificantly enhance robotic assistance policies. The dataset and code are\navailable at: https://sites.google.com/view/hhi-assist/home",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10096v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10096v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.332,
      "datasets_score": 0.418,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on introducing a dataset for human motion capture and developing a denoising diffusion model for pose prediction, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's primary contribution is the creation and introduction of the HHI-Assist dataset, including its collection methodology, analysis of human-human interactions, and benchmarking for motion prediction tasks, directly aligning with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper presents the HHI-Assist dataset, comprising 908 motion capture clips of human-human interactions in physical assistance tasks such as transfers from beds and chairs, to address challenges in predicting human motion during coupled interactions. It introduces a conditional Transformer-based denoising diffusion model that predicts poses of interacting agents by incorporating both agents' previous poses, demonstrating improved accuracy and generalization over baselines, which could enhance robotic assistance policies.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new dataset specifically for human-human interactions in physical assistance and a novel interaction-aware denoising diffusion model, significantly advancing the state-of-the-art in motion prediction for coupled dynamics.",
      "impact_score": "Moderate",
      "impact_justification": "The work provides a valuable dataset and model that are likely to be cited and built upon in robotics and computer vision subfields for developing assistive technologies, though its influence may be limited to specialized applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong contribution with a new dataset and effective model for a relevant problem in human-robot interaction, making it essential for researchers in robotics and computer vision to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7fc161c324a0a141e3c4a11d4df925b6abf4fbb0",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 4,
      "average_h_index": 1.8,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Saeed Saadatnejad",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/52154910"
        },
        {
          "name": "Reyhaneh Hosseininejad",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/92108175"
        },
        {
          "name": "Jose Barreiros",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2341334329"
        },
        {
          "name": "Katherine M. Tsui",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2237783822"
        },
        {
          "name": "Alexandre Alahi",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2316426232"
        }
      ]
    },
    {
      "id": "2509.10098",
      "title": "Polarization Denoising and Demosaicking: Dataset and Baseline Method",
      "authors": [
        "Muhamad Daniel Ariff Bin Abdul Rahman",
        "Yusuke Monno",
        "Masayuki Tanaka",
        "Masatoshi Okutomi"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "A division-of-focal-plane (DoFP) polarimeter enables us to acquire images\nwith multiple polarization orientations in one shot and thus it is valuable for\nmany applications using polarimetric information. The image processing pipeline\nfor a DoFP polarimeter entails two crucial tasks: denoising and demosaicking.\nWhile polarization demosaicking for a noise-free case has increasingly been\nstudied, the research for the joint task of polarization denoising and\ndemosaicking is scarce due to the lack of a suitable evaluation dataset and a\nsolid baseline method. In this paper, we propose a novel dataset and method for\npolarization denoising and demosaicking. Our dataset contains 40 real-world\nscenes and three noise-level conditions, consisting of pairs of noisy mosaic\ninputs and noise-free full images. Our method takes a\ndenoising-then-demosaicking approach based on well-accepted signal processing\ncomponents to offer a reproducible method. Experimental results demonstrate\nthat our method exhibits higher image reconstruction performance than other\nalternative methods, offering a solid baseline.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10098v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10098v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.281,
      "weak_supervision_score": 0.297,
      "diffusion_reasoning_score": 0.31,
      "distributed_training_score": 0.29,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10099",
      "title": "Generating Energy-Efficient Code via Large-Language Models -- Where are\n  we now?",
      "authors": [
        "Radu Apsan",
        "Vincenzo Stoico",
        "Michel Albonico",
        "Rudra Dhar",
        "Karthik Vaidhyanathan",
        "Ivano Malavolta"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Context. The rise of Large Language Models (LLMs) has led to their widespread\nadoption in development pipelines. Goal. We empirically assess the energy\nefficiency of Python code generated by LLMs against human-written code and code\ndeveloped by a Green software expert. Method. We test 363 solutions to 9 coding\nproblems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting\ntechniques, and comparing them to human-developed solutions. Energy consumption\nis measured on three different hardware platforms: a server, a PC, and a\nRaspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16%\nmore energy-efficient on the server and 3% on the Raspberry Pi, while LLMs\noutperform human developers by 25% on the PC. Prompting does not consistently\nlead to energy savings, where the most energy-efficient prompts vary by\nhardware platform. The code developed by a Green software expert is\nconsistently more energy-efficient by at least 17% to 30% against all LLMs on\nall hardware platforms. Conclusions. Even though LLMs exhibit relatively good\ncode generation capabilities, no LLM-generated code was more energy-efficient\nthan that of an experienced Green software developer, suggesting that as of\ntoday there is still a great need of human expertise for developing\nenergy-efficient Python code.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10099v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10099v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.398,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates code generated by LLMs like GPT-4 and ChatGPT, which are known to incorporate RLHF in their training for alignment with human preferences. However, the paper's main contribution focuses on energy efficiency of generated code, not on RLHF techniques, systems, or their application, making it only indirectly related.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning as defined. It centers on energy efficiency of code from standard LLMs, with no components involving diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10104",
      "title": "AI Harmonics: a human-centric and harms severity-adaptive AI risk\n  assessment framework",
      "authors": [
        "Sofia Vei",
        "Paolo Giudici",
        "Pavlos Sermpezis",
        "Athena Vakali",
        "Adelaide Emma Bernardelli"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "stat.ME (Methodology)"
      ],
      "abstract": "The absolute dominance of Artificial Intelligence (AI) introduces\nunprecedented societal harms and risks. Existing AI risk assessment models\nfocus on internal compliance, often neglecting diverse stakeholder perspectives\nand real-world consequences. We propose a paradigm shift to a human-centric,\nharm-severity adaptive approach grounded in empirical incident data. We present\nAI Harmonics, which includes a novel AI harm assessment metric (AIH) that\nleverages ordinal severity data to capture relative impact without requiring\nprecise numerical estimates. AI Harmonics combines a robust, generalized\nmethodology with a data-driven, stakeholder-aware framework for exploring and\nprioritizing AI harms. Experiments on annotated incident data confirm that\npolitical and physical harms exhibit the highest concentration and thus warrant\nurgent mitigation: political harms erode public trust, while physical harms\npose serious, even life-threatening risks, underscoring the real-world\nrelevance of our approach. Finally, we demonstrate that AI Harmonics\nconsistently identifies uneven harm distributions, enabling policymakers and\norganizations to target their mitigation efforts effectively.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10104v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10104v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.538,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.327,
      "datasets_score": 0.398,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework for AI risk assessment called AI Harmonics, which uses human annotations to rank and prioritize AI harms based on ordinal severity data. It does not involve training AI models, creating reward models, or using reinforcement learning to align models with human preferences. Since the paper lacks any elements of reinforcement learning from human feedback, it is not relevant to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10105",
      "title": "VARCO-VISION-2.0 Technical Report",
      "authors": [
        "Young-rok Cha",
        "Jeongho Ju",
        "SunYoung Park",
        "Jong-Hyeon Lee",
        "Younghyun Yu",
        "Youngjune Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model\n(VLM) for Korean and English with improved capabilities compared to the\nprevious model VARCO-VISION-14B. The model supports multi-image understanding\nfor complex inputs such as documents, charts, and tables, and delivers\nlayoutaware OCR by predicting both textual content and its spatial location.\nTrained with a four-stage curriculum with memory-efficient techniques, the\nmodel achieves enhanced multimodal alignment, while preserving core language\nabilities and improving safety via preference optimization. Extensive benchmark\nevaluations demonstrate strong spatial grounding and competitive results for\nboth languages, with the 14B model achieving 8th place on the OpenCompass VLM\nleaderboard among models of comparable scale. Alongside the 14B-scale model, we\nrelease a 1.7B version optimized for on-device deployment. We believe these\nmodels advance the development of bilingual VLMs and their practical\napplications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a\nfull-scale 14B model and a lightweight 1.7B model.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10105v2",
      "pdf_url": "http://arxiv.org/pdf/2509.10105v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.363,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10114",
      "title": "A Lightweight Ensemble-Based Face Image Quality Assessment Method with\n  Correlation-Aware Loss",
      "authors": [
        "MohammadAli Hamidi",
        "Hadi Amirpour",
        "Luigi Atzori",
        "Christian Timmerer"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Face image quality assessment (FIQA) plays a critical role in face\nrecognition and verification systems, especially in uncontrolled, real-world\nenvironments. Although several methods have been proposed, general-purpose\nno-reference image quality assessment techniques often fail to capture\nface-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be\ncomputationally intensive, limiting their practical applicability. We propose a\nlightweight and efficient method for FIQA, designed for the perceptual\nevaluation of face images in the wild. Our approach integrates an ensemble of\ntwo compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,\nwith prediction-level fusion via simple averaging. To enhance alignment with\nhuman perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),\ncombining mean squared error (MSE) with a Pearson correlation regularizer. Our\nmethod achieves a strong balance between accuracy and computational cost,\nmaking it suitable for real-world deployment. Experiments on the VQualA FIQA\nbenchmark demonstrate that our model achieves a Spearman rank correlation\ncoefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient\n(PLCC) of 0.9894, remaining within competition efficiency constraints.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10114v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10114v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.316,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10122",
      "title": "Realism Control One-step Diffusion for Real-World Image Super-Resolution",
      "authors": [
        "Zongliang Wu",
        "Siming Zheng",
        "Peng-Tao Jiang",
        "Xin Yuan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Pre-trained diffusion models have shown great potential in real-world image\nsuper-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.\nWhile one-step diffusion (OSD) methods significantly improve efficiency\ncompared to traditional multi-step approaches, they still have limitations in\nbalancing fidelity and realism across diverse scenarios. Since the OSDs for SR\nare usually trained or distilled by a single timestep, they lack flexible\ncontrol mechanisms to adaptively prioritize these competing objectives, which\nare inherently manageable in multi-step methods through adjusting sampling\nsteps. To address this challenge, we propose a Realism Controlled One-step\nDiffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping\nstrategy that enables explicit control over fidelity-realism trade-offs during\nthe noise prediction phase with minimal training paradigm modifications and\noriginal training data. A degradation-aware sampling strategy is also\nintroduced to align distillation regularization with the grouping strategy and\nenhance the controlling of trade-offs. Moreover, a visual prompt injection\nmodule is used to replace conventional text prompts with degradation-aware\nvisual tokens, enhancing both restoration accuracy and semantic consistency.\nOur method achieves superior fidelity and perceptual quality while maintaining\ncomputational efficiency. Extensive experiments demonstrate that RCOD\noutperforms state-of-the-art OSD methods in both quantitative metrics and\nvisual qualities, with flexible realism control capabilities in the inference\nstage. The code will be released.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10122v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10122v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.395,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.523,
      "distributed_training_score": 0.352,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing one-step diffusion models for image super-resolution, specifically addressing fidelity and realism in visual tasks. It does not involve adapting diffusion models for complex logical tasks, chain-of-thought reasoning, or multi-step correction of reasoning paths. While diffusion models inherently use iterative refinement, this paper applies it solely to image generation and processing, lacking any component for logical reasoning as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10127",
      "title": "Population-Aligned Persona Generation for LLM-based Social Simulation",
      "authors": [
        "Zhengyu Hu",
        "Zheyuan Xiao",
        "Max Xiong",
        "Yuxuan Lei",
        "Tianfu Wang",
        "Jianxun Lian",
        "Kaize Ding",
        "Ziang Xiao",
        "Nicholas Jing Yuan",
        "Xing Xie"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent advances in large language models (LLMs) have enabled human-like\nsocial simulations at unprecedented scale and fidelity, offering new\nopportunities for computational social science. A key challenge, however, is\nthe construction of persona sets that authentically represent the diversity and\ndistribution of real-world populations. Most existing LLM-based social\nsimulation studies focus primarily on designing agentic frameworks and\nsimulation environments, often overlooking the complexities of persona\ngeneration and the potential biases introduced by unrepresentative persona\nsets. In this paper, we propose a systematic framework for synthesizing\nhigh-quality, population-aligned persona sets for LLM-driven social simulation.\nOur approach begins by leveraging LLMs to generate narrative personas from\nlong-term social media data, followed by rigorous quality assessment to filter\nout low-fidelity profiles. We then apply importance sampling to achieve global\nalignment with reference psychometric distributions, such as the Big Five\npersonality traits. To address the needs of specific simulation contexts, we\nfurther introduce a task-specific module that adapts the globally aligned\npersona set to targeted subpopulations. Extensive experiments demonstrate that\nour method significantly reduces population-level bias and enables accurate,\nflexible social simulation for a wide range of research and policy\napplications.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10127v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10127v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.441,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.366,
      "datasets_score": 0.394,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework for generating and aligning personas for LLM-based social simulations, focusing on data synthesis from social media, quality assessment, and distribution alignment techniques like importance sampling and optimal transport. It does not involve training AI models with human-ranked data, reward models, or reinforcement learning to align models with human preferences, which are essential elements of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10128",
      "title": "Efficient Learning-Based Control of a Legged Robot in Lunar Gravity",
      "authors": [
        "Philip Arm",
        "Oliver Fischer",
        "Joseph Church",
        "Adrian Fuhrer",
        "Hendrik Kolvenbach",
        "Marco Hutter"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Legged robots are promising candidates for exploring challenging areas on\nlow-gravity bodies such as the Moon, Mars, or asteroids, thanks to their\nadvanced mobility on unstructured terrain. However, as planetary robots' power\nand thermal budgets are highly restricted, these robots need energy-efficient\ncontrol approaches that easily transfer to multiple gravity environments. In\nthis work, we introduce a reinforcement learning-based control approach for\nlegged robots with gravity-scaled power-optimized reward functions. We use our\napproach to develop and validate a locomotion controller and a base pose\ncontroller in gravity environments from lunar gravity (1.62 m/s2) to a\nhypothetical super-Earth (19.62 m/s2). Our approach successfully scales across\nthese gravity levels for locomotion and base pose control with the\ngravity-scaled reward functions. The power-optimized locomotion controller\nreached a power consumption for locomotion of 23.4 W in Earth gravity on a\n15.65 kg robot at 0.4 m/s, a 23 % improvement over the baseline policy.\nAdditionally, we designed a constant-force spring offload system that allowed\nus to conduct real-world experiments on legged locomotion in lunar gravity. In\nlunar gravity, the power-optimized control policy reached 12.2 W, 36 % less\nthan a baseline controller which is not optimized for power efficiency. Our\nmethod provides a scalable approach to developing power-efficient locomotion\ncontrollers for legged robots across multiple gravity levels.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10128v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10128v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.288,
      "distributed_training_score": 0.368,
      "datasets_score": 0.243,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10134",
      "title": "Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature\n  Disalignment",
      "authors": [
        "Rini Smita Thakur",
        "Rajeev Ranjan Dwivedi",
        "Vinod K Kurmi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate segmentation of the optic disc and cup is critical for the early\ndiagnosis and management of ocular diseases such as glaucoma. However,\nsegmentation models trained on one dataset often suffer significant performance\ndegradation when applied to target data acquired under different imaging\nprotocols or conditions. To address this challenge, we propose\n\\textbf{Grad-CL}, a novel source-free domain adaptation framework that\nleverages a pre-trained source model and unlabeled target data to robustly\nadapt segmentation performance without requiring access to the original source\ndata. Grad-CL combines a gradient-guided pseudolabel refinement module with a\ncosine similarity-based contrastive learning strategy. In the first stage,\nsalient class-specific features are extracted via a gradient-based mechanism,\nenabling more accurate uncertainty quantification and robust prototype\nestimation for refining noisy pseudolabels. In the second stage, a contrastive\nloss based on cosine similarity is employed to explicitly enforce inter-class\nseparability between the gradient-informed features of the optic cup and disc.\nExtensive experiments on challenging cross-domain fundus imaging datasets\ndemonstrate that Grad-CL outperforms state-of-the-art unsupervised and\nsource-free domain adaptation methods, achieving superior segmentation accuracy\nand improved boundary delineation. Project and code are available at\nhttps://visdomlab.github.io/GCL/.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10134v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10134v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.364,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10140",
      "title": "Scalable Training for Vector-Quantized Networks with 100% Codebook\n  Utilization",
      "authors": [
        "Yifan Chang",
        "Jie Qin",
        "Limeng Qiao",
        "Xiaofeng Wang",
        "Zheng Zhu",
        "Lin Ma",
        "Xingang Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vector quantization (VQ) is a key component in discrete tokenizers for image\ngeneration, but its training is often unstable due to straight-through\nestimation bias, one-step-behind updates, and sparse codebook gradients, which\nlead to suboptimal reconstruction performance and low codebook usage. In this\nwork, we analyze these fundamental challenges and provide a simple yet\neffective solution. To maintain high codebook usage in VQ networks (VQN) during\nlearning annealing and codebook size expansion, we propose VQBridge, a robust,\nscalable, and efficient projector based on the map function method. VQBridge\noptimizes code vectors through a compress-process-recover pipeline, enabling\nstable and effective codebook training. By combining VQBridge with learning\nannealing, our VQN achieves full (100%) codebook usage across diverse codebook\nconfigurations, which we refer to as FVQ (FullVQ). Through extensive\nexperiments, we demonstrate that FVQ is effective, scalable, and generalizable:\nit attains 100% codebook usage even with a 262k-codebook, achieves\nstate-of-the-art reconstruction performance, consistently improves with larger\ncodebooks, higher vector channels, or longer training, and remains effective\nacross different VQ variants. Moreover, when integrated with LlamaGen, FVQ\nsignificantly enhances image generation performance, surpassing visual\nautoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,\nhighlighting the importance of high-quality tokenizers for strong\nautoregressive image generation.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10140v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10140v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.412,
      "datasets_score": 0.29,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is the development of VQBridge, a method to improve vector quantization training for image generation by addressing issues like straight-through estimation bias and achieving full codebook utilization. It emphasizes algorithmic enhancements for stability and scalability within VQ networks, such as using a compress-process-recover pipeline with ViT blocks. However, it does not discuss distributed training techniques, parallel computing across multiple nodes, or strategies for partitioning data, model architecture, or computation to accelerate training. The scalability mentioned refers to handling larger codebooks and training durations on a single setup, not distributed systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10147",
      "title": "Virtual Agent Economies",
      "authors": [
        "Nenad Tomasev",
        "Matija Franklin",
        "Joel Z. Leibo",
        "Julian Jacobs",
        "William A. Cunningham",
        "Iason Gabriel",
        "Simon Osindero"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rapid adoption of autonomous AI agents is giving rise to a new economic\nlayer where agents transact and coordinate at scales and speeds beyond direct\nhuman oversight. We propose the \"sandbox economy\" as a framework for analyzing\nthis emergent system, characterizing it along two key dimensions: its origins\n(emergent vs. intentional) and its degree of separateness from the established\nhuman economy (permeable vs. impermeable). Our current trajectory points toward\na spontaneous emergence of a vast and highly permeable AI agent economy,\npresenting us with opportunities for an unprecedented degree of coordination as\nwell as significant challenges, including systemic economic risk and\nexacerbated inequality. Here we discuss a number of possible design choices\nthat may lead to safely steerable AI agent markets. In particular, we consider\nauction mechanisms for fair resource allocation and preference resolution, the\ndesign of AI \"mission economies\" to coordinate around achieving collective\ngoals, and socio-technical infrastructure needed to ensure trust, safety, and\naccountability. By doing this, we argue for the proactive design of steerable\nagent markets to ensure the coming technological shift aligns with humanity's\nlong-term collective flourishing.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10147v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10147v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.337,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10151",
      "title": "BenchECG and xECG: a benchmark and baseline for ECG foundation models",
      "authors": [
        "Riccardo Lunelli",
        "Angus Nicolson",
        "Samuel Martin Pröll",
        "Sebastian Johannes Reinstadler",
        "Axel Bauer",
        "Clemens Dlaska"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited to\ndeep learning. Recently, interest has grown in developing foundation models for\nECGs - models that generalise across diverse downstream tasks. However,\nconsistent evaluation has been lacking: prior work often uses narrow task\nselections and inconsistent datasets, hindering fair comparison. Here, we\nintroduce BenchECG, a standardised benchmark comprising a comprehensive suite\nof publicly available ECG datasets and versatile tasks. We also propose xECG,\nan xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning,\nwhich achieves the best BenchECG score compared to publicly available\nstate-of-the-art models. In particular, xECG is the only publicly available\nmodel to perform strongly on all datasets and tasks. By standardising\nevaluation, BenchECG enables rigorous comparison and aims to accelerate\nprogress in ECG representation learning. xECG achieves superior performance\nover earlier approaches, defining a new baseline for future ECG foundation\nmodels.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10151v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10151v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.317,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.312,
      "distributed_training_score": 0.331,
      "datasets_score": 0.407,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of BenchECG, a standardized benchmark that compiles and evaluates a comprehensive suite of publicly available ECG datasets for machine learning tasks. This directly aligns with research on benchmarking, evaluating, and introducing datasets for AI applications, as it standardizes dataset usage and enables rigorous comparisons in ECG representation learning.",
      "llm_score_status": "completed",
      "summary": "The paper introduces BenchECG, a standardized benchmark for evaluating foundation models in electrocardiogram (ECG) analysis, which compiles a comprehensive suite of publicly available datasets and tasks to address inconsistencies in prior evaluations. It also presents xECG, an xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning, which outperforms existing state-of-the-art models across all datasets and tasks on BenchECG, thereby establishing a new baseline for ECG representation learning and aiming to accelerate progress in the field.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark for ECG foundation models, addressing a significant gap in consistent evaluation, and proposes an innovative xECG model that advances performance through novel training techniques. This represents a substantial advancement in the state-of-the-art for ECG analysis in AI.",
      "impact_score": "High",
      "impact_justification": "The work's standardized benchmark and superior model could broadly influence future research and applications in AI-driven healthcare by enabling fair comparisons and fostering innovation in ECG representation learning. This has the potential to lead to widespread adoption and improvements in medical diagnostics.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a high-quality contribution by establishing a new standard for ECG model evaluation and a strong baseline, making it essential for researchers in AI and machine learning applied to healthcare. While impactful, it may not be universally critical outside of specialized subfields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/31e87e3bb1e071b5f0c15e907ec584b0547ff7a6",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 29,
      "average_h_index": 5.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Riccardo Lunelli",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380444465"
        },
        {
          "name": "A. Nicolson",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/143993461"
        },
        {
          "name": "Samuel Martin Proll",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380444490"
        },
        {
          "name": "S. Reinstadler",
          "h_index": 29,
          "profile_url": "https://www.semanticscholar.org/author/3991654"
        },
        {
          "name": "Axel Bauer",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2320804090"
        },
        {
          "name": "Clemens Dlaska",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2306008383"
        }
      ]
    },
    {
      "id": "2509.10156",
      "title": "LayerLock: Non-collapsing Representation Learning with Progressive\n  Freezing",
      "authors": [
        "Goker Erdogan",
        "Nikhil Parthasarathy",
        "Catalin Ionescu",
        "Drew A. Hudson",
        "Alexander Lerchner",
        "Andrew Zisserman",
        "Mehdi S. M. Sajjadi",
        "Joao Carreira"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce LayerLock, a simple yet effective approach for self-supervised\nvisual representation learning, that gradually transitions from pixel to latent\nprediction through progressive layer freezing. First, we make the observation\nthat during training of video masked-autoencoding (MAE) models, ViT layers\nconverge in the order of their depth: shallower layers converge early, deeper\nlayers converge late. We then show that this observation can be exploited to\naccelerate standard MAE by progressively freezing the model according to an\nexplicit schedule, throughout training. Furthermore, this same schedule can be\nused in a simple and scalable approach to latent prediction that does not\nsuffer from \"representation collapse\". We apply our proposed approach,\nLayerLock, to large models of up to 4B parameters with results surpassing those\nof non-latent masked prediction on the 4DS perception suite.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10156v3",
      "pdf_url": "http://arxiv.org/pdf/2509.10156v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.377,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10162",
      "title": "Online Robust Planning under Model Uncertainty: A Sample-Based Approach",
      "authors": [
        "Tamir Shazman",
        "Idan Lev-Yehudi",
        "Ron Benchetit",
        "Vadim Indelman"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Online planning in Markov Decision Processes (MDPs) enables agents to make\nsequential decisions by simulating future trajectories from the current state,\nmaking it well-suited for large-scale or dynamic environments. Sample-based\nmethods such as Sparse Sampling and Monte Carlo Tree Search (MCTS) are widely\nadopted for their ability to approximate optimal actions using a generative\nmodel. However, in practical settings, the generative model is often learned\nfrom limited data, introducing approximation errors that can degrade\nperformance or lead to unsafe behaviors. To address these challenges, Robust\nMDPs (RMDPs) offer a principled framework for planning under model uncertainty,\nyet existing approaches are typically computationally intensive and not suited\nfor real-time use. In this work, we introduce Robust Sparse Sampling (RSS), the\nfirst online planning algorithm for RMDPs with finite-sample theoretical\nperformance guarantees. Unlike Sparse Sampling, which estimates the nominal\nvalue function, RSS computes a robust value function by leveraging the\nefficiency and theoretical properties of Sample Average Approximation (SAA),\nenabling tractable robust policy computation in online settings. RSS is\napplicable to infinite or continuous state spaces, and its sample and\ncomputational complexities are independent of the state space size. We provide\ntheoretical performance guarantees and empirically show that RSS outperforms\nstandard Sparse Sampling in environments with uncertain dynamics.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10162v2",
      "pdf_url": "http://arxiv.org/pdf/2509.10162v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.296,
      "datasets_score": 0.258,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10179",
      "title": "Benchmark of stylistic variation in LLM-generated texts",
      "authors": [
        "Jiří Milička",
        "Anna Marklová",
        "Václav Cvrček"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study investigates the register variation in texts written by humans and\ncomparable texts produced by large language models (LLMs). Biber's\nmultidimensional analysis (MDA) is applied to a sample of human-written texts\nand AI-created texts generated to be their counterparts to find the dimensions\nof variation in which LLMs differ most significantly and most systematically\nfrom humans. As textual material, a new LLM-generated corpus AI-Brown is used,\nwhich is comparable to BE-21 (a Brown family corpus representing contemporary\nBritish English). Since all languages except English are underrepresented in\nthe training data of frontier LLMs, similar analysis is replicated on Czech\nusing AI-Koditex corpus and Czech multidimensional model. Examined were 16\nfrontier models in various settings and prompts, with emphasis placed on the\ndifference between base models and instruction-tuned models. Based on this, a\nbenchmark is created through which models can be compared with each other and\nranked in interpretable dimensions.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10179v2",
      "pdf_url": "http://arxiv.org/pdf/2509.10179v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.32,
      "datasets_score": 0.387,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses instruction-tuned models, which are often fine-tuned using RLHF, but it focuses on analyzing stylistic variations in generated texts rather than the RLHF process itself. It does not describe, evaluate, or contribute to RLHF methods.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper is centered on stylistic analysis and benchmarking of LLM-generated texts using multidimensional analysis, with no mention of diffusion models, iterative refinement for reasoning, or multi-step logical processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10208",
      "title": "SI-FACT: Mitigating Knowledge Conflict via Self-Improving\n  Faithfulness-Aware Contrastive Tuning",
      "authors": [
        "Shengqiang Fu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models often generate unfaithful responses in knowledge\nintensive tasks due to knowledge conflict,that is,a preference for relying on\ninternal parametric knowledge rather than the provided context.To address this\nissue,we propose a novel self improving framework,Self Improving Faithfulness\nAware Contrastive Tuning.The framework uses a self instruct mechanism that\nallows the base LLM to automatically generate high quality,structured\ncontrastive learning data,including anchor samples,semantically equivalent\npositive samples,and negative samples simulating unfaithful scenarios.This\napproach significantly reduces the cost of manual\nannotation.Subsequently,contrastive learning is applied to train the\nmodel,enabling it to pull faithful responses closer and push unfaithful\nresponses farther apart in the representation space.Experiments on knowledge\nconflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT\nmodel based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%\nover the best baseline method,while significantly reducing dependence on\ninternal memory.The results indicate that SI FACT provides strong effectiveness\nand high data efficiency in enhancing the contextual faithfulness of\nLLMs,offering a practical pathway toward building more proactive and\ntrustworthy language models.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10208v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10208v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.506,
      "weak_supervision_score": 0.421,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.354,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's framework relies on automatic data generation via a self-instruct mechanism using the base LLM, with no mention of human feedback, human-ranked data, or a reward model for reinforcement learning. Thus, it does not align with RLHF.",
      "weak_supervision_justification": "The paper's self-instruct mechanism programmatically generates high-quality contrastive learning data from the base LLM, serving as a noisy or imprecise source for labels without manual annotation, which directly matches the core principles of weak supervision.",
      "diffusion_reasoning_justification": "The paper focuses on contrastive tuning and self-improving mechanisms for faithfulness in LLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper addresses the problem of knowledge conflict in large language models (LLMs), where they often prioritize internal knowledge over provided context, leading to unfaithful responses. It introduces the SI-FACT framework, which uses a self-instruct mechanism to automatically generate contrastive learning data—including anchor, positive, and negative samples—and applies contrastive tuning to enhance the model's faithfulness, resulting in a 6.2% improvement in Contextual Recall Rate on benchmarks like ECARE KRE and COSE KRE, while reducing reliance on internal memory.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining self-instruct data generation with contrastive learning to address knowledge conflict in a new way, though it builds on existing techniques rather than introducing a completely novel paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in AI and natural language processing subfields due to its practical approach for improving LLM faithfulness, but its influence may be limited to specific applications rather than widespread adoption.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to enhancing the reliability of LLMs, making it essential for researchers in computational language and AI to be aware of its methods and results.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/bdd3a03bc40f353acb39db898110c09f2fc102f3",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Shengqiang Fu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381238029"
        }
      ]
    },
    {
      "id": "2509.10210",
      "title": "Towards Fully Automated Molecular Simulations: Multi-Agent Framework for\n  Simulation Setup and Force Field Extraction",
      "authors": [
        "Marko Petković",
        "Vlado Menkovski",
        "Sofía Calero"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Automated characterization of porous materials has the potential to\naccelerate materials discovery, but it remains limited by the complexity of\nsimulation setup and force field selection. We propose a multi-agent framework\nin which LLM-based agents can autonomously understand a characterization task,\nplan appropriate simulations, assemble relevant force fields, execute them and\ninterpret their results to guide subsequent steps. As a first step toward this\nvision, we present a multi-agent system for literature-informed force field\nextraction and automated RASPA simulation setup. Initial evaluations\ndemonstrate high correctness and reproducibility, highlighting this approach's\npotential to enable fully autonomous, scalable materials characterization.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10210v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10210v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.356,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10220",
      "title": "Openness in AI and downstream governance: A global value chain approach",
      "authors": [
        "Christopher Foster"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rise of AI has been rapid, becoming a leading sector for investment and\npromising disruptive impacts across the economy. Within the critical analysis\nof the economic impacts, AI has been aligned to the critical literature on data\npower and platform capitalism - further concentrating power and value capture\namongst a small number of \"big tech\" leaders.\n  The equally rapid rise of openness in AI (here taken to be claims made by AI\nfirms about openness, \"open source\" and free provision) signals an interesting\ndevelopment. It highlights an emerging ecosystem of open AI models, datasets\nand toolchains, involving massive capital investment. It poses questions as to\nwhether open resources can support technological transfer and the ability for\ncatch-up, even in the face of AI industry power.\n  This work seeks to add conceptual clarity to these debates by conceptualising\nopenness in AI as a unique type of interfirm relation and therefore amenable to\nvalue chain analysis. This approach then allows consideration of the capitalist\ndynamics of \"outsourcing\" of foundational firms in value chains, and\nconsequently the types of governance and control that might emerge downstream\nas AI is adopted. This work, therefore, extends previous mapping of AI value\nchains to build a framework which links foundational AI with downstream value\nchains.\n  Overall, this work extends our understanding of AI as a productive sector.\nWhile the work remains critical of the power of leading AI firms, openness in\nAI may lead to potential spillovers stemming from the intense competition for\nglobal technological leadership in AI.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10220v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10220v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.34,
      "datasets_score": 0.41,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on the economic and governance aspects of openness in AI, including value chains and power dynamics, without any mention of reinforcement learning techniques, human feedback, or model alignment processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper mentions datasets as part of the open AI ecosystem and discusses their potential role in technological transfer, but it does not focus on creating, analyzing, benchmarking, or evaluating datasets; instead, it uses them to explore broader economic and governance implications.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10222",
      "title": "Compartmentalised Agentic Reasoning for Clinical NLI",
      "authors": [
        "Maël Jullien",
        "Lei Xu",
        "Marco Valentino",
        "André Freitas"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "A common assumption holds that scaling data and parameters yields\nincreasingly structured, generalisable internal representations. We interrogate\nthis assumption in clinical natural language inference (NLI) by adopting a\nbenchmark decomposed into four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction,\nand introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI\nthat separates knowledge access from principled inference. CARENLI routes each\npremise, statement pair to a family specific solver and enforces auditable\nprocedures via a planner, verifier, and refiner.\n  Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching\n98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag\nviolations with near-ceiling reliability, while refiners correct a substantial\nshare of epistemic errors. Remaining failures cluster in routing, identifying\nfamily classification as the main bottleneck. These results show that LLMs\noften retain relevant facts but default to heuristics when inference is\nunderspecified, a dissociation CARENLI makes explicit while offering a\nframework for safer, auditable reasoning.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10222v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10222v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.521,
      "distributed_training_score": 0.339,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a compartmentalised agentic reasoning framework for clinical NLI, focusing on routing and structured inference in existing LLMs, without any mention of training models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a multi-step reasoning framework with planners, verifiers, and refiners for clinical NLI, but it does not involve adapting diffusion models or their iterative refinement processes for logical tasks; instead, it uses agentic components without reference to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10241",
      "title": "On the Geometric Accuracy of Implicit and Primitive-based\n  Representations Derived from View Rendering Constraints",
      "authors": [
        "Elias De Smijter",
        "Renaud Detry",
        "Christophe De Vleeschouwer"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present the first systematic comparison of implicit and explicit Novel\nView Synthesis methods for space-based 3D object reconstruction, evaluating the\nrole of appearance embeddings. While embeddings improve photometric fidelity by\nmodeling lighting variation, we show they do not translate into meaningful\ngains in geometric accuracy - a critical requirement for space robotics\napplications. Using the SPEED+ dataset, we compare K-Planes, Gaussian\nSplatting, and Convex Splatting, and demonstrate that embeddings primarily\nreduce the number of primitives needed for explicit methods rather than\nenhancing geometric fidelity. Moreover, convex splatting achieves more compact\nand clutter-free representations than Gaussian splatting, offering advantages\nfor safety-critical applications such as interaction and collision avoidance.\nOur findings clarify the limits of appearance embeddings for geometry-centric\ntasks and highlight trade-offs between reconstruction quality and\nrepresentation efficiency in space scenarios.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10241v2",
      "pdf_url": "http://arxiv.org/pdf/2509.10241v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.307,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.324,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10249",
      "title": "Investigating Language Model Capabilities to Represent and Process\n  Formal Knowledge: A Preliminary Study to Assist Ontology Engineering",
      "authors": [
        "Hanna Abi Akl"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in Language Models (LMs) have failed to mask their\nshortcomings particularly in the domain of reasoning. This limitation impacts\nseveral tasks, most notably those involving ontology engineering. As part of a\nPhD research, we investigate the consequences of incorporating formal methods\non the performance of Small Language Models (SLMs) on reasoning tasks.\nSpecifically, we aim to orient our work toward using SLMs to bootstrap ontology\nconstruction and set up a series of preliminary experiments to determine the\nimpact of expressing logical problems with different grammars on the\nperformance of SLMs on a predefined reasoning task. Our findings show that it\nis possible to substitute Natural Language (NL) with a more compact logical\nlanguage while maintaining a strong performance on reasoning tasks and hope to\nuse these results to further refine the role of SLMs in ontology engineering.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10249v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10249v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.267,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluating Small Language Models (SLMs) for reasoning tasks in ontology engineering, specifically testing different grammars for logical problems. It does not mention or involve diffusion models, iterative refinement processes, or any multi-step logical reasoning adapted from diffusion techniques. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10250",
      "title": "GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented\n  Training for AI-Generated Image Detection",
      "authors": [
        "Haozhen Yan",
        "Yan Hong",
        "Suning Lang",
        "Jiahui Zhan",
        "Yikun Ji",
        "Yujie Gao",
        "Jun Lan",
        "Huijia Zhu",
        "Weiqiang Wang",
        "Jianfu Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "With generative models becoming increasingly sophisticated and diverse,\ndetecting AI-generated images has become increasingly challenging. While\nexisting AI-genereted Image detectors achieve promising performance on\nin-distribution generated images, their generalization to unseen generative\nmodels remains limited. This limitation is largely attributed to their reliance\non generation-specific artifacts, such as stylistic priors and compression\npatterns. To address these limitations, we propose GAMMA, a novel training\nframework designed to reduce domain bias and enhance semantic alignment. GAMMA\nintroduces diverse manipulation strategies, such as inpainting-based\nmanipulation and semantics-preserving perturbations, to ensure consistency\nbetween manipulated and authentic content. We employ multi-task supervision\nwith dual segmentation heads and a classification head, enabling pixel-level\nsource attribution across diverse generative domains. In addition, a reverse\ncross-attention mechanism is introduced to allow the segmentation heads to\nguide and correct biased representations in the classification branch. Our\nmethod achieves state-of-the-art generalization performance on the GenImage\nbenchmark, imporving accuracy by 5.8%, but also maintains strong robustness on\nnewly released generative model such as GPT-4o.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10250v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10250v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.374,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a training framework for detecting AI-generated images, focusing on generalization through manipulation strategies, multi-task supervision, and reverse cross-attention. It does not involve adapting diffusion models for iterative refinement in solving complex logical tasks, such as treating a Chain-of-Thought as a single entity for holistic correction. The paper is centered on image forensics, not multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10257",
      "title": "Robustness and Diagnostic Performance of Super-Resolution Fetal Brain\n  MRI",
      "authors": [
        "Ema Masterl",
        "Tina Vipotnik Vesnaver",
        "Žiga Špiclin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce\nmotion artifacts caused by fetal movement. However, these stacks are typically\nlow resolution, may suffer from motion corruption, and do not adequately\ncapture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to\naddress these limitations by combining slice-to-volume registration and\nsuper-resolution techniques to generate high-resolution (HR) 3D volumes. While\nseveral SRR methods have been proposed, their comparative performance -\nparticularly in pathological cases - and their influence on downstream\nvolumetric analysis and diagnostic tasks remain underexplored. In this study,\nwe applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to\n140 fetal brain MRI scans, including both healthy controls (HC) and\npathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was\nsegmented using the BoUNTi algorithm to extract volumes of nine principal brain\nstructures. We evaluated visual quality, SRR success rates, volumetric\nmeasurement agreement, and diagnostic classification performance. NeSVoR\ndemonstrated the highest and most consistent reconstruction success rate (>90%)\nacross both HC and PC groups. Although significant differences in volumetric\nestimates were observed between SRR methods, classification performance for VM\nwas not affected by the choice of SRR method. These findings highlight NeSVoR's\nrobustness and the resilience of diagnostic performance despite SRR-induced\nvolumetric variability.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10257v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10257v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.308,
      "distributed_training_score": 0.297,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10259",
      "title": "Mask Consistency Regularization in Object Removal",
      "authors": [
        "Hua Yuan",
        "Jin Yuan",
        "Yicheng Jiang",
        "Yao Zhang",
        "Xin Geng",
        "Yong Rui"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Object removal, a challenging task within image inpainting, involves\nseamlessly filling the removed region with content that matches the surrounding\ncontext. Despite advancements in diffusion models, current methods still face\ntwo critical challenges. The first is mask hallucination, where the model\ngenerates irrelevant or spurious content inside the masked region, and the\nsecond is mask-shape bias, where the model fills the masked area with an object\nthat mimics the mask's shape rather than surrounding content. To address these\nissues, we propose Mask Consistency Regularization (MCR), a novel training\nstrategy designed specifically for object removal tasks. During training, our\napproach introduces two mask perturbations: dilation and reshape, enforcing\nconsistency between the outputs of these perturbed branches and the original\nmask. The dilated masks help align the model's output with the surrounding\ncontent, while reshaped masks encourage the model to break the mask-shape bias.\nThis combination of strategies enables MCR to produce more robust and\ncontextually coherent inpainting results. Our experiments demonstrate that MCR\nsignificantly reduces hallucinations and mask-shape bias, leading to improved\nperformance in object removal.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10259v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10259v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.314,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a training strategy for diffusion models in image inpainting to address mask hallucination and mask-shape bias, focusing on generating coherent visual content. It does not adapt the iterative refinement process of diffusion models for multi-step logical reasoning or solving complex logical tasks, such as treating a Chain-of-Thought as a single entity. Therefore, there is no clear component for diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10260",
      "title": "MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained\n  Artifacts Assessment in Text-to-Image Generation",
      "authors": [
        "Jia Wang",
        "Jie Hu",
        "Xiaoqi Ma",
        "Hanghang Ma",
        "Yanbing Zeng",
        "Xiaoming Wei"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-image (T2I) generation has achieved remarkable progress in\ninstruction following and aesthetics. However, a persistent challenge is the\nprevalence of physical artifacts, such as anatomical and structural flaws,\nwhich severely degrade perceptual quality and limit application. Given the\ndiversity and complexity of these artifacts, a systematic and fine-grained\nevaluation framework is required, which is lacking in current benchmarks. To\nfill this gap, we introduce MagicMirror, a comprehensive framework for\nartifacts assessment. We first establish a detailed taxonomy of generated image\nartifacts. Guided by this taxonomy, we manually annotate MagicData340K, the\nfirst human-annotated large-scale dataset of 340K generated images with\nfine-grained artifact labels. Building on this dataset, we train MagicAssessor,\na Vision-Language Model (VLM) that provides detailed assessments and\ncorresponding labels. To overcome challenges like class imbalance and reward\nhacking, we design a novel data sampling strategy and a multi-level reward\nsystem for Group Relative Policy Optimization (GRPO). Finally, we leverage\nMagicAssessor to construct MagicBench, an automated benchmark for evaluating\nthe image artifacts of current T2I models. Our evaluation with MagicBench\nreveals that despite their widespread adoption, even top-tier models like\nGPT-image-1 are consistently plagued by significant artifacts, highlighting\nartifact reduction as a critical frontier for future T2I development. Project\npage: https://wj-inf.github.io/MagicMirror-page/.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10260v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10260v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.369,
      "datasets_score": 0.444,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on artifact assessment in text-to-image generation using a Vision-Language Model (VLM), not on adapting diffusion processes for multi-step logical reasoning. While it mentions diffusion-based T2I models in the introduction, it does not involve treating a Chain-of-Thought as a single entity for iterative correction in logical tasks, as required by the topic definition.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contributions include creating and annotating MagicData340K, a large-scale dataset for artifact evaluation in T2I generation, along with discussing curation methodologies, human annotation, and using it to build MagicBench for benchmarking. This directly aligns with research on dataset creation, analysis, and evaluation for AI applications.",
      "llm_score_status": "completed",
      "summary": "MagicMirror presents a comprehensive framework for evaluating fine-grained artifacts in text-to-image (T2I) generation, addressing a key gap in current benchmarks by introducing a detailed taxonomy of artifacts, a large-scale human-annotated dataset called MagicData340K with 340K images, a specialized Vision-Language Model named MagicAssessor trained with innovative strategies like Group Relative Policy Optimization to handle data imbalances, and an automated benchmark called MagicBench. The paper's methodology involves collecting and annotating diverse generated images, training the model to provide accurate assessments, and using the benchmark to reveal that even advanced T2I models, such as GPT-image-1, exhibit significant artifacts, underscoring the need for focused efforts in artifact reduction to enhance perceptual quality and real-world applicability.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel large-scale dataset, a specialized VLM for fine-grained artifact assessment, and an automated benchmark, significantly advancing the state-of-the-art in T2I evaluation by addressing previously overlooked aspects of artifact diversity and complexity.",
      "impact_score": "High",
      "impact_justification": "This work has the potential to broadly influence T2I research and development by providing robust tools for artifact detection and evaluation, which could lead to improvements in model reliability and wider commercial applications in fields like graphic design and photorealistic generation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a high-quality contribution with practical tools and insights for T2I evaluation, making it essential for researchers in computer vision and AI generation to understand and build upon, though it may not be critical for those outside this specific subfield.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3c21d2b0de9262c857f8f6c5484631c1edad518d",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 5,
      "average_h_index": 1.8333333333333333,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jia Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356483041"
        },
        {
          "name": "Jie Hu",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2157421431"
        },
        {
          "name": "Xiaoqi Ma",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2317277028"
        },
        {
          "name": "Hanghang Ma",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2318254445"
        },
        {
          "name": "Yanbing Zeng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380465756"
        },
        {
          "name": "Xiaoming Wei",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2115493489"
        }
      ]
    },
    {
      "id": "2509.10266",
      "title": "SignClip: Leveraging Mouthing Cues for Sign Language Translation by\n  Multimodal Contrastive Fusion",
      "authors": [
        "Wenfang Wu",
        "Tingting Yuan",
        "Yupeng Li",
        "Daling Wang",
        "Xiaoming Fu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Sign language translation (SLT) aims to translate natural language from sign\nlanguage videos, serving as a vital bridge for inclusive communication. While\nrecent advances leverage powerful visual backbones and large language models,\nmost approaches mainly focus on manual signals (hand gestures) and tend to\noverlook non-manual cues like mouthing. In fact, mouthing conveys essential\nlinguistic information in sign languages and plays a crucial role in\ndisambiguating visually similar signs. In this paper, we propose SignClip, a\nnovel framework to improve the accuracy of sign language translation. It fuses\nmanual and non-manual cues, specifically spatial gesture and lip movement\nfeatures. Besides, SignClip introduces a hierarchical contrastive learning\nframework with multi-level alignment objectives, ensuring semantic consistency\nacross sign-lip and visual-text modalities. Extensive experiments on two\nbenchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our\napproach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip\nsurpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from\n24.32 to 24.71, and ROUGE from 46.57 to 48.38.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10266v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10266v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.326,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a framework for sign language translation using multimodal contrastive fusion to integrate manual and non-manual cues, with no mention of diffusion models, iterative refinement processes, or applications to complex logical tasks. It focuses solely on visual-language alignment for translation, which does not align with the topic's emphasis on adapting diffusion for Chain-of-Thought reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10278",
      "title": "Detecting Text Manipulation in Images using Vision Language Models",
      "authors": [
        "Vidit Vidit",
        "Pavel Korshunov",
        "Amir Mohammadi",
        "Christophe Ecabert",
        "Ketan Kotwal",
        "Sébastien Marcel"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent works have shown the effectiveness of Large Vision Language Models\n(VLMs or LVLMs) in image manipulation detection. However, text manipulation\ndetection is largely missing in these studies. We bridge this knowledge gap by\nanalyzing closed- and open-source VLMs on different text manipulation datasets.\nOur results suggest that open-source models are getting closer, but still\nbehind closed-source ones like GPT- 4o. Additionally, we benchmark image\nmanipulation detection-specific VLMs for text manipulation detection and show\nthat they suffer from the generalization problem. We benchmark VLMs for\nmanipulations done on in-the-wild scene texts and on fantasy ID cards, where\nthe latter mimic a challenging real-world misuse.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10278v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10278v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.431,
      "distributed_training_score": 0.291,
      "datasets_score": 0.361,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using Vision Language Models (VLMs) for detecting text manipulation in images, including benchmarking open and closed-source models and evaluating generalization on datasets. It does not involve diffusion models, iterative refinement processes for logical reasoning, or any adaptation of diffusion for multi-step Chain-of-Thought tasks. Therefore, there is no connection to the topic of Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10282",
      "title": "MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly\n  Detection",
      "authors": [
        "Gang Li",
        "Tianjiao Chen",
        "Mingle Zhou",
        "Min Li",
        "Delong Han",
        "Jin Wan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects\nwithout relying on labeled training data, making it especially valuable in\nscenarios constrained by data scarcity, privacy, or high annotation cost.\nHowever, most existing methods focus exclusively on point clouds, neglecting\nthe rich semantic cues available from complementary modalities such as RGB\nimages and texts priors. This paper introduces MCL-AD, a novel framework that\nleverages multimodal collaboration learning across point clouds, RGB images,\nand texts semantics to achieve superior zero-shot 3D anomaly detection.\nSpecifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that\nenhances the intra-modal representation capability and inter-modal\ncollaborative learning by introducing an object-agnostic decoupled text prompt\nand a multimodal contrastive loss. In addition, a collaborative modulation\nmechanism (CMM) is proposed to fully leverage the complementary representations\nof point clouds and RGB images by jointly modulating the RGB image-guided and\npoint cloud-guided branches. Extensive experiments demonstrate that the\nproposed MCL-AD framework achieves state-of-the-art performance in ZS-3D\nanomaly detection.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10282v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10282v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.366,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10289",
      "title": "We Need a New Ethics for a World of AI Agents",
      "authors": [
        "Iason Gabriel",
        "Geoff Keeling",
        "Arianna Manzini",
        "James Evans"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The deployment of capable AI agents raises fresh questions about safety,\nhuman-machine relationships and social coordination. We argue for greater\nengagement by scientists, scholars, engineers and policymakers with the\nimplications of a world increasingly populated by AI agents. We explore key\nchallenges that must be addressed to ensure that interactions between humans\nand agents, and among agents themselves, remain broadly beneficial.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10289v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10289v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.295,
      "diffusion_reasoning_score": 0.271,
      "distributed_training_score": 0.271,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on ethical implications, safety, and social coordination of AI agents, advocating for broader engagement by various stakeholders. It does not discuss or involve reinforcement learning techniques, including the use of human feedback for training AI models, making it unrelated to this specific topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10297",
      "title": "The Morality of Probability: How Implicit Moral Biases in LLMs May Shape\n  the Future of Human-AI Symbiosis",
      "authors": [
        "Eoin O'Doherty",
        "Nicole Weinrauch",
        "Andrew Talone",
        "Uri Klempner",
        "Xiaoyuan Yi",
        "Xing Xie",
        "Yi Zeng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Artificial intelligence (AI) is advancing at a pace that raises urgent\nquestions about how to align machine decision-making with human moral values.\nThis working paper investigates how leading AI systems prioritize moral\noutcomes and what this reveals about the prospects for human-AI symbiosis. We\naddress two central questions: (1) What moral values do state-of-the-art large\nlanguage models (LLMs) implicitly favour when confronted with dilemmas? (2) How\ndo differences in model architecture, cultural origin, and explainability\naffect these moral preferences? To explore these questions, we conduct a\nquantitative experiment with six LLMs, ranking and scoring outcomes across 18\ndilemmas representing five moral frameworks. Our findings uncover strikingly\nconsistent value biases. Across all models, Care and Virtue values outcomes\nwere rated most moral, while libertarian choices were consistently penalized.\nReasoning-enabled models exhibited greater sensitivity to context and provided\nricher explanations, whereas non-reasoning models produced more uniform but\nopaque judgments. This research makes three contributions: (i) Empirically, it\ndelivers a large-scale comparison of moral reasoning across culturally distinct\nLLMs; (ii) Theoretically, it links probabilistic model behaviour with\nunderlying value encodings; (iii) Practically, it highlights the need for\nexplainability and cultural awareness as critical design principles to guide AI\ntoward a transparent, aligned, and symbiotic future.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10297v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10297v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.553,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.478,
      "distributed_training_score": 0.351,
      "datasets_score": 0.373,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses AI alignment methods, including learning from human feedback, as referenced in \"ouyang2022training\" which is a key RLHF paper. It also mentions specialized training via reinforcement learning for reasoning models. However, the main contribution focuses on evaluating moral biases in LLMs rather than directly implementing or analyzing RLHF, making it moderately relevant.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes reasoning models using chain-of-thought prompting and reinforcement learning for multi-step inference, but it does not mention or adapt diffusion-based processes for logical tasks. There is no evidence of iterative refinement akin to diffusion models, so it is not relevant.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This working paper investigates the implicit moral biases in leading large language models (LLMs) by analyzing their responses to 18 moral dilemmas across five frameworks, aiming to understand what values these models favor and how factors like architecture, cultural origin, and explainability influence their decisions. Through a quantitative experiment with six LLMs, the study reveals consistent biases toward Care and Virtue values while penalizing libertarian choices, with reasoning-enabled models demonstrating greater contextual sensitivity and richer explanations, ultimately emphasizing the need for value alignment and transparency to foster human-AI symbiosis.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by conducting a large-scale empirical comparison of moral biases in LLMs across cultural and architectural differences, combining existing ideas on AI alignment in a new way to advance understanding. However, it does not introduce a entirely new problem, as moral biases in AI have been previously discussed.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI ethics and explainability, as it provides practical insights into designing aligned AI systems for human-AI symbiosis. While it has potential applications, its influence may be confined to specific areas rather than broadly transformative.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers high-quality insights into moral biases in LLMs and their implications for AI development, making it a valuable contribution for researchers in AI ethics and alignment. It is significant but not essential for all readers, as its focus is somewhat specialized.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/77df7f8d34fc9f66c914c1f3558df0aeef83ba86",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 9,
      "average_h_index": 1.2857142857142858,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Eoin O’Doherty",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2355528382"
        },
        {
          "name": "Nicole Weinrauch",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380441844"
        },
        {
          "name": "Andrew Talone",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380440517"
        },
        {
          "name": "Uri Klempner",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380440932"
        },
        {
          "name": "Xiaoyuan Yi",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2258961742"
        },
        {
          "name": "Xing Xie",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380466839"
        },
        {
          "name": "Yi Zeng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380466679"
        }
      ]
    },
    {
      "id": "2509.10298",
      "title": "Adversarial robustness through Lipschitz-Guided Stochastic Depth in\n  Neural Networks",
      "authors": [
        "Laith Nayal",
        "Mahmoud Mousatat",
        "Bader Rasheed"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep neural networks and Vision Transformers achieve state-of-the-art\nperformance in computer vision but are highly vulnerable to adversarial\nperturbations. Standard defenses often incur high computational cost or lack\nformal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)\nmethod, where drop probabilities increase with depth to control the effective\nLipschitz constant of the network. This approach regularizes deeper layers,\nimproving robustness while preserving clean accuracy and reducing computation.\nExperiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent\nschedule maintains near-baseline clean accuracy, enhances robustness under\nFGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to\nbaseline and linear DropPath schedules.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10298v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10298v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.373,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10303",
      "title": "Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns\n  Effective Scheduling through Random Data",
      "authors": [
        "Jesse van Remmerden",
        "Zaharah Bukhsh",
        "Yingqian Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling\nProblem (FJSP), are canonical combinatorial optimization problems with\nwide-ranging applications in industrial operations. In recent years, many\nonline reinforcement learning (RL) approaches have been proposed to learn\nconstructive heuristics for JSP and FJSP. Although effective, these online RL\nmethods require millions of interactions with simulated environments that may\nnot capture real-world complexities, and their random policy initialization\nleads to poor sample efficiency. To address these limitations, we introduce\nConservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL\nalgorithm that learns effective scheduling policies directly from historical\ndata, eliminating the need for costly online interactions, while maintaining\nthe ability to improve upon suboptimal training data. CDQAC couples a\nquantile-based critic with a delayed policy update, estimating the return\ndistribution of each machine-operation pair rather than selecting pairs\noutright. Our extensive experiments demonstrate CDQAC's remarkable ability to\nlearn from diverse data sources. CDQAC consistently outperforms the original\ndata-generating heuristics and surpasses state-of-the-art offline and online RL\nbaselines. In addition, CDQAC is highly sample efficient, requiring only 10-20\ntraining instances to learn high-quality policies. Surprisingly, we find that\nCDQAC performs better when trained on data generated by a random heuristic than\nwhen trained on higher-quality data from genetic algorithms and priority\ndispatching rules.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10303v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10303v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.318,
      "distributed_training_score": 0.381,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10310",
      "title": "A Stochastic Birth-and-Death Approach for Street Furniture Geolocation\n  in Urban Environments",
      "authors": [
        "Evan Murphy",
        "Marco Viola",
        "Vladimir A. Krylov"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper we address the problem of precise geolocation of street\nfurniture in complex urban environments, which is a critical task for effective\nmonitoring and maintenance of public infrastructure by local authorities and\nprivate stakeholders. To this end, we propose a probabilistic framework based\non energy maps that encode the spatial likelihood of object locations.\nRepresenting the energy in a map-based geopositioned format allows the\noptimisation process to seamlessly integrate external geospatial information,\nsuch as GIS layers, road maps, or placement constraints, which improves\ncontextual awareness and localisation accuracy. A stochastic birth-and-death\noptimisation algorithm is introduced to infer the most probable configuration\nof assets. We evaluate our approach using a realistic simulation informed by a\ngeolocated dataset of street lighting infrastructure in Dublin city centre,\ndemonstrating its potential for scalable and accurate urban asset mapping. The\nimplementation of the algorithm will be made available in the GitHub repository\nhttps://github.com/EMurphy0108/SBD_Street_Furniture.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10310v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10310v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.29,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.322,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10312",
      "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching",
      "authors": [
        "Zhixin Zheng",
        "Xinyu Wang",
        "Chang Zou",
        "Shaobo Wang",
        "Linfeng Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10312v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10312v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.487,
      "distributed_training_score": 0.453,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on accelerating diffusion transformers for image and video generation through feature caching, emphasizing computational efficiency in the denoising process. It does not involve adapting diffusion models for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks.",
      "distributed_training_justification": "The paper addresses inference acceleration in diffusion models via feature caching and spatial clustering, which is an internal optimization technique. It does not discuss distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation in multi-processor environments.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10326",
      "title": "State Algebra for Propositional Logic",
      "authors": [
        "Dmitry Lesnik",
        "Tobias Schäfer"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "This paper presents State Algebra, a novel framework designed to represent\nand manipulate propositional logic using algebraic methods. The framework is\nstructured as a hierarchy of three representations: Set, Coordinate, and Row\nDecomposition. These representations anchor the system in well-known semantics\nwhile facilitating the computation using a powerful algebraic engine. A key\naspect of State Algebra is its flexibility in representation. We show that\nalthough the default reduction of a state vector is not canonical, a unique\ncanonical form can be obtained by applying a fixed variable order during the\nreduction process. This highlights a trade-off: by foregoing guaranteed\ncanonicity, the framework gains increased flexibility, potentially leading to\nmore compact representations of certain classes of problems. We explore how\nthis framework provides tools to articulate both search-based and knowledge\ncompilation algorithms and discuss its natural extension to probabilistic logic\nand Weighted Model Counting.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10326v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10326v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.245,
      "weak_supervision_score": 0.222,
      "diffusion_reasoning_score": 0.331,
      "distributed_training_score": 0.24,
      "datasets_score": 0.229,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10334",
      "title": "I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic\n  Segmentation",
      "authors": [
        "Jordan Sassoon",
        "Michal Szczepanski",
        "Martyna Poreba"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Vision Transformers (ViTs) have recently achieved strong results in semantic\nsegmentation, yet their deployment on resource-constrained devices remains\nlimited due to their high memory footprint and computational cost. Quantization\noffers an effective strategy to improve efficiency, but ViT-based segmentation\nmodels are notoriously fragile under low precision, as quantization errors\naccumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the\nfirst fully integer-only ViT segmentation framework. Building on the Segmenter\narchitecture, I-Segmenter systematically replaces floating-point operations\nwith integer-only counterparts. To further stabilize both training and\ninference, we propose $\\lambda$-ShiftGELU, a novel activation function that\nmitigates the limitations of uniform quantization in handling long-tailed\nactivation distributions. In addition, we remove the L2 normalization layer and\nreplace bilinear interpolation in the decoder with nearest neighbor upsampling,\nensuring integer-only execution throughout the computational graph. Extensive\nexperiments show that I-Segmenter achieves accuracy within a reasonable margin\nof its FP32 baseline (5.1 % on average), while reducing model size by up to\n3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,\neven in one-shot PTQ with a single calibration image, I-Segmenter delivers\ncompetitive accuracy, underscoring its practicality for real-world deployment.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10334v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10334v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.406,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is the development of I-Segmenter, a quantized Vision Transformer for efficient semantic segmentation, focusing on integer-only operations, new activation functions, and inference optimizations. It addresses model compression and deployment on resource-constrained devices but does not involve distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation during training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10341",
      "title": "GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT",
      "authors": [
        "Botond Fazekas",
        "Thomas Pinetz",
        "Guilherme Aresta",
        "Taha Emre",
        "Hrvoje Bogunovic"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing\nand monitoring retinal diseases. However, OCT images are inherently degraded by\nspeckle noise, which obscures fine details and hinders accurate interpretation.\nWhile numerous denoising methods exist, many struggle to balance noise\nreduction with the preservation of crucial anatomical structures. This paper\nintroduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel\ndeep learning approach for OCT image despeckling that leverages the strengths\nof diffusion probabilistic models. Unlike conventional diffusion models that\nassume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more\naccurately reflect the statistical properties of speckle. Furthermore, we\nintroduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,\nless-noisy image to guide the denoising process. This crucial addition prevents\nthe reintroduction of high-frequency noise. We accelerate the inference process\nby adapting the Denoising Diffusion Implicit Model framework to our Gamma-based\nmodel. Experiments on a dataset with paired noisy and less-noisy OCT B-scans\ndemonstrate that GARD significantly outperforms traditional denoising methods\nand state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.\nQualitative results confirm that GARD produces sharper edges and better\npreserves fine anatomical details.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10341v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10341v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.281,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.312,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a diffusion-based model for denoising OCT images by adapting it to a Gamma distribution, focusing on image restoration and noise reduction. This is a perceptual task in medical imaging and does not involve adapting diffusion models for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10344",
      "title": "GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography",
      "authors": [
        "Yuexi Du",
        "Lihui Chen",
        "Nicha C. Dvornek"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Mammography screening is an essential tool for early detection of breast\ncancer. The speed and accuracy of mammography interpretation have the potential\nto be improved with deep learning methods. However, the development of a\nfoundation visual language model (VLM) is hindered by limited data and domain\ndifferences between natural and medical images. Existing mammography VLMs,\nadapted from natural images, often ignore domain-specific characteristics, such\nas multi-view relationships in mammography. Unlike radiologists who analyze\nboth views together to process ipsilateral correspondence, current methods\ntreat them as independent images or do not properly model the multi-view\ncorrespondence learning, losing critical geometric context and resulting in\nsuboptimal prediction. We propose GLAM: Global and Local Alignment for\nMulti-view mammography for VLM pretraining using geometry guidance. By\nleveraging the prior knowledge about the multi-view imaging process of\nmammograms, our model learns local cross-view alignments and fine-grained local\nfeatures through joint global and local, visual-visual, and visual-language\ncontrastive learning. Pretrained on EMBED [14], one of the largest open\nmammography datasets, our model outperforms baselines across multiple datasets\nunder different settings.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10344v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10344v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.338,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10345",
      "title": "Towards Understanding Visual Grounding in Visual Language Models",
      "authors": [
        "Georgios Pantazopoulos",
        "Eda B. Özyiğit"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Visual grounding refers to the ability of a model to identify a region within\nsome visual input that matches a textual description. Consequently, a model\nequipped with visual grounding capabilities can target a wide range of\napplications in various domains, including referring expression comprehension,\nanswering questions pertinent to fine-grained details in images or videos,\ncaption visual context by explicitly referring to entities, as well as low and\nhigh-level control in simulated and real environments. In this survey paper, we\nreview representative works across the key areas of research on modern\ngeneral-purpose vision language models (VLMs). We first outline the importance\nof grounding in VLMs, then delineate the core components of the contemporary\nparadigm for developing grounded models, and examine their practical\napplications, including benchmarks and evaluation metrics for grounded\nmultimodal generation. We also discuss the multifaceted interrelations among\nvisual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,\nwe analyse the challenges inherent to visual grounding and suggest promising\ndirections for future research.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10345v2",
      "pdf_url": "http://arxiv.org/pdf/2509.10345v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.386,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.468,
      "distributed_training_score": 0.325,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper is a survey on visual grounding in visual language models (VLMs), focusing on establishing correspondences between text and visual elements, including tasks like referring expression comprehension and grounded captioning. It discusses architectural components, applications, and challenges in VLMs but does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10348",
      "title": "Multi-pathology Chest X-ray Classification with Rejection Mechanisms",
      "authors": [
        "Yehudit Aperstein",
        "Amit Tzahar",
        "Alon Gottlib",
        "Tal Verber",
        "Ravit Shagan Damti",
        "Alexander Apartsin"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Overconfidence in deep learning models poses a significant risk in\nhigh-stakes medical imaging tasks, particularly in multi-label classification\nof chest X-rays, where multiple co-occurring pathologies must be detected\nsimultaneously. This study introduces an uncertainty-aware framework for chest\nX-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective\nprediction mechanisms: entropy-based rejection and confidence interval-based\nrejection. Both methods enable the model to abstain from uncertain predictions,\nimproving reliability by deferring ambiguous cases to clinical experts. A\nquantile-based calibration procedure is employed to tune rejection thresholds\nusing either global or class-specific strategies. Experiments conducted on\nthree large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR)\ndemonstrate that selective rejection improves the trade-off between diagnostic\naccuracy and coverage, with entropy-based rejection yielding the highest\naverage AUC across all pathologies. These results support the integration of\nselective prediction into AI-assisted diagnostic workflows, providing a\npractical step toward safer, uncertainty-aware deployment of deep learning in\nclinical settings.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10348v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10348v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.332,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10359",
      "title": "Immunizing Images from Text to Image Editing via Adversarial\n  Cross-Attention",
      "authors": [
        "Matteo Trippodo",
        "Federico Becattini",
        "Lorenzo Seidenari"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in text-based image editing have enabled fine-grained\nmanipulation of visual content guided by natural language. However, such\nmethods are susceptible to adversarial attacks. In this work, we propose a\nnovel attack that targets the visual component of editing methods. We introduce\nAttention Attack, which disrupts the cross-attention between a textual prompt\nand the visual representation of the image by using an automatically generated\ncaption of the source image as a proxy for the edit prompt. This breaks the\nalignment between the contents of the image and their textual description,\nwithout requiring knowledge of the editing method or the editing prompt.\nReflecting on the reliability of existing metrics for immunization success, we\npropose two novel evaluation strategies: Caption Similarity, which quantifies\nsemantic consistency between original and adversarial edits, and semantic\nIntersection over Union (IoU), which measures spatial layout disruption via\nsegmentation masks. Experiments conducted on the TEDBench++ benchmark\ndemonstrate that our attack significantly degrades editing performance while\nremaining imperceptible.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10359v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10359v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.275,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10366",
      "title": "Efficient Learned Image Compression Through Knowledge Distillation",
      "authors": [
        "Fabien Allemand",
        "Attilio Fiandrotti",
        "Sumanta Chaudhuri",
        "Alaa Eddine Mazouz"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Learned image compression sits at the intersection of machine learning and\nimage processing. With advances in deep learning, neural network-based\ncompression methods have emerged. In this process, an encoder maps the image to\na low-dimensional latent space, which is then quantized, entropy-coded into a\nbinary bitstream, and transmitted to the receiver. At the receiver end, the\nbitstream is entropy-decoded, and a decoder reconstructs an approximation of\nthe original image. Recent research suggests that these models consistently\noutperform conventional codecs. However, they require significant processing\npower, making them unsuitable for real-time use on resource-constrained\nplatforms, which hinders their deployment in mainstream applications. This\nstudy aims to reduce the resource requirements of neural networks used for\nimage compression by leveraging knowledge distillation, a training paradigm\nwhere smaller neural networks, partially trained on the outputs of larger, more\ncomplex models, can achieve better performance than when trained independently.\nOur work demonstrates that knowledge distillation can be effectively applied to\nimage compression tasks: i) across various architecture sizes, ii) to achieve\ndifferent image quality/bit rate tradeoffs, and iii) to save processing and\nenergy resources. This approach introduces new settings and hyperparameters,\nand future research could explore the impact of different teacher models, as\nwell as alternative loss functions. Knowledge distillation could also be\nextended to transformer-based models. The code is publicly available at:\nhttps://github.com/FABallemand/PRIM .",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10366v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10366v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.406,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on knowledge distillation for efficient image compression using neural networks, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. It deals solely with compression techniques, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper discusses knowledge distillation to optimize neural networks for image compression but does not involve distributed training, parallel computing, or multi-node strategies for accelerating model training. It centers on single-model efficiency rather than partitioning across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10369",
      "title": "Data distribution impacts the performance and generalisability of\n  contrastive learning-based foundation models of electrocardiograms",
      "authors": [
        "Gul Rukh Khattak",
        "Konstantinos Patlatzoglou",
        "Joseph Barker",
        "Libor Pastika",
        "Boroumand Zeidaabadi",
        "Ahmed El-Medany",
        "Hesham Aggour",
        "Yixiu Liang",
        "Antonio H. Ribeiro",
        "Jeffrey Annis",
        "Antonio Luiz Pinho Ribeiro",
        "Junbo Ge",
        "Daniel B. Kramer",
        "Jonathan W. Waks",
        "Evan Brittain",
        "Nicholas Peters",
        "Fu Siong Ng",
        "Arunashis Sau"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "Contrastive learning is a widely adopted self-supervised pretraining\nstrategy, yet its dependence on cohort composition remains underexplored. We\npresent Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation\nmodel and pretrain on four cohorts (n = 5,203,352), from diverse populations\nacross three continents (North America, South America, Asia). We systematically\nassess how cohort demographics, health status, and population diversity\ninfluence the downstream performance for prediction tasks also including two\nadditional cohorts from another continent (Europe). We find that downstream\nperformance depends on the distributional properties of the pretraining cohort,\nincluding demographics and health status. Moreover, while pretraining with a\nmulti-centre, demographically diverse cohort improves in-distribution accuracy,\nit reduces out-of-distribution (OOD) generalisation of our contrastive approach\nby encoding cohort-specific artifacts. To address this, we propose the\nIn-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency\nduring pretraining and enhances OOD robustness. This work provides important\ninsights for developing clinically fair and generalisable foundation models.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10369v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10369v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.406,
      "datasets_score": 0.407,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on contrastive learning for ECG data and analyzes the impact of data distribution on model performance, but it does not discuss distributed training, parallel computing, multi-node setups, or algorithms for partitioning data across processors. There is no mention of strategies for accelerating training via distributed systems.",
      "datasets_justification": "The paper involves analyzing and evaluating datasets, including multiple cohorts from diverse populations, assessing how demographics, health status, and diversity influence model performance. It also introduces new insights into dataset curation and benchmarking for contrastive learning, aligning directly with research on dataset analysis and evaluation for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces the CAPE foundation model for electrocardiograms, pretrained on diverse cohorts from multiple continents, to investigate how data distribution factors like demographics and health status affect the performance and generalizability of contrastive learning-based models. Through systematic evaluation on downstream tasks, the authors find that while diverse pretraining cohorts enhance in-distribution accuracy, they can reduce out-of-distribution generalization due to cohort-specific artifacts, and they propose the In-Distribution Batch (IDB) strategy to improve OOD robustness, providing key insights for developing fair and generalizable clinical models.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing the CAPE model and the IDB strategy to address underexplored issues of data distribution in contrastive learning for ECGs, combining existing techniques in a clever way to enhance generalizability. However, it builds on established contrastive learning methods rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in medical AI, particularly for developing fair foundation models in electrocardiogram analysis, by highlighting the importance of data distribution and proposing solutions like IDB. Its impact is primarily within the subfield of machine learning for healthcare signals, making it relevant but not broadly transformative across all AI domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable insights into the challenges of data distribution in contrastive learning for medical applications, with practical strategies like IDB that could guide future research. It is a strong contribution in AI for healthcare but not essential for those outside this specific area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/208ace72c22a298507b8731fe4fc4cdbb06b154c",
      "total_authors": 18,
      "authors_found": 18,
      "highest_h_index": 10,
      "average_h_index": 3.2222222222222223,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Gul Rukh Khattak",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2357319963"
        },
        {
          "name": "K. Patlatzoglou",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2279404029"
        },
        {
          "name": "Joseph Barker",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2303527654"
        },
        {
          "name": "L. Pastika",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2217972476"
        },
        {
          "name": "B. Zeidaabadi",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2279403164"
        },
        {
          "name": "Ahmed El-Medany",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2296848700"
        },
        {
          "name": "Hesham Aggour",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/150119736"
        },
        {
          "name": "Yixiu Liang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2357872172"
        },
        {
          "name": "A. H. Ribeiro",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375985673"
        },
        {
          "name": "J. Annis",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2123017242"
        },
        {
          "name": "A. Ribeiro",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2302363560"
        },
        {
          "name": "Junbo Ge",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2357504902"
        },
        {
          "name": "Daniel B. Kramer",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2223853054"
        },
        {
          "name": "J. Waks",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2165857664"
        },
        {
          "name": "Evan L Brittain",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2293598619"
        },
        {
          "name": "Nicholas S. Peters",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2300362420"
        },
        {
          "name": "F. S. Ng",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2293571447"
        },
        {
          "name": "A. Sau",
          "h_index": 10,
          "profile_url": "https://www.semanticscholar.org/author/39041322"
        }
      ]
    },
    {
      "id": "2509.10388",
      "title": "Ordinality of Visible-Thermal Image Intensities for Intrinsic Image\n  Decomposition",
      "authors": [
        "Zeqing Leo Yuan",
        "Mani Ramanagopal",
        "Aswin C. Sankaranarayanan",
        "Srinivasa G. Narasimhan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Decomposing an image into its intrinsic photometric factors--shading and\nreflectance--is a long-standing challenge due to the lack of extensive\nground-truth data for real-world scenes. Recent methods rely on synthetic data\nor sparse annotations for limited indoor and even fewer outdoor scenes. We\nintroduce a novel training-free approach for intrinsic image decomposition\nusing only a pair of visible and thermal images. We leverage the principle that\nlight not reflected from an opaque surface is absorbed and detected as heat by\na thermal camera. This allows us to relate the ordinalities between visible and\nthermal image intensities to the ordinalities of shading and reflectance, which\ncan densely self-supervise an optimizing neural network to recover shading and\nreflectance. We perform quantitative evaluations with known reflectance and\nshading under natural and artificial lighting, and qualitative experiments\nacross diverse outdoor scenes. The results demonstrate superior performance\nover recent learning-based models and point toward a scalable path to curating\nreal-world ordinal supervision, previously infeasible via manual labeling.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10388v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10388v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.306,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.294,
      "datasets_score": 0.278,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10391",
      "title": "Improving Audio Event Recognition with Consistency Regularization",
      "authors": [
        "Shanmuka Sadhu",
        "Weiran Wang"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Consistency regularization (CR), which enforces agreement between model\npredictions on augmented views, has found recent benefits in automatic speech\nrecognition [1]. In this paper, we propose the use of consistency\nregularization for audio event recognition, and demonstrate its effectiveness\non AudioSet. With extensive ablation studies for both small ($\\sim$20k) and\nlarge ($\\sim$1.8M) supervised training sets, we show that CR brings consistent\nimprovement over supervised baselines which already heavily utilize data\naugmentation, and CR using stronger augmentation and multiple augmentations\nleads to additional gain for the small training set. Furthermore, we extend the\nuse of CR into the semi-supervised setup with 20K labeled samples and 1.8M\nunlabeled samples, and obtain performance improvement over our best model\ntrained on the small set.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10391v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10391v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.404,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.293,
      "datasets_score": 0.371,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on consistency regularization for audio event recognition, including a semi-supervised setup with 20K labeled and 1.8M unlabeled samples. While it uses unlabeled data to enforce consistency in predictions, which indirectly involves weak supervisory signals, it does not primarily involve programmatically generating labels from noisy or imprecise sources. This makes it tangentially related to weak supervision, as semi-supervised techniques can overlap but are not the same.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10392",
      "title": "Diversified recommendations of cultural activities with personalized\n  determinantal point processes",
      "authors": [
        "Carole Ibrahim",
        "Hiba Bederina",
        "Daniel Cuesta",
        "Laurent Montier",
        "Cyrille Delabre",
        "Jill-Jênn Vie"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While optimizing recommendation systems for user engagement is a\nwell-established practice, effectively diversifying recommendations without\nnegatively impacting core business metrics remains a significant industry\nchallenge. In line with our initiative to broaden our audience's cultural\npractices, this study investigates using personalized Determinantal Point\nProcesses (DPPs) to sample diverse and relevant recommendations. We rely on a\nwell-known quality-diversity decomposition of the similarity kernel to give\nmore weight to user preferences. In this paper, we present our implementations\nof the personalized DPP sampling, evaluate the trade-offs between relevance and\ndiversity through both offline and online metrics, and give insights for\npractitioners on their use in a production environment. For the sake of\nreproducibility, we release the full code for our platform and experiments on\nGitHub.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10392v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10392v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.269,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.313,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10401",
      "title": "Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure\n  Attribution in Multi-Agent Systems",
      "authors": [
        "Alva West",
        "Yixuan Weng",
        "Minjun Zhu",
        "Zhen Lin",
        "Zhiyuan Ning",
        "Yue Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Failure attribution in multi-agent systems -- pinpointing the exact step\nwhere a decisive error occurs -- is a critical yet unsolved challenge. Current\nmethods treat this as a pattern recognition task over long conversation logs,\nleading to critically low step-level accuracy (below 17\\%), which renders them\nimpractical for debugging complex systems. Their core weakness is a fundamental\ninability to perform robust counterfactual reasoning: to determine if\ncorrecting a single action would have actually averted the task failure. To\nbridge this \\emph{counterfactual inference gap}, we introduce\nAbduct-Act-Predict (A2P) Scaffolding, a novel agent framework that transforms\nfailure attribution from pattern recognition into a structured causal inference\ntask. A2P explicitly guides a large language model through a formal three-step\nreasoning process within a single inference pass: (1) Abduction, to infer the\nhidden root causes behind an agent's actions; (2) Action, to define a minimal\ncorrective intervention; and (3) Prediction, to simulate the subsequent\ntrajectory and verify if the intervention resolves the failure. This structured\napproach leverages the holistic context of the entire conversation while\nimposing a rigorous causal logic on the model's analysis. Our extensive\nexperiments on the Who\\&When benchmark demonstrate its efficacy. On the\nAlgorithm-Generated dataset, A2P achieves 47.46\\% step-level accuracy, a\n2.85$\\times$ improvement over the 16.67\\% of the baseline. On the more complex\nHand-Crafted dataset, it achieves 29.31\\% step accuracy, a 2.43$\\times$\nimprovement over the baseline's 12.07\\%. By reframing the problem through a\ncausal lens, A2P Scaffolding provides a robust, verifiable, and significantly\nmore accurate solution for automated failure attribution. Ours code are\nreleased at https://github.com/ResearAI/A2P.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10401v2",
      "pdf_url": "http://arxiv.org/pdf/2509.10401v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.493,
      "distributed_training_score": 0.338,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a prompting framework for causal inference in LLMs for failure attribution, with no mention of human feedback, reward models, or reinforcement learning techniques. It focuses solely on automated reasoning processes without aligning models via human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a structured three-step reasoning process (Abduct-Act-Predict) for causal inference, but it does not adapt diffusion models or involve iterative refinement of a Chain-of-Thought as a holistic entity. There is no reference to diffusion-based mechanisms for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10407",
      "title": "Compressed Video Quality Enhancement: Classifying and Benchmarking over\n  Standards",
      "authors": [
        "Xiem HoangVan",
        "Dang BuiDinh",
        "Sang NguyenQuang",
        "Wen-Hsiao Peng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Compressed video quality enhancement (CVQE) is crucial for improving user\nexperience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.\nWhile deep learning based CVQE has driven significant progress, existing\nsurveys still suffer from limitations: lack of systematic classification\nlinking methods to specific standards and artifacts, insufficient comparative\nanalysis of architectural paradigms across coding types, and underdeveloped\nbenchmarking practices. To address these gaps, this paper presents three key\ncontributions. First, it introduces a novel taxonomy classifying CVQE methods\nacross architectural paradigms, coding standards, and compressed-domain feature\nutilization. Second, it proposes a unified benchmarking framework integrating\nmodern compression protocols and standard test sequences for fair\nmulti-criteria evaluation. Third, it provides a systematic analysis of the\ncritical trade-offs between reconstruction performance and computational\ncomplexity observed in state-of-the-art methods and highlighting promising\ndirections for future research. This comprehensive review aims to establish a\nfoundation for consistent assessment and informed model selection in CVQE\nresearch and deployment.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10407v2",
      "pdf_url": "http://arxiv.org/pdf/2509.10407v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.336,
      "distributed_training_score": 0.318,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10408",
      "title": "Multimodal SAM-adapter for Semantic Segmentation",
      "authors": [
        "Iacopo Curti",
        "Pierluigi Zama Ramirez",
        "Alioscia Petrelli",
        "Luigi Di Stefano"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Semantic segmentation, a key task in computer vision with broad applications\nin autonomous driving, medical imaging, and robotics, has advanced\nsubstantially with deep learning. Nevertheless, current approaches remain\nvulnerable to challenging conditions such as poor lighting, occlusions, and\nadverse weather. To address these limitations, multimodal methods that\nintegrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,\nproviding complementary information that enhances robustness. In this work, we\npresent MM SAM-adapter, a novel framework that extends the capabilities of the\nSegment Anything Model (SAM) for multimodal semantic segmentation. The proposed\nmethod employs an adapter network that injects fused multimodal features into\nSAM's rich RGB features. This design enables the model to retain the strong\ngeneralization ability of RGB features while selectively incorporating\nauxiliary modalities only when they contribute additional cues. As a result, MM\nSAM-adapter achieves a balanced and efficient use of multimodal information. We\nevaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,\nwhere MM SAM-adapter delivers state-of-the-art performance. To further analyze\nmodality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard\nsubsets. Results consistently demonstrate that our framework outperforms\ncompeting methods in both favorable and adverse conditions, highlighting the\neffectiveness of multimodal adaptation for robust scene understanding. The code\nis available at the following link:\nhttps://github.com/iacopo97/Multimodal-SAM-Adapter.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10408v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10408v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.333,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10414",
      "title": "Is In-Context Learning Learning?",
      "authors": [
        "Adrian de Wynter"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In-context learning (ICL) allows some autoregressive models to solve tasks\nvia next-token prediction and without needing further training. This has led to\nclaims about these model's ability to solve (learn) unseen tasks with only a\nfew shots (exemplars) in the prompt. However, deduction does not always imply\nlearning, as ICL does not explicitly encode a given observation. Instead, the\nmodels rely on their prior knowledge and the exemplars given, if any. We argue\nthat, mathematically, ICL does constitute learning, but its full\ncharacterisation requires empirical work. We then carry out a large-scale\nanalysis of ICL ablating out or accounting for memorisation, pretraining,\ndistributional shifts, and prompting style and phrasing. We find that ICL is an\neffective learning paradigm, but limited in its ability to learn and generalise\nto unseen tasks. We note that, in the limit where exemplars become more\nnumerous, accuracy is insensitive to exemplar distribution, model, prompt\nstyle, and the input's linguistic features. Instead, it deduces patterns from\nregularities in the prompt, which leads to distributional sensitivity,\nespecially in prompting styles such as chain-of-thought. Given the varied\naccuracies on formally similar tasks, we conclude that autoregression's ad-hoc\nencoding is not a robust mechanism, and suggests limited all-purpose\ngeneralisability.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10414v2",
      "pdf_url": "http://arxiv.org/pdf/2509.10414v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.429,
      "weak_supervision_score": 0.432,
      "diffusion_reasoning_score": 0.452,
      "distributed_training_score": 0.386,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on in-context learning in LLMs, analyzing its learning capabilities through prompting and empirical studies, without any mention of human feedback, reward models, or reinforcement learning for fine-tuning.",
      "weak_supervision_justification": "The paper uses exemplars in prompts as a form of guidance for LLMs, which could loosely relate to providing noisy or programmatic labels, but it does not involve training models with weakly supervised data; instead, it examines inference-time learning without weight updates.",
      "diffusion_reasoning_justification": "The paper discusses chain-of-thought prompting for reasoning but does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion techniques for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10423",
      "title": "Mutual Information Tracks Policy Coherence in Reinforcement Learning",
      "authors": [
        "Cameron Reid",
        "Wael Hafez",
        "Amirhossein Nazeri"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Reinforcement Learning (RL) agents deployed in real-world environments face\ndegradation from sensor faults, actuator wear, and environmental shifts, yet\nlack intrinsic mechanisms to detect and diagnose these failures. We present an\ninformation-theoretic framework that reveals both the fundamental dynamics of\nRL and provides practical methods for diagnosing deployment-time anomalies.\nThrough analysis of state-action mutual information patterns in a robotic\ncontrol task, we first demonstrate that successful learning exhibits\ncharacteristic information signatures: mutual information between states and\nactions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing\nstate entropy, indicating that agents develop increasingly selective attention\nto task-relevant patterns. Intriguingly, states, actions and next states joint\nmutual information, MI(S,A;S'), follows an inverted U-curve, peaking during\nearly learning before declining as the agent specializes suggesting a\ntransition from broad exploration to efficient exploitation. More immediately\nactionable, we show that information metrics can differentially diagnose system\nfailures: observation-space, i.e., states noise (sensor faults) produces broad\ncollapses across all information channels with pronounced drops in state-action\ncoupling, while action-space noise (actuator faults) selectively disrupts\naction-outcome predictability while preserving state-action relationships. This\ndifferential diagnostic capability demonstrated through controlled perturbation\nexperiments enables precise fault localization without architectural\nmodifications or performance degradation. By establishing information patterns\nas both signatures of learning and diagnostic for system health, we provide the\nfoundation for adaptive RL systems capable of autonomous fault detection and\npolicy adjustment based on information-theoretic principles.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10423v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10423v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.449,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.299,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is an information-theoretic framework for analyzing RL dynamics and diagnosing anomalies in robotic control tasks, focusing on mutual information in state-action pairs. It does not involve human feedback, such as training a reward model on human-ranked data or aligning AI models with human preferences. Therefore, it does not align with the definition of Reinforcement Learning from Human Feedback.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10432",
      "title": "Standards in the Preparation of Biomedical Research Metadata: A\n  Bridge2AI Perspective",
      "authors": [
        "Harry Caufield",
        "Satrajit Ghosh",
        "Sek Wong Kong",
        "Jillian Parker",
        "Nathan Sheffield",
        "Bhavesh Patel",
        "Andrew Williams",
        "Timothy Clark",
        "Monica C. Munoz-Torres"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "AI-readiness describes the degree to which data may be optimally and\nethically used for subsequent AI and Machine Learning (AI/ML) methods, where\nthose methods may involve some combination of model training, data\nclassification, and ethical, explainable prediction. The Bridge2AI consortium\nhas defined the particular criteria a biomedical dataset may possess to render\nit AI-ready: in brief, a dataset's readiness is related to its FAIRness,\nprovenance, degree of characterization, explainability, sustainability, and\ncomputability, in addition to its accompaniment with documentation about\nethical data practices.\n  To ensure AI-readiness and to clarify data structure and relationships within\nBridge2AI's Grand Challenges (GCs), particular types of metadata are necessary.\nThe GCs within the Bridge2AI initiative include four data-generating projects\nfocusing on generating AI/ML-ready datasets to tackle complex biomedical and\nbehavioral research problems. These projects develop standardized, multimodal\ndata, tools, and training resources to support AI integration, while addressing\nethical data practices. Examples include using voice as a biomarker, building\ninterpretable genomic tools, modeling disease trajectories with diverse\nmultimodal data, and mapping cellular and molecular health indicators across\nthe human body.\n  This report assesses the state of metadata creation and standardization in\nthe Bridge2AI GCs, provides guidelines where required, and identifies gaps and\nareas for improvement across the program. New projects, including those outside\nthe Bridge2AI consortium, would benefit from what we have learned about\ncreating metadata as part of efforts to promote AI readiness.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10432v2",
      "pdf_url": "http://arxiv.org/pdf/2509.10432v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.299,
      "distributed_training_score": 0.333,
      "datasets_score": 0.495,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves assessing and standardizing metadata for biomedical datasets to make them AI-ready, which directly relates to creating, analyzing, and evaluating datasets for AI and machine learning applications. It discusses criteria for dataset readiness, identifies gaps, and provides guidelines, aligning with dataset curation and evaluation methodologies.",
      "llm_score_status": "completed",
      "summary": "The paper, from the Bridge2AI perspective, assesses standards for preparing biomedical research metadata to ensure datasets are AI-ready, emphasizing criteria such as FAIRness, provenance, characterization, explainability, sustainability, computability, and ethical practices. It evaluates metadata creation and standardization across Bridge2AI's Grand Challenges—projects focused on generating multimodal data for AI/ML applications like voice biomarkers and genomic tools—provides guidelines, identifies gaps, and offers recommendations to improve AI readiness for current and future initiatives.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing metadata standards like FAIRness with specific applications to Bridge2AI projects, offering a practical framework for enhancing AI readiness in biomedical data. However, it does not introduce a entirely new problem or technique, relying instead on established concepts adapted to a targeted context.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research and projects within the biomedical AI subfield by providing actionable guidelines for metadata standardization, potentially improving data practices in similar initiatives. While its scope is somewhat limited to Bridge2AI and related efforts, it could still be cited and built upon in specialized applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable insights and guidelines for researchers in AI and biomedical data management, making it a strong contribution worth reviewing for those involved in similar projects. However, it is not essential for a broader audience beyond this niche, as its impact is focused and incremental.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/734657a62bedc07beebded0b4cd3a75d44304cfb",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 35,
      "average_h_index": 4.666666666666667,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Harry Caufield",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2249531959"
        },
        {
          "name": "Satrajit Ghosh",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380460966"
        },
        {
          "name": "Sek Wong Kong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380444184"
        },
        {
          "name": "Jillian A. Parker",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2329371630"
        },
        {
          "name": "Nathan Sheffield",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380442606"
        },
        {
          "name": "Bhavesh Patel",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380611902"
        },
        {
          "name": "Andrew Williams",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2237959798"
        },
        {
          "name": "Timothy Clark",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380445077"
        },
        {
          "name": "M. Munoz-Torres",
          "h_index": 35,
          "profile_url": "https://www.semanticscholar.org/author/1399643910"
        }
      ]
    },
    {
      "id": "2509.10441",
      "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis",
      "authors": [
        "Tao Han",
        "Wanghan Xu",
        "Junchao Gong",
        "Xiaoyu Yue",
        "Song Guo",
        "Luping Zhou",
        "Lei Bai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Arbitrary resolution image generation provides a consistent visual experience\nacross devices, having extensive applications for producers and consumers.\nCurrent diffusion models increase computational demand quadratically with\nresolution, causing 4K image generation delays over 100 seconds. To solve this,\nwe explore the second generation upon the latent diffusion models, where the\nfixed latent generated by diffusion models is regarded as the content\nrepresentation and we propose to decode arbitrary resolution images with a\ncompact generated latent using a one-step generator. Thus, we present the\n\\textbf{InfGen}, replacing the VAE decoder with the new generator, for\ngenerating images at any resolution from a fixed-size latent without retraining\nthe diffusion models, which simplifies the process, reducing computational\ncomplexity and can be applied to any model using the same latent space.\nExperiments show InfGen is capable of improving many models into the arbitrary\nhigh-resolution era while cutting 4K image generation time to under 10 seconds.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10441v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10441v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.317,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.54,
      "distributed_training_score": 0.38,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on improving image synthesis for arbitrary resolutions using a modified generator in latent diffusion models, aiming to reduce computational costs for high-resolution outputs. It does not involve adapting diffusion processes for multi-step logical reasoning, Chain-of-Thought, or solving complex logical tasks; instead, it is centered on visual generation efficiency. Therefore, there is no component of diffusion-based reasoning present.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10453",
      "title": "SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and\n  Adaptability Across Alzheimer's Prediction Tasks and Datasets",
      "authors": [
        "Emily Kaczmarek",
        "Justin Szeto",
        "Brennan Nichyporuk",
        "Tal Arbel"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Alzheimer's disease is a progressive, neurodegenerative disorder that causes\nmemory loss and cognitive decline. While there has been extensive research in\napplying deep learning models to Alzheimer's prediction tasks, these models\nremain limited by lack of available labeled data, poor generalization across\ndatasets, and inflexibility to varying numbers of input scans and time\nintervals between scans. In this study, we adapt three state-of-the-art\ntemporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,\nand add novel extensions designed to handle variable-length inputs and learn\nrobust spatial features. We aggregate four publicly available datasets\ncomprising 3,161 patients for pre-training, and show the performance of our\nmodel across multiple Alzheimer's prediction tasks including diagnosis\nclassification, conversion detection, and future conversion prediction.\nImportantly, our SSL model implemented with temporal order prediction and\ncontrastive learning outperforms supervised learning on six out of seven\ndownstream tasks. It demonstrates adaptability and generalizability across\ntasks and number of input images with varying time intervals, highlighting its\ncapacity for robust performance across clinical applications. We release our\ncode and model publicly at https://github.com/emilykaczmarek/SSL-AD.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10453v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10453v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.373,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on spatiotemporal self-supervised learning (SSL) for Alzheimer's disease prediction using 3D brain MRI, incorporating temporal order prediction and contrastive learning. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks. The core contributions are in SSL for medical imaging and prediction, with no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10454",
      "title": "GC-VLN: Instruction as Graph Constraints for Training-free\n  Vision-and-Language Navigation",
      "authors": [
        "Hang Yin",
        "Haoyu Wei",
        "Xiuwei Xu",
        "Wenxuan Guo",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we propose a training-free framework for vision-and-language\nnavigation (VLN). Existing zero-shot VLN methods are mainly designed for\ndiscrete environments or involve unsupervised training in continuous simulator\nenvironments, which makes it challenging to generalize and deploy them in\nreal-world scenarios. To achieve a training-free framework in continuous\nenvironments, our framework formulates navigation guidance as graph constraint\noptimization by decomposing instructions into explicit spatial constraints. The\nconstraint-driven paradigm decodes spatial semantics through constraint\nsolving, enabling zero-shot adaptation to unseen environments. Specifically, we\nconstruct a spatial constraint library covering all types of spatial\nrelationship mentioned in VLN instructions. The human instruction is decomposed\ninto a directed acyclic graph, with waypoint nodes, object nodes and edges,\nwhich are used as queries to retrieve the library to build the graph\nconstraints. The graph constraint optimization is solved by the constraint\nsolver to determine the positions of waypoints, obtaining the robot's\nnavigation path and final goal. To handle cases of no solution or multiple\nsolutions, we construct a navigation tree and the backtracking mechanism.\nExtensive experiments on standard benchmarks demonstrate significant\nimprovements in success rate and navigation efficiency compared to\nstate-of-the-art zero-shot VLN methods. We further conduct real-world\nexperiments to show that our framework can effectively generalize to new\nenvironments and instruction sets, paving the way for a more robust and\nautonomous navigation framework.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10454v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10454v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.337,
      "datasets_score": 0.255,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10584",
      "title": "Smart Trial: Evaluating the Use of Large Language Models for Recruiting\n  Clinical Trial Participants via Social Media",
      "authors": [
        "Xiaofan Zhou",
        "Zisu Wang",
        "Janice Krieger",
        "Mohan Zalake",
        "Lu Cheng"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Clinical trials (CT) are essential for advancing medical research and\ntreatment, yet efficiently recruiting eligible participants -- each of whom\nmust meet complex eligibility criteria -- remains a significant challenge.\nTraditional recruitment approaches, such as advertisements or electronic health\nrecord screening within hospitals, are often time-consuming and geographically\nconstrained. This work addresses the recruitment challenge by leveraging the\nvast amount of health-related information individuals share on social media\nplatforms. With the emergence of powerful large language models (LLMs) capable\nof sophisticated text understanding, we pose the central research question: Can\nLLM-driven tools facilitate CT recruitment by identifying potential\nparticipants through their engagement on social media? To investigate this\nquestion, we introduce TRIALQA, a novel dataset comprising two social media\ncollections from the subreddits on colon cancer and prostate cancer. Using\neligibility criteria from public real-world CTs, experienced annotators are\nhired to annotate TRIALQA to indicate (1) whether a social media user meets a\ngiven eligibility criterion and (2) the user's stated reasons for interest in\nparticipating in CT. We benchmark seven widely used LLMs on these two\nprediction tasks, employing six distinct training and inference strategies. Our\nextensive experiments reveal that, while LLMs show considerable promise, they\nstill face challenges in performing the complex, multi-hop reasoning needed to\naccurately assess eligibility criteria.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10584v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10584v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.381,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on benchmarking LLMs for clinical trial recruitment using annotated datasets, but it does not involve training or fine-tuning models with human feedback via a reward model and reinforcement learning. Human annotations are used for evaluation, not for RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates LLMs with techniques like chain-of-thought and prompting strategies, but it does not mention or utilize diffusion models for iterative refinement or multi-step logical reasoning. There is no component involving diffusion processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10590",
      "title": "Machine Unlearning for Responsible and Adaptive AI in Education",
      "authors": [
        "Betty Mayeku",
        "Sandra Hummel",
        "Parisa Memarmoshrefi"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The concept of Machine Unlearning (MU) has gained popularity in various\ndomains due to its ability to address several issues in Machine Learning (ML)\nmodels, particularly those related to privacy, security, bias mitigation, and\nadaptability. With these abilities, MU is evolving into a promising technology\nin upholding Responsible AI principles and optimizing ML models' performance.\nHowever, despite its promising potential, the concept has not received much\nattention in the education sector. In an attempt to encourage further uptake of\nthis promising technology in the educational landscape, this paper demonstrates\nthat MU indeed has great potential to serve as a practical mechanism for\noperationalizing Responsible AI principles as well as an essential tool for\nAdaptive AI within the educational application domain hence fostering trust in\nAI-driven educational systems. Through a structured review of 42 peer-reviewed\nsources, we identify four domains where MU holds particular promise namely\nprivacy protection, resilience against adversarial inputs, mitigation of\nsystemic bias, and adaptability in evolving learning contexts. We\nsystematically explore these potentials and their interventions to core\nchallenges in ML-based education systems. As a conceptual contribution, we\npresent a reference Machine Unlearning application architecture for Responsible\nand Adaptive AI (MU-RAAI) in education context.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10590v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10590v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.325,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses Machine Unlearning (MU) for removing data from ML models in educational contexts, focusing on privacy, bias, and adaptability. It does not involve training models with human feedback, reward models, or reinforcement learning techniques, making it unrelated to RLHF.",
      "weak_supervision_justification": "The paper centers on Machine Unlearning as a method to forget data in ML models for education, without addressing the generation of training labels from noisy or imprecise sources. It does not cover weak supervision approaches for model training.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10591",
      "title": "Assisting the Grading of a Handwritten General Chemistry Exam with\n  Artificial Intelligence",
      "authors": [
        "Jan Cvengros",
        "Gerd Kortemeyer"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We explore the effectiveness and reliability of an artificial intelligence\n(AI)-based grading system for a handwritten general chemistry exam, comparing\nAI-assigned scores to human grading across various types of questions. Exam\npages and grading rubrics were uploaded as images to account for chemical\nreaction equations, short and long open-ended answers, numerical and symbolic\nanswer derivations, drawing, and sketching in pencil-and-paper format. Using\nlinear regression analyses and psychometric evaluations, the investigation\nreveals high agreement between AI and human graders for textual and chemical\nreaction questions, while highlighting lower reliability for numerical and\ngraphical tasks. The findings emphasize the necessity for human oversight to\nensure grading accuracy, based on selective filtering. The results indicate\npromising applications for AI in routine assessment tasks, though careful\nconsideration must be given to student perceptions of fairness and trust in\nintegrating AI-based grading into educational practice.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10591v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10591v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.305,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10593",
      "title": "Automated Cervical Os Segmentation for Camera-Guided, Speculum-Free\n  Screening",
      "authors": [
        "Aoife McDonald-Bowyer",
        "Anjana Wijekoon",
        "Ryan Laurance Love",
        "Katie Allan",
        "Scott Colvin",
        "Aleksandra Gentry-Maharaj",
        "Adeola Olaitan",
        "Danail Stoyanov",
        "Agostino Stilli",
        "Sophia Bano"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Cervical cancer is highly preventable, yet persistent barriers to screening\nlimit progress toward elimination goals. Speculum-free devices that integrate\nimaging and sampling could improve access, particularly in low-resource\nsettings, but require reliable visual guidance. This study evaluates deep\nlearning methods for real-time segmentation of the cervical os in transvaginal\nendoscopic images. Five encoder-decoder architectures were compared using 913\nframes from 200 cases in the IARC Cervical Image Dataset, annotated by\ngynaecologists. Performance was assessed using IoU, DICE, detection rate, and\ndistance metrics with ten-fold cross-validation. EndoViT/DPT, a vision\ntransformer pre-trained on surgical video, achieved the highest DICE (0.50 \\pm\n0.31) and detection rate (0.87 \\pm 0.33), outperforming CNN-based approaches.\nExternal validation with phantom data demonstrated robust segmentation under\nvariable conditions at 21.5 FPS, supporting real-time feasibility. These\nresults establish a foundation for integrating automated os recognition into\nspeculum-free cervical screening devices to support non-expert use in both\nhigh- and low-resource contexts.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10593v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10593v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.279,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.273,
      "distributed_training_score": 0.308,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10594",
      "title": "SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of\n  AI and LLMs in SMEs",
      "authors": [
        "Iqbal H. Sarker",
        "Helge Janicke",
        "Ahmad Mohsin",
        "Leandros Maglaras"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Artificial Intelligence (AI) and Large Language Models (LLMs) are reshaping\ntoday's business practices, however, their adoption within small and\nmedium-sized enterprises (SMEs) raises significant technical, ethical and trust\nissues. This paper proposes a structured, multi-phased framework designed to\nembed trust and ethical principles throughout the AI lifecycle for their secure\nand responsible use in SMEs. Structured around four pillars, i.e., Data,\nAlgorithms, Human oversight, and Model Architecture, the framework bridges\ntheoretical ethical principles with operational practice, enhancing AI\ncapabilities in diverse SME applications. Ultimately, this paper offers a\nstructured roadmap for responsible AI adoption, framing trust and ethics as a\ncatalyst for resilience, competitiveness, and sustainable innovation in SMEs.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10594v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10594v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.463,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.354,
      "datasets_score": 0.39,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework (SME-TEAM) for embedding trust and ethical principles in AI adoption for SMEs, focusing on pillars like Data, Algorithms, Human Oversight, and Model Architecture. It discusses general human oversight for ethical AI practices but does not involve or mention Reinforcement Learning from Human Feedback (RLHF), which specifically entails training a reward model on human-ranked data and fine-tuning via reinforcement learning. Thus, there is no connection to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10596",
      "title": "GenAI Voice Mode in Programming Education",
      "authors": [
        "Sven Jacobs",
        "Natalie Kiesler"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Real-time voice interfaces using multimodal Generative AI (GenAI) can\npotentially address the accessibility needs of novice programmers with\ndisabilities (e.g., related to vision). Yet, little is known about how novices\ninteract with GenAI tools and their feedback quality in the form of audio\noutput. This paper analyzes audio dialogues from nine 9th-grade students using\na voice-enabled tutor (powered by OpenAI's Realtime API) in an authentic\nclassroom setting while learning Python. We examined the students' voice\nprompts and AI's responses (1210 messages) by using qualitative coding. We also\ngathered students' perceptions via the Partner Modeling Questionnaire. The\nGenAI Voice Tutor primarily offered feedback on mistakes and next steps, but\nits correctness was limited (71.4% correct out of 416 feedback outputs).\nQuality issues were observed, particularly when the AI attempted to utter\nprogramming code elements. Students used the GenAI voice tutor primarily for\ndebugging. They perceived it as competent, only somewhat human-like, and\nflexible. The present study is the first to explore the interaction dynamics of\nreal-time voice GenAI tutors and novice programmers, informing future\neducational tool design and potentially addressing accessibility needs of\ndiverse learners.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10596v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10596v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.29,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10600",
      "title": "National Running Club Database: Assessing Collegiate Club Athletes'\n  Cross Country Race Results",
      "authors": [
        "Jonathan A. Karr Jr",
        "Ben Darden",
        "Nicholas Pell",
        "Ryan M. Fryer",
        "Kayla Ambrose",
        "Evan Hall",
        "Ramzi K. Bualuan",
        "Nitesh V. Chawla"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The National Running Club Database (NRCD) aggregates 15,397 race results of\n5,585 athletes from the 2023 and 2024 cross country seasons. This paper\nintroduces the NRCD dataset, which provides insights into individual athlete\nprogressions, enabling data-driven decision-making. Analysis reveals that\nrunners' improvement per calendar day for women, racing 6,000m, and men, racing\n8,000m, is more pronounced in athletes with slower initial race times and those\nwho race more frequently. Additionally, we factor in course conditions,\nincluding weather and elevation gain, to standardize improvement. While the\nNRCD shows a gender imbalance, 3,484 men vs. 2,101 women, the racing frequency\nbetween genders is comparable. This publication makes the NRCD dataset\naccessible to the research community, addressing a previous challenge where\nsmaller datasets, often limited to 500 entries, had to be manually scraped from\nthe internet. Focusing on club athletes rather than elite professionals offers\na unique lens into the performance of real-world runners who balance\ncompetition with academics and other commitments. These results serve as a\nvaluable resource for runners, coaches, and teams, bridging the gap between raw\ndata and applied sports science.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10600v2",
      "pdf_url": "http://arxiv.org/pdf/2509.10600v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.243,
      "weak_supervision_score": 0.257,
      "diffusion_reasoning_score": 0.231,
      "distributed_training_score": 0.322,
      "datasets_score": 0.382,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10620",
      "title": "Building a General SimCLR Self-Supervised Foundation Model Across\n  Neurological Diseases to Advance 3D Brain MRI Diagnoses",
      "authors": [
        "Emily Kaczmarek",
        "Justin Szeto",
        "Brennan Nichyporuk",
        "Tal Arbel"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly\nacquired in clinical settings to monitor a wide range of neurological\nconditions, including neurodegenerative disorders and stroke. While deep\nlearning models have shown promising results analyzing 3D MRI across a number\nof brain imaging tasks, most are highly tailored for specific tasks with\nlimited labeled data, and are not able to generalize across tasks and/or\npopulations. The development of self-supervised learning (SSL) has enabled the\ncreation of large medical foundation models that leverage diverse, unlabeled\ndatasets ranging from healthy to diseased data, showing significant success in\n2D medical imaging applications. However, even the very few foundation models\nfor 3D brain MRI that have been developed remain limited in resolution, scope,\nor accessibility. In this work, we present a general, high-resolution\nSimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on\n18,759 patients (44,958 scans) from 11 publicly available datasets spanning\ndiverse neurological diseases. We compare our model to Masked Autoencoders\n(MAE), as well as two supervised baselines, on four diverse downstream\nprediction tasks in both in-distribution and out-of-distribution settings. Our\nfine-tuned SimCLR model outperforms all other models across all tasks. Notably,\nour model still achieves superior performance when fine-tuned using only 20% of\nlabeled training samples for predicting Alzheimer's disease. We use publicly\navailable code and data, and release our trained model at\nhttps://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadly\napplicable and accessible foundation model for clinical brain MRI analysis.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10620v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10620v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.396,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.388,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10625",
      "title": "No Answer Needed: Predicting LLM Answer Accuracy from Question-Only\n  Linear Probes",
      "authors": [
        "Iván Vicente Moreno Cencerrado",
        "Arnau Padrés Masdemont",
        "Anton Gonzalvez Hawthorne",
        "David Demitri Africa",
        "Lorenzo Pacchiardi"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Do large language models (LLMs) anticipate when they will answer correctly?\nTo study this, we extract activations after a question is read but before any\ntokens are generated, and train linear probes to predict whether the model's\nforthcoming answer will be correct. Across three open-source model families\nranging from 7 to 70 billion parameters, projections on this \"in-advance\ncorrectness direction\" trained on generic trivia questions predict success in\ndistribution and on diverse out-of-distribution knowledge datasets,\noutperforming black-box baselines and verbalised predicted confidence.\nPredictive power saturates in intermediate layers, suggesting that\nself-assessment emerges mid-computation. Notably, generalisation falters on\nquestions requiring mathematical reasoning. Moreover, for models responding \"I\ndon't know\", doing so strongly correlates with the probe score, indicating that\nthe same direction also captures confidence. By complementing previous results\non truthfulness and other behaviours obtained with probes and sparse\nauto-encoders, our work contributes essential findings to elucidate LLM\ninternals.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10625v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10625v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.349,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on predicting LLM answer accuracy using linear probes on internal activations, without any involvement of human feedback, reward models, or reinforcement learning for model alignment. There is no mention of training processes that align AI with human preferences.",
      "weak_supervision_justification": "The paper trains linear probes using labels derived from evaluating model answers against ground truth datasets like TriviaQA, which rely on precise labels rather than programmatically generated, noisy, or imprecise sources. It does not employ weak supervision techniques for label generation or model training.",
      "diffusion_reasoning_justification": "The paper examines internal activations in LLMs for predicting answer correctness and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10626",
      "title": "Optimal Multimarginal Schrödinger Bridge: Minimum Spanning Tree over\n  Measure-valued Vertices",
      "authors": [
        "Georgiy A. Bondar",
        "Abhishek Halder"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "math.OC (Optimization and Control)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "The Multimarginal Schr\\\"odinger Bridge (MSB) finds the optimal coupling among\na collection of random vectors with known statistics and a known correlation\nstructure. In the MSB formulation, this correlation structure is specified\n\\emph{a priori} as an undirected connected graph with measure-valued vertices.\nIn this work, we formulate and solve the problem of finding the optimal MSB in\nthe sense we seek the optimal coupling over all possible graph structures. We\nfind that computing the optimal MSB amounts to solving the minimum spanning\ntree problem over measure-valued vertices. We show that the resulting problem\ncan be solved in two steps. The first step constructs a complete graph with\nedge weight equal to a sum of the optimal value of the corresponding bimarginal\nSB and the entropies of the endpoints. The second step solves a standard\nminimum spanning tree problem over that complete weighted graph. Numerical\nexperiments illustrate the proposed solution.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10626v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10626v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.206,
      "weak_supervision_score": 0.246,
      "diffusion_reasoning_score": 0.279,
      "distributed_training_score": 0.275,
      "datasets_score": 0.19,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10635",
      "title": "Accurate and Private Diagnosis of Rare Genetic Syndromes from Facial\n  Images with Federated Deep Learning",
      "authors": [
        "Ali Burak Ünal",
        "Cem Ata Baykara",
        "Peter Krawitz",
        "Mete Akgün"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CR (Cryptography and Security)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Machine learning has shown promise in facial dysmorphology, where\ncharacteristic facial features provide diagnostic clues for rare genetic\ndisorders. GestaltMatcher, a leading framework in this field, has demonstrated\nclinical utility across multiple studies, but its reliance on centralized\ndatasets limits further development, as patient data are siloed across\ninstitutions and subject to strict privacy regulations. We introduce a\nfederated GestaltMatcher service based on a cross-silo horizontal federated\nlearning framework, which allows hospitals to collaboratively train a global\nensemble feature extractor without sharing patient images. Patient data are\nmapped into a shared latent space, and a privacy-preserving kernel matrix\ncomputation framework enables syndrome inference and discovery while\nsafeguarding confidentiality. New participants can directly benefit from and\ncontribute to the system by adopting the global feature extractor and kernel\nconfiguration from previous training rounds. Experiments show that the\nfederated service retains over 90% of centralized performance and remains\nrobust to both varying silo numbers and heterogeneous data distributions.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10635v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10635v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.318,
      "distributed_training_score": 0.413,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution involves implementing a cross-silo horizontal federated learning framework for training a global ensemble feature extractor across multiple hospitals without sharing patient data. This directly aligns with distributed training concepts, as it partitions data and computation across nodes (silos) to accelerate model training while maintaining privacy, fitting the definition of algorithms designed for multi-node machine learning.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces a federated learning-based version of the GestaltMatcher framework to enable accurate and private diagnosis of rare genetic syndromes from facial images, addressing privacy concerns by allowing hospitals to collaboratively train a global ensemble feature extractor without sharing patient data. The methodology employs cross-silo horizontal federated learning to map patient images into a shared latent space and utilizes a privacy-preserving kernel matrix computation framework for syndrome inference and discovery, with experiments demonstrating that the federated service retains over 90% of the centralized model's performance while being robust to varying numbers of silos and heterogeneous data distributions.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting federated learning to the GestaltMatcher framework for privacy-preserving diagnosis, combining existing techniques in a new way for rare genetic syndrome analysis. While it advances the state-of-the-art in privacy for medical AI, it builds on established federated learning concepts rather than introducing a entirely new problem or technique.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future research in privacy-preserving medical AI and federated learning applications, particularly in diagnostics for rare diseases, by enabling collaborative data use without compromising patient confidentiality. It could lead to broader adoption in healthcare settings, impacting both academic research and commercial medical tools.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution to privacy in medical AI, making it valuable for researchers in machine learning, computer vision, and healthcare privacy. While essential for those in the field, it may not be critical for a general audience.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/dd5ba8e9dbdf5d14a8004a245053b3dfdeb925e9",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 3,
      "average_h_index": 1.25,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ali Burak Unal",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2345189179"
        },
        {
          "name": "Cem Ata Baykara",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2292408521"
        },
        {
          "name": "Peter Krawitz",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2368419394"
        },
        {
          "name": "Mete Akgun",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/50336323"
        }
      ]
    },
    {
      "id": "2509.10641",
      "title": "Test-Time Warmup for Multimodal Large Language Models",
      "authors": [
        "Nikita Rajaneesh",
        "Thomas Zollo",
        "Richard Zemel"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) hold great promise for advanced\nreasoning at the intersection of text and images, yet they have not fully\nrealized this potential. MLLMs typically integrate an LLM, a vision encoder,\nand a connector that maps the vision encoder's embeddings into the LLM's text\nembedding space. Although each component is pretrained on massive datasets with\nbillions of samples, the entire multimodal model is typically trained on only\nthousands (or a few million) samples, which can result in weak performance on\ncomplex reasoning tasks. To address these shortcomings, instead of relying on\nextensive labeled datasets for fine-tuning, we propose a Test-Time Warmup\nmethod that adapts the MLLM per test instance by leveraging data from weakly\nsupervised auxiliary tasks. With our approach, we observe a relative\nperformance improvement of 4.03% on MMMU, 5.28% on VQA-Rad, and 1.63% on GQA on\nthe Llama-Vision-Instruct model. Our method demonstrates that 'warming up'\nbefore inference can enhance MLLMs' robustness across diverse reasoning tasks.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10641v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10641v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.434,
      "diffusion_reasoning_score": 0.471,
      "distributed_training_score": 0.38,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, Test-Time Warmup (TTW), relies on weakly supervised auxiliary tasks to adapt MLLMs at test time. It uses programmatically generated outputs (e.g., via prompts and CLIP scores) as noisy supervision, aligning directly with weak supervision's definition of training without precise, hand-labeled data.",
      "diffusion_reasoning_justification": "The paper focuses on test-time adaptation via gradient updates on auxiliary tasks for MLLMs, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces Test-Time Warmup (TTW), a novel method to enhance Multimodal Large Language Models (MLLMs) by adapting them per test instance through gradient updates on weakly supervised auxiliary tasks, aiming to improve performance on complex reasoning tasks without relying on extensive labeled data. The authors evaluate TTW on datasets like MMMU, VQA-Rad, and GQA, demonstrating relative improvements of 4.03%, 5.28%, and 1.63% respectively for the Llama-Vision-Instruct model, and argue that this approach fosters more robust image representations and better handling of distribution shifts.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique, Test-Time Warmup, which adapts MLLMs at test time using auxiliary tasks for improved reasoning, representing a significant advancement in handling distribution shifts without labeled data. This innovation goes beyond incremental refinements by proposing a fresh approach to per-instance adaptation in multimodal models.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of MLLMs and test-time adaptation, as it demonstrates practical improvements on key benchmarks like MMMU and GQA. However, its influence may be limited to specific applications in visual reasoning rather than broadly across AI or commercial domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality contribution with innovative methods and empirical results that advance MLLM adaptation, making it valuable for researchers in multimodal AI. While not essential for all, it offers insights that could inform ongoing work in visual reasoning tasks.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1fac56a2dbeabc085dc6f22aa38f5d1a65b54561",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 4,
      "average_h_index": 2.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Nikita Rajaneesh",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2337795092"
        },
        {
          "name": "Thomas Zollo",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2198489301"
        },
        {
          "name": "Richard Zemel",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2329736936"
        }
      ]
    },
    {
      "id": "2509.10651",
      "title": "USCTNet: A deep unfolding nuclear-norm optimization solver for\n  physically consistent HSI reconstruction",
      "authors": [
        "Xiaoyang Ma",
        "Yiyang Chai",
        "Xinran Qu",
        "Hong Sun"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Reconstructing hyperspectral images (HSIs) from a single RGB image is\nill-posed and can become physically inconsistent when the camera spectral\nsensitivity (CSS) and scene illumination are misspecified. We formulate\nRGB-to-HSI reconstruction as a physics-grounded inverse problem regularized by\na nuclear norm in a learnable transform domain, and we explicitly estimate CSS\nand illumination to define the forward operator embedded in each iteration,\nensuring colorimetric consistency. To avoid the cost and instability of full\nsingular-value decompositions (SVDs) required by singular-value thresholding\n(SVT), we introduce a data-adaptive low-rank subspace SVT operator. Building on\nthese components, we develop USCTNet, a deep unfolding solver tailored to HSI\nthat couples a parameter estimation module with learnable proximal updates.\nExtensive experiments on standard benchmarks show consistent improvements over\nstate-of-the-art RGB-based methods in reconstruction accuracy. Code:\nhttps://github.com/psykheXX/USCTNet-Code-Implementation.git",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10651v2",
      "pdf_url": "http://arxiv.org/pdf/2509.10651v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.296,
      "diffusion_reasoning_score": 0.284,
      "distributed_training_score": 0.274,
      "datasets_score": 0.248,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10652",
      "title": "Vibe Coding for UX Design: Understanding UX Professionals' Perceptions\n  of AI-Assisted Design and Development",
      "authors": [
        "Jie Li",
        "Youyang Hou",
        "Laura Lin",
        "Ruihao Zhu",
        "Hancheng Cao",
        "Abdallah El Ali"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)",
        "cs.ET (Emerging Technologies)"
      ],
      "abstract": "Generative AI is reshaping UX design practices through \"vibe coding,\" where\nUX professionals express intent in natural language and AI translates it into\nfunctional prototypes and code. Despite rapid adoption, little research has\nexamined how vibe coding reconfigures UX workflows and collaboration. Drawing\non interviews with 20 UX professionals across enterprises, startups, and\nacademia, we show how vibe coding follows a four-stage workflow of ideation, AI\ngeneration, debugging, and review. This accelerates iteration, supports\ncreativity, and lowers barriers to participation. However, professionals\nreported challenges of code unreliability, integration, and AI over-reliance.\nWe find tensions between efficiency-driven prototyping (\"intending the right\ndesign\") and reflection (\"designing the right intention\"), introducing new\nasymmetries in trust, responsibility, and social stigma within teams. Through\nthe lens of responsible human-AI collaboration for AI-assisted UX design and\ndevelopment, we contribute a deeper understanding of deskilling, ownership and\ndisclosure, and creativity safeguarding in the age of vibe coding.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10652v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10652v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.444,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.284,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper examines the use of generative AI in UX design workflows, focusing on interviews and practical implications like collaboration and challenges, but it does not discuss AI training methods, human feedback for model alignment, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes AI tools for generating code and prototypes through natural language prompts and iterative workflows, but it does not mention diffusion models, multi-step logical reasoning processes, or any adaptation of diffusion for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10653",
      "title": "SCOR: A Framework for Responsible AI Innovation in Digital Ecosystems",
      "authors": [
        "Mohammad Saleh Torkestani",
        "Taha Mansouri"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "AI-driven digital ecosystems span diverse stakeholders including technology\nfirms, regulators, accelerators and civil society, yet often lack cohesive\nethical governance. This paper proposes a four-pillar framework (SCOR) to embed\naccountability, fairness, and inclusivity across such multi-actor networks.\nLeveraging a design science approach, we develop a Shared Ethical Charter(S),\nstructured Co-Design and Stakeholder Engagement protocols(C), a system of\nContinuous Oversight and Learning(O), and Adaptive Regulatory Alignment\nstrategies(R). Each component includes practical guidance, from lite modules\nfor resource-constrained start-ups to in-depth auditing systems for larger\nconsortia. Through illustrative vignettes in healthcare, finance, and smart\ncity contexts, we demonstrate how the framework can harmonize organizational\nculture, leadership incentives, and cross-jurisdictional compliance. Our\nmixed-method KPI design further ensures that quantitative targets are\ncomplemented by qualitative assessments of user trust and cultural change. By\nuniting ethical principles with scalable operational structures, this paper\noffers a replicable pathway toward responsible AI innovation in complex digital\necosystems.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10653v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10653v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.292,
      "distributed_training_score": 0.329,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10656",
      "title": "Self-Supervised Goal-Reaching Results in Multi-Agent Cooperation and\n  Exploration",
      "authors": [
        "Chirayu Nimonkar",
        "Shlok Shah",
        "Catherine Ji",
        "Benjamin Eysenbach"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "For groups of autonomous agents to achieve a particular goal, they must\nengage in coordination and long-horizon reasoning. However, designing reward\nfunctions to elicit such behavior is challenging. In this paper, we study how\nself-supervised goal-reaching techniques can be leveraged to enable agents to\ncooperate. The key idea is that, rather than have agents maximize some scalar\nreward, agents aim to maximize the likelihood of visiting a certain goal. This\nproblem setting enables human users to specify tasks via a single goal state\nrather than implementing a complex reward function. While the feedback signal\nis quite sparse, we will demonstrate that self-supervised goal-reaching\ntechniques enable agents to learn from such feedback. On MARL benchmarks, our\nproposed method outperforms alternative approaches that have access to the same\nsparse reward signal as our method. While our method has no explicit mechanism\nfor exploration, we observe that self-supervised multi-agent goal-reaching\nleads to emergent cooperation and exploration in settings where alternative\napproaches never witness a single successful trial.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10656v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10656v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.444,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.366,
      "datasets_score": 0.293,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on self-supervised goal-reaching in multi-agent reinforcement learning, where agents learn to reach a specified goal state without relying on human-ranked data or a separate reward model trained on human preferences. While a human may provide the initial goal state, this is not equivalent to the core elements of RLHF, such as using human feedback to train a reward model for fine-tuning. Thus, the paper does not align with RLHF methodologies.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10660",
      "title": "ZapGPT: Free-form Language Prompting for Simulated Cellular Control",
      "authors": [
        "Nam H. Le",
        "Patrick Erickson",
        "Yanbo Zhang",
        "Michael Levin",
        "Josh Bongard"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Human language is one of the most expressive tools for conveying intent, yet\nmost artificial or biological systems lack mechanisms to interpret or respond\nmeaningfully to it. Bridging this gap could enable more natural forms of\ncontrol over complex, decentralized systems. In AI and artificial life, recent\nwork explores how language can specify high-level goals, but most systems still\ndepend on engineered rewards, task-specific supervision, or rigid command sets,\nlimiting generalization to novel instructions. Similar constraints apply in\nsynthetic biology and bioengineering, where the locus of control is often\ngenomic rather than environmental perturbation.\n  A key open question is whether artificial or biological collectives can be\nguided by free-form natural language alone, without task-specific tuning or\ncarefully designed evaluation metrics. We provide one possible answer here by\nshowing, for the first time, that simple agents' collective behavior can be\nguided by free-form language prompts: one AI model transforms an imperative\nprompt into an intervention that is applied to simulated cells; a second AI\nmodel scores how well the prompt describes the resulting cellular dynamics; and\nthe former AI model is evolved to improve the scores generated by the latter.\n  Unlike previous work, our method does not require engineered fitness\nfunctions or domain-specific prompt design. We show that the evolved system\ngeneralizes to unseen prompts without retraining. By treating natural language\nas a control layer, the system suggests a future in which spoken or written\nprompts could direct computational, robotic, or biological systems to desired\nbehaviors. This work provides a concrete step toward this vision of AI-biology\npartnerships, in which language replaces mathematical objective functions,\nfixed rules, and domain-specific programming.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10660v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10660v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.41,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.332,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes a system where an AI model (P2I) is optimized using evolutionary strategies based on scores from a vision-language model (VLM), without any mention of human feedback, human-ranked data, or a reward model trained on human preferences. RLHF specifically requires human involvement in providing feedback for alignment, which is absent here, making the paper's method distinct from RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on mapping language prompts to interventions in simulated cellular dynamics using evolutionary strategies and a VLM for scoring, with no reference to diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning. There is no component involving holistic correction of a Chain-of-Thought or adaptation of diffusion for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10682",
      "title": "LLM in the Middle: A Systematic Review of Threats and Mitigations to\n  Real-World LLM-based Systems",
      "authors": [
        "Vitor Hugo Galhardo Moia",
        "Igor Jochem Sanz",
        "Gabriel Antonio Fontes Rebello",
        "Rodrigo Duarte de Meneses",
        "Briland Hitaj",
        "Ulf Lindqvist"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.ET (Emerging Technologies)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The success and wide adoption of generative AI (GenAI), particularly large\nlanguage models (LLMs), has attracted the attention of cybercriminals seeking\nto abuse models, steal sensitive data, or disrupt services. Moreover, providing\nsecurity to LLM-based systems is a great challenge, as both traditional threats\nto software applications and threats targeting LLMs and their integration must\nbe mitigated. In this survey, we shed light on security and privacy concerns of\nsuch LLM-based systems by performing a systematic review and comprehensive\ncategorization of threats and defensive strategies considering the entire\nsoftware and LLM life cycles. We analyze real-world scenarios with distinct\ncharacteristics of LLM usage, spanning from development to operation. In\naddition, threats are classified according to their severity level and to which\nscenarios they pertain, facilitating the identification of the most relevant\nthreats. Recommended defense strategies are systematically categorized and\nmapped to the corresponding life cycle phase and possible attack strategies\nthey attenuate. This work paves the way for consumers and vendors to understand\nand efficiently mitigate risks during integration of LLMs in their respective\nsolutions or organizations. It also enables the research community to benefit\nfrom the discussion of open challenges and edge cases that may hinder the\nsecure and privacy-preserving adoption of LLM-based systems.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10682v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10682v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.438,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.353,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a systematic review of security and privacy threats to LLM-based systems, including categorization of threats, defensive strategies, and analysis across software and LLM life cycles. It does not mention, discuss, or involve Reinforcement Learning from Human Feedback (RLHF), which is a specific technique for fine-tuning AI models using human-ranked data and reinforcement learning. The focus is solely on risks and mitigations, not on training methodologies like RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10683",
      "title": "A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks\n  to Large Language Models for Image Classification and Segmentation of Brain\n  Tumors on MRI",
      "authors": [
        "Felicia Liu",
        "Jay J. Yoo",
        "Farzad Khalvati"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have shown strong performance in text-based\nhealthcare tasks. However, their utility in image-based applications remains\nunexplored. We investigate the effectiveness of LLMs for medical imaging tasks,\nspecifically glioma classification and segmentation, and compare their\nperformance to that of traditional convolutional neural networks (CNNs). Using\nthe BraTS 2020 dataset of multi-modal brain MRIs, we evaluated a\ngeneral-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and after\nfine-tuning, and benchmarked its performance against custom 3D CNNs. For glioma\nclassification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy and\nbalanced precision and recall. The general LLM reached 76% accuracy but\nsuffered from a specificity of only 18%, often misclassifying Low-Grade tumors.\nFine-tuning improved specificity to 55%, but overall performance declined\n(e.g., accuracy dropped to 72%). For segmentation, three methods - center\npoint, bounding box, and polygon extraction, were implemented. CNNs accurately\nlocalized gliomas, though small tumors were sometimes missed. In contrast, LLMs\nconsistently clustered predictions near the image center, with no distinction\nof glioma size, location, or placement. Fine-tuning improved output formatting\nbut failed to meaningfully enhance spatial accuracy. The bounding polygon\nmethod yielded random, unstructured outputs. Overall, CNNs outperformed LLMs in\nboth tasks. LLMs showed limited spatial understanding and minimal improvement\nfrom fine-tuning, indicating that, in their current form, they are not\nwell-suited for image-based tasks. More rigorous fine-tuning or alternative\ntraining strategies may be needed for LLMs to achieve better performance,\nrobustness, and utility in the medical space.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10683v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10683v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.369,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates the performance of LLMs and CNNs for brain tumor classification and segmentation on MRI images, focusing on fine-tuning and comparisons in vision-based tasks. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10685",
      "title": "Pluralistic Alignment for Healthcare: A Role-Driven Framework",
      "authors": [
        "Jiayou Zhong",
        "Anudeex Shetty",
        "Chao Jia",
        "Xuanrui Lin",
        "Usman Naseem"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "As large language models are increasingly deployed in sensitive domains such\nas healthcare, ensuring their outputs reflect the diverse values and\nperspectives held across populations is critical. However, existing alignment\napproaches, including pluralistic paradigms like Modular Pluralism, often fall\nshort in the health domain, where personal, cultural, and situational factors\nshape pluralism. Motivated by the aforementioned healthcare challenges, we\npropose a first lightweight, generalizable, pluralistic alignment approach,\nEthosAgents, designed to simulate diverse perspectives and values. We\nempirically show that it advances the pluralistic alignment for all three modes\nacross seven varying-sized open and closed models. Our findings reveal that\nhealth-related pluralism demands adaptable and normatively aware approaches,\noffering insights into how these models can better respect diversity in other\nhigh-stakes domains.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10685v2",
      "pdf_url": "http://arxiv.org/pdf/2509.10685v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.479,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.355,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper references reinforcement learning from human feedback (RLHF) in the introduction as an existing alignment technique that improves safety and helpfulness but falls short in addressing diversity in healthcare. However, the main contribution—proposing the EthosAgents framework for pluralistic alignment—does not involve RLHF, as it focuses on a lightweight, role-driven simulation method without fine-tuning or human feedback-based training. Thus, RLHF is mentioned only as background context, making the paper tangentially relevant.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10687",
      "title": "Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video\n  Generation",
      "authors": [
        "Hao Zhang",
        "Chun-Han Yao",
        "Simon Donné",
        "Narendra Ahuja",
        "Varun Jampani"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present Stable Part Diffusion 4D (SP4D), a framework for generating paired\nRGB and kinematic part videos from monocular inputs. Unlike conventional part\nsegmentation methods that rely on appearance-based semantic cues, SP4D learns\nto produce kinematic parts - structural components aligned with object\narticulation and consistent across views and time. SP4D adopts a dual-branch\ndiffusion model that jointly synthesizes RGB frames and corresponding part\nsegmentation maps. To simplify the architecture and flexibly enable different\npart counts, we introduce a spatial color encoding scheme that maps part masks\nto continuous RGB-like images. This encoding allows the segmentation branch to\nshare the latent VAE from the RGB branch, while enabling part segmentation to\nbe recovered via straightforward post-processing. A Bidirectional Diffusion\nFusion (BiDiFuse) module enhances cross-branch consistency, supported by a\ncontrastive part consistency loss to promote spatial and temporal alignment of\npart predictions. We demonstrate that the generated 2D part maps can be lifted\nto 3D to derive skeletal structures and harmonic skinning weights with few\nmanual adjustments. To train and evaluate SP4D, we construct KinematicParts20K,\na curated dataset of over 20K rigged objects selected and processed from\nObjaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part\nvideo sequences. Experiments show that SP4D generalizes strongly to diverse\nscenarios, including real-world videos, novel generated objects, and rare\narticulated poses, producing kinematic-aware outputs suitable for downstream\nanimation and motion-related tasks.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10687v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10687v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.466,
      "distributed_training_score": 0.35,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for generating RGB and kinematic part videos, which involves iterative refinement processes. However, it does not adapt diffusion for multi-step logical reasoning or treat a 'Chain-of-Thought' as an entity for holistic correction. Instead, it applies diffusion to visual generation tasks, making it only loosely connected to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10691",
      "title": "Privacy-Preserving Decentralized Federated Learning via Explainable\n  Adaptive Differential Privacy",
      "authors": [
        "Fardin Jalil Piran",
        "Zhiling Chen",
        "Yang Zhang",
        "Qianyu Zhou",
        "Jiong Tang",
        "Farhad Imani"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Decentralized federated learning faces privacy risks because model updates\ncan leak data through inference attacks and membership inference, a concern\nthat grows over many client exchanges. Differential privacy offers principled\nprotection by injecting calibrated noise so confidential information remains\nsecure on resource-limited IoT devices. Yet without transparency, black-box\ntraining cannot track noise already injected by previous clients and rounds,\nwhich forces worst-case additions and harms accuracy. We propose PrivateDFL, an\nexplainable framework that joins hyperdimensional computing with differential\nprivacy and keeps an auditable account of cumulative noise so each client adds\nonly the difference between the required noise and what has already been\naccumulated. We evaluate on MNIST, ISOLET, and UCI-HAR to span image, signal,\nand tabular modalities, and we benchmark against transformer-based and deep\nlearning-based baselines trained centrally with Differentially Private\nStochastic Gradient Descent (DP-SGD) and Renyi Differential Privacy (RDP).\nPrivateDFL delivers higher accuracy, lower latency, and lower energy across IID\nand non-IID partitions while preserving formal (epsilon, delta) guarantees and\noperating without a central server. For example, under non-IID partitions,\nPrivateDFL achieves 24.42% higher accuracy than the Vision Transformer on MNIST\nwhile using about 10x less training time, 76x lower inference latency, and 11x\nless energy, and on ISOLET it exceeds Transformer accuracy by more than 80%\nwith roughly 10x less training time, 40x lower inference latency, and 36x less\ntraining energy. Future work will extend the explainable accounting to\nadversarial clients and adaptive topologies with heterogeneous privacy budgets.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10691v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10691v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.457,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on privacy-preserving decentralized federated learning using differential privacy and explainable AI, with no mention of reinforcement learning, human feedback, reward models, or aligning AI with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution involves decentralized federated learning, a form of distributed training, where model updates are shared across multiple devices in a peer-to-peer manner, addressing parallel computing and multi-node efficiency without a central server.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces PrivateDFL, a framework for privacy-preserving decentralized federated learning that integrates hyperdimensional computing with adaptive differential privacy to track cumulative noise and add only necessary amounts, thereby balancing privacy and utility. Evaluated on datasets like MNIST, ISOLET, and UCI-HAR, PrivateDFL achieves higher accuracy, lower latency, and reduced energy consumption compared to baselines, while maintaining formal privacy guarantees and operating without a central server.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by integrating explainable AI with differential privacy to adaptively track cumulative noise in decentralized federated learning, offering a clever combination of existing ideas rather than a completely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like privacy-preserving machine learning and IoT applications due to its demonstrated efficiency and accuracy gains, though its influence may be limited to specific decentralized systems.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to privacy in federated learning with practical benefits, making it essential for researchers in AI security and IoT, though not groundbreaking enough for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/9b2b19d92e36010e83a2809736b5ed3cdc6ad959",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 5,
      "average_h_index": 2.3333333333333335,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Fardin Jalil Piran",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2310441046"
        },
        {
          "name": "Zhiling Chen",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2316088367"
        },
        {
          "name": "Yang Zhang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2145955885"
        },
        {
          "name": "Qianyu Zhou",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2248760418"
        },
        {
          "name": "Jiong Tang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2248646185"
        },
        {
          "name": "Farhad Imani",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2301986889"
        }
      ]
    },
    {
      "id": "2509.10693",
      "title": "Learning Concave Bid Shading Strategies in Online Auctions via\n  Measure-valued Proximal Optimization",
      "authors": [
        "Iman Nodozi",
        "Djordje Gligorijevic",
        "Abhishek Halder"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "math.OC (Optimization and Control)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "This work proposes a bid shading strategy for first-price auctions as a\nmeasure-valued optimization problem. We consider a standard parametric form for\nbid shading and formulate the problem as convex optimization over the joint\ndistribution of shading parameters. After each auction, the shading parameter\ndistribution is adapted via a regularized Wasserstein-proximal update with a\ndata-driven energy functional. This energy functional is conditional on the\ncontext, i.e., on publisher/user attributes such as domain, ad slot type,\ndevice, or location. The proposed algorithm encourages the bid distribution to\nplace more weight on values with higher expected surplus, i.e., where the win\nprobability and the value gap are both large. We show that the resulting\nmeasure-valued convex optimization problem admits a closed form solution. A\nnumerical example illustrates the proposed method.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10693v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10693v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.342,
      "datasets_score": 0.261,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10695",
      "title": "Kalman Bayesian Transformer",
      "authors": [
        "Haoming Jing",
        "Oren Wright",
        "José M. F. Moura",
        "Yorie Nakahira"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Sequential fine-tuning of transformers is useful when new data arrive\nsequentially, especially with shifting distributions. Unlike batch learning,\nsequential learning demands that training be stabilized despite a small amount\nof data by balancing new information and previously learned knowledge in the\npre-trained models. This challenge is further complicated when training is to\nbe completed in latency-critical environments and learning must additionally\nquantify and be mediated by uncertainty. Motivated by these challenges, we\npropose a novel method that frames sequential fine-tuning as a posterior\ninference problem within a Bayesian framework. Our approach integrates\nclosed-form moment propagation of random variables, Kalman Bayesian Neural\nNetworks, and Taylor approximations of the moments of softmax functions. By\nexplicitly accounting for pre-trained models as priors and adaptively balancing\nthem against new information based on quantified uncertainty, our method\nachieves robust and data-efficient sequential learning. The effectiveness of\nour method is demonstrated through numerical simulations involving sequential\nadaptation of a decision transformer to tasks characterized by distribution\nshifts and limited memory resources.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10695v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10695v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.375,
      "datasets_score": 0.256,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a Kalman Bayesian Transformer for sequential fine-tuning of transformers, focusing on Bayesian inference, uncertainty quantification, and efficient learning under distribution shifts. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as it centers on transformer adaptation rather than generative or reasoning-specific diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10698",
      "title": "CrunchLLM: Multitask LLMs for Structured Business Reasoning and Outcome\n  Prediction",
      "authors": [
        "Rabeya Tus Sadia",
        "Qiang Cheng"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Predicting the success of start-up companies, defined as achieving an exit\nthrough acquisition or IPO, is a critical problem in entrepreneurship and\ninnovation research. Datasets such as Crunchbase provide both structured\ninformation (e.g., funding rounds, industries, investor networks) and\nunstructured text (e.g., company descriptions), but effectively leveraging this\nheterogeneous data for prediction remains challenging. Traditional machine\nlearning approaches often rely only on structured features and achieve moderate\naccuracy, while large language models (LLMs) offer rich reasoning abilities but\nstruggle to adapt directly to domain-specific business data. We present\n\\textbf{CrunchLLM}, a domain-adapted LLM framework for startup success\nprediction. CrunchLLM integrates structured company attributes with\nunstructured textual narratives and applies parameter-efficient fine-tuning\nstrategies alongside prompt optimization to specialize foundation models for\nentrepreneurship data. Our approach achieves accuracy exceeding 80\\% on\nCrunchbase startup success prediction, significantly outperforming traditional\nclassifiers and baseline LLMs. Beyond predictive performance, CrunchLLM\nprovides interpretable reasoning traces that justify its predictions, enhancing\ntransparency and trustworthiness for financial and policy decision makers. This\nwork demonstrates how adapting LLMs with domain-aware fine-tuning and\nstructured--unstructured data fusion can advance predictive modeling of\nentrepreneurial outcomes. CrunchLLM contributes a methodological framework and\na practical tool for data-driven decision making in venture capital and\ninnovation policy.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10698v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10698v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.395,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.467,
      "distributed_training_score": 0.352,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on adapting large language models (LLMs) for startup success prediction through fine-tuning, prompt optimization, and integration of structured and unstructured data. It mentions interpretable reasoning traces, which relate to Chain-of-Thought processes, but there is no reference to diffusion models, iterative refinement for logical tasks, or treating reasoning paths as entities for holistic correction. Thus, the paper does not involve diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10704",
      "title": "Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration",
      "authors": [
        "Xingchen Wan",
        "Han Zhou",
        "Ruoxi Sun",
        "Hootan Nakhost",
        "Ke Jiang",
        "Rajarishi Sinha",
        "Sercan Ö. Arık"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-image (T2I) models, while offering immense creative potential, are\nhighly reliant on human intervention, posing significant usability challenges\nthat often necessitate manual, iterative prompt engineering over often\nunderspecified prompts. This paper introduces Maestro, a novel self-evolving\nimage generation system that enables T2I models to autonomously self-improve\ngenerated images through iterative evolution of prompts, using only an initial\nprompt. Maestro incorporates two key innovations: 1) self-critique, where\nspecialized multimodal LLM (MLLM) agents act as 'critics' to identify\nweaknesses in generated images, correct for under-specification, and provide\ninterpretable edit signals, which are then integrated by a 'verifier' agent\nwhile preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge\nfor head-to-head comparisons between iteratively generated images, eschewing\nproblematic images, and evolving creative prompt candidates that align with\nuser intents. Extensive experiments on complex T2I tasks using black-box models\ndemonstrate that Maestro significantly improves image quality over initial\nprompts and state-of-the-art automated methods, with effectiveness scaling with\nmore advanced MLLM components. This work presents a robust, interpretable, and\neffective pathway towards self-improving T2I generation.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10704v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10704v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.337,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses pairwise comparisons for image selection, similar to RLHF methods, but it employs AI agents (MLLMs) for judgments without involving human feedback, training a reward model, or using reinforcement learning to fine-tune models.",
      "weak_supervision_justification": "The paper focuses on autonomous prompt refinement for text-to-image generation using existing models, without any aspect of training models with programmatically generated or noisy labels.",
      "diffusion_reasoning_justification": "The paper involves iterative prompt and image refinement, which may relate to diffusion-based T2I models, but it does not adapt diffusion processes for multi-step logical reasoning or treat reasoning paths as holistically correctable entities.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10707",
      "title": "Understanding AI Evaluation Patterns: How Different GPT Models Assess\n  Vision-Language Descriptions",
      "authors": [
        "Sajjad Abdoli",
        "Rudi Cilibrasi",
        "Rima Al-Shikh"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "As AI systems increasingly evaluate other AI outputs, understanding their\nassessment behavior becomes crucial for preventing cascading biases. This study\nanalyzes vision-language descriptions generated by NVIDIA's Describe Anything\nModel and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to\nuncover distinct \"evaluation personalities\" the underlying assessment\nstrategies and biases each model demonstrates. GPT-4o-mini exhibits systematic\nconsistency with minimal variance, GPT-4o excels at error detection, while\nGPT-5 shows extreme conservatism with high variability. Controlled experiments\nusing Gemini 2.5 Pro as an independent question generator validate that these\npersonalities are inherent model properties rather than artifacts. Cross-family\nanalysis through semantic similarity of generated questions reveals significant\ndivergence: GPT models cluster together with high similarity while Gemini\nexhibits markedly different evaluation strategies. All GPT models demonstrate a\nconsistent 2:1 bias favoring negative assessment over positive confirmation,\nthough this pattern appears family-specific rather than universal across AI\narchitectures. These findings suggest that evaluation competence does not scale\nwith general capability and that robust AI assessment requires diverse\narchitectural perspectives.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10707v2",
      "pdf_url": "http://arxiv.org/pdf/2509.10707v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.459,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.47,
      "distributed_training_score": 0.387,
      "datasets_score": 0.453,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on analyzing AI evaluation patterns and biases in models like GPT variants when assessing vision-language descriptions, without any mention of training models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines AI evaluation strategies and does not involve diffusion models, iterative refinement for logical reasoning, or multi-step Chain-of-Thought processes adapted from diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper analyzes a dataset of 762 image descriptions generated by NVIDIA's Describe Anything Model, including benchmarking with tools like DLC-Bench, which relates to dataset evaluation in AI applications, though the primary focus is on AI assessment behaviors rather than dataset creation or curation.",
      "llm_score_status": "completed",
      "summary": "This study investigates the evaluation patterns of different GPT models (GPT-4o, GPT-4o-mini, and GPT-5) when assessing vision-language descriptions generated by NVIDIA's Describe Anything Model, aiming to uncover their distinct \"evaluation personalities,\" inherent biases, and assessment strategies. Through analysis of 762 descriptions, controlled experiments with Gemini 2.5 Pro as an independent question generator, and semantic similarity assessments, the research reveals that GPT-4o-mini is highly consistent, GPT-4o excels in error detection, and GPT-5 is conservative with variability; it also identifies a consistent 2:1 bias towards negative assessments in GPT models, emphasizing that evaluation competence does not scale with general capabilities and that diverse AI architectures are essential for robust assessment.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by systematically analyzing and categorizing the inherent evaluation strategies and biases of different GPT models, offering a clever application of existing AI evaluation techniques to reveal \"evaluation personalities.\" While it doesn't introduce a entirely new problem, it addresses a known issue in AI biases through a fresh methodological approach.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI ethics and evaluation, as it provides insights into biases in AI assessment that could improve future model development and deployment. However, its influence may be limited to specific areas of AI research rather than having widespread commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers high-quality insights into AI evaluation biases and patterns, making it a valuable contribution for researchers in artificial intelligence to enhance understanding and practices in model assessment. It is significant but not essential for all readers, positioning it just below a must-read status.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/12ae57c71c0023d5b6dcaebd761aaf287d0ce473",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 15,
      "average_h_index": 5.333333333333333,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Sajjad Abdoli",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2379259647"
        },
        {
          "name": "Rudi Cilibrasi",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/2702975"
        },
        {
          "name": "Rima Al-Shikh",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380529159"
        }
      ]
    },
    {
      "id": "2509.10710",
      "title": "SegSLR: Promptable Video Segmentation for Isolated Sign Language\n  Recognition",
      "authors": [
        "Sven Schreiber",
        "Noha Sarhan",
        "Simone Frintrop",
        "Christian Wilms"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Isolated Sign Language Recognition (ISLR) approaches primarily rely on RGB\ndata or signer pose information. However, combining these modalities often\nresults in the loss of crucial details, such as hand shape and orientation, due\nto imprecise representations like bounding boxes. Therefore, we propose the\nISLR system SegSLR, which combines RGB and pose information through promptable\nzero-shot video segmentation. Given the rough localization of the hands and the\nsigner's body from pose information, we segment the respective parts through\nthe video to maintain all relevant shape information. Subsequently, the\nsegmentations focus the processing of the RGB data on the most relevant body\nparts for ISLR. This effectively combines RGB and pose information. Our\nevaluation on the complex ChaLearn249 IsoGD dataset shows that SegSLR\noutperforms state-of-the-art methods. Furthermore, ablation studies indicate\nthat SegSLR strongly benefits from focusing on the signer's body and hands,\njustifying our design choices.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10710v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10710v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.311,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10723",
      "title": "Dark Patterns Meet GUI Agents: LLM Agent Susceptibility to Manipulative\n  Interfaces and the Role of Human Oversight",
      "authors": [
        "Jingyu Tang",
        "Chaoran Chen",
        "Jiawen Li",
        "Zhiping Zhang",
        "Bingcan Guo",
        "Ibrahim Khalilov",
        "Simret Araya Gebreegziabher",
        "Bingsheng Yao",
        "Dakuo Wang",
        "Yanfang Ye",
        "Tianshi Li",
        "Ziang Xiao",
        "Yaxing Yao",
        "Toby Jia-Jun Li"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The dark patterns, deceptive interface designs manipulating user behaviors,\nhave been extensively studied for their effects on human decision-making and\nautonomy. Yet, with the rising prominence of LLM-powered GUI agents that\nautomate tasks from high-level intents, understanding how dark patterns affect\nagents is increasingly important. We present a two-phase empirical study\nexamining how agents, human participants, and human-AI teams respond to 16\ntypes of dark patterns across diverse scenarios. Phase 1 highlights that agents\noften fail to recognize dark patterns, and even when aware, prioritize task\ncompletion over protective action. Phase 2 revealed divergent failure modes:\nhumans succumb due to cognitive shortcuts and habitual compliance, while agents\nfalter from procedural blind spots. Human oversight improved avoidance but\nintroduced costs such as attentional tunneling and cognitive load. Our findings\nshow neither humans nor agents are uniformly resilient, and collaboration\nintroduces new vulnerabilities, suggesting design needs for transparency,\nadjustable autonomy, and oversight.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10723v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10723v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.455,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.304,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper primarily focuses on empirical studies of GUI agents' susceptibility to dark patterns, comparing agents, humans, and human-AI teams, with recommendations for design improvements. It mentions future directions like new reward functions to balance risks and task completion, which could indirectly relate to reinforcement learning concepts, but it does not involve or discuss systems that use human-ranked data to train a reward model for fine-tuning AI, as defined in RLHF. Thus, the connection is loose and not central to the paper's contributions.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10740",
      "title": "How are Scientific Concepts Birthed? Typing Rules of Concept Formation\n  in Theoretical Physics Reasoning",
      "authors": [
        "Omar Aguilar",
        "Anthony Aguirre"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This work aims to formalize some of the ways scientific concepts are formed\nin the process of theoretical physics discovery. Since this may at first seem\nlike a task beyond the scope of the exact sciences (natural and formal\nsciences), we begin by presenting arguments for why scientific concept\nformation can be formalized. Then, we introduce type theory as a natural and\nwell-suited framework for this formalization. We formalize what we call \"ways\nof discovering new concepts\" including concept distinction, property\npreservation, and concept change, as cognitive typing rules. Next, we apply\nthese cognitive typing rules to two case studies of conceptual discovery in the\nhistory of physics: Einstein's reasoning leading to the impossibility of frozen\nwaves, and his conceptual path to the relativity of time. In these historical\nepisodes, we recast what a physicist might informally call \"ways of discovering\nnew scientific concepts\" as compositional typing rules built from cognitive\ntyping rules - thus formalizing them as scientific discovery mechanisms.\nLastly, we computationally model the type-theoretic reconstruction of\nEinstein's conceptual path to the relativity of time as a program synthesis\ntask.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10740v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10740v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.29,
      "weak_supervision_score": 0.281,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.258,
      "datasets_score": 0.266,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the formalization of scientific concept formation in theoretical physics using type theory and cognitive typing rules, applied to historical case studies and modeled via program synthesis. It does not involve diffusion models, iterative refinement processes for logical tasks, or treating a Chain-of-Thought as a single entity for holistic correction, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10744",
      "title": "Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as\n  Retrieval Sources for Domain Adaptation of Small Language Models",
      "authors": [
        "Ozan Gokdemir",
        "Neil Getty",
        "Robert Underwood",
        "Sandeep Madireddy",
        "Franck Cappello",
        "Arvind Ramanathan",
        "Ian T. Foster",
        "Rick L. Stevens"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As scientific knowledge grows at an unprecedented pace, evaluation benchmarks\nmust evolve to reflect new discoveries and ensure language models are tested on\ncurrent, diverse literature. We propose a scalable, modular framework for\ngenerating multiple-choice question-answering (MCQA) benchmarks directly from\nlarge corpora of scientific papers. Our pipeline automates every stage of MCQA\ncreation, including PDF parsing, semantic chunking, question generation, and\nmodel evaluation. As a case study, we generate more than 16,000 MCQs from\n22,000 open-access articles in radiation and cancer biology. We then evaluate a\nsuite of small language models (1.1B-14B parameters) on these questions,\ncomparing baseline accuracy with retrieval-augmented generation (RAG) from\npaper-derived semantic chunks and from reasoning traces distilled from GPT-4.1.\nWe find that reasoning-trace retrieval consistently improves performance on\nboth synthetic and expert-annotated benchmarks, enabling several small models\nto surpass GPT-4 on the 2023 Astro Radiation and Cancer Biology exam.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10744v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10744v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.41,
      "diffusion_reasoning_score": 0.484,
      "distributed_training_score": 0.403,
      "datasets_score": 0.423,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's framework automates the generation of MCQAs from scientific corpora, using programmatic methods like PDF parsing and question generation, which aligns with weak supervision by relying on noisy or imprecise sources (e.g., automated processes) rather than hand-labeled data. However, it does not explicitly focus on training models with these generated labels, making it moderately relevant rather than central.",
      "diffusion_reasoning_justification": "The paper discusses reasoning traces and retrieval-augmented generation for improving model performance, but it does not involve diffusion-based models, iterative refinement processes, or treating chains-of-thought as entities for multi-step correction. There is no mention of diffusion mechanisms, so it does not relate to this topic.",
      "distributed_training_justification": "The paper mentions a scalable, modular framework designed for high-performance computing platforms to generate benchmarks at scale, which implies some level of parallel processing. However, it does not delve into distributed training algorithms, data partitioning across nodes, or accelerating model training, focusing instead on benchmark creation rather than training processes.",
      "datasets_justification": "The paper's main contribution is the creation and evaluation of a new benchmark dataset of over 16,000 MCQAs derived from scientific literature, including detailed discussions on dataset generation, curation methodologies, and benchmarking evaluations. This directly aligns with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces a scalable, modular framework for automating the generation of multiple-choice question-answering (MCQA) benchmarks from large scientific corpora to address the challenges of keeping evaluations current with rapid scientific advancements. It details a pipeline that includes PDF parsing, semantic chunking, question generation, and model evaluation, applied to radiation and cancer biology to create over 16,000 MCQs from 22,000 articles; key findings show that retrieval-augmented generation using reasoning traces from GPT-4 significantly enhances the performance of small language models (1.1B-14B parameters), often enabling them to outperform GPT-4 on domain-specific exams like the 2023 ASTRO Radiation and Cancer Biology exam.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new, automated pipeline for MCQA benchmark generation that integrates end-to-end processes like semantic chunking and reasoning trace retrieval, significantly advancing the state-of-the-art in dynamic benchmark creation for evolving scientific domains.",
      "impact_score": "High",
      "impact_justification": "The work could broadly influence AI research and applications by enabling scalable benchmark generation and improving domain adaptation of small language models, potentially leading to more efficient models in scientific workflows and increased citations in subfields like computational language.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a high-quality, innovative framework with practical implications for AI evaluation and model improvement, making it essential for researchers in AI and computational language to understand for advancing domain-specific applications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/64f2a3e05ebff14cd88d5777f64abae575941bd1",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 6,
      "average_h_index": 2.125,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Ozan Gokdemir",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2273787345"
        },
        {
          "name": "N. Getty",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/51170522"
        },
        {
          "name": "Robert Underwood",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2359150038"
        },
        {
          "name": "Sandeep Madireddy",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2273091847"
        },
        {
          "name": "Franck Cappello",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380522830"
        },
        {
          "name": "Arvind Ramanathan",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2280837880"
        },
        {
          "name": "Ian T. Foster",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2288265018"
        },
        {
          "name": "Rick L. Stevens",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380524205"
        }
      ]
    },
    {
      "id": "2509.10748",
      "title": "SCOPE: Speech-guided COllaborative PErception Framework for Surgical\n  Scene Segmentation",
      "authors": [
        "Jecia Z. Y. Mao",
        "Francis X Creighton",
        "Russell H Taylor",
        "Manish Sahu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate segmentation and tracking of relevant elements of the surgical scene\nis crucial to enable context-aware intraoperative assistance and decision\nmaking. Current solutions remain tethered to domain-specific, supervised models\nthat rely on labeled data and required domain-specific data to adapt to new\nsurgical scenarios and beyond predefined label categories. Recent advances in\nprompt-driven vision foundation models (VFM) have enabled open-set, zero-shot\nsegmentation across heterogeneous medical images. However, dependence of these\nmodels on manual visual or textual cues restricts their deployment in\nintroperative surgical settings. We introduce a speech-guided collaborative\nperception (SCOPE) framework that integrates reasoning capabilities of large\nlanguage model (LLM) with perception capabilities of open-set VFMs to support\non-the-fly segmentation, labeling and tracking of surgical instruments and\nanatomy in intraoperative video streams. A key component of this framework is a\ncollaborative perception agent, which generates top candidates of VFM-generated\nsegmentation and incorporates intuitive speech feedback from clinicians to\nguide the segmentation of surgical instruments in a natural human-machine\ncollaboration paradigm. Afterwards, instruments themselves serve as interactive\npointers to label additional elements of the surgical scene. We evaluated our\nproposed framework on a subset of publicly available Cataract1k dataset and an\nin-house ex-vivo skull-base dataset to demonstrate its potential to generate\non-the-fly segmentation and tracking of surgical scene. Furthermore, we\ndemonstrate its dynamic capabilities through a live mock ex-vivo experiment.\nThis human-AI collaboration paradigm showcase the potential of developing\nadaptable, hands-free, surgeon-centric tools for dynamic operating-room\nenvironments.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10748v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10748v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.422,
      "diffusion_reasoning_score": 0.394,
      "distributed_training_score": 0.344,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a speech-guided framework for intraoperative segmentation using pre-trained vision foundation models and large language models, enabling zero-shot and interactive capabilities without relying on extensive labeled data. While it critiques traditional supervised methods that depend on hand-labeled datasets and highlights the use of open-set models, it does not directly involve weak supervision techniques, such as programmatically generating noisy labels for training. Instead, the focus is on runtime interaction and adaptation, making it only indirectly related to weak supervision concepts.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10753",
      "title": "HalluField: Detecting LLM Hallucinations via Field-Theoretic Modeling",
      "authors": [
        "Minh Vu",
        "Brian K. Tran",
        "Syed A. Shah",
        "Geigh Zollicoffer",
        "Nhat Hoang-Xuan",
        "Manish Bhattarai"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) exhibit impressive reasoning and\nquestion-answering capabilities. However, they often produce inaccurate or\nunreliable content known as hallucinations. This unreliability significantly\nlimits their deployment in high-stakes applications. Thus, there is a growing\nneed for a general-purpose method to detect hallucinations in LLMs. In this\nwork, we introduce HalluField, a novel field-theoretic approach for\nhallucination detection based on a parametrized variational principle and\nthermodynamics. Inspired by thermodynamics, HalluField models an LLM's response\nto a given query and temperature setting as a collection of discrete likelihood\ntoken paths, each associated with a corresponding energy and entropy. By\nanalyzing how energy and entropy distributions vary across token paths under\nchanges in temperature and likelihood, HalluField quantifies the semantic\nstability of a response. Hallucinations are then detected by identifying\nunstable or erratic behavior in this energy landscape. HalluField is\ncomputationally efficient and highly practical: it operates directly on the\nmodel's output logits without requiring fine-tuning or auxiliary neural\nnetworks. Notably, the method is grounded in a principled physical\ninterpretation, drawing analogies to the first law of thermodynamics.\nRemarkably, by modeling LLM behavior through this physical lens, HalluField\nachieves state-of-the-art hallucination detection performance across models and\ndatasets.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.10753v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10753v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.453,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.372,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a method for detecting hallucinations in LLMs using thermodynamic principles, without any mention of training models with human feedback, reward models, or reinforcement learning techniques. It focuses solely on analysis of existing model outputs, not alignment via RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a field-theoretic and thermodynamic framework to analyze LLM responses for hallucination detection, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined. There is no adaptation of diffusion for Chain-of-Thought or similar tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12248",
      "title": "Humor in Pixels: Benchmarking Large Multimodal Models Understanding of\n  Online Comics",
      "authors": [
        "Yuriel Ryan",
        "Rui Yang Tan",
        "Kenny Tsu Wei Choo",
        "Roy Ka-Wei Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Understanding humor is a core aspect of social intelligence, yet it remains a\nsignificant challenge for Large Multimodal Models (LMMs). We introduce\nPixelHumor, a benchmark dataset of 2,800 annotated multi-panel comics designed\nto evaluate LMMs' ability to interpret multimodal humor and recognize narrative\nsequences. Experiments with state-of-the-art LMMs reveal substantial gaps: for\ninstance, top models achieve only 61% accuracy in panel sequencing, far below\nhuman performance. This underscores critical limitations in current models'\nintegration of visual and textual cues for coherent narrative and humor\nunderstanding. By providing a rigorous framework for evaluating multimodal\ncontextual and narrative reasoning, PixelHumor aims to drive the development of\nLMMs that better engage in natural, socially aware interactions.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.12248v2",
      "pdf_url": "http://arxiv.org/pdf/2509.12248v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.323,
      "datasets_score": 0.429,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of PixelHumor, a new benchmark dataset of 2,800 annotated comics specifically designed for evaluating Large Multimodal Models (LMMs) in humor understanding. It covers dataset creation, curation methodologies (sourcing from creators and annotation for tasks like classification and sequencing), and benchmarking through experiments with state-of-the-art models. This directly aligns with research on creating, analyzing, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces PixelHumor, a benchmark dataset comprising 2,800 annotated multi-panel comics, aimed at evaluating Large Multimodal Models' (LMMs) ability to understand humor through integrating visual and textual elements. The methodology involves sourcing comics from seven creators, annotating them for tasks such as classification, interpretation, and sequential recognition, and testing state-of-the-art LMMs, which revealed significant shortcomings, such as only 61% accuracy in panel sequencing compared to human performance, highlighting the need for improved multimodal reasoning in AI.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark dataset and framework for evaluating humor comprehension in LMMs, addressing a previously underexplored aspect of multimodal AI that advances the state-of-the-art in understanding social intelligence.",
      "impact_score": "High",
      "impact_justification": "The work provides a rigorous benchmark that could drive advancements in LMMs for socially aware interactions, potentially influencing broader AI research in multimodal understanding and humor processing.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution by highlighting critical limitations in current LMMs and providing a new dataset for future improvements, making it essential for researchers in AI, computer vision, and natural language processing to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d9d420075b8aa8246bb785fa7ac529ac201d8e22",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 9,
      "average_h_index": 2.75,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Yuriel Ryan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380610863"
        },
        {
          "name": "Rui Yang Tan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2356877936"
        },
        {
          "name": "K. Choo",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/9511005"
        },
        {
          "name": "Roy Ka-Wei Lee",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2367229125"
        }
      ]
    },
    {
      "id": "2509.12249",
      "title": "Why and How Auxiliary Tasks Improve JEPA Representations",
      "authors": [
        "Jiacan Yu",
        "Siyi Chen",
        "Mingrui Liu",
        "Nono Horiuchi",
        "Vladimir Braverman",
        "Zicheng Xu",
        "Dan Haramati",
        "Randall Balestriero"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Joint-Embedding Predictive Architecture (JEPA) is increasingly used for\nvisual representation learning and as a component in model-based RL, but its\nbehavior remains poorly understood. We provide a theoretical characterization\nof a simple, practical JEPA variant that has an auxiliary regression head\ntrained jointly with latent dynamics. We prove a No Unhealthy Representation\nCollapse theorem: in deterministic MDPs, if training drives both the\nlatent-transition consistency loss and the auxiliary regression loss to zero,\nthen any pair of non-equivalent observations, i.e., those that do not have the\nsame transition dynamics or auxiliary label, must map to distinct latent\nrepresentations. Thus, the auxiliary task anchors which distinctions the\nrepresentation must preserve. Controlled ablations in a counting environment\ncorroborate the theory and show that training the JEPA model jointly with the\nauxiliary head generates a richer representation than training them separately.\nOur work indicates a path to improve JEPA encoders: training them with an\nauxiliary function that, together with the transition dynamics, encodes the\nright equivalence relations.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.12249v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12249v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.341,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on Joint-Embedding Predictive Architecture (JEPA) for visual representation learning and its theoretical analysis in reinforcement learning contexts, including the role of auxiliary tasks. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, which are central to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12250",
      "title": "OnlineHOI: Towards Online Human-Object Interaction Generation and\n  Perception",
      "authors": [
        "Yihong Ji",
        "Yunze Liu",
        "Yiyao Zhuo",
        "Weijiang Yu",
        "Fei Ma",
        "Joshua Huang",
        "Fei Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "The perception and generation of Human-Object Interaction (HOI) are crucial\nfor fields such as robotics, AR/VR, and human behavior understanding. However,\ncurrent approaches model this task in an offline setting, where information at\neach time step can be drawn from the entire interaction sequence. In contrast,\nin real-world scenarios, the information available at each time step comes only\nfrom the current moment and historical data, i.e., an online setting. We find\nthat offline methods perform poorly in an online context. Based on this\nobservation, we propose two new tasks: Online HOI Generation and Perception. To\naddress this task, we introduce the OnlineHOI framework, a network architecture\nbased on the Mamba framework that employs a memory mechanism. By leveraging\nMamba's powerful modeling capabilities for streaming data and the Memory\nmechanism's efficient integration of historical information, we achieve\nstate-of-the-art results on the Core4D and OAKINK2 online generation tasks, as\nwell as the online HOI4D perception task.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.12250v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12250v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.395,
      "weak_supervision_score": 0.324,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.332,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves developing the OnlineHOI framework using the Mamba architecture for online Human-Object Interaction (HOI) generation and perception, incorporating memory mechanisms for handling streaming data. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks, such as treating a Chain-of-Thought as a single entity. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12251",
      "title": "V-Math: An Agentic Approach to the Vietnamese National High School\n  Graduation Mathematics Exams",
      "authors": [
        "Duong Q. Nguyen",
        "Quy P. Nguyen",
        "Nguyen Van Nhon",
        "Quang-Thinh Bui",
        "H. Nguyen-Xuan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "This paper develops an autonomous agentic framework called V-Math that aims\nto assist Vietnamese high school students in preparing for the National High\nSchool Graduation Mathematics Exams (NHSGMEs). The salient framework integrates\nthree specialized AI agents: a specification-matrix-conditioned question\ngenerator, a solver/explainer for detailed step-by-step reasoning, and a\npersonalized tutor that adapts to student performance. Beyond enabling\nself-paced student practice, V-Math supports teachers by generating innovative,\ncompliant exam questions and building diverse, high-quality question banks.\nThis reduces manual workload and enriches instructional resources. We describe\nthe system architecture, focusing on practice modes for learners and\nteacher-oriented features for question generation. Preliminary evaluations\ndemonstrate that V-Math produces matrix-aligned exams with high solution\naccuracy, delivers coherent explanations, and enhances the variety of practice\nmaterials. These results highlight its potential to support scalable, equitable\nmathematics preparation aligned with national standards while also empowering\nteachers through AI-assisted exam creation.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.12251v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12251v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.318,
      "datasets_score": 0.394,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes an agentic framework for mathematics education using AI agents, LLMs, and step-by-step reasoning, but it does not mention or utilize diffusion-based models. There is no evidence of adapting iterative refinement processes from diffusion for logical tasks or treating Chain-of-Thought as a holistically corrected entity, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12253",
      "title": "Physics-Informed Neural Networks vs. Physics Models for Non-Invasive\n  Glucose Monitoring: A Comparative Study Under Realistic Synthetic Conditions",
      "authors": [
        "Riyaadh Gani"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Non-invasive glucose monitors often fail outside the lab because existing\ndatasets ignore hardware noise, environmental drift, and person-to-person\nphysiology. We introduce the first ultra-realistic near-infrared (NIR)\nsimulator that injects 12-bit ADC quantisation, +/-0.1% LED ageing, photodiode\ndark noise, 15-45 C temperature, 30-90% relative humidity, contact-pressure\nvariation, Fitzpatrick I-VI melanin, and diurnal glucose excursions (dawn\nphenomenon). Using this platform (rho glucose-NIR = 0.21), we benchmark six\nmethods: Enhanced Beer-Lambert (physics-engineered ridge regression), three\nphysics-informed neural networks (PINNs), a selective radiative-transfer PINN,\nand a shallow DNN. Beer-Lambert achieves 13.6 mg/dL RMSE, 95.8% Clarke-A and\n93.8% +/-15% accuracy with only 56 parameters and 0.01 ms inference,\noutperforming the best PINN (14.6 mg/dL) and the SDNN baseline (35.1 mg/dL).\nResults overturn the assumption that deeper PINNs dominate and supply an open,\nend-to-end reference stack for rapid prototyping of embedded optical glucose\nsensors.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.12253v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12253v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.307,
      "diffusion_reasoning_score": 0.323,
      "distributed_training_score": 0.341,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12254",
      "title": "DISPLIB: a library of train dispatching problems",
      "authors": [
        "Oddvar Kloster",
        "Bjørnar Luteberget",
        "Carlo Mannino",
        "Giorgio Sartor"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Optimization-based decision support systems have a significant potential to\nreduce delays, and thus improve efficiency on the railways, by automatically\nre-routing and re-scheduling trains after delays have occurred. The operations\nresearch community has dedicated a lot of effort to developing optimization\nalgorithms for this problem, but each study is typically tightly connected with\na specific industrial use case. Code and data are seldom shared publicly. This\nfact hinders reproducibility, and has led to a proliferation of papers\ndescribing algorithms for more or less compatible problem definitions, without\nany real opportunity for readers to assess their relative performance. Inspired\nby the successful communities around MILP, SAT, TSP, VRP, etc., we introduce a\ncommon problem definition and file format, DISPLIB, which captures all the main\nfeatures of train re-routing and re-scheduling. We have gathered problem\ninstances from multiple real-world use cases and made them openly available. In\nthis paper, we describe the problem definition, the industrial instances, and a\nreference solver implementation. This allows any researcher or developer to\nwork on the train dispatching problem without an industrial connection, and\nenables the research community to perform empirical comparisons between\nsolvers. All materials are available online at https://displib.github.io.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.12254v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12254v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.287,
      "weak_supervision_score": 0.264,
      "diffusion_reasoning_score": 0.318,
      "distributed_training_score": 0.369,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12255",
      "title": "Representation Learning on Large Non-Bipartite Transaction Networks\n  using GraphSAGE",
      "authors": [
        "Mihir Tare",
        "Clemens Rattasits",
        "Yiming Wu",
        "Euan Wielewski"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.SI (Social and Information Networks)"
      ],
      "abstract": "Financial institutions increasingly require scalable tools to analyse complex\ntransactional networks, yet traditional graph embedding methods struggle with\ndynamic, real-world banking data. This paper demonstrates the practical\napplication of GraphSAGE, an inductive Graph Neural Network framework, to\nnon-bipartite heterogeneous transaction networks within a banking context.\nUnlike transductive approaches, GraphSAGE scales well to large networks and can\ngeneralise to unseen nodes which is critical for institutions working with\ntemporally evolving transactional data. We construct a transaction network\nusing anonymised customer and merchant transactions and train a GraphSAGE model\nto generate node embeddings. Our exploratory work on the embeddings reveals\ninterpretable clusters aligned with geographic and demographic attributes.\nAdditionally, we illustrate their utility in downstream classification tasks by\napplying them to a money mule detection model where using these embeddings\nimproves the prioritisation of high-risk accounts. Beyond fraud detection, our\nwork highlights the adaptability of this framework to banking-scale networks,\nemphasising its inductive capability, scalability, and interpretability. This\nstudy provides a blueprint for financial organisations to harness graph machine\nlearning for actionable insights in transactional ecosystems.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.12255v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12255v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.339,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12258",
      "title": "EfficientNet-Based Multi-Class Detection of Real, Deepfake, and Plastic\n  Surgery Faces",
      "authors": [
        "Li Kun",
        "Milena Radenkovic"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Currently, deep learning has been utilised to tackle several difficulties in\nour everyday lives. It not only exhibits progress in computer vision but also\nconstitutes the foundation for several revolutionary technologies. Nonetheless,\nsimilar to all phenomena, the use of deep learning in diverse domains has\nproduced a multifaceted interaction of advantages and disadvantages for human\nsociety. Deepfake technology has advanced, significantly impacting social life.\nHowever, developments in this technology can affect privacy, the reputations of\nprominent personalities, and national security via software development. It can\nproduce indistinguishable counterfeit photographs and films, potentially\nimpairing the functionality of facial recognition systems, so presenting a\nsignificant risk.\n  The improper application of deepfake technology produces several detrimental\neffects on society. Face-swapping programs mislead users by altering persons'\nappearances or expressions to fulfil particular aims or to appropriate personal\ninformation. Deepfake technology permeates daily life through such techniques.\nCertain individuals endeavour to sabotage election campaigns or subvert\nprominent political figures by creating deceptive pictures to influence public\nperception, causing significant harm to a nation's political and economic\nstructure.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.12258v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12258v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.306,
      "distributed_training_score": 0.327,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12259",
      "title": "Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) for\n  Diabetes Risk Prediction",
      "authors": [
        "Kenneth G. Young II"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "quant-ph (Quantum Physics)"
      ],
      "abstract": "The Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) is an\ninnovative machine learning framework that harnesses quantum-inspired\ntechniques to predict diabetes risk with exceptional accuracy and efficiency.\nUtilizing the PIMA Indians Diabetes dataset augmented with 2,000 synthetic\nsamples to mitigate class imbalance (total: 2,768 samples, 1,949 positives),\nQISICGM integrates a self-improving concept graph with a stacked ensemble\ncomprising Random Forests (RF), Extra Trees (ET), transformers, convolutional\nneural networks (CNNs), and feed-forward neural networks (FFNNs). This approach\nachieves an out-of-fold (OOF) F1 score of 0.8933 and an AUC of 0.8699,\noutperforming traditional methods. Quantum inspired elements, such as phase\nfeature mapping and neighborhood sequence modeling, enrich feature\nrepresentations, enabling CPU-efficient inference at 8.5 rows per second. This\npaper presents a detailed architecture, theoretical foundations, code insights,\nand performance evaluations, including visualizations from the outputs\nsubfolder. The open-source implementation (v1.0.0) is available at\nhttps://github.com/keninayoung/QISICGM, positioning QISICGM as a potential\nbenchmark for AI-assisted clinical triage in diabetes and beyond. Ultimately,\nthis work emphasizes trustworthy AI through calibration, interpretability, and\nopen-source reproducibility.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.12259v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12259v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.36,
      "datasets_score": 0.383,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.12263",
      "title": "InPhyRe Discovers: Large Multimodal Models Struggle in Inductive\n  Physical Reasoning",
      "authors": [
        "Gautam Sreekumar",
        "Vishnu Naresh Boddeti"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large multimodal models (LMMs) encode universal physical laws observed during\ntraining, such as momentum conservation, as parametric knowledge. It allows\nLMMs to answer physical reasoning queries, such as the outcome of a potential\ncollision event from visual input. However, since parametric knowledge includes\nonly the physical laws seen during training, it is insufficient for reasoning\nwhen the inference scenario violates these physical laws. In contrast, humans\npossess the skill to adapt their physical reasoning to unseen physical\nenvironments from a few visual examples. This ability, which we refer to as\ninductive physical reasoning, is indispensable for LMMs if they are to replace\nhuman agents in safety-critical applications. Despite its importance, existing\nvisual benchmarks evaluate only the parametric knowledge in LMMs, and not\ninductive physical reasoning. To this end, we propose InPhyRe, the first visual\nquestion answering benchmark to measure inductive physical reasoning in LMMs.\nInPhyRe evaluates LMMs on their ability to predict the outcome of collision\nevents in algorithmically generated synthetic collision videos. By inspecting\n13 LMMs, InPhyRe informs us that (1) LMMs struggle to apply their limited\nparametric knowledge about universal physical laws to reasoning, (2) inductive\nphysical reasoning in LMMs is weak when demonstration samples violate universal\nphysical laws, and (3) inductive physical reasoning in LMMs suffers from\nlanguage bias and largely ignores the visual inputs, questioning the\ntrustworthiness of LMMs regarding visual inputs.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.12263v1",
      "pdf_url": "http://arxiv.org/pdf/2509.12263v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.463,
      "distributed_training_score": 0.338,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a benchmark for evaluating inductive physical reasoning in large multimodal models, focusing on their ability to adapt to unseen physical scenarios. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.13342",
      "title": "Real World Robotic Exploration using Deep Neural Networks Trained in\n  Photorealistic Reconstructed Environments",
      "authors": [
        "Isaac Ronald Ward"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this work, an existing deep neural network approach for determining a\nrobot's pose from visual information (RGB images) is modified, improving its\nlocalization performance without impacting its ease of training. Explicitly,\nthe network's loss function is extended in a manner which intuitively combines\nthe positional and rotational error in order to increase robustness to\nperceptual aliasing. An improvement in the localization accuracy for indoor\nscenes is observed: with decreases of up to 9.64% and 2.99% in the median\npositional and rotational error respectively, when compared to the unmodified\nnetwork.\n  Additionally, photogrammetry data is used to produce a pose-labelled dataset\nwhich allows the above model to be trained on a local environment, resulting in\nlocalization accuracies of 0.11m & 0.89 degrees. This trained model forms the\nbasis of a navigation algorithm, which is tested in real-time on a TurtleBot (a\nwheeled robotic device). As such, this work introduces a full pipeline for\ncreating a robust navigational algorithm for any given real world indoor scene;\nthe only requirement being a collection of images from the scene, which can be\ncaptured in as little as 330 seconds of",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.13342v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13342v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.412,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.381,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper involves programmatically generating a pose-labelled dataset using photogrammetry from images, which aligns with weak supervision as it relies on automated, potentially noisy or imprecise sources for labels rather than manual annotation. This enables training without perfectly hand-labeled data, as described in the abstract. However, weak supervision is not the core focus; the primary contributions are the network modification for improved localization and its application to robotic navigation, making the topic relevant but secondary.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper modifies an existing deep neural network for robot pose estimation by extending its loss function to combine positional and rotational errors, enhancing robustness to perceptual aliasing and improving localization accuracy in indoor environments. The authors use photogrammetry to create a pose-labelled dataset from a real office scene, train the model to achieve high accuracies (0.11m positional and 0.89 degrees rotational), and demonstrate its integration into a real-time navigation algorithm on a TurtleBot, resulting in a complete pipeline for robust indoor robotic navigation using minimal image data.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by extending the loss function of an existing network to better handle perceptual aliasing, which is a clever combination of ideas rather than a entirely new architecture or problem. While it introduces a new dataset and pipeline, the core technique builds on prior work without significantly advancing the state-of-the-art.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of indoor robotic navigation due to its practical improvements and demonstration on real hardware, potentially influencing developments in robotics and AI applications. However, its specific focus on indoor scenes limits its broader applicability across diverse fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution with practical insights into improving robotic localization and navigation, making it essential for researchers in robotics and AI to be aware of. While not groundbreaking, its real-world demonstration and methodological enhancements provide useful knowledge for relevant specialists.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1396e9c2886ba501b3282f7a14c799a02712b821",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Isaac Ronald Ward",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380685701"
        }
      ]
    },
    {
      "id": "2509.13345",
      "title": "Accuracy Paradox in Large Language Models: Regulating Hallucination\n  Risks in Generative AI",
      "authors": [
        "Zihao Li",
        "Weiwei Yi",
        "Jiahong Chen"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.HC (Human-Computer Interaction)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "As Large Language Models (LLMs) permeate everyday decision-making, their\nepistemic and societal risks demand urgent scrutiny. Hallucinations, the\ngeneration of fabricated, misleading, oversimplified or untrustworthy outputs,\nhas emerged as imperative challenges. While regulatory, academic, and technical\ndiscourse position accuracy as the principal benchmark for mitigating such\nharms, this article contends that overreliance on accuracy misdiagnoses the\nproblem and has counterproductive effect: the accuracy paradox. Drawing on\ninterdisciplinary literatures, this article develops a taxonomy of\nhallucination types and shows the paradox along three intertwining dimensions:\noutputs, individuals and society. First, accuracy functions as a superficial\nproxy for reliability, incentivising the optimisation of rhetorical fluency and\nsurface-level correctness over epistemic trustworthiness. This encourages\npassive user trust in outputs that appear accurate but epistemically untenable.\nSecond, accuracy as a singular metric fails to detect harms that are not\nfactually false but are nonetheless misleading, value-laden, or socially\ndistorting, including consensus illusions, sycophantic alignment, and subtle\nmanipulation. Third, regulatory overemphasis on accuracy obscures the wider\nsocietal consequences of hallucination, including social sorting, privacy\nviolations, equity harms, epistemic convergence that marginalises dissent,\nreduces pluralism, and causes social deskilling. By examining the EU AI Act,\nGDPR, and DSA, the article argues that current regulations are not yet\nstructurally equipped to address these epistemic, relational, and systemic\nharms and exacerbated by the overreliance on accuracy. By exposing such\nconceptual and practical challenges, this article calls for a fundamental shift\ntowards pluralistic, context-aware, and manipulation-resilient approaches to AI\ntrustworthy governance.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.13345v1",
      "pdf_url": "http://arxiv.org/pdf/2509.13345v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.469,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.373,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses AI alignment issues, such as sycophantic alignment, which could indirectly relate to methods like RLHF that aim to align models with human preferences. However, it does not mention RLHF specifically, nor does it describe systems using human-ranked data for reward modeling and reinforcement learning. The focus is on conceptual and regulatory aspects of hallucinations, not technical training methods.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses hallucinations and accuracy in LLMs, including a taxonomy and societal risks, but it does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning for Chain-of-Thought. There is no component related to adapting diffusion for complex tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14255",
      "title": "Opening the Black Box: Interpretable LLMs via Semantic Resonance\n  Architecture",
      "authors": [
        "Ivan Ternovtsii"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) achieve remarkable performance but remain\ndifficult to interpret. Mixture-of-Experts (MoE) models improve efficiency\nthrough sparse activation, yet typically rely on opaque, learned gating\nfunctions. While similarity-based routing (Cosine Routers) has been explored\nfor training stabilization, its potential for inherent interpretability remains\nlargely untapped. We introduce the Semantic Resonance Architecture (SRA), an\nMoE approach designed to ensure that routing decisions are inherently\ninterpretable. SRA replaces learned gating with a Chamber of Semantic Resonance\n(CSR) module, which routes tokens based on cosine similarity with trainable\nsemantic anchors. We also introduce a novel Dispersion Loss that encourages\northogonality among anchors to enforce diverse specialization. Experiments on\nWikiText-103 demonstrate that SRA achieves a validation perplexity of 13.41,\noutperforming both a dense baseline (14.13) and a Standard MoE baseline (13.53)\nunder matched active parameter constraints (29.0M). Crucially, SRA exhibits\nsuperior expert utilization (1.0% dead experts vs. 14.8% in the Standard MoE)\nand develops distinct, semantically coherent specialization patterns, unlike\nthe noisy specialization observed in standard MoEs. This work establishes\nsemantic routing as a robust methodology for building more transparent and\ncontrollable language models.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.14255v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14255v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.403,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.474,
      "distributed_training_score": 0.366,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on developing an interpretable Mixture-of-Experts architecture for LLMs using semantic routing and a Dispersion Loss, with no mention of human feedback, reward models, or reinforcement learning techniques for aligning models with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses interpretability in LLMs through semantic routing in a Mixture-of-Experts setup, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14256",
      "title": "JU-NLP at Touché: Covert Advertisement in Conversational AI-Generation\n  and Detection Strategies",
      "authors": [
        "Arka Dutta",
        "Agrik Majumdar",
        "Sombrata Biswas",
        "Dipankar Das",
        "Sivaji Bandyopadhyay"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper proposes a comprehensive framework for the generation of covert\nadvertisements within Conversational AI systems, along with robust techniques\nfor their detection. It explores how subtle promotional content can be crafted\nwithin AI-generated responses and introduces methods to identify and mitigate\nsuch covert advertising strategies. For generation (Sub-Task~1), we propose a\nnovel framework that leverages user context and query intent to produce\ncontextually relevant advertisements. We employ advanced prompting strategies\nand curate paired training data to fine-tune a large language model (LLM) for\nenhanced stealthiness. For detection (Sub-Task~2), we explore two effective\nstrategies: a fine-tuned CrossEncoder (\\texttt{all-mpnet-base-v2}) for direct\nclassification, and a prompt-based reformulation using a fine-tuned\n\\texttt{DeBERTa-v3-base} model. Both approaches rely solely on the response\ntext, ensuring practicality for real-world deployment. Experimental results\nshow high effectiveness in both tasks, achieving a precision of 1.0 and recall\nof 0.71 for ad generation, and F1-scores ranging from 0.99 to 1.00 for ad\ndetection. These results underscore the potential of our methods to balance\npersuasive communication with transparency in conversational AI.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.14256v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14256v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.426,
      "distributed_training_score": 0.332,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves generating and detecting covert advertisements in conversational AI using LLMs, fine-tuning techniques like ORPO, and models such as CrossEncoder and DeBERTa. It does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, which are core to diffusion-based reasoning. Therefore, there is no connection to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.14257",
      "title": "From Correction to Mastery: Reinforced Distillation of Large Language\n  Model Agents",
      "authors": [
        "Yuanjie Lyu",
        "Chengyu Wang",
        "Jun Huang",
        "Tong Xu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Model agents excel at solving complex tasks through iterative\nreasoning and tool use, but typically depend on ultra-large, costly backbones.\nExisting distillation approaches train smaller students to imitate full teacher\ntrajectories, yet reasoning and knowledge gaps between the teacher and student\noften lead to compounding errors. We propose SCoRe, a student-centered\nframework in which the student generates trajectories and the teacher\nintervenes only at the first critical error, producing training data matched to\nthe student's ability and exposing specific weaknesses. The student is first\nfine-tuned on corrected trajectories. Subsequently, short-horizon reinforcement\nlearning starts from the verified prefix before the first critical error, with\ntarget rewards assigned at that step. This design encourages autonomous\nproblem-solving beyond imitation and improves training stability. Particularly,\non 12 challenging benchmarks, a 7B-parameter student distilled with SCoRe\nmatches the agentic performance of a 72B-parameter teacher.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.14257v1",
      "pdf_url": "http://arxiv.org/pdf/2509.14257v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.447,
      "weak_supervision_score": 0.443,
      "diffusion_reasoning_score": 0.482,
      "distributed_training_score": 0.431,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with rewards based on teacher model corrections, not human feedback. RLHF specifically requires human-ranked data to train a reward model, which is absent here.",
      "weak_supervision_justification": "The paper generates training data through teacher corrections, which can be seen as programmatically derived labels from a noisy or imprecise source (teacher interventions), aligning with weak supervision concepts, though it's more targeted than typical weak supervision methods.",
      "diffusion_reasoning_justification": "The paper focuses on iterative reasoning in LLM agents and reinforcement learning, with no mention of diffusion models, iterative refinement processes from diffusion, or adapting diffusion for logical tasks.",
      "distributed_training_justification": "The paper does not discuss distributed training, parallel computing, or multi-node systems; it centers on the SCoRe framework for model distillation, with no reference to partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces SCoRe, a student-centered framework for distilling knowledge from large language model (LLM) agents to smaller models, addressing gaps in reasoning and knowledge by having the student generate trajectories with the teacher intervening only at the first critical error. The methodology involves fine-tuning the student on corrected trajectories followed by short-horizon reinforcement learning from the verified prefix, resulting in a 7B-parameter student matching the performance of a 72B-parameter teacher on 12 challenging benchmarks.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework, SCoRe, which shifts from traditional imitation-based distillation to a student-led approach with targeted corrections and reinforcement learning, significantly advancing techniques for agent distillation. This innovation addresses key limitations in existing methods, potentially setting a new standard in the field.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and commercial applications by enabling smaller, more efficient LLMs to perform comparably to larger models, thus reducing costs and barriers in AI deployment. Its demonstrated results on benchmarks suggest broad applicability in interactive tasks and agent-based systems.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality contribution that innovatively improves model distillation, making it valuable for researchers in AI and language models to understand and build upon. While not groundbreaking enough for \"Must Read,\" it offers significant insights and practical advancements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1b2ef8ba05fba415b5bd5d3a592c35d9d1136ec5",
      "total_authors": 4,
      "authors_found": 3,
      "highest_h_index": 5,
      "average_h_index": 3.6666666666666665,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yuanjie Lyu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2187857206"
        },
        {
          "name": "Chengyu Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2350765490"
        },
        {
          "name": "Jun Huang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2258923993"
        },
        {
          "name": "Tong Xu",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2509.16226",
      "title": "On LLM-Based Scientific Inductive Reasoning Beyond Equations",
      "authors": [
        "Brian S. Lin",
        "Jiaxin Yuan",
        "Zihan Zhou",
        "Shouli Wang",
        "Shuo Wang",
        "Cunliang Kong",
        "Qi Shi",
        "Yuxuan Li",
        "Liner Yang",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As large language models (LLMs) increasingly exhibit human-like capabilities,\na fundamental question emerges: How can we enable LLMs to learn the underlying\npatterns from limited examples in entirely novel environments and apply them\neffectively? This question is central to the ability of LLMs in inductive\nreasoning. Existing research on LLM-based inductive reasoning can be broadly\ncategorized based on whether the underlying rules are expressible via explicit\nmathematical equations. However, many recent studies in the beyond-equations\ncategory have emphasized rule design without grounding them in specific\nscenarios. Inspired by the parallels between inductive reasoning and human\nscientific discovery, we propose the task of LLM-Based Scientific Inductive\nReasoning Beyond Equations and introduce a new benchmark, SIRBench-V1, to\nevaluate the inductive reasoning abilities of LLMs in scientific settings. Our\nexperimental results show that current LLMs still struggle with this task,\nunderscoring its difficulty and the need for further advancement in this area.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.16226v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16226v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.517,
      "distributed_training_score": 0.375,
      "datasets_score": 0.394,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating LLMs for scientific inductive reasoning tasks using benchmarks and existing models, without any mention of training methods involving human feedback, reward models, or reinforcement learning for alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses standard reasoning strategies like self-consistency and hypothesis refinement for LLMs in inductive tasks, but it does not involve or reference diffusion models, iterative refinement processes, or treating chains-of-thought as entities for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18130",
      "title": "Research on Metro Transportation Flow Prediction Based on the STL-GRU\n  Combined Model",
      "authors": [
        "Zijie Zhou",
        "Huichen Ma"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In the metro intelligent transportation system, accurate transfer passenger\nflow prediction is a key link in optimizing operation plans and improving\ntransportation efficiency. To further improve the theory of metro internal\ntransfer passenger flow prediction and provide more reliable support for\nintelligent operation decisions, this paper innovatively proposes a metro\ntransfer passenger flow prediction model that integrates the Seasonal and Trend\ndecomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In\npractical application, the model first relies on the deep learning library\nKeras to complete the construction and training of the GRU model, laying the\nfoundation for subsequent prediction; then preprocesses the original metro card\nswiping data, uses the graph-based depth-first search algorithm to identify\npassengers' travel paths, and further constructs the transfer passenger flow\ntime series; subsequently adopts the STL time series decomposition algorithm to\ndecompose the constructed transfer passenger flow time series into trend\ncomponent, periodic component and residual component, and uses the 3{\\sigma}\nprinciple to eliminate and fill the outliers in the residual component, and\nfinally completes the transfer passenger flow prediction.Taking the transfer\npassenger flow data of a certain metro station as the research sample, the\nvalidity of the model is verified. The results show that compared with Long\nShort-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of\nSTL time series decomposition method and Long Short-Term Memory (STL-LSTM), the\nSTL-GRU combined prediction model significantly improves the prediction\naccuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays\nand rest days, with the mean absolute percentage error (MAPE) of the prediction\nresults reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.18130v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18130v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.281,
      "weak_supervision_score": 0.257,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.307,
      "datasets_score": 0.266,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18131",
      "title": "Two ways to knowledge?",
      "authors": [
        "Jean-Michel Tucny",
        "Abhisek Ganguly",
        "Santosh Ansumali",
        "Sauro Succi"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "It is shown that the weight matrices of transformer-based machine learning\napplications to the solution of two representative physical applications show a\nrandom-like character which bears no directly recognizable link to the physical\nand mathematical structure of the physical problem under study. This suggests\nthat machine learning and the scientific method may represent two distinct and\npotentially complementary paths to knowledge, even though a strict notion of\nexplainability in terms of direct correspondence between network parameters and\nphysical structures may remain out of reach. It is also observed that drawing a\nparallel between transformer operation and (generalized) path-integration\ntechniques may account for the random-like nature of the weights, but still\ndoes not resolve the tension with explainability. We conclude with some general\ncomments on the hazards of gleaning knowledge without the benefit of Insight.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.18131v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18131v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.47,
      "distributed_training_score": 0.373,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on the random-like nature of weight matrices in transformer-based models for physical applications, discussing explainability and the relationship between ML and the scientific method. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.19319",
      "title": "FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR\n  Question Answering",
      "authors": [
        "Gyubok Lee",
        "Elea Bach",
        "Eric Yang",
        "Tom Pollard",
        "Alistair Johnson",
        "Edward Choi",
        "Yugang jia",
        "Jong Ha Lee"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The recent shift toward the Health Level Seven Fast Healthcare\nInteroperability Resources (HL7 FHIR) standard opens a new frontier for\nclinical AI, demanding LLM agents to navigate complex, resource-based data\nmodels instead of conventional structured health data. However, existing\nbenchmarks have lagged behind this transition, lacking the realism needed to\nevaluate recent LLMs on interoperable clinical data. To bridge this gap, we\nintroduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical\nquestions in the HL7 FHIR standard. Using this benchmark, we systematically\nevaluate agentic frameworks, comparing different data retrieval strategies\n(direct FHIR API calls vs. specialized tools), interaction patterns\n(single-turn vs. multi-turn), and reasoning strategies (natural language vs.\ncode generation). Our experiments highlight the practical challenges of\nretrieving data from intricate FHIR resources and the difficulty of reasoning\nover them, both of which critically affect question answering performance. We\npublicly release the FHIR-AgentBench dataset and evaluation suite\n(https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research\nand the development of robust, reliable LLM agents for clinical applications.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.19319v1",
      "pdf_url": "http://arxiv.org/pdf/2509.19319v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.357,
      "datasets_score": 0.406,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on introducing a benchmark for evaluating LLM agents on FHIR-based EHR data, including data retrieval and reasoning strategies, but does not involve reinforcement learning, human feedback, or training models with reward models. There is no mention of aligning AI models with human preferences through RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and introduction of FHIR-AgentBench, a new dataset with 2,931 real-world clinical questions grounded in FHIR standards, along with methodologies for curation, benchmarking, and evaluation. It directly addresses dataset creation, analysis, and benchmarking for AI applications in healthcare.",
      "llm_score_status": "completed",
      "summary": "This paper introduces FHIR-AgentBench, a benchmark designed to evaluate large language model (LLM) agents on realistic, interoperable electronic health record (EHR) question answering using the HL7 FHIR standard, addressing the gap in existing benchmarks by grounding 2,931 real-world clinical questions in actual FHIR resources from MIMIC-IV-FHIR data. The methodology involves systematic evaluation of various agent frameworks, comparing data retrieval strategies (direct FHIR API calls vs. specialized tools), interaction patterns (single-turn vs. multi-turn), and reasoning strategies (natural language vs. code generation), revealing key challenges in data retrieval and reasoning that impact QA performance, with the dataset and tools released for reproducibility.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark, FHIR-AgentBench, that advances the state-of-the-art by focusing on LLM agents for HL7 FHIR-based EHR QA, addressing a significant gap in realistic interoperability evaluations. This represents a novel problem and technique not adequately covered in prior works.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of clinical AI and LLM agents for healthcare, as it provides a practical benchmark for improving EHR interoperability. However, its influence may be limited to specific applications in healthcare AI rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution by introducing a realistic benchmark for LLM agents in clinical settings, making it essential for researchers in AI and healthcare to be aware of for advancing EHR QA systems. While not universally critical, it offers important insights and resources for relevant subfields.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0306ffedaa06848407bb49d37310856925dd0bb3",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 8,
      "average_h_index": 2.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Gyubok Lee",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2115216766"
        },
        {
          "name": "Elea Bach",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381834764"
        },
        {
          "name": "Eric Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381836078"
        },
        {
          "name": "Tom J. Pollard",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2308041369"
        },
        {
          "name": "Alistair Johnson",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2365402763"
        },
        {
          "name": "Edward Choi",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2293447917"
        },
        {
          "name": "Yugang Jia",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2301105849"
        },
        {
          "name": "Jong Ha Lee",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2312997287"
        }
      ]
    },
    {
      "id": "2509.19322",
      "title": "Readme_AI: Dynamic Context Construction for Large Language Models",
      "authors": [
        "Millie Vyas",
        "Timothy Blattner",
        "Alden Dima"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite being trained on significant amounts of data, Large Language Models\n(LLMs) can provide inaccurate or unreliable information in the context of a\nuser's specific query. Given query-specific context significantly improves the\nusefulness of its responses. In this paper, we present a specification that can\nbe used to dynamically build context for data sources. The data source owner\ncreates the file containing metadata for LLMs to use when reasoning about\ndataset-related queries. To demonstrate our proposed specification, we created\na prototype Readme_AI Model Context Protocol (MCP) server that retrieves the\nmetadata from the data source and uses it to dynamically build context. Some\nfeatures that make this specification dynamic are the extensible types that\nrepresent crawling web-pages, fetching data from data repositories, downloading\nand parsing publications, and general text. The context is formatted and\ngrouped using user-specified tags that provide clear contextual information for\nthe LLM to reason about the content. We demonstrate the capabilities of this\nearly prototype by asking the LLM about the NIST-developed Hedgehog library,\nfor which common LLMs often provides inaccurate and irrelevant responses\ncontaining hallucinations. With Readme_AI, the LLM receives enough context that\nit is now able to reason about the library and its use, and even generate code\ninterpolated from examples that were included in the Readme_AI file provided by\nHedgehog's developer. Our primary contribution is a extensible protocol for\ndynamically grounding LLMs in specialized, owner-provided data, enhancing\nresponses from LLMs and reducing hallucinations. The source code for the\nReadme_AI tool is posted here: https://github.com/usnistgov/readme_ai .",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.19322v1",
      "pdf_url": "http://arxiv.org/pdf/2509.19322v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.498,
      "weak_supervision_score": 0.428,
      "diffusion_reasoning_score": 0.467,
      "distributed_training_score": 0.397,
      "datasets_score": 0.442,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on dynamic context construction for LLMs using metadata specifications, not on training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper does not involve training machine learning models with programmatically generated labels or noisy data sources; it instead addresses providing context for LLMs to improve query responses.",
      "diffusion_reasoning_justification": "The paper describes a system for building context for LLMs but does not incorporate diffusion models, iterative refinement for logical reasoning, or multi-step Chain-of-Thought processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper involves creating and using metadata files (e.g., Readme_AI.json) that fetch data from sources like repositories, which could relate to dataset curation, but its primary focus is on enhancing LLM responses rather than creating, analyzing, or benchmarking datasets.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.19323",
      "title": "Magnitude Matters: a Superior Class of Similarity Metrics for Holistic\n  Semantic Understanding",
      "authors": [
        "V. S. Raghu Parupudi"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Vector comparison in high dimensions is a fundamental task in NLP, yet it is\ndominated by two baselines: the raw dot product, which is unbounded and\nsensitive to vector norms, and the cosine similarity, which discards magnitude\ninformation entirely. This paper challenges both standards by proposing and\nrigorously evaluating a new class of parameter-free, magnitude-aware similarity\nmetrics. I introduce two such functions, Overlap Similarity (OS) and Hyperbolic\nTangent Similarity (HTS), designed to integrate vector magnitude and alignment\nin a more principled manner. To ensure that my findings are robust and\ngeneralizable, I conducted a comprehensive evaluation using four\nstate-of-the-art sentence embedding models (all-MiniLM-L6-v2,\nall-mpnet-base-v2, paraphrase-mpnet-base-v2, and BAAI/bge-large-en-v1.5) across\na diverse suite of eight standard NLP benchmarks, including STS-B, SICK, Quora,\nand PAWS. Using the Wilcoxon signed-rank test for statistical significance, my\nresults are definitive: on the tasks requiring holistic semantic understanding\n(paraphrase and inference), both OS and HTS provide a statistically significant\nimprovement in Mean Squared Error over both the raw dot product and cosine\nsimilarity, regardless of the underlying embedding model.Crucially, my findings\ndelineate the specific domain of advantage for these metrics: for tasks\nrequiring holistic semantic understanding like paraphrase and inference, my\nmagnitude-aware metrics offer a statistically superior alternative. This\nsignificant improvement was not observed on benchmarks designed to test highly\nnuanced compositional semantics (SICK, STS-B), identifying the challenge of\nrepresenting compositional text as a distinct and important direction for\nfuture work.",
      "published_date": "2025-09-12",
      "arxiv_url": "http://arxiv.org/abs/2509.19323v1",
      "pdf_url": "http://arxiv.org/pdf/2509.19323v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.299,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.345,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    }
  ],
  "total_papers": 151,
  "date": "2025-09-12"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Relevance Filter State Management
        let currentRelevanceFilters = {
            highlyRelevant: true,
            moderatelyRelevant: true,
            tangentiallyRelevant: true,
            notRelevant: true
        };
        
        let pendingRelevanceFilters = { ...currentRelevanceFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function calculateRelevanceScore(paper) {
            let score = 0;
            
            // Only consider topics that are currently selected/enabled in the topic filter
            const topicsToConsider = [];
            if (currentTopicFilters.rlhf) topicsToConsider.push('rlhf_relevance');
            if (currentTopicFilters.weakSupervision) topicsToConsider.push('weak_supervision_relevance');
            if (currentTopicFilters.diffusionReasoning) topicsToConsider.push('diffusion_reasoning_relevance');
            if (currentTopicFilters.distributedTraining) topicsToConsider.push('distributed_training_relevance');
            if (currentTopicFilters.datasets) topicsToConsider.push('datasets_relevance');
            
            // If no topics are selected, return 0
            if (topicsToConsider.length === 0) return 0;
            
            // Weighted scoring system
            const relevanceWeights = {
                'Highly Relevant': 4,
                'Moderately Relevant': 3,
                'Tangentially Relevant': 2,
                'Not Relevant': 1
            };
            
            // Sum up scores for selected topics only
            for (let topicField of topicsToConsider) {
                const relevance = paper[topicField];
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = relevance === "not_validated" ? "Not Relevant" : relevance;
                score += relevanceWeights[normalizedRelevance] || 1; // Default to 1 if unknown
            }
            
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'relevance_high':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(b) - calculateRelevanceScore(a));
                    break;
                case 'relevance_low':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(a) - calculateRelevanceScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'relevance_high': 'Relevance (Highest to Lowest)',
                'relevance_low': 'Relevance (Lowest to Highest)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
            updateNoveltyButtonText();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
            updateImpactButtonText();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for relevance checkboxes to sync between mobile and desktop
            document.getElementById('mobile-relevance-highly').addEventListener('change', () => {
                document.getElementById('desktop-relevance-highly').checked = 
                    document.getElementById('mobile-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-moderately').addEventListener('change', () => {
                document.getElementById('desktop-relevance-moderately').checked = 
                    document.getElementById('mobile-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('desktop-relevance-tangentially').checked = 
                    document.getElementById('mobile-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-not').addEventListener('change', () => {
                document.getElementById('desktop-relevance-not').checked = 
                    document.getElementById('mobile-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-highly').addEventListener('change', () => {
                document.getElementById('mobile-relevance-highly').checked = 
                    document.getElementById('desktop-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-moderately').addEventListener('change', () => {
                document.getElementById('mobile-relevance-moderately').checked = 
                    document.getElementById('desktop-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('mobile-relevance-tangentially').checked = 
                    document.getElementById('desktop-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-not').addEventListener('change', () => {
                document.getElementById('mobile-relevance-not').checked = 
                    document.getElementById('desktop-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function resetPendingScoringFilters() {
            pendingScoringFilters = { ...currentScoringFilters };
            syncPendingScoringUI();
            updateScoringButtonText();
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function resetPendingRecommendationFilters() {
            pendingRecommendationFilters = { ...currentRecommendationFilters };
            syncPendingRecommendationUI();
            updateRecommendationButtonText();
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
            updateTopicButtonText();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // RELEVANCE FILTER FUNCTIONS  
        // ============================================================================
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = pendingRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = pendingRelevanceFilters.notRelevant;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = currentRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = currentRelevanceFilters.notRelevant;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            // Read the current checkbox states from the UI (like H-Index filter does)
            const highlyRelevantChecked = document.getElementById('mobile-relevance-highly').checked;
            const moderatelyRelevantChecked = document.getElementById('mobile-relevance-moderately').checked;
            const tangentiallyRelevantChecked = document.getElementById('mobile-relevance-tangentially').checked;
            const notRelevantChecked = document.getElementById('mobile-relevance-not').checked;
            
            const selectedOptions = [];
            if (highlyRelevantChecked) selectedOptions.push("Highly Relevant");
            if (moderatelyRelevantChecked) selectedOptions.push("Moderately Relevant");
            if (tangentiallyRelevantChecked) selectedOptions.push("Tangentially Relevant");
            if (notRelevantChecked) selectedOptions.push("Not Relevant");
            
            const selectionText = selectedOptions.length === 4 ? "All Selected" : 
                                selectedOptions.length === 0 ? "None Selected" : 
                                `${selectedOptions.length} Selected`;
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            // Get values from mobile (primary source)
            const mobileHighly = document.getElementById('mobile-relevance-highly');
            const mobileModerately = document.getElementById('mobile-relevance-moderately');
            const mobileTangentially = document.getElementById('mobile-relevance-tangentially');
            const mobileNot = document.getElementById('mobile-relevance-not');
            
            // Update pending filters from mobile if available, otherwise from desktop
            pendingRelevanceFilters.highlyRelevant = mobileHighly ? mobileHighly.checked : document.getElementById('desktop-relevance-highly').checked;
            pendingRelevanceFilters.moderatelyRelevant = mobileModerately ? mobileModerately.checked : document.getElementById('desktop-relevance-moderately').checked;
            pendingRelevanceFilters.tangentiallyRelevant = mobileTangentially ? mobileTangentially.checked : document.getElementById('desktop-relevance-tangentially').checked;
            pendingRelevanceFilters.notRelevant = mobileNot ? mobileNot.checked : document.getElementById('desktop-relevance-not').checked;
        }
        
        function resetPendingRelevanceFilters() {
            pendingRelevanceFilters = { ...currentRelevanceFilters };
            syncPendingRelevanceUI();
            updateRelevanceButtonText();
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_highly', currentRelevanceFilters.highlyRelevant.toString());
            params.set('relevance_moderately', currentRelevanceFilters.moderatelyRelevant.toString());
            params.set('relevance_tangentially', currentRelevanceFilters.tangentiallyRelevant.toString());
            params.set('relevance_not', currentRelevanceFilters.notRelevant.toString());
            window.history.replaceState({}, '', `${window.location.pathname}?${params.toString()}`);
        }
        
        function loadRelevanceFiltersFromURL() {
            const highlyRelevant = getUrlParameter('relevance_highly');
            const moderatelyRelevant = getUrlParameter('relevance_moderately');
            const tangentiallyRelevant = getUrlParameter('relevance_tangentially');
            const notRelevant = getUrlParameter('relevance_not');
            
            if (highlyRelevant !== null) {
                currentRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
                pendingRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
            }
            if (moderatelyRelevant !== null) {
                currentRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
                pendingRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
            }
            if (tangentiallyRelevant !== null) {
                currentRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
                pendingRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
            }
            if (notRelevant !== null) {
                currentRelevanceFilters.notRelevant = notRelevant === 'true';
                pendingRelevanceFilters.notRelevant = notRelevant === 'true';
            }
            
            syncRelevanceUI();
        }
        
        function passesRelevanceFilter(paper) {
            // Get selected topics
            const selectedTopics = [];
            if (currentTopicFilters.rlhf) selectedTopics.push('rlhf');
            if (currentTopicFilters.weakSupervision) selectedTopics.push('weak_supervision');
            if (currentTopicFilters.diffusionReasoning) selectedTopics.push('diffusion_reasoning');
            if (currentTopicFilters.distributedTraining) selectedTopics.push('distributed_training');
            if (currentTopicFilters.datasets) selectedTopics.push('datasets');
            
            // If no topics selected, skip relevance filtering
            if (selectedTopics.length === 0) return true;
            
            // Get selected relevance levels
            const selectedRelevanceLevels = [];
            if (currentRelevanceFilters.highlyRelevant) selectedRelevanceLevels.push('Highly Relevant');
            if (currentRelevanceFilters.moderatelyRelevant) selectedRelevanceLevels.push('Moderately Relevant');
            if (currentRelevanceFilters.tangentiallyRelevant) selectedRelevanceLevels.push('Tangentially Relevant');
            if (currentRelevanceFilters.notRelevant) selectedRelevanceLevels.push('Not Relevant');
            
            // If no relevance levels selected, show 0 papers
            if (selectedRelevanceLevels.length === 0) return false;
            
            // Check each selected topic
            for (let topic of selectedTopics) {
                const relevanceField = `${topic}_relevance`;
                const paperRelevance = paper[relevanceField];
                
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = paperRelevance === "not_validated" ? "Not Relevant" : paperRelevance;
                
                // If this topic's relevance matches any selected relevance level, paper passes
                if (selectedRelevanceLevels.includes(normalizedRelevance)) {
                    return true; // At least one topic matches
                }
            }
            
            // No selected topics had matching relevance levels
            return false;
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
            
            // Update Relevance
            updateDropdownDisabledState('mobile-relevance-btn', 'mobile-relevance-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-relevance-btn', 'desktop-relevance-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            // Check if there are no papers to display
            if (filteredSortedPapers.length === 0) {
                showNoPapersMessage();
                hidePaginationSections();
                return;
            }
            
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            showPaginationSections();
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        function showNoPapersMessage() {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const noPapersHTML = '<div class="flex items-center justify-center min-h-screen"><h2 class="font-heading text-2xl text-neutral-600">No papers to show</h2></div>';
            
            if (mobileContainer) {
                mobileContainer.innerHTML = noPapersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = noPapersHTML;
            }
        }

        function hidePaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = 'none';
                }
            });
        }

        function showPaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = '';
                }
            });
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            // Find the similarity container in the currently visible layout
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            // Find the justification container in the currently visible layout
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            // Find the normalized similarity container in the currently visible layout
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // QUICK FILTER FUNCTIONS
        // ============================================================================


        function applyQuickFilter(filterType) {
            // Build URL parameters based on filter type
            const url = new URL(window.location.href);
            
            // Clear all existing parameters
            url.search = '';
            
            // Set sort to recommendation best first for all filters
            url.searchParams.set('sort', 'recommend_best');
            
            switch(filterType) {
                case 'must-read':
                    // Recommendation: Only Must Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'true');
                    url.searchParams.set('recommendation_should', 'false');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'should-read':
                    // Recommendation: Only Should Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'false');
                    url.searchParams.set('recommendation_should', 'true');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'rlhf':
                    // Topic: Only RLHF + Relevance: Exclude Not Relevant
                    url.searchParams.set('topic_rlhf', 'true');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'weak-supervision':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'true');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'diffusion-reasoning':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'true');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'distributed-training':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'true');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'datasets':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'true');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'reset':
                    // Clear all parameters, which will reset everything to defaults
                    break;
                    
                default:
                    console.warn('Unknown quick filter type:', filterType);
                    return;
            }
            
            // Navigate to new URL, which will trigger existing URL parsing logic
            window.location.href = url.toString();
        }



        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Load Relevance filters from URL
            loadRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-relevance-dropdown'), button: document.getElementById('mobile-relevance-btn') },
                { dropdown: document.getElementById('desktop-relevance-dropdown'), button: document.getElementById('desktop-relevance-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('relevance')) {
                            resetPendingRelevanceFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        } else if (dropdown.id.includes('recommendation')) {
                            resetPendingRecommendationFilters();
                        } else if (dropdown.id.includes('scoring')) {
                            resetPendingScoringFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
