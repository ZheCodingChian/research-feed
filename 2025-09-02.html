<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 02 September 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: 'rgba(107, 106, 101, 0.7)' // #6B6A65 with 70% opacity
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 02 September 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100 min-h-screen">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="mobile-quick-must-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="mobile-quick-should-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="mobile-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="mobile-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="mobile-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="mobile-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="mobile-quick-datasets" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="mobile-quick-reset" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-highly" checked>
                                            <label for="mobile-relevance-highly"></label>
                                        </div>
                                        <label for="mobile-relevance-highly" class="text-neutral-10 text-xl font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-moderately" checked>
                                            <label for="mobile-relevance-moderately"></label>
                                        </div>
                                        <label for="mobile-relevance-moderately" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-tangentially" checked>
                                            <label for="mobile-relevance-tangentially"></label>
                                        </div>
                                        <label for="mobile-relevance-tangentially" class="text-neutral-10 text-xl font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-not" checked>
                                            <label for="mobile-relevance-not"></label>
                                        </div>
                                        <label for="mobile-relevance-not" class="text-neutral-10 text-xl font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="desktop-quick-must-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="desktop-quick-should-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="desktop-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="desktop-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="desktop-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="desktop-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="desktop-quick-datasets" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="desktop-quick-reset" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-highly" checked>
                                            <label for="desktop-relevance-highly"></label>
                                        </div>
                                        <label for="desktop-relevance-highly" class="text-neutral-10 text-lg font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-moderately" checked>
                                            <label for="desktop-relevance-moderately"></label>
                                        </div>
                                        <label for="desktop-relevance-moderately" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-tangentially" checked>
                                            <label for="desktop-relevance-tangentially"></label>
                                        </div>
                                        <label for="desktop-relevance-tangentially" class="text-neutral-10 text-lg font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-not" checked>
                                            <label for="desktop-relevance-not"></label>
                                        </div>
                                        <label for="desktop-relevance-not" class="text-neutral-10 text-lg font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 02 September 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl min-h-screen">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2509.01845",
      "title": "Community-Centered Spatial Intelligence for Climate Adaptation at Nova\n  Scotia's Eastern Shore",
      "authors": [
        "Gabriel Spadon",
        "Oladapo Oyebode",
        "Camilo M. Botero",
        "Tushar Sharma",
        "Floris Goerlandt",
        "Ronald Pelot"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper presents an overview of a human-centered initiative aimed at\nstrengthening climate resilience along Nova Scotia's Eastern Shore. This\nregion, a collection of rural villages with deep ties to the sea, faces\nexistential threats from climate change that endanger its way of life. Our\nproject moves beyond a purely technical response, weaving together expertise\nfrom Computer Science, Industrial Engineering, and Coastal Geography to\nco-create tools with the community. By integrating generational knowledge of\nresidents, particularly elders, through the Eastern Shore Citizen Science\nCoastal Monitoring Network, this project aims to collaborate in building a\nliving digital archive. This effort is hosted under Dalhousie University's\nTransforming Climate Action (TCA) initiative, specifically through its\nTransformative Adaptations to Social-Ecological Climate Change Trajectories\n(TranSECT) and TCA Artificial Intelligence (TCA-AI) projects. This work is\ndriven by a collaboration model in which student teams work directly with\nresidents. We present a detailed project timeline and a replicable model for\nhow technology can support traditional communities, enabling them to navigate\nclimate transformation more effectively.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01845v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01845v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.281,
      "distributed_training_score": 0.319,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01864",
      "title": "Latent Gene Diffusion for Spatial Transcriptomics Completion",
      "authors": [
        "Paula Cárdenas",
        "Leonardo Manrique",
        "Daniela Vega",
        "Daniela Ruiz",
        "Pablo Arbeláez"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Computer Vision has proven to be a powerful tool for analyzing Spatial\nTranscriptomics (ST) data. However, current models that predict spatially\nresolved gene expression from histopathology images suffer from significant\nlimitations due to data dropout. Most existing approaches rely on single-cell\nRNA sequencing references, making them dependent on alignment quality and\nexternal datasets while also risking batch effects and inherited dropout. In\nthis paper, we address these limitations by introducing LGDiST, the first\nreference-free latent gene diffusion model for ST data dropout. We show that\nLGDiST outperforms the previous state-of-the-art in gene expression completion,\nwith an average Mean Squared Error that is 18% lower across 26 datasets.\nFurthermore, we demonstrate that completing ST data with LGDiST improves gene\nexpression prediction performance on six state-of-the-art methods up to 10% in\nMSE. A key innovation of LGDiST is using context genes previously considered\nuninformative to build a rich and biologically meaningful genetic latent space.\nOur experiments show that removing key components of LGDiST, such as the\ncontext genes, the ST latent space, and the neighbor conditioning, leads to\nconsiderable drops in performance. These findings underscore that the full\narchitecture of LGDiST achieves substantially better performance than any of\nits isolated components.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01864v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01864v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.511,
      "distributed_training_score": 0.368,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for completing missing gene expression data in Spatial Transcriptomics, specifically through a latent gene diffusion model for data imputation. While it employs iterative refinement via Denoising Diffusion Probabilistic Models (DDPMs), this is applied to generative tasks in biological data processing, not to multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks as defined in the topic. There is no evidence of adapting diffusion for holistic correction of reasoning paths, making the paper's contribution outside the scope of Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01868",
      "title": "Enabling Federated Object Detection for Connected Autonomous Vehicles: A\n  Deployment-Oriented Evaluation",
      "authors": [
        "Komala Subramanyam Cherukuri",
        "Kewei Sha",
        "Zhenhua Huang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "Object detection is crucial for Connected Autonomous Vehicles (CAVs) to\nperceive their surroundings and make safe driving decisions. Centralized\ntraining of object detection models often achieves promising accuracy, fast\nconvergence, and simplified training process, but it falls short in\nscalability, adaptability, and privacy-preservation. Federated learning (FL),\nby contrast, enables collaborative, privacy-preserving, and continuous training\nacross naturally distributed CAV fleets. However, deploying FL in real-world\nCAVs remains challenging due to the substantial computational demands of\ntraining and inference, coupled with highly diverse operating conditions.\nPractical deployment must address three critical factors: (i) heterogeneity\nfrom non-IID data distributions, (ii) constrained onboard computing hardware,\nand (iii) environmental variability such as lighting and weather, alongside\nsystematic evaluation to ensure reliable performance. This work introduces the\nfirst holistic deployment-oriented evaluation of FL-based object detection in\nCAVs, integrating model performance, system-level resource profiling, and\nenvironmental robustness. Using state-of-the-art detectors, YOLOv5, YOLOv8,\nYOLOv11, and Deformable DETR, evaluated on the KITTI, BDD100K, and nuScenes\ndatasets, we analyze trade-offs between detection accuracy, computational cost,\nand resource usage under diverse resolutions, batch sizes, weather and lighting\nconditions, and dynamic client participation, paving the way for robust FL\ndeployment in CAVs.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01868v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01868v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.455,
      "datasets_score": 0.389,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution centers on Federated Learning (FL) for object detection in Connected Autonomous Vehicles (CAVs), which is a specific form of distributed training. It addresses key aspects such as partitioning data across distributed devices (e.g., vehicles), handling non-IID data distributions, and managing computational resources in a multi-node environment. The evaluation framework using the Flower platform simulates large-scale distributed setups, directly aligning with distributed training concepts like parallel computing and multi-node machine learning for accelerating and scaling model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper evaluates the deployment of federated learning (FL) for object detection in Connected Autonomous Vehicles (CAVs), addressing challenges such as data heterogeneity, computational constraints, and environmental variability by introducing a comprehensive evaluation framework based on the Flower platform. Using state-of-the-art detectors like YOLOv5, YOLOv8, YOLOv11, and Deformable DETR on datasets including KITTI, BDD100K, and nuScenes, the authors analyze trade-offs in detection accuracy, computational cost, and resource usage under diverse conditions, ultimately identifying key factors for robust FL deployment and outlining open research problems.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing the first holistic deployment-oriented evaluation framework for FL-based object detection in CAVs, combining existing FL techniques with systematic analysis to address real-world challenges in a new way. However, it does not introduce a entirely new problem or technique, making it an incremental advancement rather than a groundbreaking one.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in FL for CAVs by providing a benchmarking framework and insights into deployment challenges, potentially leading to citations and developments within this specific subfield. Nonetheless, its applicability is somewhat limited to autonomous vehicle perception, reducing its broader impact.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution through its practical evaluation framework and insights for FL in CAVs, making it essential for researchers in computer vision and distributed computing focused on autonomous systems. While not universally groundbreaking, it provides important guidance for advancing real-world deployments.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/83327930e127a19c6414a06616f3f1222920c0cd",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 1,
      "average_h_index": 0.6666666666666666,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Komala Subramanyam Cherukuri",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2326192579"
        },
        {
          "name": "Kewei Sha",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2316763859"
        },
        {
          "name": "Zhenhua Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379409069"
        }
      ]
    },
    {
      "id": "2509.01873",
      "title": "Doctoral Thesis: Geometric Deep Learning For Camera Pose Prediction,\n  Registration, Depth Estimation, and 3D Reconstruction",
      "authors": [
        "Xueyang Kang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Modern deep learning developments create new opportunities for 3D mapping\ntechnology, scene reconstruction pipelines, and virtual reality development.\nDespite advances in 3D deep learning technology, direct training of deep\nlearning models on 3D data faces challenges due to the high dimensionality\ninherent in 3D data and the scarcity of labeled datasets. Structure-from-motion\n(SfM) and Simultaneous Localization and Mapping (SLAM) exhibit robust\nperformance when applied to structured indoor environments but often struggle\nwith ambiguous features in unstructured environments. These techniques often\nstruggle to generate detailed geometric representations effective for\ndownstream tasks such as rendering and semantic analysis. Current limitations\nrequire the development of 3D representation methods that combine traditional\ngeometric techniques with deep learning capabilities to generate robust\ngeometry-aware deep learning models.\n  The dissertation provides solutions to the fundamental challenges in 3D\nvision by developing geometric deep learning methods tailored for essential\ntasks such as camera pose estimation, point cloud registration, depth\nprediction, and 3D reconstruction. The integration of geometric priors or\nconstraints, such as including depth information, surface normals, and\nequivariance into deep learning models, enhances both the accuracy and\nrobustness of geometric representations. This study systematically investigates\nkey components of 3D vision, including camera pose estimation, point cloud\nregistration, depth estimation, and high-fidelity 3D reconstruction,\ndemonstrating their effectiveness across real-world applications such as\ndigital cultural heritage preservation and immersive VR/AR environments.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01873v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01873v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.359,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01874",
      "title": "Preserving Bilinear Weight Spectra with a Signed and Shrunk Quadratic\n  Activation Function",
      "authors": [
        "Jason Abohwo",
        "Thomas Mosen"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Understanding the inner workings of machine learning models is critical for\nensuring their reliability and robustness. Whilst many techniques in\nmechanistic interpretability focus on activation driven analyses, being able to\nderive meaningful features directly from the weights of a neural network would\nprovide greater guarantees and more computational efficiency. Existing\ntechniques for analyzing model features through weights suffer from drawbacks\nsuch as reduced performance and data inefficiency. In this paper, we introduce\nSigned Quadratic Shrink (SQS), an activation function designed to allow Gated\nLinear Units (GLUs) to learn interpretable features without these drawbacks.\nOur experimental results show that SQS achieves performance competitive with\nstate-of-the-art activation functions whilst enabling weight-based\ninterpretability",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01874v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01874v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.32,
      "datasets_score": 0.239,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01878",
      "title": "AI-Driven Marine Robotics: Emerging Trends in Underwater Perception and\n  Ecosystem Monitoring",
      "authors": [
        "Scarlett Raine",
        "Tobias Fischer"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Marine ecosystems face increasing pressure due to climate change, driving the\nneed for scalable, AI-powered monitoring solutions. This paper examines the\nrapid emergence of underwater AI as a major research frontier and analyzes the\nfactors that have transformed marine perception from a niche application into a\ncatalyst for AI innovation. We identify three convergent drivers: environmental\nnecessity for ecosystem-scale monitoring, democratization of underwater\ndatasets through citizen science platforms, and researcher migration from\nsaturated terrestrial computer vision domains. Our analysis reveals how unique\nunderwater challenges - turbidity, cryptic species detection, expert annotation\nbottlenecks, and cross-ecosystem generalization - are driving fundamental\nadvances in weakly supervised learning, open-set recognition, and robust\nperception under degraded conditions. We survey emerging trends in datasets,\nscene understanding and 3D reconstruction, highlighting the paradigm shift from\npassive observation toward AI-driven, targeted intervention capabilities. The\npaper demonstrates how underwater constraints are pushing the boundaries of\nfoundation models, self-supervised learning, and perception, with\nmethodological innovations that extend far beyond marine applications to\nbenefit general computer vision, robotics, and environmental monitoring.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01878v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01878v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.426,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.372,
      "datasets_score": 0.421,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper explicitly discusses weakly supervised learning as a fundamental advance driven by underwater challenges, such as turbidity and cryptic species detection, which require training models with noisy or imprecise data sources. This aligns directly with the topic's definition, as the paper highlights how these methods address annotation bottlenecks in marine AI.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper surveys emerging trends in datasets for underwater AI, including their democratization through citizen science, creation of large-scale datasets, and their role in enabling modern AI applications. This directly involves analyzing and evaluating datasets, fitting the topic's focus on dataset curation, introduction, and benchmarking for machine learning.",
      "llm_score_status": "completed",
      "summary": "This paper analyzes the emerging field of AI-driven underwater perception and marine robotics, driven by environmental imperatives, the availability of large-scale datasets from citizen science, and the migration of researchers from saturated domains. It identifies key challenges such as turbidity and cryptic species detection that are spurring innovations in AI techniques like self-supervised learning and open-set recognition, surveys trends in datasets, scene understanding, and 3D reconstruction, and highlights how these advancements could extend to broader applications in computer vision, robotics, and environmental monitoring.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable synthesis of existing AI ideas applied to the underwater domain, highlighting how unique challenges are driving innovations in techniques like weakly supervised learning, which represents a clever adaptation rather than a completely new problem. While it advances the state-of-the-art by identifying new research frontiers, it primarily builds on established methods rather than introducing entirely novel architectures.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in marine AI and environmental monitoring by providing a comprehensive analysis of trends and innovations, potentially extending to general computer vision applications. However, its impact may be confined to specific subfields rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers valuable insights into an emerging and important area of AI with real-world applications in ecosystem monitoring, making it a significant contribution for researchers in robotics and computer vision. While not essential for all, it is highly relevant for those working in environmental AI or related domains.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/61a49cdd69949357a55e788ad74c7963cc1cdc19",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 4,
      "average_h_index": 3.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Scarlett Raine",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/1955624212"
        },
        {
          "name": "Tobias Fischer",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2296712508"
        }
      ]
    },
    {
      "id": "2509.01882",
      "title": "HydroVision: Predicting Optically Active Parameters in Surface Water\n  Using Computer Vision",
      "authors": [
        "Shubham Laxmikant Deshmukh",
        "Matthew Wilchek",
        "Feras A. Batarseh"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Ongoing advancements in computer vision, particularly in pattern recognition\nand scene classification, have enabled new applications in environmental\nmonitoring. Deep learning now offers non-contact methods for assessing water\nquality and detecting contamination, both critical for disaster response and\npublic health protection. This work introduces HydroVision, a deep\nlearning-based scene classification framework that estimates optically active\nwater quality parameters including Chlorophyll-Alpha, Chlorophylls, Colored\nDissolved Organic Matter (CDOM), Phycocyanins, Suspended Sediments, and\nTurbidity from standard Red-Green-Blue (RGB) images of surface water.\nHydroVision supports early detection of contamination trends and strengthens\nmonitoring by regulatory agencies during external environmental stressors,\nindustrial activities, and force majeure events. The model is trained on more\nthan 500,000 seasonally varied images collected from the United States\nGeological Survey Hydrologic Imagery Visualization and Information System\nbetween 2022 and 2024. This approach leverages widely available RGB imagery as\na scalable, cost-effective alternative to traditional multispectral and\nhyperspectral remote sensing. Four state-of-the-art convolutional neural\nnetworks (VGG-16, ResNet50, MobileNetV2, DenseNet121) and a Vision Transformer\nare evaluated through transfer learning to identify the best-performing\narchitecture. DenseNet121 achieves the highest validation performance, with an\nR2 score of 0.89 in predicting CDOM, demonstrating the framework's promise for\nreal-world water quality monitoring across diverse conditions. While the\ncurrent model is optimized for well-lit imagery, future work will focus on\nimproving robustness under low-light and obstructed scenarios to expand its\noperational utility.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01882v2",
      "pdf_url": "http://arxiv.org/pdf/2509.01882v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.352,
      "datasets_score": 0.396,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01885",
      "title": "Extracting OPQRST in Electronic Health Records using Large Language\n  Models with Reasoning",
      "authors": [
        "Zhimeng Luo",
        "Abhibha Gupta",
        "Adam Frisch",
        "Daqing He"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The extraction of critical patient information from Electronic Health Records\n(EHRs) poses significant challenges due to the complexity and unstructured\nnature of the data. Traditional machine learning approaches often fail to\ncapture pertinent details efficiently, making it difficult for clinicians to\nutilize these tools effectively in patient care. This paper introduces a novel\napproach to extracting the OPQRST assessment from EHRs by leveraging the\ncapabilities of Large Language Models (LLMs). We propose to reframe the task\nfrom sequence labeling to text generation, enabling the models to provide\nreasoning steps that mimic a physician's cognitive processes. This approach\nenhances interpretability and adapts to the limited availability of labeled\ndata in healthcare settings. Furthermore, we address the challenge of\nevaluating the accuracy of machine-generated text in clinical contexts by\nproposing a modification to traditional Named Entity Recognition (NER) metrics.\nThis includes the integration of semantic similarity measures, such as the BERT\nScore, to assess the alignment between generated text and the clinical intent\nof the original records. Our contributions demonstrate a significant\nadvancement in the use of AI in healthcare, offering a scalable solution that\nimproves the accuracy and usability of information extraction from EHRs,\nthereby aiding clinicians in making more informed decisions and enhancing\npatient care outcomes.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01885v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01885v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.437,
      "weak_supervision_score": 0.419,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.355,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using LLMs for text generation and reasoning in EHR extraction, with no mention of human feedback, reward models, or reinforcement learning techniques. It does not involve aligning models with human preferences through RLHF.",
      "weak_supervision_justification": "The paper employs few-shot learning with LLMs to handle limited labeled data, reducing the need for extensive annotations, which aligns with weak supervision's goal of training on noisy or minimal data sources. However, it does not explicitly describe programmatic label generation or other core weak supervision methods.",
      "diffusion_reasoning_justification": "The paper uses LLMs for generating reasoning steps in text extraction but does not involve diffusion models, iterative refinement processes, or treating chain-of-thought as a holistically corrected entity over multiple steps.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the challenge of extracting OPQRST assessments from Electronic Health Records (EHRs) by proposing a novel approach that reframes the task from sequence labeling to text generation using Large Language Models (LLMs) like Llama 2 and GPT-4, allowing for reasoning steps that mimic a physician's cognitive process and improving interpretability with limited labeled data. The methodology includes adapting traditional Named Entity Recognition metrics with semantic similarity measures such as BERTScore to evaluate generated text more accurately, demonstrating significant advancements in accuracy, usability, and scalability for clinical decision-making in healthcare.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by reframing EHR extraction as text generation with LLMs for reasoning and interpretability, which is a clever adaptation of existing techniques rather than a completely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in medical NLP and AI applications for healthcare by providing a more interpretable and efficient method for EHR analysis, though its impact may be primarily within specific subfields rather than broadly across disciplines.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a high-quality contribution with practical innovations in AI for healthcare, making it essential for researchers in computational linguistics and medical AI to be aware of its methods and insights.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f32e663048a86510b02063dab740de7fa94d28b6",
      "total_authors": 4,
      "authors_found": 3,
      "highest_h_index": 1,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zhimeng Luo",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Abhibha Gupta",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2266812290"
        },
        {
          "name": "Adam Frisch",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2266790702"
        },
        {
          "name": "Daqing He",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2291477832"
        }
      ]
    },
    {
      "id": "2509.01895",
      "title": "Automated Wildfire Damage Assessment from Multi view Ground level\n  Imagery Via Vision Language Models",
      "authors": [
        "Miguel Esparza",
        "Archit Gupta",
        "Ali Mostafavi",
        "Kai Yin",
        "Yiming Xiao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The escalating intensity and frequency of wildfires demand innovative\ncomputational methods for rapid and accurate property damage assessment.\nTraditional methods are often time consuming, while modern computer vision\napproaches typically require extensive labeled datasets, hindering immediate\npost-disaster deployment. This research introduces a novel, zero-shot framework\nleveraging pre-trained vision language models (VLMs) to classify damage from\nground-level imagery. We propose and evaluate two pipelines applied to the 2025\nEaton and Palisades fires in California, a VLM (Pipeline A) and a VLM + large\nlanguage model (LLM) approach (Pipeline B), that integrate structured prompts\nbased on specific wildfire damage indicators. A primary scientific contribution\nof this study is demonstrating the VLMs efficacy in synthesizing information\nfrom multiple perspectives to identify nuanced damage, a critical limitation in\nexisting literature. Our findings reveal that while single view assessments\nstruggled to classify affected structures (F1 scores ranging from 0.225 to\n0.511), the multi-view analysis yielded dramatic improvements (F1 scores\nranging from 0.857 to 0.947). Moreover, the McNemar test confirmed that\npipelines with a multi-view image assessment yields statistically significant\nclassification improvements; however, the improvements this research observed\nbetween Pipeline A and B were not statistically significant. Thus, future\nresearch can explore the potential of LLM prompting in damage assessment. The\npractical contribution is an immediately deployable, flexible, and\ninterpretable workflow that bypasses the need for supervised training,\nsignificantly accelerating triage and prioritization for disaster response\npractitioners.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01895v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01895v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.354,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01898",
      "title": "DroneSR: Rethinking Few-shot Thermal Image Super-Resolution from\n  Drone-based Perspective",
      "authors": [
        "Zhipeng Weng",
        "Xiaopeng Liu",
        "Ce Liu",
        "Xingyuan Guo",
        "Yukai Shi",
        "Liang Lin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Although large scale models achieve significant improvements in performance,\nthe overfitting challenge still frequently undermines their generalization\nability. In super resolution tasks on images, diffusion models as\nrepresentatives of generative models typically adopt large scale architectures.\nHowever, few-shot drone-captured infrared training data frequently induces\nsevere overfitting in large-scale architectures. To address this key challenge,\nour method proposes a new Gaussian quantization representation learning method\noriented to diffusion models that alleviates overfitting and enhances\nrobustness. At the same time, an effective monitoring mechanism tracks large\nscale architectures during training to detect signs of overfitting. By\nintroducing Gaussian quantization representation learning, our method\neffectively reduces overfitting while maintaining architecture complexity. On\nthis basis, we construct a multi source drone-based infrared image benchmark\ndataset for detection and use it to emphasize overfitting issues of large scale\narchitectures in few sample, drone-based diverse drone-based image\nreconstruction scenarios. To verify the efficacy of the method in mitigating\noverfitting, experiments are conducted on the constructed benchmark.\nExperimental results demonstrate that our method outperforms existing super\nresolution approaches and significantly mitigates overfitting of large scale\narchitectures under complex conditions. The code and DroneSR dataset will be\navailable at: https://github.com/wengzp1/GARLSR.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01898v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01898v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.383,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is developing a method to mitigate overfitting in diffusion models for thermal image super-resolution, specifically for drone-captured infrared images. It uses diffusion models for iterative image generation and refinement, but this is limited to visual tasks like enhancing image quality, not for multi-step logical reasoning or solving complex logical tasks as defined in the topic. There is no evidence of treating a 'Chain-of-Thought' as an entity for holistic correction in reasoning processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01903",
      "title": "VISP: Volatility Informed Stochastic Projection for Adaptive\n  Regularization",
      "authors": [
        "Tanvir Islam"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We propose VISP: Volatility Informed Stochastic Projection, an adaptive\nregularization method that leverages gradient volatility to guide stochastic\nnoise injection in deep neural networks. Unlike conventional techniques that\napply uniform noise or fixed dropout rates, VISP dynamically computes\nvolatility from gradient statistics and uses it to scale a stochastic\nprojection matrix. This mechanism selectively regularizes inputs and hidden\nnodes that exhibit higher gradient volatility while preserving stable\nrepresentations, thereby mitigating overfitting. Extensive experiments on\nMNIST, CIFAR-10, and SVHN demonstrate that VISP consistently improves\ngeneralization performance over baseline models and fixed-noise alternatives.\nIn addition, detailed analyses of the evolution of volatility, the spectral\nproperties of the projection matrix, and activation distributions reveal that\nVISP not only stabilizes the internal dynamics of the network but also fosters\na more robust feature representation.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01903v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01903v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.358,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01907",
      "title": "RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster\n  Events",
      "authors": [
        "Zhenyuan Chen",
        "Chenxi Wang",
        "Ningyu Zhang",
        "Feng Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Remote sensing is critical for disaster monitoring, yet existing datasets\nlack temporal image pairs and detailed textual annotations. While\nsingle-snapshot imagery dominates current resources, it fails to capture\ndynamic disaster impacts over time. To address this gap, we introduce the\nRemote Sensing Change Caption (RSCC) dataset, a large-scale benchmark\ncomprising 62,315 pre-/post-disaster image pairs (spanning earthquakes, floods,\nwildfires, and more) paired with rich, human-like change captions. By bridging\nthe temporal and semantic divide in remote sensing data, RSCC enables robust\ntraining and evaluation of vision-language models for disaster-aware\nbi-temporal understanding. Our results highlight RSCC's ability to facilitate\ndetailed disaster-related analysis, paving the way for more accurate,\ninterpretable, and scalable vision-language applications in remote sensing.\nCode and dataset are available at https://github.com/Bili-Sakura/RSCC.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01907v4",
      "pdf_url": "http://arxiv.org/pdf/2509.01907v4",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.338,
      "datasets_score": 0.427,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of the RSCC dataset, a large-scale collection of pre-/post-disaster image pairs with detailed captions for remote sensing applications. It covers dataset creation through sourcing and annotation processes, analysis via comparisons with existing datasets (as shown in the table), and benchmarking by evaluating models on this dataset. This directly aligns with research on creating, analyzing, benchmarking, and evaluating datasets for AI and machine learning, making it a core focus of the paper.",
      "llm_score_status": "completed",
      "summary": "The paper introduces the RSCC dataset, a large-scale collection of 62,315 pre- and post-disaster image pairs from 31 global events such as earthquakes, floods, and wildfires, each accompanied by detailed human-like change captions, to address the shortcomings of existing remote sensing datasets that lack temporal pairs and rich textual annotations. The authors detail the dataset's construction, train a specialized multimodal large language model (MLLM) for remote sensing change captioning using RSCC, and establish a benchmark to evaluate state-of-the-art temporal MLLMs, demonstrating that RSCC significantly enhances models' capabilities in disaster-aware bi-temporal image understanding and analysis.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new dataset specifically tailored for disaster events with bi-temporal image pairs and detailed change captions, addressing a significant gap in existing remote sensing resources and advancing the state-of-the-art in vision-language models for dynamic event analysis.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of remote sensing and disaster monitoring by providing a new benchmark for vision-language models, though its influence may be limited to specialized applications rather than broader commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution by introducing a valuable dataset and benchmark that could advance research in remote sensing for disasters, making it essential for specialists in computer vision and multimodal learning to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b29d9127d3d357f83f20bdabfa109abd9db2b3c5",
      "total_authors": 4,
      "authors_found": 3,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zhenyuan Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378972270"
        },
        {
          "name": "Chenxi Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378712620"
        },
        {
          "name": "Ningyu Zhang",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Feng Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378707789"
        }
      ]
    },
    {
      "id": "2509.01909",
      "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for\n  Responsible Language Models",
      "authors": [
        "Ranjie Duan",
        "Jiexi Liu",
        "Xiaojun Jia",
        "Shiji Zhao",
        "Ruoxi Cheng",
        "Fengxiang Wang",
        "Cheng Wei",
        "Yong Xie",
        "Chang Liu",
        "Defeng Li",
        "Yinpeng Dong",
        "Yichi Zhang",
        "Yuefeng Chen",
        "Chongwen Wang",
        "Xingjun Ma",
        "Xingxing Wei",
        "Yang Liu",
        "Hang Su",
        "Jun Zhu",
        "Xinfeng Li",
        "Yitong Sun",
        "Jie Zhang",
        "Jinzhao Hu",
        "Sha Xu",
        "Wenchao Yang",
        "Yitong Yang",
        "Jialing Tao",
        "Hui Xue"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CY (Computers and Society)",
        "cs.HC (Human-Computer Interaction)",
        "cs.SC (Symbolic Computation)"
      ],
      "abstract": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01909v5",
      "pdf_url": "http://arxiv.org/pdf/2509.01909v5",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.47,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.339,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Constructive Safety Alignment (CSA) with components like game-theoretic modeling and Linguistic Backpropagation, but it does not describe using human feedback to train a reward model or fine-tune the model via reinforcement learning. Evaluations mention human assessments for benchmarks, but this is for testing, not training.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's Linguistic Backpropagation involves iterative refinement of reasoning trajectories, which shares conceptual similarities with diffusion-based iterative processes for logical tasks. However, it does not explicitly adapt or use diffusion models, focusing instead on a custom mechanism for structured reasoning without holistic Chain-of-Thought correction as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01910",
      "title": "Towards Interpretable Geo-localization: a Concept-Aware Global Image-GPS\n  Alignment Framework",
      "authors": [
        "Furong Jia",
        "Lanxin Liu",
        "Ce Hou",
        "Fan Zhang",
        "Xinyan Liu",
        "Yu Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Worldwide geo-localization involves determining the exact geographic location\nof images captured globally, typically guided by geographic cues such as\nclimate, landmarks, and architectural styles. Despite advancements in\ngeo-localization models like GeoCLIP, which leverages images and location\nalignment via contrastive learning for accurate predictions, the\ninterpretability of these models remains insufficiently explored. Current\nconcept-based interpretability methods fail to align effectively with\nGeo-alignment image-location embedding objectives, resulting in suboptimal\ninterpretability and performance. To address this gap, we propose a novel\nframework integrating global geo-localization with concept bottlenecks. Our\nmethod inserts a Concept-Aware Alignment Module that jointly projects image and\nlocation embeddings onto a shared bank of geographic concepts (e.g., tropical\nclimate, mountain, cathedral) and minimizes a concept-level loss, enhancing\nalignment in a concept-specific subspace and enabling robust interpretability.\nTo our knowledge, this is the first work to introduce interpretability into\ngeo-localization. Extensive experiments demonstrate that our approach surpasses\nGeoCLIP in geo-localization accuracy and boosts performance across diverse\ngeospatial prediction tasks, revealing richer semantic insights into geographic\ndecision-making processes.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01910v2",
      "pdf_url": "http://arxiv.org/pdf/2509.01910v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.333,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01914",
      "title": "How Real Is AI Tutoring? Comparing Simulated and Human Dialogues in\n  One-on-One Instruction",
      "authors": [
        "Ruijia Li",
        "Yuan-Hao Jiang",
        "Jiatong Wang",
        "Bo Jiang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Heuristic and scaffolded teacher-student dialogues are widely regarded as\ncritical for fostering students' higher-order thinking and deep learning.\nHowever, large language models (LLMs) currently face challenges in generating\npedagogically rich interactions. This study systematically investigates the\nstructural and behavioral differences between AI-simulated and authentic human\ntutoring dialogues. We conducted a quantitative comparison using an\nInitiation-Response-Feedback (IRF) coding scheme and Epistemic Network Analysis\n(ENA). The results show that human dialogues are significantly superior to\ntheir AI counterparts in utterance length, as well as in questioning (I-Q) and\ngeneral feedback (F-F) behaviors. More importantly, ENA results reveal a\nfundamental divergence in interactional patterns: human dialogues are more\ncognitively guided and diverse, centered around a \"question-factual\nresponse-feedback\" teaching loop that clearly reflects pedagogical guidance and\nstudent-driven thinking; in contrast, simulated dialogues exhibit a pattern of\nstructural simplification and behavioral convergence, revolving around an\n\"explanation-simplistic response\" loop that is essentially a simple information\ntransfer between the teacher and student. These findings illuminate key\nlimitations in current AI-generated tutoring and provide empirical guidance for\ndesigning and evaluating more pedagogically effective generative educational\ndialogue systems.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01914v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01914v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.444,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.295,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a comparison of AI-simulated and human tutoring dialogues using analytical methods like IRF coding and Epistemic Network Analysis, focusing on pedagogical differences. It does not involve training AI models, using human feedback to create reward models, or applying reinforcement learning for alignment with human preferences. As such, it lacks any connection to Reinforcement Learning from Human Feedback (RLHF) concepts.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01919",
      "title": "A Diffusion-Based Framework for Configurable and Realistic Multi-Storage\n  Trace Generation",
      "authors": [
        "Seohyun Kim",
        "Junyoung Lee",
        "Jongho Park",
        "Jinhyung Koo",
        "Sungjin Lee",
        "Yeseong Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.PF (Performance)"
      ],
      "abstract": "We propose DiTTO, a novel diffusion-based framework for generating realistic,\nprecisely configurable, and diverse multi-device storage traces. Leveraging\nadvanced diffusion techniques, DiTTO enables the synthesis of high-fidelity\ncontinuous traces that capture temporal dynamics and inter-device dependencies\nwith user-defined configurations. Our experimental results demonstrate that\nDiTTO can generate traces with high fidelity and diversity while aligning\nclosely with guided configurations with only 8% errors.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01919v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01919v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.251,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.47,
      "distributed_training_score": 0.403,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for generating synthetic storage traces, which is a generative task involving data synthesis and temporal modeling. It does not involve adapting diffusion for multi-step logical reasoning, such as treating a 'Chain-of-Thought' as a single entity for iterative correction and improvement. There is no component for complex logical tasks or holistic reasoning paths.",
      "distributed_training_justification": "The paper discusses generating traces for distributed storage systems like RAID arrays and CephFS, but it does not address distributed training, parallel computing, or multi-node machine learning techniques. It lacks any description of partitioning data, model architecture, or computation across multiple processors or nodes for accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01920",
      "title": "Dynamic Speculative Agent Planning",
      "authors": [
        "Yilin Guan",
        "Wenyue Hua",
        "Qingfeng Lan",
        "Sun Fei",
        "Dujian Ding",
        "Devang Acharya",
        "Chi Wang",
        "William Yang Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Despite their remarkable success in complex tasks propelling widespread\nadoption, large language-model-based agents still face critical deployment\nchallenges due to prohibitive latency and inference costs. While recent work\nhas explored various methods to accelerate inference, existing approaches\nsuffer from significant limitations: they either fail to preserve performance\nfidelity, require extensive offline training of router modules, or incur\nexcessive operational costs. Moreover, they provide minimal user control over\nthe tradeoff between acceleration and other performance metrics. To address\nthese gaps, we introduce Dynamic Speculative Planning (DSP), an asynchronous\nonline reinforcement learning framework that provides lossless acceleration\nwith substantially reduced costs without requiring additional pre-deployment\npreparation. DSP explicitly optimizes a joint objective balancing end-to-end\nlatency against dollar cost, allowing practitioners to adjust a single\nparameter that steers the system toward faster responses, cheaper operation, or\nany point along this continuum. Experiments on two standard agent benchmarks\ndemonstrate that DSP achieves comparable efficiency to the fastest lossless\nacceleration method while reducing total cost by 30% and unnecessary cost up to\n60%. Our code and data are available through\nhttps://github.com/guanyilin428/Dynamic-Speculative-Planning.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01920v2",
      "pdf_url": "http://arxiv.org/pdf/2509.01920v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.421,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces Dynamic Speculative Planning (DSP) for accelerating inference in language model agents using reinforcement learning, focusing on predicting and verifying steps. It does not involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for holistic correction in logical tasks.",
      "distributed_training_justification": "The paper deals with runtime inference acceleration for agents, including step parallelization, but does not address distributed training, parallel computing for model training, or strategies for partitioning data/computation across multiple nodes during the training phase.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01938",
      "title": "EigenBench: A Comparative Behavioral Measure of Value Alignment",
      "authors": [
        "Jonathn Chang",
        "Leonhard Piff",
        "Suvadip Sana",
        "Jasmine X. Li",
        "Lionel Levine"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CY (Computers and Society)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Aligning AI with human values is a pressing unsolved problem. To address the\nlack of quantitative metrics for value alignment, we propose EigenBench: a\nblack-box method for comparatively benchmarking language models' values. Given\nan ensemble of models, a constitution describing a value system, and a dataset\nof scenarios, our method returns a vector of scores quantifying each model's\nalignment to the given constitution. To produce these scores, each model judges\nthe outputs of other models across many scenarios, and these judgments are\naggregated with EigenTrust (Kamvar et al, 2003), yielding scores that reflect a\nweighted-average judgment of the whole ensemble. EigenBench uses no ground\ntruth labels, as it is designed to quantify traits for which reasonable judges\nmay disagree on the correct label. Using prompted personas, we test whether\nEigenBench scores are more sensitive to the model or the prompt: we find that\nmost of the variance is explained by the prompt, but a small residual\nquantifies the disposition of the model itself.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01938v2",
      "pdf_url": "http://arxiv.org/pdf/2509.01938v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.494,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.331,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on EigenBench, a method for benchmarking language models' value alignment using mutual judgments from an ensemble of models, without relying on human feedback or reinforcement learning. While it touches on AI alignment and mentions LM feedback as an alternative to human feedback (e.g., in character training), it does not involve training a reward model on human-ranked data or fine-tuning via reinforcement learning, which are core to RLHF. Thus, the connection is indirect, as both address alignment but through different mechanisms.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01943",
      "title": "A Continuous Encoding-Based Representation for Efficient Multi-Fidelity\n  Multi-Objective Neural Architecture Search",
      "authors": [
        "Zhao Wei",
        "Chin Chun Ooi",
        "Yew-Soon Ong"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Neural architecture search (NAS) is an attractive approach to automate the\ndesign of optimized architectures but is constrained by high computational\nbudget, especially when optimizing for multiple, important conflicting\nobjectives. To address this, an adaptive Co-Kriging-assisted multi-fidelity\nmulti-objective NAS algorithm is proposed to further reduce the computational\ncost of NAS by incorporating a clustering-based local multi-fidelity infill\nsampling strategy, enabling efficient exploration of the search space for\nfaster convergence. This algorithm is further accelerated by the use of a novel\ncontinuous encoding method to represent the connections of nodes in each cell\nwithin a generalized cell-based U-Net backbone, thereby decreasing the search\ndimension (number of variables). Results indicate that the proposed NAS\nalgorithm outperforms previously published state-of-the-art methods under\nlimited computational budget on three numerical benchmarks, a 2D Darcy flow\nregression problem and a CHASE_DB1 biomedical image segmentation problem. The\nproposed method is subsequently used to create a wind velocity regression model\nwith application in urban modelling, with the found model able to achieve good\nprediction with less computational complexity. Further analysis revealed that\nthe NAS algorithm independently identified principles undergirding superior\nU-Net architectures in other literature, such as the importance of allowing\neach cell to incorporate information from prior cells.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01943v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01943v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.387,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01944",
      "title": "AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity\n  for VLA Model in Autonomous Driving",
      "authors": [
        "Zhenlong Yuan",
        "Jing Tang",
        "Jinguo Luo",
        "Rui Chen",
        "Chengxuan Qian",
        "Lei Sun",
        "Xiangxiang Chu",
        "Yujun Cai",
        "Dapeng Zhang",
        "Shuo Li"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Vision-Language-Action (VLA) models in autonomous driving systems have\nrecently demonstrated transformative potential by integrating multimodal\nperception with decision-making capabilities. However, the interpretability and\ncoherence of the decision process and the plausibility of action sequences\nremain largely underexplored. To address these issues, we propose\nAutoDrive-R$^2$, a novel VLA framework that enhances both reasoning and\nself-reflection capabilities of autonomous driving systems through\nchain-of-thought (CoT) processing and reinforcement learning (RL).\nSpecifically, we first propose an innovative CoT dataset named nuScenesR$^2$-6K\nfor supervised fine-tuning, which effectively builds cognitive bridges between\ninput information and output trajectories through a four-step logical chain\nwith self-reflection for validation. Moreover, to maximize both reasoning and\nself-reflection during the RL stage, we further employ the Group Relative\nPolicy Optimization (GRPO) algorithm within a physics-grounded reward framework\nthat incorporates spatial alignment, vehicle dynamic, and temporal smoothness\ncriteria to ensure reliable and realistic trajectory planning. Extensive\nevaluation results across both nuScenes and Waymo datasets demonstrates the\nstate-of-the-art performance and robust generalization capacity of our proposed\nmethod.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01944v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01944v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.468,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.542,
      "distributed_training_score": 0.388,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning (RL) with Group Relative Policy Optimization (GRPO) and a physics-grounded reward framework, but it does not involve human feedback, a reward model trained on human-ranked data, or any alignment with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes chain-of-thought (CoT) processing for reasoning and self-reflection, but it does not employ diffusion models, iterative refinement processes, or treat the reasoning path as a holistically corrected entity over multiple steps as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01959",
      "title": "Structure-aware Contrastive Learning for Diagram Understanding of\n  Multimodal Models",
      "authors": [
        "Hiroshi Sasaki"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Multimodal models, such as the Contrastive Language-Image Pre-training (CLIP)\nmodel, have demonstrated remarkable success in aligning visual and linguistic\nrepresentations. However, these models exhibit limitations when applied to\nspecialised visual domains, such as diagrams, which encode structured, symbolic\ninformation distinct from that of natural imagery.\n  In this paper, we introduce a novel training paradigm explicitly designed to\nenhance the comprehension of diagrammatic images within vision-language models.\nOur approach uses ``hard'' samples for our proposed contrastive learning that\nincorporates two specialised loss functions that leverage the inherent\nstructural properties of diagrams. By integrating these objectives into model\ntraining, our method enables models to develop a more structured and\nsemantically coherent understanding of diagrammatic content.\n  We empirically validate our approach on a benchmark dataset of flowcharts, as\na representative class of diagrammatic imagery, demonstrating substantial\nimprovements over standard CLIP and conventional hard negative CLIP learning\nparadigms for both image-text matching and visual question answering tasks. Our\nfindings underscore the significance of tailored training strategies for\nspecialised tasks and contribute to advancing diagrammatic understanding within\nthe broader landscape of vision-language integration.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01959v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01959v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.479,
      "distributed_training_score": 0.333,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on structure-aware contrastive learning to improve diagram understanding in vision-language models, using specialized loss functions and hard samples. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01964",
      "title": "2D Gaussian Splatting with Semantic Alignment for Image Inpainting",
      "authors": [
        "Hongyu Li",
        "Chaofeng Chen",
        "Xiaoming Li",
        "Guangming Lu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Gaussian Splatting (GS), a recent technique for converting discrete points\ninto continuous spatial representations, has shown promising results in 3D\nscene modeling and 2D image super-resolution. In this paper, we explore its\nuntapped potential for image inpainting, which demands both locally coherent\npixel synthesis and globally consistent semantic restoration. We propose the\nfirst image inpainting framework based on 2D Gaussian Splatting, which encodes\nincomplete images into a continuous field of 2D Gaussian splat coefficients and\nreconstructs the final image via a differentiable rasterization process. The\ncontinuous rendering paradigm of GS inherently promotes pixel-level coherence\nin the inpainted results. To improve efficiency and scalability, we introduce a\npatch-wise rasterization strategy that reduces memory overhead and accelerates\ninference. For global semantic consistency, we incorporate features from a\npretrained DINO model. We observe that DINO's global features are naturally\nrobust to small missing regions and can be effectively adapted to guide\nsemantic alignment in large-mask scenarios, ensuring that the inpainted content\nremains contextually consistent with the surrounding scene. Extensive\nexperiments on standard benchmarks demonstrate that our method achieves\ncompetitive performance in both quantitative metrics and perceptual quality,\nestablishing a new direction for applying Gaussian Splatting to 2D image\nprocessing.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01964v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01964v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.31,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a framework for image inpainting using 2D Gaussian Splatting, focusing on continuous spatial representations, patch-wise rasterization, and semantic alignment with DINO features. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning on a 'Chain-of-Thought'. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01968",
      "title": "Ensemble-Based Event Camera Place Recognition Under Varying Illumination",
      "authors": [
        "Therese Joseph",
        "Tobias Fischer",
        "Michael Milford"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Compared to conventional cameras, event cameras provide a high dynamic range\nand low latency, offering greater robustness to rapid motion and challenging\nlighting conditions. Although the potential of event cameras for visual place\nrecognition (VPR) has been established, developing robust VPR frameworks under\nsevere illumination changes remains an open research problem. In this paper, we\nintroduce an ensemble-based approach to event camera place recognition that\ncombines sequence-matched results from multiple event-to-frame reconstructions,\nVPR feature extractors, and temporal resolutions. Unlike previous event-based\nensemble methods, which only utilise temporal resolution, our broader fusion\nstrategy delivers significantly improved robustness under varied lighting\nconditions (e.g., afternoon, sunset, night), achieving a 57% relative\nimprovement in Recall@1 across day-night transitions. We evaluate our approach\non two long-term driving datasets (with 8 km per traverse) without metric\nsubsampling, thereby preserving natural variations in speed and stop duration\nthat influence event density. We also conduct a comprehensive analysis of key\ndesign choices, including binning strategies, polarity handling, reconstruction\nmethods, and feature extractors, to identify the most critical components for\nrobust performance. Additionally, we propose a modification to the standard\nsequence matching framework that enhances performance at longer sequence\nlengths. To facilitate future research, we will release our codebase and\nbenchmarking framework.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01968v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01968v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.288,
      "weak_supervision_score": 0.278,
      "diffusion_reasoning_score": 0.334,
      "distributed_training_score": 0.295,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01977",
      "title": "MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware\n  Alignment and Disentanglement",
      "authors": [
        "Dong She",
        "Siming Fu",
        "Mushui Liu",
        "Qiaoqiao Jin",
        "Hualiang Wang",
        "Mu Liu",
        "Jidong Jiang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multi-subject personalized generation presents unique challenges in\nmaintaining identity fidelity and semantic coherence when synthesizing images\nconditioned on multiple reference subjects. Existing methods often suffer from\nidentity blending and attribute leakage due to inadequate modeling of how\ndifferent subjects should interact within shared representation spaces. We\npresent MOSAIC, a representation-centric framework that rethinks multi-subject\ngeneration through explicit semantic correspondence and orthogonal feature\ndisentanglement. Our key insight is that multi-subject generation requires\nprecise semantic alignment at the representation level - knowing exactly which\nregions in the generated image should attend to which parts of each reference.\nTo enable this, we introduce SemAlign-MS, a meticulously annotated dataset\nproviding fine-grained semantic correspondences between multiple reference\nsubjects and target images, previously unavailable in this domain. Building on\nthis foundation, we propose the semantic correspondence attention loss to\nenforce precise point-to-point semantic alignment, ensuring high consistency\nfrom each reference to its designated regions. Furthermore, we develop the\nmulti-reference disentanglement loss to push different subjects into orthogonal\nattention subspaces, preventing feature interference while preserving\nindividual identity characteristics. Extensive experiments demonstrate that\nMOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably,\nwhile existing methods typically degrade beyond 3 subjects, MOSAIC maintains\nhigh fidelity with 4+ reference subjects, opening new possibilities for complex\nmulti-subject synthesis applications.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01977v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01977v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.433,
      "distributed_training_score": 0.372,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on MOSAIC, a framework for multi-subject image generation using diffusion models to enhance semantic alignment and disentanglement in visual synthesis. While it involves diffusion processes for iterative image refinement, it does not adapt these processes for solving complex logical tasks, treating a Chain-of-Thought as an entity, or enabling multi-step logical reasoning. Instead, the work is centered on visual generation tasks, lacking any components for logical inference or reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01984",
      "title": "Discrete Noise Inversion for Next-scale Autoregressive Text-based Image\n  Editing",
      "authors": [
        "Quan Dao",
        "Xiaoxiao He",
        "Ligong Han",
        "Ngan Hoai Nguyen",
        "Amin Heyrani Nobar",
        "Faez Ahmed",
        "Han Zhang",
        "Viet Anh Nguyen",
        "Dimitris Metaxas"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Visual autoregressive models (VAR) have recently emerged as a promising class\nof generative models, achieving performance comparable to diffusion models in\ntext-to-image generation tasks. While conditional generation has been widely\nexplored, the ability to perform prompt-guided image editing without additional\ntraining is equally critical, as it supports numerous practical real-world\napplications. This paper investigates the text-to-image editing capabilities of\nVAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise\ninversion-based editing technique designed explicitly for VAR models. VARIN\nleverages a novel pseudo-inverse function for argmax sampling, named\nLocation-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These\ninverse noises enable precise reconstruction of the source image and facilitate\ntargeted, controllable edits aligned with textual prompts. Extensive\nexperiments demonstrate that VARIN effectively modifies source images according\nto specified prompts while significantly preserving the original background and\nstructural details, thus validating its efficacy as a practical editing\napproach.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01984v2",
      "pdf_url": "http://arxiv.org/pdf/2509.01984v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.449,
      "distributed_training_score": 0.304,
      "datasets_score": 0.285,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on developing a noise inversion technique for visual autoregressive models to enable text-based image editing, drawing inspiration from diffusion models for image generation tasks. However, it does not involve adapting diffusion processes for multi-step logical reasoning, Chain-of-Thought processing, or solving complex logical tasks. The core contribution is in visual editing, not reasoning, making it unrelated to the specified topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01986",
      "title": "Draw-In-Mind: Learning Precise Image Editing via Chain-of-Thought\n  Imagination",
      "authors": [
        "Ziyun Zeng",
        "Junhao Zhang",
        "Wei Li",
        "Mike Zheng Shou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In recent years, integrating multimodal understanding and generation into a\nsingle unified model has emerged as a promising paradigm. While this approach\nachieves strong results in text-to-image (T2I) generation, it still struggles\nwith precise image editing. We attribute this limitation to an imbalanced\ndivision of responsibilities. The understanding module primarily functions as a\ntranslator that encodes user instructions into semantic conditions, while the\ngeneration module must simultaneously act as designer and painter, inferring\nthe original layout, identifying the target editing region, and rendering the\nnew content. This imbalance is counterintuitive because the understanding\nmodule is typically trained with several times more data on complex reasoning\ntasks than the generation module. To address this issue, we introduce\nDraw-In-Mind (DIM), a dataset comprising two complementary subsets: (i)\nDIM-T2I, containing 14M long-context image-text pairs to enhance complex\ninstruction comprehension; and (ii) DIM-Edit, consisting of 233K\nchain-of-thought imaginations generated by GPT-4o, serving as explicit design\nblueprints for image edits. We connect a frozen Qwen2.5-VL-3B with a trainable\nSANA1.5-1.6B via a lightweight two-layer MLP, and train it on the proposed DIM\ndataset, resulting in DIM-4.6B-T2I/Edit. Despite its modest parameter scale,\nDIM-4.6B-Edit achieves SOTA or competitive performance on the ImgEdit and\nGEdit-Bench benchmarks, outperforming much larger models such as UniWorld-V1\nand Step1X-Edit. These findings demonstrate that explicitly assigning the\ndesign responsibility to the understanding module provides significant benefits\nfor image editing. Our dataset and models will be available at\nhttps://github.com/showlab/DIM.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01986v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01986v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.499,
      "distributed_training_score": 0.366,
      "datasets_score": 0.388,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a diffusion model (SANA1.5-1.6B) primarily for image generation and editing, which involves iterative refinement processes typical of diffusion models. However, it does not adapt diffusion for multi-step logical reasoning or treat a chain-of-thought as a single entity for holistic correction. Instead, chain-of-thought is handled by GPT-4o or the understanding module, making the diffusion component supportive but not central to reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01991",
      "title": "Explaining What Machines See: XAI Strategies in Deep Object Detection\n  Models",
      "authors": [
        "FatemehSadat Seyedmomeni",
        "Mohammad Ali Keyvanrad"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In recent years, deep learning has achieved unprecedented success in various\ncomputer vision tasks, particularly in object detection. However, the black-box\nnature and high complexity of deep neural networks pose significant challenges\nfor interpretability, especially in critical domains such as autonomous\ndriving, medical imaging, and security systems. Explainable Artificial\nIntelligence (XAI) aims to address this challenge by providing tools and\nmethods to make model decisions more transparent, interpretable, and\ntrust-worthy for humans. This review provides a comprehensive analysis of\nstate-of-the-art explain-ability methods specifically applied to object\ndetection models. The paper be-gins by categorizing existing XAI techniques\nbased on their underlying mechanisms-perturbation-based, gradient-based,\nbackpropagation-based, and graph-based methods. Notable methods such as D-RISE,\nBODEM, D-CLOSE, and FSOD are discussed in detail. Furthermore, the paper\ninvestigates their applicability to various object detection architectures,\nincluding YOLO, SSD, Faster R-CNN, and EfficientDet. Statistical analysis of\npublication trends from 2022 to mid-2025 shows an accelerating interest in\nexplainable object detection, indicating its increasing importance. The study\nalso explores common datasets and evaluation metrics, and highlights the major\nchallenges associated with model interpretability. By providing a structured\ntaxonomy and a critical assessment of existing methods, this review aims to\nguide researchers and practitioners in selecting suitable explainability\ntechniques for object detection applications and to foster the development of\nmore interpretable AI systems.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01991v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01991v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.305,
      "datasets_score": 0.385,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.01997",
      "title": "ACA-Net: Future Graph Learning for Logistical Demand-Supply Forecasting",
      "authors": [
        "Jiacheng Shi",
        "Haibin Wei",
        "Jiang Wang",
        "Xiaowei Xu",
        "Longzhi Du",
        "Taixu Jiang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Logistical demand-supply forecasting that evaluates the alignment between\nprojected supply and anticipated demand, is essential for the efficiency and\nquality of on-demand food delivery platforms and serves as a key indicator for\nscheduling decisions. Future order distribution information, which reflects the\ndistribution of orders in on-demand food delivery, is crucial for the\nperformance of logistical demand-supply forecasting. Current studies utilize\nspatial-temporal analysis methods to model future order distribution\ninformation from serious time slices. However, learning future order\ndistribution in online delivery platform is a time-series-insensitive problem\nwith strong randomness. These approaches often struggle to effectively capture\nthis information while remaining efficient. This paper proposes an innovative\nspatiotemporal learning model that utilizes only two graphs (ongoing and\nglobal) to learn future order distribution information, achieving superior\nperformance compared to traditional spatial-temporal long-series methods. The\nmain contributions are as follows: (1) The introduction of ongoing and global\ngraphs in logistical demand-supply pressure forecasting compared to traditional\nlong time series significantly enhances forecasting performance. (2) An\ninnovative graph learning network framework using adaptive future graph\nlearning and innovative cross attention mechanism (ACA-Net) is proposed to\nextract future order distribution information, effectively learning a robust\nfuture graph that substantially improves logistical demand-supply pressure\nforecasting outcomes. (3) The effectiveness of the proposed method is validated\nin real-world production environments.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.01997v1",
      "pdf_url": "http://arxiv.org/pdf/2509.01997v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.372,
      "distributed_training_score": 0.367,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02000",
      "title": "Palette Aligned Image Diffusion",
      "authors": [
        "Elad Aharoni",
        "Noy Porat",
        "Dani Lischinski",
        "Ariel Shamir"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce the Palette-Adapter, a novel method for conditioning\ntext-to-image diffusion models on a user-specified color palette. While\npalettes are a compact and intuitive tool widely used in creative workflows,\nthey introduce significant ambiguity and instability when used for conditioning\nimage generation. Our approach addresses this challenge by interpreting\npalettes as sparse histograms and introducing two scalar control parameters:\nhistogram entropy and palette-to-histogram distance, which allow flexible\ncontrol over the degree of palette adherence and color variation. We further\nintroduce a negative histogram mechanism that allows users to suppress specific\nundesired hues, improving adherence to the intended palette under the standard\nclassifier-free guidance mechanism. To ensure broad generalization across the\ncolor space, we train on a carefully curated dataset with balanced coverage of\nrare and common colors. Our method enables stable, semantically coherent\ngeneration across a wide range of palettes and prompts. We evaluate our method\nqualitatively, quantitatively, and through a user study, and show that it\nconsistently outperforms existing approaches in achieving both strong palette\nadherence and high image quality.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02000v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02000v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.473,
      "distributed_training_score": 0.309,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing text-to-image diffusion models for color palette conditioning in image generation, emphasizing visual control and iterative refinement for aesthetics. It does not involve adapting the diffusion process for multi-step logical reasoning, chain-of-thought processing, or solving complex logical tasks, which are the core elements of the topic. Thus, there is no clear component of diffusion-based reasoning in the paper.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02007",
      "title": "mFARM: Towards Multi-Faceted Fairness Assessment based on HARMs in\n  Clinical Decision Support",
      "authors": [
        "Shreyash Adappanavar",
        "Krithi Shailya",
        "Gokul S Krishnan",
        "Sriraam Natarajan",
        "Balaraman Ravindran"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The deployment of Large Language Models (LLMs) in high-stakes medical\nsettings poses a critical AI alignment challenge, as models can inherit and\namplify societal biases, leading to significant disparities. Existing fairness\nevaluation methods fall short in these contexts as they typically use\nsimplistic metrics that overlook the multi-dimensional nature of medical harms.\nThis also promotes models that are fair only because they are clinically inert,\ndefaulting to safe but potentially inaccurate outputs. To address this gap, our\ncontributions are mainly two-fold: first, we construct two large-scale,\ncontrolled benchmarks (ED-Triage and Opioid Analgesic Recommendation) from\nMIMIC-IV, comprising over 50,000 prompts with twelve race x gender variants and\nthree context tiers. Second, we propose a multi-metric framework -\nMulti-faceted Fairness Assessment based on hARMs ($mFARM$) to audit fairness\nfor three distinct dimensions of disparity (Allocational, Stability, and\nLatent) and aggregate them into an $mFARM$ score. We also present an aggregated\nFairness-Accuracy Balance (FAB) score to benchmark and observe trade-offs\nbetween fairness and prediction accuracy. We empirically evaluate four\nopen-source LLMs (Mistral-7B, BioMistral-7B, Qwen-2.5-7B, Bio-LLaMA3-8B) and\ntheir finetuned versions under quantization and context variations. Our\nfindings showcase that the proposed $mFARM$ metrics capture subtle biases more\neffectively under various settings. We find that most models maintain robust\nperformance in terms of $mFARM$ score across varying levels of quantization but\ndeteriorate significantly when the context is reduced. Our benchmarks and\nevaluation code are publicly released to enhance research in aligned AI for\nhealthcare.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02007v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02007v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.511,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.408,
      "datasets_score": 0.398,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on fairness assessment frameworks, benchmarks, and fine-tuning of LLMs for clinical decision support, but it does not involve reinforcement learning, human feedback, or training a reward model based on human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses evaluating LLMs under quantization and context variations, as well as fine-tuning, but it does not cover distributed training, parallel computing, multi-node setups, or strategies for partitioning data and computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02017",
      "title": "Empowering Large Language Model for Sequential Recommendation via\n  Multimodal Embeddings and Semantic IDs",
      "authors": [
        "Yuhao Wang",
        "Junwei Pan",
        "Xinhang Li",
        "Maolin Wang",
        "Yuan Wang",
        "Yue Liu",
        "Dapeng Liu",
        "Jie Jiang",
        "Xiangyu Zhao"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Sequential recommendation (SR) aims to capture users' dynamic interests and\nsequential patterns based on their historical interactions. Recently, the\npowerful capabilities of large language models (LLMs) have driven their\nadoption in SR. However, we identify two critical challenges in existing\nLLM-based SR methods: 1) embedding collapse when incorporating pre-trained\ncollaborative embeddings and 2) catastrophic forgetting of quantized embeddings\nwhen utilizing semantic IDs. These issues dampen the model scalability and lead\nto suboptimal recommendation performance. Therefore, based on LLMs like\nLlama3-8B-instruct, we introduce a novel SR framework named MME-SID, which\nintegrates multimodal embeddings and quantized embeddings to mitigate embedding\ncollapse. Additionally, we propose a Multimodal Residual Quantized Variational\nAutoencoder (MM-RQ-VAE) with maximum mean discrepancy as the reconstruction\nloss and contrastive learning for alignment, which effectively preserve\nintra-modal distance information and capture inter-modal correlations,\nrespectively. To further alleviate catastrophic forgetting, we initialize the\nmodel with the trained multimodal code embeddings. Finally, we fine-tune the\nLLM efficiently using LoRA in a multimodal frequency-aware fusion manner.\nExtensive experiments on three public datasets validate the superior\nperformance of MME-SID thanks to its capability to mitigate embedding collapse\nand catastrophic forgetting. The implementation code and datasets are publicly\navailable for reproduction:\nhttps://github.com/Applied-Machine-Learning-Lab/MME-SID.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02017v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02017v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.399,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on enhancing sequential recommendation using LLMs with multimodal embeddings and semantic IDs, addressing embedding collapse and catastrophic forgetting through techniques like MM-RQ-VAE and LoRA fine-tuning. It does not involve training a reward model based on human-ranked data or using reinforcement learning to align with human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a framework for sequential recommendation that uses variational autoencoders and contrastive learning, but it does not adapt diffusion processes for multi-step logical reasoning or treat Chain-of-Thought as an entity for iterative refinement, as required for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02018",
      "title": "Vision-Based Embedded System for Noncontact Monitoring of Preterm Infant\n  Behavior in Low-Resource Care Settings",
      "authors": [
        "Stanley Mugisha",
        "Rashid Kisitu",
        "Francis Komakech",
        "Excellence Favor"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CY (Computers and Society)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Preterm birth remains a leading cause of neonatal mortality,\ndisproportionately affecting low-resource settings with limited access to\nadvanced neonatal intensive care units (NICUs).Continuous monitoring of infant\nbehavior, such as sleep/awake states and crying episodes, is critical but\nrelies on manual observation or invasive sensors, which are prone to error,\nimpractical, and can cause skin damage. This paper presents a novel,\nnoninvasive, and automated vision-based framework to address this gap. We\nintroduce an embedded monitoring system that utilizes a quantized MobileNet\nmodel deployed on a Raspberry Pi for real-time behavioral state detection. When\ntrained and evaluated on public neonatal image datasets, our system achieves\nstate-of-the-art accuracy (91.8% for sleep detection and 97.7% for\ncrying/normal classification) while maintaining computational efficiency\nsuitable for edge deployment. Through comparative benchmarking, we provide a\ncritical analysis of the trade-offs between model size, inference latency, and\ndiagnostic accuracy. Our findings demonstrate that while larger architectures\n(e.g., ResNet152, VGG19) offer marginal gains in accuracy, their computational\ncost is prohibitive for real-time edge use. The proposed framework integrates\nthree key innovations: model quantization for memory-efficient inference (68%\nreduction in size), Raspberry Pi-optimized vision pipelines, and secure IoT\ncommunication for clinical alerts. This work conclusively shows that\nlightweight, optimized models such as the MobileNet offer the most viable\nfoundation for scalable, low-cost, and clinically actionable NICU monitoring\nsystems, paving the way for improved preterm care in resource-constrained\nenvironments.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02018v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02018v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.354,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02024",
      "title": "Unsupervised Training of Vision Transformers with Synthetic Negatives",
      "authors": [
        "Nikolaos Giakoumoglou",
        "Andreas Floros",
        "Kleanthis Marios Papadopoulos",
        "Tania Stathaki"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper does not introduce a novel method per se. Instead, we address the\nneglected potential of hard negative samples in self-supervised learning.\nPrevious works explored synthetic hard negatives but rarely in the context of\nvision transformers. We build on this observation and integrate synthetic hard\nnegatives to improve vision transformer representation learning. This simple\nyet effective technique notably improves the discriminative power of learned\nrepresentations. Our experiments show performance improvements for both DeiT-S\nand Swin-T architectures.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02024v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02024v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.444,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.375,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is improving unsupervised self-supervised learning for vision transformers by incorporating synthetic hard negatives in contrastive training, without any reliance on programmatically generated labels or weak supervisory signals. Weak supervision specifically involves using noisy or imprecise sources to create training labels, which is not addressed or utilized in this work.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02028",
      "title": "See No Evil: Adversarial Attacks Against Linguistic-Visual Association\n  in Referring Multi-Object Tracking Systems",
      "authors": [
        "Halima Bouzidi",
        "Haoyu Liu",
        "Mohammad Abdullah Al Faruque"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Language-vision understanding has driven the development of advanced\nperception systems, most notably the emerging paradigm of Referring\nMulti-Object Tracking (RMOT). By leveraging natural-language queries, RMOT\nsystems can selectively track objects that satisfy a given semantic\ndescription, guided through Transformer-based spatial-temporal reasoning\nmodules. End-to-End (E2E) RMOT models further unify feature extraction,\ntemporal memory, and spatial reasoning within a Transformer backbone, enabling\nlong-range spatial-temporal modeling over fused textual-visual representations.\nDespite these advances, the reliability and robustness of RMOT remain\nunderexplored. In this paper, we examine the security implications of RMOT\nsystems from a design-logic perspective, identifying adversarial\nvulnerabilities that compromise both the linguistic-visual referring and\ntrack-object matching components. Additionally, we uncover a novel\nvulnerability in advanced RMOT models employing FIFO-based memory, whereby\ntargeted and consistent attacks on their spatial-temporal reasoning introduce\nerrors that persist within the history buffer over multiple subsequent frames.\nWe present VEIL, a novel adversarial framework designed to disrupt the unified\nreferring-matching mechanisms of RMOT models. We show that carefully crafted\ndigital and physical perturbations can corrupt the tracking logic reliability,\ninducing track ID switches and terminations. We conduct comprehensive\nevaluations using the Refer-KITTI dataset to validate the effectiveness of VEIL\nand demonstrate the urgent need for security-aware RMOT designs for critical\nlarge-scale applications.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02028v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02028v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.415,
      "distributed_training_score": 0.314,
      "datasets_score": 0.287,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of an adversarial attack framework (VEIL) targeting vulnerabilities in Referring Multi-Object Tracking (RMOT) systems, which rely on Transformer-based architectures for language-vision fusion and spatial-temporal reasoning. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks. The core elements discussed, such as cross-attention and adversarial perturbations, are unrelated to the topic of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02029",
      "title": "Fake & Square: Training Self-Supervised Vision Transformers with\n  Synthetic Data and Synthetic Hard Negatives",
      "authors": [
        "Nikolaos Giakoumoglou",
        "Andreas Floros",
        "Kleanthis Marios Papadopoulos",
        "Tania Stathaki"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper does not introduce a new method per se. Instead, we build on\nexisting self-supervised learning approaches for vision, drawing inspiration\nfrom the adage \"fake it till you make it\". While contrastive self-supervised\nlearning has achieved remarkable success, it typically relies on vast amounts\nof real-world data and carefully curated hard negatives. To explore\nalternatives to these requirements, we investigate two forms of \"faking it\" in\nvision transformers. First, we study the potential of generative models for\nunsupervised representation learning, leveraging synthetic data to augment\nsample diversity. Second, we examine the feasibility of generating synthetic\nhard negatives in the representation space, creating diverse and challenging\ncontrasts. Our framework - dubbed Syn2Co - combines both approaches and\nevaluates whether synthetically enhanced training can lead to more robust and\ntransferable visual representations on DeiT-S and Swin-T architectures. Our\nfindings highlight the promise and limitations of synthetic data in\nself-supervised learning, offering insights for future work in this direction.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02029v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02029v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.442,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.381,
      "datasets_score": 0.396,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper explores using synthetic data and synthetic hard negatives for self-supervised learning in vision transformers, which reduces reliance on hand-labeled data by programmatically generating training samples. This aligns with weak supervision's core idea of deriving training signals from noisy or imprecise sources, as synthetic data serves as an alternative to real-world data. However, the paper focuses on data augmentation for contrastive learning rather than directly generating labels, making it only moderately relevant rather than highly so.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper builds on existing self-supervised learning methods for vision transformers by introducing synthetic data generated from diffusion models and synthetic hard negatives in the representation space to enhance training diversity and create challenging contrasts. The authors propose the Syn2Co framework, applied to DeiT-S and Swin-T architectures, to investigate whether these synthetic elements can lead to more robust and transferable visual representations, revealing promising results in improving accuracy on tasks like ImageNet-100 while highlighting limitations in fully replacing real data.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by cleverly combining existing self-supervised techniques with synthetic data and hard negatives, offering a new way to address data diversity without introducing an entirely novel method or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of self-supervised learning for computer vision, as it provides practical insights into using synthetic data to overcome real-world data limitations, though its influence may be confined to specific applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution with valuable insights into synthetic data usage in self-supervised learning, making it important for researchers in computer vision and AI to be aware of, though it is not essential for those outside the immediate subfield.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7f55b4dfd79f1c739b108bf3dc580de3ca1737e2",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 7,
      "average_h_index": 2.25,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Nikolaos Giakoumoglou",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2196360101"
        },
        {
          "name": "Andreas Floros",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378706169"
        },
        {
          "name": "Kleanthis Marios Papadopoulos",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2291706510"
        },
        {
          "name": "Tania Stathaki",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2292259667"
        }
      ]
    },
    {
      "id": "2509.02031",
      "title": "Synesthesia of Machines (SoM)-Based Task-Driven MIMO System for Image\n  Transmission",
      "authors": [
        "Sijiang Li",
        "Rongqing Zhang",
        "Xiang Cheng",
        "Jian Tang"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "To support cooperative perception (CP) of networked mobile agents in dynamic\nscenarios, the efficient and robust transmission of sensory data is a critical\nchallenge. Deep learning-based joint source-channel coding (JSCC) has\ndemonstrated promising results for image transmission under adverse channel\nconditions, outperforming traditional rule-based codecs. While recent works\nhave explored to combine JSCC with the widely adopted multiple-input\nmultiple-output (MIMO) technology, these approaches are still limited to the\ndiscrete-time analog transmission (DTAT) model and simple tasks. Given the\nlimited performance of existing MIMO JSCC schemes in supporting complex CP\ntasks for networked mobile agents with digital MIMO communication systems, this\npaper presents a Synesthesia of Machines (SoM)-based task-driven MIMO system\nfor image transmission, referred to as SoM-MIMO. By leveraging the structural\nproperties of the feature pyramid for perceptual tasks and the channel\nproperties of the closed-loop MIMO communication system, SoM-MIMO enables\nefficient and robust digital MIMO transmission of images. Experimental results\nhave shown that compared with two JSCC baseline schemes, our approach achieves\naverage mAP improvements of 6.30 and 10.48 across all SNR levels, while\nmaintaining identical communication overhead.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02031v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02031v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.382,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a deep learning-based system for efficient image transmission using MIMO technology and joint source-channel coding, aimed at cooperative perception tasks. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning. Therefore, it has no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02032",
      "title": "ContextFusion and Bootstrap: An Effective Approach to Improve Slot\n  Attention-Based Object-Centric Learning",
      "authors": [
        "Pinzhuo Tian",
        "Shengjie Yang",
        "Hang Yu",
        "Alex C. Kot"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "A key human ability is to decompose a scene into distinct objects and use\ntheir relationships to understand the environment. Object-centric learning aims\nto mimic this process in an unsupervised manner. Recently, the slot\nattention-based framework has emerged as a leading approach in this area and\nhas been widely used in various downstream tasks. However, existing slot\nattention methods face two key limitations: (1) a lack of high-level semantic\ninformation. In current methods, image areas are assigned to slots based on\nlow-level features such as color and texture. This makes the model overly\nsensitive to low-level features and limits its understanding of object\ncontours, shapes, or other semantic characteristics. (2) The inability to\nfine-tune the encoder. Current methods require a stable feature space\nthroughout training to enable reconstruction from slots, which restricts the\nflexibility needed for effective object-centric learning. To address these\nlimitations, we propose a novel ContextFusion stage and a Bootstrap Branch,\nboth of which can be seamlessly integrated into existing slot attention models.\nIn the ContextFusion stage, we exploit semantic information from the foreground\nand background, incorporating an auxiliary indicator that provides additional\ncontextual cues about them to enrich the semantic content beyond low-level\nfeatures. In the Bootstrap Branch, we decouple feature adaptation from the\noriginal reconstruction phase and introduce a bootstrap strategy to train a\nfeature-adaptive mechanism, allowing for more flexible adaptation. Experimental\nresults show that our method significantly improves the performance of\ndifferent SOTA slot attention models on both simulated and real-world datasets.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02032v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02032v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.352,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on improving slot attention-based object-centric learning by introducing ContextFusion for semantic information and a Bootstrap Branch for feature adaptation, primarily in computer vision tasks. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02036",
      "title": "DeepSeek performs better than other Large Language Models in Dental\n  Cases",
      "authors": [
        "Hexian Zhang",
        "Xinyu Yan",
        "Yanqi Yang",
        "Lijian Jin",
        "Ping Yang",
        "Junwen Wang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) hold transformative potential in healthcare, yet\ntheir capacity to interpret longitudinal patient narratives remains\ninadequately explored. Dentistry, with its rich repository of structured\nclinical data, presents a unique opportunity to rigorously assess LLMs'\nreasoning abilities. While several commercial LLMs already exist, DeepSeek, a\nmodel that gained significant attention earlier this year, has also joined the\ncompetition. This study evaluated four state-of-the-art LLMs (GPT-4o, Gemini\n2.0 Flash, Copilot, and DeepSeek V3) on their ability to analyze longitudinal\ndental case vignettes through open-ended clinical tasks. Using 34 standardized\nlongitudinal periodontal cases (comprising 258 question-answer pairs), we\nassessed model performance via automated metrics and blinded evaluations by\nlicensed dentists. DeepSeek emerged as the top performer, demonstrating\nsuperior faithfulness (median score = 0.528 vs. 0.367-0.457) and higher expert\nratings (median = 4.5/5 vs. 4.0/5), without significantly compromising\nreadability. Our study positions DeepSeek as the leading LLM for case analysis,\nendorses its integration as an adjunct tool in both medical education and\nresearch, and highlights its potential as a domain-specific agent.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02036v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02036v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.354,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates the performance of various LLMs, including DeepSeek, on dental case analysis tasks, focusing on metrics like faithfulness and expert ratings. It does not mention or involve diffusion-based reasoning, such as adapting iterative refinement processes for multi-step logical tasks or treating Chain-of-Thought as a holistic entity. The study is centered on general LLM capabilities in healthcare, with no reference to diffusion models or related techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02046",
      "title": "Fantastic Pretraining Optimizers and Where to Find Them",
      "authors": [
        "Kaiyue Wen",
        "David Hall",
        "Tengyu Ma",
        "Percy Liang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "AdamW has long been the dominant optimizer in language model pretraining,\ndespite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We\nposit that two methodological shortcomings have obscured fair comparisons and\nhindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited\nor misleading evaluation setups. To address these two issues, we conduct a\nsystematic study of ten deep learning optimizers across four model scales\n(0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum).\nWe find that fair and informative comparisons require rigorous hyperparameter\ntuning and evaluations across a range of model scales and data-to-model ratios,\nperformed at the end of training. First, optimal hyperparameters for one\noptimizer may be suboptimal for another, making blind hyperparameter transfer\nunfair. Second, the actual speedup of many proposed optimizers over well-tuned\nbaselines is lower than claimed and decreases with model size to only 1.1x for\n1.2B parameter models. Thirdly, comparing intermediate checkpoints before\nreaching the target training budgets can be misleading, as rankings between two\noptimizers can flip during training due to learning rate decay. Through our\nthorough investigation, we find that all the fastest optimizers such as Muon\nand Soap, use matrices as preconditioners -- multiplying gradients with\nmatrices rather than entry-wise scalars. However, the speedup of matrix-based\noptimizers is inversely proportional to model scale, decreasing from 1.4x over\nAdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02046v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02046v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.325,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.423,
      "datasets_score": 0.26,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper primarily evaluates optimizers for language model pretraining, focusing on their efficiency and speedup in terms of computation and hyperparameter tuning. While it discusses computational costs and speedups that could indirectly relate to accelerating training in distributed environments, it does not directly address distributed training techniques, such as data partitioning, model parallelism, or multi-node strategies. Thus, the relevance is tangential, as efficient optimizers might enhance distributed systems but are not the paper's main focus.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02048",
      "title": "Privacy-Utility Trade-off in Data Publication: A Bilevel Optimization\n  Framework with Curvature-Guided Perturbation",
      "authors": [
        "Yi Yin",
        "Guangquan Zhang",
        "Hua Zuo",
        "Jie Lu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Machine learning models require datasets for effective training, but directly\nsharing raw data poses significant privacy risk such as membership inference\nattacks (MIA). To mitigate the risk, privacy-preserving techniques such as data\nperturbation, generalization, and synthetic data generation are commonly\nutilized. However, these methods often degrade data accuracy, specificity, and\ndiversity, limiting the performance of downstream tasks and thus reducing data\nutility. Therefore, striking an optimal balance between privacy preservation\nand data utility remains a critical challenge.\n  To address this issue, we introduce a novel bilevel optimization framework\nfor the publication of private datasets, where the upper-level task focuses on\ndata utility and the lower-level task focuses on data privacy. In the\nupper-level task, a discriminator guides the generation process to ensure that\nperturbed latent variables are mapped to high-quality samples, maintaining\nfidelity for downstream tasks. In the lower-level task, our framework employs\nlocal extrinsic curvature on the data manifold as a quantitative measure of\nindividual vulnerability to MIA, providing a geometric foundation for targeted\nprivacy protection. By perturbing samples toward low-curvature regions, our\nmethod effectively suppresses distinctive feature combinations that are\nvulnerable to MIA. Through alternating optimization of both objectives, we\nachieve a synergistic balance between privacy and utility. Extensive\nexperimental evaluations demonstrate that our method not only enhances\nresistance to MIA in downstream tasks but also surpasses existing methods in\nterms of sample quality and diversity.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02048v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02048v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.357,
      "datasets_score": 0.372,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02053",
      "title": "Generative KI für TA",
      "authors": [
        "Wolfgang Eppler",
        "Reinhard Heil"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Many scientists use generative AI in their scientific work. People working in\ntechnology assessment (TA) are no exception. TA's approach to generative AI is\ntwofold: on the one hand, generative AI is used for TA work, and on the other\nhand, generative AI is the subject of TA research. After briefly outlining the\nphenomenon of generative AI and formulating requirements for its use in TA, the\nfollowing article discusses in detail the structural causes of the problems\nassociated with it. Although generative AI is constantly being further\ndeveloped, the structurally induced risks remain. The article concludes with\nproposed solutions and brief notes on their feasibility, as well as some\nexamples of the use of generative AI in TA work.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02053v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02053v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.271,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02055",
      "title": "Align-Then-stEer: Adapting the Vision-Language Action Models through\n  Unified Latent Guidance",
      "authors": [
        "Yang Zhang",
        "Chenwei Wang",
        "Ouyang Lu",
        "Yuan Zhao",
        "Yunfei Ge",
        "Zhenglong Sun",
        "Xiu Li",
        "Chi Zhang",
        "Chenjia Bai",
        "Xuelong Li"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Vision-Language-Action (VLA) models pre-trained on large, diverse datasets\nshow remarkable potential for general-purpose robotic manipulation. However, a\nprimary bottleneck remains in adapting these models to downstream tasks,\nespecially when the robot's embodiment or the task itself differs from the\npre-training data. This discrepancy leads to a significant mismatch in action\ndistributions, demanding extensive data and compute for effective fine-tuning.\nTo address this challenge, we introduce \\textbf{Align-Then-stEer\n(\\texttt{ATE})}, a novel, data-efficient, and plug-and-play adaptation\nframework. \\texttt{ATE} first aligns disparate action spaces by constructing a\nunified latent space, where a variational autoencoder constrained by reverse KL\ndivergence embeds adaptation actions into modes of the pre-training action\nlatent distribution. Subsequently, it steers the diffusion- or flow-based VLA's\ngeneration process during fine-tuning via a guidance mechanism that pushes the\nmodel's output distribution towards the target domain. We conduct extensive\nexperiments on cross-embodiment and cross-task manipulation in both simulation\nand real world. Compared to direct fine-tuning of representative VLAs, our\nmethod improves the average multi-task success rate by up to \\textbf{9.8\\%} in\nsimulation and achieves a striking \\textbf{32\\% success rate gain} in a\nreal-world cross-embodiment setting. Our work presents a general and\nlightweight solution that greatly enhances the practicality of deploying VLA\nmodels to new robotic platforms and tasks.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02055v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02055v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.378,
      "datasets_score": 0.32,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on adapting Vision-Language-Action (VLA) models using VAEs and guidance mechanisms for robotic tasks, relying on imitation learning and fine-tuning on datasets, without any mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper involves diffusion-based VLAs for action generation and steering during adaptation, but it applies this to robotic manipulation tasks rather than multi-step logical reasoning or holistic correction of a Chain-of-Thought as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02075",
      "title": "How Instruction-Tuning Imparts Length Control: A Cross-Lingual\n  Mechanistic Analysis",
      "authors": [
        "Elisabetta Rocchetti",
        "Alfio Ferrara"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Adhering to explicit length constraints, such as generating text with a\nprecise word count, remains a significant challenge for Large Language Models\n(LLMs). This study aims at investigating the differences between foundation\nmodels and their instruction-tuned counterparts, on length-controlled text\ngeneration in English and Italian. We analyze both performance and internal\ncomponent contributions using Cumulative Weighted Attribution, a metric derived\nfrom Direct Logit Attribution. Our findings reveal that instruction-tuning\nsubstantially improves length control, primarily by specializing components in\ndeeper model layers. Specifically, attention heads in later layers of IT models\nshow increasingly positive contributions, particularly in English. In Italian,\nwhile attention contributions are more attenuated, final-layer MLPs exhibit a\nstronger positive role, suggesting a compensatory mechanism. These results\nindicate that instruction-tuning reconfigures later layers for task adherence,\nwith component-level strategies potentially adapting to linguistic context.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02075v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02075v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.42,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.342,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on instruction-tuning for length control in LLMs and analyzes internal mechanisms, but it does not mention using human feedback to train a reward model or applying reinforcement learning for fine-tuning. Therefore, it does not align with the specific definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines mechanistic interpretability for length-controlled text generation and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined. It is solely about attribution analysis in LLMs.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02076",
      "title": "Forecasting Future DDoS Attacks Using Long Short Term Memory (LSTM)\n  Model",
      "authors": [
        "Kong Mun Yeen",
        "Rafidah Md Noor",
        "Wahidah Md Shah",
        "Aslinda Hassan",
        "Muhammad Umair Munir"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper forecasts future Distributed Denial of Service (DDoS) attacks\nusing deep learning models. Although several studies address forecasting DDoS\nattacks, they remain relatively limited compared to detection-focused research.\nBy studying the current trends and forecasting based on newer and updated\ndatasets, mitigation plans against the attacks can be planned and formulated.\nThe methodology used in this research work conforms to the Cross Industry\nStandard Process for Data Mining (CRISP-DM) model.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02076v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02076v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.291,
      "weak_supervision_score": 0.307,
      "diffusion_reasoning_score": 0.329,
      "distributed_training_score": 0.317,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02089",
      "title": "AGI as Second Being: The Structural-Generative Ontology of Intelligence",
      "authors": [
        "Maijunxian Wang",
        "Ran Ji"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Artificial intelligence is often measured by the range of tasks it can\nperform. Yet wide ability without depth remains only an imitation. This paper\nproposes a Structural-Generative Ontology of Intelligence: true intelligence\nexists only when a system can generate new structures, coordinate them into\nreasons, and sustain its identity over time. These three conditions --\ngenerativity, coordination, and sustaining -- define the depth that underlies\nreal intelligence. Current AI systems, however broad in function, remain\nsurface simulations because they lack this depth. Breadth is not the source of\nintelligence but the growth that follows from depth. If future systems were to\nmeet these conditions, they would no longer be mere tools, but could be seen as\na possible Second Being, standing alongside yet distinct from human existence.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02089v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02089v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.284,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.291,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02093",
      "title": "Better by Comparison: Retrieval-Augmented Contrastive Reasoning for\n  Automatic Prompt Optimization",
      "authors": [
        "Juhyeon Lee",
        "Wonduk Seo",
        "Hyunjin An",
        "Seunghyun Lee",
        "Yi Bu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Automatic prompt optimization has recently emerged as a strategy for\nimproving the quality of prompts used in Large Language Models (LLMs), with the\ngoal of generating more accurate and useful responses. However, most prior work\nfocuses on direct prompt refinement or model fine-tuning, overlooking the\npotential of leveraging LLMs' inherent reasoning capability to learn from\ncontrasting examples. In this paper, we present Contrastive Reasoning Prompt\nOptimization (CRPO), a novel framework that formulates prompt optimization as a\nretrieval augmented reasoning process. Our approach retrieves top k reference\nprompts from the HelpSteer2 dataset, an open-source collection annotated for\nhelpfulness, correctness, coherence, complexity, and verbosity, and constructs\ntwo complementary optimization paradigms: (1) tiered contrastive reasoning,\nwhere the LLM compares high, medium, and low quality prompts to refine its own\ngeneration through reflective reasoning, and (2) multi-metric contrastive\nreasoning, where the LLM analyzes the best prompts along each evaluation\ndimension and integrates their strengths into an optimized prompt. By\nexplicitly contrasting high and low quality exemplars, CRPO enables the model\nto deduce why certain prompts succeed while others fail, thereby achieving more\nrobust and interpretable optimization. Experimental results on the HelpSteer2\nbenchmark demonstrate that CRPO significantly outperforms baselines. Our\nfindings highlight the promise of contrastive, retrieval-augmented reasoning\nfor advancing automatic prompt optimization.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02093v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02093v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.469,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.467,
      "distributed_training_score": 0.327,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses human-annotated data from HelpSteer2 for prompt retrieval and comparison, which involves human feedback, but it does not train a reward model or fine-tune the main model using reinforcement learning. Instead, it focuses on contrastive reasoning for prompt optimization, making it only indirectly related to RLHF.",
      "weak_supervision_justification": "The paper relies on the pre-annotated HelpSteer2 dataset for retrieving prompts, which appears to be high-quality human annotations rather than programmatically generated noisy labels. There is no indication of training models with weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper's approach involves retrieval-augmented contrastive reasoning for prompt optimization, with no mention of diffusion models, iterative refinement processes, or treating reasoning as a holistically corrected entity over multiple steps.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02097",
      "title": "JudgeAgent: Dynamically Evaluate LLMs with Agent-as-Interviewer",
      "authors": [
        "Zhichao Shi",
        "Xuhui Jiang",
        "Chengjin Xu",
        "Cangli Yao",
        "Zhenxin Huang",
        "Shengjie Ma",
        "Yinghan Shen",
        "Yuanzhuo Wang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Evaluating the capabilities of large language models (LLMs) is an essential\nstep to ensure the successful application of LLMs across various domains. The\ncurrent evaluation of LLMs is based on a paradigm that involves querying them\nwith predefined question sets and assessing their outputs. This paradigm offers\ncontrollable processes and simplicity, but faces challenges such as limited\ninteraction with targets, insufficient difficulty control, and difficulties in\nverifying the validity of evaluation results, making it hard to precisely\ndetermine the knowledge and capability boundaries of target models. To address\nthese challenges, we propose JudgeAgent, a knowledge-target adaptive dynamic\nevaluation framework based on a new interviewer-style evaluation paradigm.\nJudgeAgent employs a comprehensive evaluation approach consisting of benchmark\ngrading, interactive extension, and evaluation feedback. It utilizes\nknowledge-driven data synthesis and target-adaptive difficulty adjustment\nmethods to conduct extended testing, providing accurate and effective\nevaluation results. We also introduce a novel insight into validating\nevaluation methods, demonstrating the effectiveness of JudgeAgent and its\ndynamic evaluation paradigm through extensive experiments.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02097v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02097v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.347,
      "datasets_score": 0.391,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a dynamic evaluation framework for LLMs, specifically JudgeAgent, which assesses model capabilities through interactive questioning and feedback. It does not involve training or fine-tuning models using human feedback, a reward model, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces an interviewer-style evaluation paradigm for LLMs, involving benchmark grading and interactive extension, but it does not use diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02099",
      "title": "A Data-Centric Approach to Pedestrian Attribute Recognition: Synthetic\n  Augmentation via Prompt-driven Diffusion Models",
      "authors": [
        "Alejandro Alonso",
        "Sawaiz A. Chaudhry",
        "Juan C. SanMiguel",
        "Álvaro García-Martín",
        "Pablo Ayuso-Albizu",
        "Pablo Carballeira"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Pedestrian Attribute Recognition (PAR) is a challenging task as models are\nrequired to generalize across numerous attributes in real-world data.\nTraditional approaches focus on complex methods, yet recognition performance is\noften constrained by training dataset limitations, particularly the\nunder-representation of certain attributes. In this paper, we propose a\ndata-centric approach to improve PAR by synthetic data augmentation guided by\ntextual descriptions. First, we define a protocol to identify weakly recognized\nattributes across multiple datasets. Second, we propose a prompt-driven\npipeline that leverages diffusion models to generate synthetic pedestrian\nimages while preserving the consistency of PAR datasets. Finally, we derive a\nstrategy to seamlessly incorporate synthetic samples into training data, which\nconsiders prompt-based annotation rules and modifies the loss function. Results\non popular PAR datasets demonstrate that our approach not only boosts\nrecognition of underrepresented attributes but also improves overall model\nperformance beyond the targeted attributes. Notably, this approach strengthens\nzero-shot generalization without requiring architectural changes of the model,\npresenting an efficient and scalable solution to improve the recognition of\nattributes of pedestrians in the real world.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02099v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02099v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.4,
      "weak_supervision_score": 0.441,
      "diffusion_reasoning_score": 0.509,
      "distributed_training_score": 0.37,
      "datasets_score": 0.437,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on synthetic data augmentation for Pedestrian Attribute Recognition using diffusion models, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper generates synthetic data and labels programmatically via diffusion models to augment training, which aligns with weak supervision by using noisy or derived labels instead of fully hand-annotated data, though it is not the primary focus.",
      "diffusion_reasoning_justification": "The paper uses diffusion models solely for generating synthetic images, not for multi-step logical reasoning or iterative refinement of a chain-of-thought process.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper involves augmenting existing datasets (e.g., RAPv1, RAPv2) with synthetic data and evaluating their impact on attribute recognition, which relates to dataset enhancement and analysis, but it does not introduce new datasets or comprehensive benchmarking.",
      "llm_score_status": "completed",
      "summary": "This paper introduces a data-centric approach to enhance Pedestrian Attribute Recognition (PAR) by addressing data imbalances through synthetic image generation using prompt-driven diffusion models. The methodology involves identifying underrepresented attributes, generating consistent synthetic pedestrian images, and integrating them into training via a modified loss function, resulting in improved recognition performance for targeted attributes, overall model accuracy, and zero-shot generalization on datasets like RAPv1, RAPv2, and RAPzs.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining diffusion models with a prompt-driven pipeline for synthetic data augmentation in PAR, effectively addressing data imbalances in a new way for this specific domain. While diffusion models are not novel, their targeted application to enhance underrepresented attributes in PAR constitutes a clever adaptation rather than a groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in PAR and data augmentation within computer vision by providing a scalable method to handle imbalanced datasets, potentially leading to citations and adaptations in related subfields. However, its impact may be limited to specific applications like pedestrian analysis rather than broader AI domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, practical contribution to PAR through innovative data augmentation techniques, making it valuable for researchers in computer vision focused on attribute recognition and dataset imbalances. While not essential for all, it provides useful insights that could inform ongoing work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/efda34abbf20b6349fd3ec3c91f08545974704a8",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 17,
      "average_h_index": 3.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Alejandro Alonso",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378708637"
        },
        {
          "name": "Sawaiz A. Chaudhry",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378557937"
        },
        {
          "name": "Juan C. Sanmiguel",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/1748121"
        },
        {
          "name": "Álvaro García-Martín",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2309599151"
        },
        {
          "name": "Pablo Ayuso-Albizu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378703703"
        },
        {
          "name": "Pablo Carballeira",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2305683384"
        }
      ]
    },
    {
      "id": "2509.02101",
      "title": "SALAD -- Semantics-Aware Logical Anomaly Detection",
      "authors": [
        "Matic Fučka",
        "Vitjan Zavrtanik",
        "Danijel Skočaj"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent surface anomaly detection methods excel at identifying structural\nanomalies, such as dents and scratches, but struggle with logical anomalies,\nsuch as irregular or missing object components. The best-performing logical\nanomaly detection approaches rely on aggregated pretrained features or\nhandcrafted descriptors (most often derived from composition maps), which\ndiscard spatial and semantic information, leading to suboptimal performance. We\npropose SALAD, a semantics-aware discriminative logical anomaly detection\nmethod that incorporates a newly proposed composition branch to explicitly\nmodel the distribution of object composition maps, consequently learning\nimportant semantic relationships. Additionally, we introduce a novel procedure\nfor extracting composition maps that requires no hand-made labels or\ncategory-specific information, in contrast to previous methods. By effectively\nmodelling the composition map distribution, SALAD significantly improves upon\nstate-of-the-art methods on the standard benchmark for logical anomaly\ndetection, MVTec LOCO, achieving an impressive image-level AUROC of 96.1%.\nCode: https://github.com/MaticFuc/SALAD",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02101v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02101v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.294,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.271,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02111",
      "title": "NOOUGAT: Towards Unified Online and Offline Multi-Object Tracking",
      "authors": [
        "Benjamin Missaoui",
        "Orcun Cetintas",
        "Guillem Brasó",
        "Tim Meinhardt",
        "Laura Leal-Taixé"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The long-standing division between \\textit{online} and \\textit{offline}\nMulti-Object Tracking (MOT) has led to fragmented solutions that fail to\naddress the flexible temporal requirements of real-world deployment scenarios.\nCurrent \\textit{online} trackers rely on frame-by-frame hand-crafted\nassociation strategies and struggle with long-term occlusions, whereas\n\\textit{offline} approaches can cover larger time gaps, but still rely on\nheuristic stitching for arbitrarily long sequences. In this paper, we introduce\nNOOUGAT, the first tracker designed to operate with arbitrary temporal\nhorizons. NOOUGAT leverages a unified Graph Neural Network (GNN) framework that\nprocesses non-overlapping subclips, and fuses them through a novel\nAutoregressive Long-term Tracking (ALT) layer. The subclip size controls the\ntrade-off between latency and temporal context, enabling a wide range of\ndeployment scenarios, from frame-by-frame to batch processing. NOOUGAT achieves\nstate-of-the-art performance across both tracking regimes, improving\n\\textit{online} AssA by +2.3 on DanceTrack, +9.2 on SportsMOT, and +5.0 on\nMOT20, with even greater gains in \\textit{offline} mode.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02111v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02111v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.294,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.355,
      "datasets_score": 0.281,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02113",
      "title": "HiGraph: A Large-Scale Hierarchical Graph Dataset for Malware Analysis",
      "authors": [
        "Han Chen",
        "Hanchen Wang",
        "Hongmei Chen",
        "Ying Zhang",
        "Lu Qin",
        "Wenjie Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)",
        "cs.SI (Social and Information Networks)"
      ],
      "abstract": "The advancement of graph-based malware analysis is critically limited by the\nabsence of large-scale datasets that capture the inherent hierarchical\nstructure of software. Existing methods often oversimplify programs into single\nlevel graphs, failing to model the crucial semantic relationship between\nhigh-level functional interactions and low-level instruction logic. To bridge\nthis gap, we introduce \\dataset, the largest public hierarchical graph dataset\nfor malware analysis, comprising over \\textbf{200M} Control Flow Graphs (CFGs)\nnested within \\textbf{595K} Function Call Graphs (FCGs). This two-level\nrepresentation preserves structural semantics essential for building robust\ndetectors resilient to code obfuscation and malware evolution. We demonstrate\nHiGraph's utility through a large-scale analysis that reveals distinct\nstructural properties of benign and malicious software, establishing it as a\nfoundational benchmark for the community. The dataset and tools are publicly\navailable at https://higraph.org.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02113v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02113v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.293,
      "distributed_training_score": 0.34,
      "datasets_score": 0.404,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of HiGraph, a large-scale hierarchical graph dataset for malware analysis, which directly involves creating a new dataset, analyzing its properties (e.g., structural differences between benign and malicious software), and providing it as a benchmark for AI applications like graph neural networks. This aligns closely with research on dataset creation, curation, analysis, and benchmarking for machine learning and AI.",
      "llm_score_status": "completed",
      "summary": "The paper introduces HiGraph, a large-scale hierarchical graph dataset for malware analysis, comprising over 200 million Control Flow Graphs (CFGs) nested within 595,000 Function Call Graphs (FCGs), to address the limitations of existing flat-graph representations that fail to capture software hierarchies. By providing a two-level structure that models both intra-procedural logic and inter-procedural dependencies, the authors demonstrate its utility through large-scale analysis, revealing distinct structural properties between benign and malicious software, and position it as a foundational benchmark for developing robust, obfuscation-resilient malware detectors, with the dataset made publicly available.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new dataset that employs hierarchical graphs for malware analysis, significantly advancing the state-of-the-art by addressing the gap in modeling software hierarchies and providing the largest such dataset publicly available.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence a wide range of future research in AI-driven cybersecurity by serving as a standard benchmark dataset, enabling the development of more effective malware detection techniques and fostering innovations in graph-based analysis.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, valuable contribution through its innovative dataset and analysis, making it essential for researchers in machine learning and security to be aware of for advancing malware detection methods.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/17498cffba485ae8f6de830259f1823179f359af",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 24,
      "average_h_index": 7.666666666666667,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Han Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2225236719"
        },
        {
          "name": "Hanchen Wang",
          "h_index": 14,
          "profile_url": "https://www.semanticscholar.org/author/97699982"
        },
        {
          "name": "Hongmei Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378578096"
        },
        {
          "name": "Ying Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2278054342"
        },
        {
          "name": "Lu Qin",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2266855976"
        },
        {
          "name": "Wenjie Zhang",
          "h_index": 24,
          "profile_url": "https://www.semanticscholar.org/author/2402302"
        }
      ]
    },
    {
      "id": "2509.02129",
      "title": "Scale, Don't Fine-tune: Guiding Multimodal LLMs for Efficient Visual\n  Place Recognition at Test-Time",
      "authors": [
        "Jintao Cheng",
        "Weibin Li",
        "Jiehao Luo",
        "Xiaoyu Tang",
        "Zhijian He",
        "Jin Wu",
        "Yao Zou",
        "Wei Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Visual Place Recognition (VPR) has evolved from handcrafted descriptors to\ndeep learning approaches, yet significant challenges remain. Current\napproaches, including Vision Foundation Models (VFMs) and Multimodal Large\nLanguage Models (MLLMs), enhance semantic understanding but suffer from high\ncomputational overhead and limited cross-domain transferability when\nfine-tuned. To address these limitations, we propose a novel zero-shot\nframework employing Test-Time Scaling (TTS) that leverages MLLMs'\nvision-language alignment capabilities through Guidance-based methods for\ndirect similarity scoring. Our approach eliminates two-stage processing by\nemploying structured prompts that generate length-controllable JSON outputs.\nThe TTS framework with Uncertainty-Aware Self-Consistency (UASC) enables\nreal-time adaptation without additional training costs, achieving superior\ngeneralization across diverse environments. Experimental results demonstrate\nsignificant improvements in cross-domain VPR performance with up to 210$\\times$\ncomputational efficiency gains.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02129v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02129v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.411,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Test-Time Scaling (TTS) using Chain-of-Thought (CoT) and Self-Consistency for Visual Place Recognition, but it does not involve diffusion models or iterative refinement processes for reasoning. There is no mention of adapting diffusion mechanisms for logical tasks or holistically correcting a Chain-of-Thought, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper discusses computational efficiency in inference for Visual Place Recognition using MLLMs, including gains like 210x efficiency, but it does not address distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes or processors. The focus is on zero-shot test-time adaptation, not training methodologies.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02134",
      "title": "Learning Social Heuristics for Human-Aware Path Planning",
      "authors": [
        "Andrea Eirale",
        "Matteo Leonetti",
        "Marcello Chiaberge"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Social robotic navigation has been at the center of numerous studies in\nrecent years. Most of the research has focused on driving the robotic agent\nalong obstacle-free trajectories, respecting social distances from humans, and\npredicting their movements to optimize navigation. However, in order to really\nbe socially accepted, the robots must be able to attain certain social norms\nthat cannot arise from conventional navigation, but require a dedicated\nlearning process. We propose Heuristic Planning with Learned Social Value\n(HPLSV), a method to learn a value function encapsulating the cost of social\nnavigation, and use it as an additional heuristic in heuristic-search path\nplanning. In this preliminary work, we apply the methodology to the common\nsocial scenario of joining a queue of people, with the intention of\ngeneralizing to further human activities.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02134v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02134v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.485,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.26,
      "datasets_score": 0.265,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes using reinforcement learning to learn a value function for social navigation in robots, specifically for scenarios like joining a queue, and integrating it into path planning. However, it does not mention training a reward model on human-ranked data or incorporating human feedback to align the model with human preferences. RLHF requires explicit human involvement in the feedback loop, which is absent here, making the paper's contribution unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02141",
      "title": "GRMM: Real-Time High-Fidelity Gaussian Morphable Head Model with Learned\n  Residuals",
      "authors": [
        "Mohit Mendiratta",
        "Mayur Deshmukh",
        "Kartik Teotia",
        "Vladislav Golyanik",
        "Adam Kortylewski",
        "Christian Theobalt"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D Morphable Models (3DMMs) enable controllable facial geometry and\nexpression editing for reconstruction, animation, and AR/VR, but traditional\nPCA-based mesh models are limited in resolution, detail, and photorealism.\nNeural volumetric methods improve realism but remain too slow for interactive\nuse. Recent Gaussian Splatting (3DGS) based facial models achieve fast,\nhigh-quality rendering but still depend solely on a mesh-based 3DMM prior for\nexpression control, limiting their ability to capture fine-grained geometry,\nexpressions, and full-head coverage. We introduce GRMM, the first full-head\nGaussian 3D morphable model that augments a base 3DMM with residual geometry\nand appearance components, additive refinements that recover high-frequency\ndetails such as wrinkles, fine skin texture, and hairline variations. GRMM\nprovides disentangled control through low-dimensional, interpretable parameters\n(e.g., identity shape, facial expressions) while separately modelling residuals\nthat capture subject- and expression-specific detail beyond the base model's\ncapacity. Coarse decoders produce vertex-level mesh deformations, fine decoders\nrepresent per-Gaussian appearance, and a lightweight CNN refines rasterised\nimages for enhanced realism, all while maintaining 75 FPS real-time rendering.\nTo learn consistent, high-fidelity residuals, we present EXPRESS-50, the first\ndataset with 60 aligned expressions across 50 identities, enabling robust\ndisentanglement of identity and expression in Gaussian-based 3DMMs. Across\nmonocular 3D face reconstruction, novel-view synthesis, and expression\ntransfer, GRMM surpasses state-of-the-art methods in fidelity and expression\naccuracy while delivering interactive real-time performance.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02141v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02141v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.343,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02144",
      "title": "A Theoretical Framework of the Processes of Change in Psychotherapy\n  Delivered by Artificial Agents",
      "authors": [
        "Arthur Bran Herbener",
        "Malene Flensborg Damholdt"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The question of whether artificial agents (e.g., chatbots and social robots)\ncan replace human therapists has received notable attention following the\nrecent launch of large language models. However, little is known about the\nprocesses of change in psychotherapy delivered by artificial agents. To\nfacilitate hypothesis development and stimulate scientific debate, the present\narticle offers the first theoretical framework of the processes of change in\npsychotherapy delivered by artificial agents. The theoretical framework rests\nupon a conceptual analysis of what active ingredients may be inherently linked\nto the presence of human therapists. We propose that human therapists'\nontological status as human beings and sociocultural status as socially\nsanctioned healthcare professionals play crucial roles in promoting treatment\noutcomes. In the absence of the ontological and sociocultural status of human\ntherapists, we propose what we coin the genuineness gap and credibility gap can\nemerge and undermine key processes of change in psychotherapy. Based on these\npropositions, we propose avenues for scientific investigations and practical\napplications aimed at leveraging the strengths of artificial agents and human\ntherapists respectively. We also highlight the intricate agentic nature of\nartificial agents and discuss how this complicates endeavors to establish\nuniversally applicable propositions regarding the processes of change in these\ninterventions.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02144v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02144v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.258,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.238,
      "datasets_score": 0.267,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02154",
      "title": "Conditional-$t^3$VAE: Equitable Latent Space Allocation for Fair\n  Generation",
      "authors": [
        "Aymene Mohammed Bouayed",
        "Samuel Deslauriers-Gauthier",
        "Adrian Iaccovelli",
        "David Naccache"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Variational Autoencoders (VAEs) with global priors mirror the training set's\nclass frequency in latent space, underrepresenting tail classes and reducing\ngenerative fairness on imbalanced datasets. While $t^3$VAE improves robustness\nvia heavy-tailed Student's t-distribution priors, it still allocates latent\nvolume proportionally to the class frequency.In this work, we address this\nissue by explicitly enforcing equitable latent space allocation across classes.\nTo this end, we propose Conditional-$t^3$VAE, which defines a per-class\n\\mbox{Student's t} joint prior over latent and output variables, preventing\ndominance by majority classes. Our model is optimized using a closed-form\nobjective derived from the $\\gamma$-power divergence. Moreover, for\nclass-balanced generation, we derive an equal-weight latent mixture of\nStudent's t-distributions. On SVHN-LT, CIFAR100-LT, and CelebA,\nConditional-$t^3$VAE consistently achieves lower FID scores than both $t^3$VAE\nand Gaussian-based VAE baselines, particularly under severe class imbalance. In\nper-class F1 evaluations, Conditional-$t^3$VAE also outperforms the conditional\nGaussian VAE across all highly imbalanced settings. While Gaussian-based models\nremain competitive under mild imbalance ratio ($\\rho \\lesssim 3$), our approach\nsubstantially improves generative fairness and diversity in more extreme\nregimes.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02154v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02154v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.355,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02156",
      "title": "SegFormer Fine-Tuning with Dropout: Advancing Hair Artifact Removal in\n  Skin Lesion Analysis",
      "authors": [
        "Asif Mohammed Saad",
        "Umme Niraj Mahi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Hair artifacts in dermoscopic images present significant challenges for\naccurate skin lesion analysis, potentially obscuring critical diagnostic\nfeatures in dermatological assessments. This work introduces a fine-tuned\nSegFormer model augmented with dropout regularization to achieve precise hair\nmask segmentation. The proposed SegformerWithDropout architecture leverages the\nMiT-B2 encoder, pretrained on ImageNet, with an in-channel count of 3 and 2\noutput classes, incorporating a dropout probability of 0.3 in the segmentation\nhead to prevent overfitting. Training is conducted on a specialized dataset of\n500 dermoscopic skin lesion images with fine-grained hair mask annotations,\nemploying 10-fold cross-validation, AdamW optimization with a learning rate of\n0.001, and cross-entropy loss. Early stopping is applied based on validation\nloss, with a patience of 3 epochs and a maximum of 20 epochs per fold.\nPerformance is evaluated using a comprehensive suite of metrics, including\nIntersection over Union (IoU), Dice coefficient, Peak Signal-to-Noise Ratio\n(PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch\nSimilarity (LPIPS). Experimental results from the cross-validation demonstrate\nrobust performance, with average Dice coefficients reaching approximately 0.96\nand IoU values of 0.93, alongside favorable PSNR (around 34 dB), SSIM (0.97),\nand low LPIPS (0.06), highlighting the model's effectiveness in accurate hair\nartifact segmentation and its potential to enhance preprocessing for downstream\nskin cancer detection tasks.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02156v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02156v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.336,
      "distributed_training_score": 0.332,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02160",
      "title": "Meta-Pretraining for Zero-Shot Cross-Lingual Named Entity Recognition in\n  Low-Resource Philippine Languages",
      "authors": [
        "David Demitri Africa",
        "Suchir Salhan",
        "Yuval Weiss",
        "Paula Buttery",
        "Richard Diehl Martinez"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Named-entity recognition (NER) in low-resource languages is usually tackled\nby finetuning very large multilingual LMs, an option that is often infeasible\nin memory- or latency-constrained settings. We ask whether small decoder LMs\ncan be pretrained so that they adapt quickly and transfer zero-shot to\nlanguages unseen during pretraining. To this end we replace part of the\nautoregressive objective with first-order model-agnostic meta-learning (MAML).\nTagalog and Cebuano are typologically similar yet structurally different in\ntheir actor/non-actor voice systems, and hence serve as a challenging test-bed.\nAcross four model sizes (11 M - 570 M) MAML lifts zero-shot micro-F1 by 2-6 pp\nunder head-only tuning and 1-3 pp after full tuning, while cutting convergence\ntime by up to 8%. Gains are largest for single-token person entities that\nco-occur with Tagalog case particles si/ni, highlighting the importance of\nsurface anchors.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02160v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02160v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.402,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.369,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper explores meta-pretraining with MAML for zero-shot NER in low-resource languages, which indirectly relates to handling limited labeled data. However, it does not involve programmatically generating labels from noisy or imprecise sources, as defined in weak supervision. Instead, it focuses on improving model adaptation through meta-learning, making the connection loose.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02161",
      "title": "Enhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data\n  Generation: A Comparative Study with Image-To-Image Diffusion Models",
      "authors": [
        "Pablo Ayuso-Albizu",
        "Juan C. SanMiguel",
        "Pablo Carballeira"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Pedestrian Attribute Recognition (PAR) involves identifying various human\nattributes from images with applications in intelligent monitoring systems. The\nscarcity of large-scale annotated datasets hinders the generalization of PAR\nmodels, specially in complex scenarios involving occlusions, varying poses, and\ndiverse environments. Recent advances in diffusion models have shown promise\nfor generating diverse and realistic synthetic images, allowing to expand the\nsize and variability of training data. However, the potential of\ndiffusion-based data expansion for generating PAR-like images remains\nunderexplored. Such expansion may enhance the robustness and adaptability of\nPAR models in real-world scenarios. This paper investigates the effectiveness\nof diffusion models in generating synthetic pedestrian images tailored to PAR\ntasks. We identify key parameters of img2img diffusion-based data expansion;\nincluding text prompts, image properties, and the latest enhancements in\ndiffusion-based data augmentation, and examine their impact on the quality of\ngenerated images for PAR. Furthermore, we employ the best-performing expansion\napproach to generate synthetic images for training PAR models, by enriching the\nzero-shot datasets. Experimental results show that prompt alignment and image\nproperties are critical factors in image generation, with optimal selection\nleading to a 4.5% improvement in PAR recognition performance.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02161v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02161v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.499,
      "distributed_training_score": 0.371,
      "datasets_score": 0.411,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for generating synthetic images to augment datasets for Pedestrian Attribute Recognition (PAR), which is a standard application of diffusion in image synthesis. It does not involve adapting diffusion models for multi-step logical reasoning, Chain-of-Thought processes, or iterative refinement of reasoning paths as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper involves generating synthetic images to expand and enrich datasets for PAR, including evaluating their impact on model performance, which relates to dataset augmentation and benchmarking. However, it primarily focuses on diffusion-based image generation for PAR rather than core aspects like new dataset creation, curation methodologies, or comprehensive analysis.",
      "llm_score_status": "completed",
      "summary": "This paper investigates the use of image-to-image diffusion models to generate synthetic pedestrian images for enhancing zero-shot Pedestrian Attribute Recognition (PAR), addressing the challenges posed by scarce annotated datasets in complex scenarios. The authors conduct a comparative study examining key parameters such as text prompts and image properties, demonstrating that optimal configurations lead to a 4.5% improvement in PAR performance when synthetic data is incorporated into training, thereby improving model robustness and generalization.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing diffusion models applied to the underexplored area of PAR synthetic data generation, offering a notable improvement in dataset expansion techniques for this specific task.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of computer vision for PAR, as it demonstrates practical benefits in model robustness through synthetic data augmentation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable insights and empirical evidence on enhancing PAR with diffusion models, making it a significant contribution for researchers focused on computer vision and synthetic data techniques.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f8cd363ae55dc747efcb5435e29909964090a439",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 17,
      "average_h_index": 6.333333333333333,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Pablo Ayuso-Albizu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378703703"
        },
        {
          "name": "Juan C. Sanmiguel",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/1748121"
        },
        {
          "name": "Pablo Carballeira",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2305683384"
        }
      ]
    },
    {
      "id": "2509.02163",
      "title": "Enhancing Reliability in LLM-Integrated Robotic Systems: A Unified\n  Approach to Security and Safety",
      "authors": [
        "Wenxiao Zhang",
        "Xiangrui Kong",
        "Conan Dewitt",
        "Thomas Bräunl",
        "Jin B. Hong"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Integrating large language models (LLMs) into robotic systems has\nrevolutionised embodied artificial intelligence, enabling advanced\ndecision-making and adaptability. However, ensuring reliability, encompassing\nboth security against adversarial attacks and safety in complex environments,\nremains a critical challenge. To address this, we propose a unified framework\nthat mitigates prompt injection attacks while enforcing operational safety\nthrough robust validation mechanisms. Our approach combines prompt assembling,\nstate management, and safety validation, evaluated using both performance and\nsecurity metrics. Experiments show a 30.8% improvement under injection attacks\nand up to a 325% improvement in complex environment settings under adversarial\nconditions compared to baseline scenarios. This work bridges the gap between\nsafety and security in LLM-based robotic systems, offering actionable insights\nfor deploying reliable LLM-integrated mobile robots in real-world settings. The\nframework is open-sourced with simulation and physical deployment demos at\nhttps://llmeyesim.vercel.app/",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02163v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02163v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.487,
      "weak_supervision_score": 0.421,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.378,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a unified framework for security and safety in LLM-integrated robotic systems, emphasizing prompt assembly, state management, and validation mechanisms. It does not involve training AI models with human feedback, reward models, or reinforcement learning techniques for alignment, which are core to RLHF.",
      "weak_supervision_justification": "The paper proposes a framework for enhancing reliability in robotic systems using existing LLMs, without any discussion of training models with programmatically generated labels, noisy sources, or alternatives to hand-labeled data, which define weak supervision.",
      "diffusion_reasoning_justification": "The paper addresses LLM integration for decision-making and safety in robotics but does not describe any use of diffusion models, iterative refinement processes, or multi-step logical reasoning akin to diffusion-based approaches for tasks like Chain-of-Thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02164",
      "title": "Omnidirectional Spatial Modeling from Correlated Panoramas",
      "authors": [
        "Xinshen Zhang",
        "Tongxi Fu",
        "Xu Zheng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Omnidirectional scene understanding is vital for various downstream\napplications, such as embodied AI, autonomous driving, and immersive\nenvironments, yet remains challenging due to geometric distortion and complex\nspatial relations in 360{\\deg} imagery. Existing omnidirectional methods\nachieve scene understanding within a single frame while neglecting cross-frame\ncorrelated panoramas. To bridge this gap, we introduce \\textbf{CFpano}, the\n\\textbf{first} benchmark dataset dedicated to cross-frame correlated panoramas\nvisual question answering in the holistic 360{\\deg} scenes. CFpano consists of\nover 2700 images together with over 8000 question-answer pairs, and the\nquestion types include both multiple choice and open-ended VQA. Building upon\nour CFpano, we further present \\methodname, a multi-modal large language model\n(MLLM) fine-tuned with Group Relative Policy Optimization (GRPO) and a set of\ntailored reward functions for robust and consistent reasoning with cross-frame\ncorrelated panoramas. Benchmark experiments with existing MLLMs are conducted\nwith our CFpano. The experimental results demonstrate that \\methodname achieves\nstate-of-the-art performance across both multiple-choice and open-ended VQA\ntasks, outperforming strong baselines on all major reasoning categories\n(\\textbf{+5.37\\%} in overall performance). Our analyses validate the\neffectiveness of GRPO and establish a new benchmark for panoramic scene\nunderstanding.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02164v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02164v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.352,
      "datasets_score": 0.345,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves introducing a dataset (CFpano) for panoramic visual question answering and a model (Pano-R1) fine-tuned using Group Relative Policy Optimization (GRPO), a reinforcement learning technique, along with reward functions for robust reasoning. There is no mention or utilization of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion. Thus, it does not align with diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02170",
      "title": "Avoidance Decoding for Diverse Multi-Branch Story Generation",
      "authors": [
        "Kyeongman Park",
        "Nakyeong Yang",
        "Kyomin Jung"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) often generate repetitive and monotonous\noutputs, especially in tasks like story generation, due to limited creative\ndiversity when given the same input prompt. To address this challenge, we\npropose a novel decoding strategy, Avoidance Decoding, that modifies token\nlogits by penalizing similarity to previously generated outputs, thereby\nencouraging more diverse multi-branch stories. This penalty adaptively balances\ntwo similarity measures: (1) Concept-level Similarity Penalty, which is\nprioritized in early stages to diversify initial story concepts, and (2)\nNarrative-level Similarity Penalty, which is increasingly emphasized later to\nensure natural yet diverse plot development. Notably, our method achieves up to\n2.6 times higher output diversity and reduces repetition by an average of 30%\ncompared to strong baselines, while effectively mitigating text degeneration.\nFurthermore, we reveal that our method activates a broader range of neurons,\ndemonstrating that it leverages the model's intrinsic creativity.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02170v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02170v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.496,
      "distributed_training_score": 0.341,
      "datasets_score": 0.297,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a decoding strategy for improving diversity in story generation by penalizing similarity in token logits, focusing on creative text outputs from LLMs. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. As the topic specifically requires adaptation of diffusion for holistic Chain-of-Thought correction, there is no connection.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02175",
      "title": "Understanding Space Is Rocket Science -- Only Top Reasoning Models Can\n  Solve Spatial Understanding Tasks",
      "authors": [
        "Nils Hoehing",
        "Mayug Maniparambil",
        "Ellen Rushe",
        "Noel E. O'Connor",
        "Anthony Ventresque"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We propose RocketScience, an open-source contrastive VLM benchmark that tests\nfor spatial relation understanding. It is comprised of entirely new real-world\nimage-text pairs covering mostly relative spatial understanding and the order\nof objects. The benchmark is designed to be very easy for humans and hard for\nthe current generation of VLMs, and this is empirically verified. Our results\nshow a striking lack of spatial relation understanding in open source and\nfrontier commercial VLMs and a surprisingly high performance of reasoning\nmodels. Additionally, we perform a disentanglement analysis to separate the\ncontributions of object localization and spatial reasoning in\nchain-of-thought-based models and find that the performance on the benchmark is\nbottlenecked by spatial reasoning and not object localization capabilities. We\nrelease the dataset with a CC-BY-4.0 license and make the evaluation code\navailable at: https://github.com/nilshoehing/rocketscience",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02175v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02175v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.507,
      "distributed_training_score": 0.346,
      "datasets_score": 0.406,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a benchmark for spatial understanding in VLMs and evaluating models using chain-of-thought reasoning, but it does not mention or involve diffusion-based methods, such as iterative refinement for logical tasks. There is no component of multi-step logical reasoning using diffusion models.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and evaluation of a new benchmark dataset, RocketScience, which includes dataset curation from real-world images, analysis of its design to address limitations of existing datasets, and empirical evaluations. This directly aligns with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces RocketScience, a new open-source benchmark consisting of 482 manually curated real-world image-text pairs designed to evaluate spatial relation understanding in vision-language models (VLMs), addressing limitations in existing datasets by using contrastive, non-synthetic data. Through evaluations of various model types—including dual-encoders, vanilla MLLMs, and reasoning-based MLLMs—the authors demonstrate that most models perform poorly on spatial tasks, while advanced reasoning models excel, and a disentanglement analysis reveals that spatial reasoning, rather than object localization, is the key bottleneck.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark with entirely original real-world data and a contrastive structure, significantly advancing the evaluation of spatial understanding in VLMs by addressing flaws in prior datasets. This represents a substantial innovation in methodology for assessing model capabilities.",
      "impact_score": "Moderate",
      "impact_justification": "The work provides a valuable new benchmark that could be adopted and built upon in research on VLMs, particularly for improving spatial reasoning, though its influence may remain largely within subfields of computer vision and AI. This targeted contribution is likely to generate citations and inspire refinements in model development.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a high-quality, innovative benchmark and insightful analysis that advances understanding of VLM limitations, making it essential for researchers in AI and computer vision to stay informed. While not groundbreaking across all fields, its specific contributions warrant attention for those working on spatial tasks.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2a648de5cb13fece2d943b0f1bdf5eaeb6712393",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 4,
      "average_h_index": 2.2,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Nils Hoehing",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2267332305"
        },
        {
          "name": "Mayug Maniparambil",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/46223705"
        },
        {
          "name": "Ellen Rushe",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/114919979"
        },
        {
          "name": "Noel E. O'Connor",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2278840809"
        },
        {
          "name": "Anthony Ventresque",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2330427534"
        }
      ]
    },
    {
      "id": "2509.02182",
      "title": "ADVMEM: Adversarial Memory Initialization for Realistic Test-Time\n  Adaptation via Tracklet-Based Benchmarking",
      "authors": [
        "Shyma Alhuwaider",
        "Motasem Alfarra",
        "Juan C. Perez",
        "Merey Ramazanova",
        "Bernard Ghanem"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce a novel tracklet-based dataset for benchmarking test-time\nadaptation (TTA) methods. The aim of this dataset is to mimic the intricate\nchallenges encountered in real-world environments such as images captured by\nhand-held cameras, self-driving cars, etc. The current benchmarks for TTA focus\non how models face distribution shifts, when deployed, and on violations to the\ncustomary independent-and-identically-distributed (i.i.d.) assumption in\nmachine learning. Yet, these benchmarks fail to faithfully represent realistic\nscenarios that naturally display temporal dependencies, such as how consecutive\nframes from a video stream likely show the same object across time. We address\nthis shortcoming of current datasets by proposing a novel TTA benchmark we call\nthe \"Inherent Temporal Dependencies\" (ITD) dataset. We ensure the instances in\nITD naturally embody temporal dependencies by collecting them from\ntracklets-sequences of object-centric images we compile from the bounding boxes\nof an object-tracking dataset. We use ITD to conduct a thorough experimental\nanalysis of current TTA methods, and shed light on the limitations of these\nmethods when faced with the challenges of temporal dependencies. Moreover, we\nbuild upon these insights and propose a novel adversarial memory initialization\nstrategy to improve memory-based TTA methods. We find this strategy\nsubstantially boosts the performance of various methods on our challenging\nbenchmark.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02182v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02182v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.376,
      "datasets_score": 0.401,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes introducing a novel tracklet-based dataset called \"Inherent Temporal Dependencies\" (ITD), which is specifically designed for benchmarking test-time adaptation methods in machine learning. It details the creation process from object-tracking data, analyzes its properties to mimic real-world scenarios, and evaluates existing methods using this dataset. This directly aligns with research on creating, benchmarking, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces the Inherent Temporal Dependencies (ITD) dataset, a novel tracklet-based benchmark derived from object-tracking data to evaluate test-time adaptation (TTA) methods under realistic conditions with temporal dependencies and distribution shifts. It assesses existing TTA approaches, highlights their limitations in handling these challenges, and proposes ADVMEM, an adversarial memory initialization strategy that significantly enhances the performance of memory-based TTA methods, reducing error rates by up to 44% in experiments.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark incorporating temporal dependencies and a novel adversarial memory initialization technique, advancing the state-of-the-art in test-time adaptation by addressing overlooked real-world challenges.",
      "impact_score": "High",
      "impact_justification": "The work provides a more realistic evaluation framework for TTA, which could influence future research in computer vision and applications like autonomous driving by improving model robustness in dynamic environments.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a significant contribution to TTA methodologies with practical improvements and a new benchmark, making it valuable for researchers focused on real-world machine learning adaptations.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d01d15f88712b2d7703133f2d0d2ed2b8b50746e",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 6,
      "average_h_index": 2.6,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Shyma Alhuwaider",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2191617246"
        },
        {
          "name": "Motasem Alfarra",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2297847213"
        },
        {
          "name": "J. C. Perez",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2333324795"
        },
        {
          "name": "Merey Ramazanova",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/4042496"
        },
        {
          "name": "Bernard Ghanem",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2367329933"
        }
      ]
    },
    {
      "id": "2509.02196",
      "title": "Beyond Ensembles: Simulating All-Atom Protein Dynamics in a Learned\n  Latent Space",
      "authors": [
        "Aditya Sengar",
        "Ali Hariri",
        "Pierre Vandergheynst",
        "Patrick Barth"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Simulating the long-timescale dynamics of biomolecules is a central challenge\nin computational science. While enhanced sampling methods can accelerate these\nsimulations, they rely on pre-defined collective variables that are often\ndifficult to identify. A recent generative model, LD-FPG, demonstrated that\nthis problem could be bypassed by learning to sample the static equilibrium\nensemble as all-atom deformations from a reference structure, establishing a\npowerful method for all-atom ensemble generation. However, while this approach\nsuccessfully captures a system's probable conformations, it does not model the\ntemporal evolution between them. Here we extend LD-FPG with a temporal\npropagator that operates within the learned latent space and compare three\nclasses: (i) score-guided Langevin dynamics, (ii) Koopman-based linear\noperators, and (iii) autoregressive neural networks. Within a unified\nencoder-propagator-decoder framework, we evaluate long-horizon stability,\nbackbone and side-chain ensemble fidelity, and functional free-energy\nlandscapes. Autoregressive neural networks deliver the most robust long\nrollouts; score-guided Langevin best recovers side-chain thermodynamics when\nthe score is well learned; and Koopman provides an interpretable, lightweight\nbaseline that tends to damp fluctuations. These results clarify the trade-offs\namong propagators and offer practical guidance for latent-space simulators of\nall-atom protein dynamics.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02196v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02196v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.361,
      "datasets_score": 0.264,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses score-guided Langevin dynamics, a technique related to diffusion models for generating protein conformations, but it focuses on simulating molecular dynamics rather than adapting diffusion for multi-step logical reasoning or Chain-of-Thought processes. There is no component for solving complex logical tasks, making the connection indirect.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02208",
      "title": "Baichuan-M2: Scaling Medical Capability with Large Verifier System",
      "authors": [
        "Baichuan-M2 Team",
        ":",
        "Chengfeng Dou",
        "Chong Liu",
        "Fan Yang",
        "Fei Li",
        "Jiyuan Jia",
        "Mingyang Chen",
        "Qiang Ju",
        "Shuai Wang",
        "Shunya Dang",
        "Tianpeng Li",
        "Xiangrong Zeng",
        "Yijie Zhou",
        "Chenzheng Zhu",
        "Da Pan",
        "Fei Deng",
        "Guangwei Ai",
        "Guosheng Dong",
        "Hongda Zhang",
        "Jinyang Tai",
        "Jixiang Hong",
        "Kai Lu",
        "Linzhuang Sun",
        "Peidong Guo",
        "Qian Ma",
        "Rihui Xin",
        "Shihui Yang",
        "Shusen Zhang",
        "Yichuan Mo",
        "Zheng Liang",
        "Zhishou Zhang",
        "Hengfu Cui",
        "Zuyi Zhu",
        "Xiaochuan Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As large language models (LLMs) advance in conversational and reasoning\ncapabilities, their practical application in healthcare has become a critical\nresearch focus. However, there is a notable gap between the performance of\nmedical LLMs on static benchmarks such as USMLE and their utility in real-world\nclinical decision-making. This discrepancy arises because traditional exams\nfail to capture the dynamic, interactive nature of medical consultations. To\naddress this challenge, we introduce a novel dynamic verification framework\nthat moves beyond static answer verifier, establishing a large-scale,\nhigh-fidelity interactive reinforcement learning system. Our framework\ncomprises two key components: a Patient Simulator that creates realistic\nclinical environments using de-identified medical records, and a Clinical\nRubrics Generator that dynamically produces multi-dimensional evaluation\nmetrics. Building on this foundation, we develop Baichuan-M2, a 32B-parameter\nmedical augmented reasoning model trained through a multi-stage reinforcement\nlearning strategy with an improved Group Relative Policy Optimization (GRPO)\nalgorithm. Evaluated on HealthBench, Baichuan-M2 outperforms all other\nopen-source models and most advanced closed-source counterparts, achieving a\nscore above 32 on the challenging HealthBench Hard benchmark-previously\nexceeded only by GPT-5. Our work demonstrates that robust dynamic verifier\nsystem is essential for aligning LLM capabilities with practical clinical\napplications, establishing a new Pareto front in the performance-parameter\ntrade-off for medical AI deployment.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02208v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02208v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.429,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes a reinforcement learning approach using an improved GRPO algorithm with a simulated verifier system, including a Patient Simulator and Clinical Rubrics Generator. However, it does not involve training a reward model on human-ranked data or incorporate direct human feedback for alignment. Instead, it relies on automated simulations based on medical records, making it distinct from RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for medical reasoning through interactive simulations and a GRPO algorithm, but it does not mention or adapt diffusion models for iterative refinement in logical tasks. There is no component for multi-step reasoning via diffusion processes.",
      "distributed_training_justification": "The paper discusses training Baichuan-M2 using reinforcement learning strategies but does not address distributed training techniques, such as parallel computing, data partitioning, or multi-node systems to accelerate training. The focus is on the RL framework and verifier system, not computational distribution.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02217",
      "title": "ST-Hyper: Learning High-Order Dependencies Across Multiple\n  Spatial-Temporal Scales for Multivariate Time Series Forecasting",
      "authors": [
        "Binqing Wu",
        "Jianlong Huang",
        "Zongjiang Shang",
        "Ling Chen"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In multivariate time series (MTS) forecasting, many deep learning based\nmethods have been proposed for modeling dependencies at multiple spatial\n(inter-variate) or temporal (intra-variate) scales. However, existing methods\nmay fail to model dependencies across multiple spatial-temporal scales\n(ST-scales, i.e., scales that jointly consider spatial and temporal scopes). In\nthis work, we propose ST-Hyper to model the high-order dependencies across\nmultiple ST-scales through adaptive hypergraph modeling. Specifically, we\nintroduce a Spatial-Temporal Pyramid Modeling (STPM) module to extract features\nat multiple ST-scales. Furthermore, we introduce an Adaptive Hypergraph\nModeling (AHM) module that learns a sparse hypergraph to capture robust\nhigh-order dependencies among features. In addition, we interact with these\nfeatures through tri-phase hypergraph propagation, which can comprehensively\ncapture multi-scale spatial-temporal dynamics. Experimental results on six\nreal-world MTS datasets demonstrate that ST-Hyper achieves the state-of-the-art\nperformance, outperforming the best baselines with an average MAE reduction of\n3.8\\% and 6.8\\% for long-term and short-term forecasting, respectively.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02217v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02217v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.381,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02220",
      "title": "Towards Multi-Aspect Diversification of News Recommendations Using\n  Neuro-Symbolic AI for Individual and Societal Benefit",
      "authors": [
        "Markus Reiter-Haas",
        "Elisabeth Lex"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "News recommendations are complex, with diversity playing a vital role. So\nfar, existing literature predominantly focuses on specific aspects of news\ndiversity, such as viewpoints. In this paper, we introduce multi-aspect\ndiversification in four distinct recommendation modes and outline the nuanced\nchallenges in diversifying lists, sequences, summaries, and interactions. Our\nproposed research direction combines symbolic and subsymbolic artificial\nintelligence, leveraging both knowledge graphs and rule learning. We plan to\nevaluate our models using user studies to not only capture behavior but also\ntheir perceived experience. Our vision to balance news consumption points to\nother positive effects for users (e.g., increased serendipity) and society\n(e.g., decreased polarization).",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02220v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02220v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.448,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.463,
      "distributed_training_score": 0.346,
      "datasets_score": 0.399,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions using user studies to evaluate models and capture user behavior and perceived experience, which involves human feedback. However, it does not describe training a reward model or fine-tuning via reinforcement learning based on human-ranked data, making it only loosely related to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on neuro-symbolic AI for news recommendation diversification using knowledge graphs and rule learning, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02227",
      "title": "Application Of Large Language Models For The Extraction Of Information\n  From Particle Accelerator Technical Documentation",
      "authors": [
        "Qing Dai",
        "Rasmus Ischebeck",
        "Maruisz Sapinski",
        "Adam Grycner"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The large set of technical documentation of legacy accelerator systems,\ncoupled with the retirement of experienced personnel, underscores the urgent\nneed for efficient methods to preserve and transfer specialized knowledge. This\npaper explores the application of large language models (LLMs), to automate and\nenhance the extraction of information from particle accelerator technical\ndocuments. By exploiting LLMs, we aim to address the challenges of knowledge\nretention, enabling the retrieval of domain expertise embedded in legacy\ndocumentation. We present initial results of adapting LLMs to this specialized\ndomain. Our evaluation demonstrates the effectiveness of LLMs in extracting,\nsummarizing, and organizing knowledge, significantly reducing the risk of\nlosing valuable insights as personnel retire. Furthermore, we discuss the\nlimitations of current LLMs, such as interpretability and handling of rare\ndomain-specific terms, and propose strategies for improvement. This work\nhighlights the potential of LLMs to play a pivotal role in preserving\ninstitutional knowledge and ensuring continuity in highly specialized fields.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02227v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02227v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.387,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on applying and adapting large language models (LLMs) for information extraction from technical documentation, but it does not mention using human feedback, reward models, or reinforcement learning to fine-tune the models. There is no indication of aligning AI with human preferences through RLHF.",
      "weak_supervision_justification": "The paper discusses using LLMs to extract and organize information from existing technical documentation, but it does not describe training models with programmatically generated labels, noisy sources, or any weak supervision techniques. The focus is on application rather than the training paradigm.",
      "diffusion_reasoning_justification": "The paper explores LLMs for knowledge extraction and summarization from documentation, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning. It does not involve treating reasoning paths holistically.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02237",
      "title": "Autoencoder-based non-intrusive model order reduction in continuum\n  mechanics",
      "authors": [
        "Jannick Kehls",
        "Ellen Kuhl",
        "Tim Brepols",
        "Kevin Linka",
        "Hagen Holthusen"
      ],
      "categories": [
        "cs.CE (Computational Engineering, Finance, and Science)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We propose a non-intrusive, Autoencoder-based framework for reduced-order\nmodeling in continuum mechanics. Our method integrates three stages: (i) an\nunsupervised Autoencoder compresses high-dimensional finite element solutions\ninto a compact latent space, (ii) a supervised regression network maps problem\nparameters to latent codes, and (iii) an end-to-end surrogate reconstructs\nfull-field solutions directly from input parameters.\n  To overcome limitations of existing approaches, we propose two key\nextensions: a force-augmented variant that jointly predicts displacement fields\nand reaction forces at Neumann boundaries, and a multi-field architecture that\nenables coupled field predictions, such as in thermo-mechanical systems. The\nframework is validated on nonlinear benchmark problems involving heterogeneous\ncomposites, anisotropic elasticity with geometric variation, and\nthermo-mechanical coupling. Across all cases, it achieves accurate\nreconstructions of high-fidelity solutions while remaining fully non-intrusive.\n  These results highlight the potential of combining deep learning with\ndimensionality reduction to build efficient and extensible surrogate models.\nOur publicly available implementation provides a foundation for integrating\ndata-driven model order reduction into uncertainty quantification,\noptimization, and digital twin applications.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02237v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02237v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.37,
      "datasets_score": 0.268,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02241",
      "title": "LLMs for LLMs: A Structured Prompting Methodology for Long Legal\n  Documents",
      "authors": [
        "Strahinja Klem",
        "Noura Al Moubayed"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rise of Large Language Models (LLMs) has had a profoundly transformative\neffect on a number of fields and domains. However, their uptake in Law has\nproven more challenging due to the important issues of reliability and\ntransparency. In this study, we present a structured prompting methodology as a\nviable alternative to the often expensive fine-tuning, with the capability of\ntacking long legal documents from the CUAD dataset on the task of information\nretrieval. Each document is first split into chunks via a system of chunking\nand augmentation, addressing the long document problem. Then, alongside an\nengineered prompt, the input is fed into QWEN-2 to produce a set of answers for\neach question. Finally, we tackle the resulting candidate selection problem\nwith the introduction of the Distribution-based Localisation and Inverse\nCardinality Weighting heuristics. This approach leverages a general purpose\nmodel to promote long term scalability, prompt engineering to increase\nreliability and the two heuristic strategies to reduce the impact of the black\nbox effect. Whilst our model performs up to 9\\% better than the previously\npresented method, reaching state-of-the-art performance, it also highlights the\nlimiting factor of current automatic evaluation metrics for question answering,\nserving as a call to action for future research. However, the chief aim of this\nwork is to underscore the potential of structured prompt engineering as a\nuseful, yet under-explored, tool in ensuring accountability and responsibility\nof AI in the legal domain, and beyond.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02241v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02241v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.451,
      "weak_supervision_score": 0.402,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.329,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on structured prompting and heuristics for LLMs on legal documents, without any mention of training a model using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper does not involve training models with programmatically generated labels or noisy sources; it relies on prompting an existing LLM (QWEN-2) for information retrieval, bypassing any supervision methods.",
      "diffusion_reasoning_justification": "The paper describes prompt engineering and heuristics for handling long documents and question answering, but it does not use diffusion models, iterative refinement processes, or multi-step logical reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02248",
      "title": "Palmistry-Informed Feature Extraction and Analysis using Machine\n  Learning",
      "authors": [
        "Shweta Patil"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper explores the automated analysis of palmar features using machine\nlearning techniques. We present a computer vision pipeline that extracts key\ncharacteristics from palm images, such as principal line structures, texture,\nand shape metrics. These features are used to train predictive models on a\nnovel dataset curated from annotated palm images. Our approach moves beyond\ntraditional subjective interpretation by providing a data-driven, quantitative\nframework for studying the correlations between palmar morphology and\nexternally validated traits or conditions. The methodology demonstrates\nfeasibility for applications in digital anthropometry and personalized user\nanalytics, with potential for deployment on mobile platforms. Results indicate\nthat machine learning models can identify complex patterns in palm data,\nopening avenues for research that intersects cultural practices with\ncomputational analysis.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02248v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02248v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.301,
      "distributed_training_score": 0.269,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02256",
      "title": "A Multimodal Cross-View Model for Predicting Postoperative Neck Pain in\n  Cervical Spondylosis Patients",
      "authors": [
        "Jingyang Shan",
        "Qishuai Yu",
        "Jiacen Liu",
        "Shaolin Zhang",
        "Wen Shen",
        "Yanxiao Zhao",
        "Tianyi Wang",
        "Xiaolin Qin",
        "Yiheng Yin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Neck pain is the primary symptom of cervical spondylosis, yet its underlying\nmechanisms remain unclear, leading to uncertain treatment outcomes. To address\nthe challenges of multimodal feature fusion caused by imaging differences and\nspatial mismatches, this paper proposes an Adaptive Bidirectional Pyramid\nDifference Convolution (ABPDC) module that facilitates multimodal integration\nby exploiting the advantages of difference convolution in texture extraction\nand grayscale invariance, and a Feature Pyramid Registration Auxiliary Network\n(FPRAN) to mitigate structural misalignment. Experiments on the MMCSD dataset\ndemonstrate that the proposed model achieves superior prediction accuracy of\npostoperative neck pain recovery compared with existing methods, and ablation\nstudies further confirm its effectiveness.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02256v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02256v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.336,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02258",
      "title": "An Epidemiological Knowledge Graph extracted from the World Health\n  Organization's Disease Outbreak News",
      "authors": [
        "Sergio Consoli",
        "Pietro Coletti",
        "Peter V. Markov",
        "Lia Orfei",
        "Indaco Biazzo",
        "Lea Schuh",
        "Nicolas Stefanovitch",
        "Lorenzo Bertolini",
        "Mario Ceresa",
        "Nikolaos I. Stilianakis"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rapid evolution of artificial intelligence (AI), together with the\nincreased availability of social media and news for epidemiological\nsurveillance, are marking a pivotal moment in epidemiology and public health\nresearch. Leveraging the power of generative AI, we use an ensemble approach\nwhich incorporates multiple Large Language Models (LLMs) to extract valuable\nactionable epidemiological information from the World Health Organization (WHO)\nDisease Outbreak News (DONs). DONs is a collection of regular reports on global\noutbreaks curated by the WHO and the adopted decision-making processes to\nrespond to them. The extracted information is made available in a daily-updated\ndataset and a knowledge graph, referred to as eKG, derived to provide a nuanced\nrepresentation of the public health domain knowledge. We provide an overview of\nthis new dataset and describe the structure of eKG, along with the services and\ntools used to access and utilize the data that we are building on top. These\ninnovative data resources open altogether new opportunities for epidemiological\nresearch, and the analysis and surveillance of disease outbreaks.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02258v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02258v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.283,
      "datasets_score": 0.388,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02261",
      "title": "DSGC-Net: A Dual-Stream Graph Convolutional Network for Crowd Counting\n  via Feature Correlation Mining",
      "authors": [
        "Yihong Wu",
        "Jinqiao Wei",
        "Xionghui Zhao",
        "Yidi Li",
        "Shaoyi Du",
        "Bin Ren",
        "Nicu Sebe"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep learning-based crowd counting methods have achieved remarkable progress\nin recent years. However, in complex crowd scenarios, existing models still\nface challenges when adapting to significant density distribution differences\nbetween regions. Additionally, the inconsistency of individual representations\ncaused by viewpoint changes and body posture differences further limits the\ncounting accuracy of the models. To address these challenges, we propose\nDSGC-Net, a Dual-Stream Graph Convolutional Network based on feature\ncorrelation mining. DSGC-Net introduces a Density Approximation (DA) branch and\na Representation Approximation (RA) branch. By modeling two semantic graphs, it\ncaptures the potential feature correlations in density variations and\nrepresentation distributions. The DA branch incorporates a density prediction\nmodule that generates the density distribution map, and constructs a\ndensity-driven semantic graph based on density similarity. The RA branch\nestablishes a representation-driven semantic graph by computing global\nrepresentation similarity. Then, graph convolutional networks are applied to\nthe two semantic graphs separately to model the latent semantic relationships,\nwhich enhance the model's ability to adapt to density variations and improve\ncounting accuracy in multi-view and multi-pose scenarios. Extensive experiments\non three widely used datasets demonstrate that DSGC-Net outperforms current\nstate-of-the-art methods. In particular, we achieve MAE of 48.9 and 5.9 in\nShanghaiTech Part A and Part B datasets, respectively. The released code is\navailable at: https://github.com/Wu-eon/CrowdCounting-DSGCNet.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02261v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02261v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.272,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.364,
      "datasets_score": 0.37,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02271",
      "title": "VariAntNet: Learning Decentralized Control of Multi-Agent Systems",
      "authors": [
        "Yigal Koifman",
        "Erez Koifman",
        "Eran Iceland",
        "Ariel Barel",
        "Alfred M. Bruckstein"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "A simple multi-agent system can be effectively utilized in disaster response\napplications, such as firefighting. Such a swarm is required to operate in\ncomplex environments with limited local sensing and no reliable inter-agent\ncommunication or centralized control. These simple robotic agents, also known\nas Ant Robots, are defined as anonymous agents that possess limited sensing\ncapabilities, lack a shared coordinate system, and do not communicate\nexplicitly with one another. A key challenge for simple swarms lies in\nmaintaining cohesion and avoiding fragmentation despite limited-range sensing.\nRecent advances in machine learning offer effective solutions to some of the\nclassical decentralized control challenges. We propose VariAntNet, a deep\nlearning-based decentralized control model designed to facilitate agent\nswarming and collaborative task execution. VariAntNet includes geometric\nfeatures extraction from unordered, variable-sized local observations. It\nincorporates a neural network architecture trained with a novel,\ndifferentiable, multi-objective, mathematically justified loss function that\npromotes swarm cohesiveness by utilizing the properties of the visibility graph\nLaplacian matrix. VariAntNet is demonstrated on the fundamental multi-agent\ngathering task, where agents with bearing-only and limited-range sensing must\ngather at some location. VariAntNet significantly outperforms an existing\nanalytical solution, achieving more than double the convergence rate while\nmaintaining high swarm connectivity across varying swarm sizes. While the\nanalytical solution guarantees cohesion, it is often too slow in practice. In\ntime-critical scenarios, such as emergency response operations where lives are\nat risk, slower analytical methods are impractical and justify the loss of some\nagents within the swarm. This paper presents and analyzes this trade-off in\ndetail.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02271v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02271v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.374,
      "distributed_training_score": 0.427,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses Centralized Training with Decentralized Execution (CTDE) for a neural network in multi-agent systems, where training is performed centrally using global swarm data. While this involves processing data from multiple agents, it does not address distributed training techniques such as parallel computing, data partitioning across nodes, or multi-node acceleration. The main focus is on the control model and loss function, not on strategies for scaling training infrastructure, making it only loosely related to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02273",
      "title": "RS-OOD: A Vision-Language Augmented Framework for Out-of-Distribution\n  Detection in Remote Sensing",
      "authors": [
        "Yingrui Ji",
        "Jiansheng Chen",
        "Jingbo Chen",
        "Anzhi Yue",
        "Chenhao Wang",
        "Kai Li",
        "Yao Zhu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Out-of-distribution (OOD) detection represents a critical challenge in remote\nsensing applications, where reliable identification of novel or anomalous\npatterns is essential for autonomous monitoring, disaster response, and\nenvironmental assessment. Despite remarkable progress in OOD detection for\nnatural images, existing methods and benchmarks remain poorly suited to remote\nsensing imagery due to data scarcity, complex multi-scale scene structures, and\npronounced distribution shifts. To this end, we propose RS-OOD, a novel\nframework that leverages remote sensing-specific vision-language modeling to\nenable robust few-shot OOD detection. Our approach introduces three key\ninnovations: spatial feature enhancement that improved scene discrimination, a\ndual-prompt alignment mechanism that cross-verifies scene context against\nfine-grained semantics for spatial-semantic consistency, and a\nconfidence-guided self-training loop that dynamically mines pseudo-labels to\nexpand training data without manual annotation. RS-OOD consistently outperforms\nexisting methods across multiple remote sensing benchmarks and enables\nefficient adaptation with minimal labeled data, demonstrating the critical\nvalue of spatial-semantic integration.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02273v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02273v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.37,
      "datasets_score": 0.369,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02274",
      "title": "Look: AI at Work! -- Analysing Key Aspects of AI-support at the Work\n  Place",
      "authors": [
        "Stefan Schiffer",
        "Anna Milena Rothermel",
        "Alexander Ferrein",
        "Astrid Rosenthal-von der Pütten"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this paper we present an analysis of technological and psychological\nfactors of applying artificial intelligence (AI) at the work place. We do so\nfor a number of twelve application cases in the context of a project where AI\nis integrated at work places and in work systems of the future. From a\ntechnological point of view we mainly look at the areas of AI that the\napplications are concerned with. This allows to formulate recommendations in\nterms of what to look at in developing an AI application and what to pay\nattention to with regards to building AI literacy with different stakeholders\nusing the system. This includes the importance of high-quality data for\ntraining learning-based systems as well as the integration of human expertise,\nespecially with knowledge-based systems. In terms of the psychological factors\nwe derive research questions to investigate in the development of AI supported\nwork systems and to consider in future work, mainly concerned with topics such\nas acceptance, openness, and trust in an AI system.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02274v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02274v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.323,
      "distributed_training_score": 0.299,
      "datasets_score": 0.385,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on general technological and psychological factors of AI applications in the workplace, such as AI sub-fields, data quality, human expertise integration, acceptance, and trust. It does not mention, discuss, or involve Reinforcement Learning from Human Feedback (RLHF), which specifically entails training AI models using human-ranked data for reward modeling and fine-tuning. There is no reference to human feedback in the context of reinforcement learning, making the paper's content unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02276",
      "title": "Rewarding Explainability in Drug Repurposing with Knowledge Graphs",
      "authors": [
        "Susana Nunes",
        "Samy Badreddine",
        "Catia Pesquita"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Knowledge graphs (KGs) are powerful tools for modelling complex,\nmulti-relational data and supporting hypothesis generation, particularly in\napplications like drug repurposing. However, for predictive methods to gain\nacceptance as credible scientific tools, they must ensure not only accuracy but\nalso the capacity to offer meaningful scientific explanations. This paper\npresents a novel approach REx, for generating scientific explanations based in\nlink prediction in knowledge graphs. It employs reward and policy mechanisms\nthat consider desirable properties of scientific explanation to guide a\nreinforcement learning agent in the identification of explanatory paths within\na KG. The approach further enriches explanatory paths with domain-specific\nontologies, ensuring that the explanations are both insightful and grounded in\nestablished biomedical knowledge. We evaluate our approach in drug repurposing\nusing three popular knowledge graph benchmarks. The results clearly demonstrate\nits ability to generate explanations that validate predictive insights against\nbiomedical knowledge and that outperform the state-of-the-art approaches in\npredictive performance, establishing REx as a relevant contribution to advance\nAI-driven scientific discovery.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02276v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02276v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.279,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with rewards based on scientific explanation properties, but it does not involve human feedback, human-ranked data, or a reward model trained on human preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for identifying explanatory paths in knowledge graphs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02278",
      "title": "Think2Sing: Orchestrating Structured Motion Subtitles for Singing-Driven\n  3D Head Animation",
      "authors": [
        "Zikai Huang",
        "Yihan Zhou",
        "Xuemiao Xu",
        "Cheng Xu",
        "Xiaofen Xing",
        "Jing Qin",
        "Shengfeng He"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Singing-driven 3D head animation is a challenging yet promising task with\napplications in virtual avatars, entertainment, and education. Unlike speech,\nsinging involves richer emotional nuance, dynamic prosody, and lyric-based\nsemantics, requiring the synthesis of fine-grained, temporally coherent facial\nmotion. Existing speech-driven approaches often produce oversimplified,\nemotionally flat, and semantically inconsistent results, which are insufficient\nfor singing animation. To address this, we propose Think2Sing, a\ndiffusion-based framework that leverages pretrained large language models to\ngenerate semantically coherent and temporally consistent 3D head animations,\nconditioned on both lyrics and acoustics. A key innovation is the introduction\nof motion subtitles, an auxiliary semantic representation derived through a\nnovel Singing Chain-of-Thought reasoning process combined with acoustic-guided\nretrieval. These subtitles contain precise timestamps and region-specific\nmotion descriptions, serving as interpretable motion priors. We frame the task\nas a motion intensity prediction problem, enabling finer control over facial\nregions and improving the modeling of expressive motion. To support this, we\ncreate a multimodal singing dataset with synchronized video, acoustic\ndescriptors, and motion subtitles, enabling diverse and expressive motion\nlearning. Extensive experiments show that Think2Sing outperforms\nstate-of-the-art methods in realism, expressiveness, and emotional fidelity,\nwhile also offering flexible, user-controllable animation editing.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02278v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02278v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.44,
      "distributed_training_score": 0.32,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes Think2Sing, a diffusion-based framework for generating 3D head animations from singing inputs, which uses diffusion models primarily for motion synthesis and intensity prediction. However, the Chain-of-Thought reasoning is handled by large language models (LLMs) to generate motion subtitles, not by the diffusion model itself. The diffusion process does not adapt iterative refinement for multi-step logical tasks or treat a Chain-of-Thought as a single entity for correction; it focuses on generative animation tasks. Thus, it does not meet the criteria for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02287",
      "title": "SynthGenNet: a self-supervised approach for test-time generalization\n  using synthetic multi-source domain mixing of street view images",
      "authors": [
        "Pushpendra Dhakara",
        "Prachi Chachodhia",
        "Vaibhav Kumar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Unstructured urban environments present unique challenges for scene\nunderstanding and generalization due to their complex and diverse layouts. We\nintroduce SynthGenNet, a self-supervised student-teacher architecture designed\nto enable robust test-time domain generalization using synthetic multi-source\nimagery. Our contributions include the novel ClassMix++ algorithm, which blends\nlabeled data from various synthetic sources while maintaining semantic\nintegrity, enhancing model adaptability. We further employ Grounded Mask\nConsistency Loss (GMC), which leverages source ground truth to improve\ncross-domain prediction consistency and feature alignment. The Pseudo-Label\nGuided Contrastive Learning (PLGCL) mechanism is integrated into the student\nnetwork to facilitate domain-invariant feature learning through iterative\nknowledge distillation from the teacher network. This self-supervised strategy\nimproves prediction accuracy, addresses real-world variability, bridges the\nsim-to-real domain gap, and reliance on labeled target data, even in complex\nurban areas. Outcomes show our model outperforms the state-of-the-art (relying\non single source) by achieving 50% Mean Intersection-Over-Union (mIoU) value on\nreal-world datasets like Indian Driving Dataset (IDD).",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02287v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02287v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.429,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.378,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper employs synthetic data and pseudo-labels generated programmatically from a teacher network, which aligns with weak supervision by using noisy or imprecise labels instead of hand-labeled data. However, its primary focus is on self-supervised domain generalization for image segmentation, not solely on weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or any iterative refinement process for complex logical tasks; it centers on self-supervised learning, contrastive learning, and domain generalization for image segmentation, with no mention of multi-step logical reasoning via diffusion.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "SynthGenNet is a self-supervised student-teacher architecture aimed at improving test-time domain generalization for semantic segmentation in street view images by leveraging synthetic multi-source data to bridge the sim-to-real gap. The methodology includes the novel ClassMix++ algorithm for blending labeled data while maintaining semantic integrity, the Grounded Mask Consistency Loss (GMC) for enhancing prediction consistency and feature alignment, and Pseudo-Label Guided Contrastive Learning (PLGCL) for domain-invariant feature learning in the student network. Key findings show that this approach outperforms state-of-the-art methods using single sources, achieving a 50% Mean Intersection-over-Union (mIoU) on real-world datasets like the Indian Driving Dataset, thus reducing reliance on labeled target data in complex urban environments.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement through the introduction of ClassMix++, GMC loss, and PLGCL, which cleverly combine and extend existing techniques like CutMix and Masked Image Consistency for better domain generalization in unstructured environments. While it advances the field by addressing multi-source synthetic data mixing, it builds on known problems in domain adaptation rather than introducing a entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in computer vision subfields like domain generalization and semantic segmentation, particularly for applications in autonomous driving and urban planning in diverse environments. However, its impact may be limited to specific scenarios involving synthetic data, rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution with innovative techniques for domain generalization, making it important for researchers in computer vision focused on real-world adaptability. While not essential for all, it provides insights that could be built upon in related work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/58e31dfa355a11f4ec8941b06f2dbc008c183499",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Pushpendra Dhakara",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378705246"
        },
        {
          "name": "Prachi Chachodhia",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378705592"
        },
        {
          "name": "Vaibhav Kumar",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378732539"
        }
      ]
    },
    {
      "id": "2509.02295",
      "title": "Data-Driven Loss Functions for Inference-Time Optimization in\n  Text-to-Image Generation",
      "authors": [
        "Sapir Esther Yiflach",
        "Yuval Atzmon",
        "Gal Chechik"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-image diffusion models can generate stunning visuals, yet they often\nfail at tasks children find trivial--like placing a dog to the right of a teddy\nbear rather than to the left. When combinations get more unusual--a giraffe\nabove an airplane--these failures become even more pronounced. Existing methods\nattempt to fix these spatial reasoning failures through model fine-tuning or\ntest-time optimization with handcrafted losses that are suboptimal. Rather than\nimposing our assumptions about spatial encoding, we propose learning these\nobjectives directly from the model's internal representations. We introduce\nLearn-to-Steer, a novel framework that learns data-driven objectives for\ntest-time optimization rather than handcrafting them. Our key insight is to\ntrain a lightweight classifier that decodes spatial relationships from the\ndiffusion model's cross-attention maps, then deploy this classifier as a\nlearned loss function during inference. Training such classifiers poses a\nsurprising challenge: they can take shortcuts by detecting linguistic traces\nrather than learning true spatial patterns. We solve this with a dual-inversion\nstrategy that enforces geometric understanding. Our method dramatically\nimproves spatial accuracy: from 0.20 to 0.61 on FLUX.1-dev and from 0.07 to\n0.54 on SD2.1 across standard benchmarks. Moreover, our approach generalizes to\nmultiple relations and significantly improves accuracy.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02295v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02295v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.552,
      "distributed_training_score": 0.364,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on learning loss functions from model internals for text-to-image generation, without any involvement of human feedback, reward models, or reinforcement learning for alignment.",
      "weak_supervision_justification": "The paper trains a classifier on model representations with data augmentation to handle noisy signals like \"relation leakage,\" which aligns with weak supervision by using programmatically derived or imperfect labels, though it does not explicitly rely on large-scale noisy sources.",
      "diffusion_reasoning_justification": "The paper uses diffusion models for image generation and spatial optimization but does not involve adapting diffusion for multi-step logical reasoning or treating reasoning paths as entities for iterative refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces Learn-to-Steer, a framework designed to enhance spatial accuracy in text-to-image diffusion models by learning data-driven loss functions from the model's cross-attention maps, rather than relying on handcrafted ones. The methodology involves training a lightweight classifier to decode spatial relationships and using it as a loss for inference-time optimization, addressing challenges like \"relation leakage\" through a dual-inversion strategy, resulting in significant accuracy improvements—from 0.20 to 0.61 on FLUX.1-dev and 0.07 to 0.54 on SD2.1—while supporting multiple relations without model fine-tuning.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework that learns data-driven objectives from internal representations, significantly advancing test-time optimization for spatial accuracy in text-to-image generation beyond existing handcrafted methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields focused on text-to-image alignment and optimization, as it provides a flexible method for improving spatial reasoning without fine-tuning. However, its influence may be limited to specific applications rather than broadly transformative across all AI research.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a strong, innovative contribution with clear improvements and novel techniques, making it valuable for researchers in computer vision and text-to-image generation to stay informed. While not essential for all, it offers practical insights that could inspire further advancements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/fb82287c684aed1b090055e44e746a4a55143260",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 20,
      "average_h_index": 12.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Sapir Esther Yiflach",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378710669"
        },
        {
          "name": "Y. Atzmon",
          "h_index": 20,
          "profile_url": "https://www.semanticscholar.org/author/34815079"
        },
        {
          "name": "Gal Chechik",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/2065001062"
        }
      ]
    },
    {
      "id": "2509.02297",
      "title": "Re-evaluating LLM-based Heuristic Search: A Case Study on the 3D Packing\n  Problem",
      "authors": [
        "Guorui Quan",
        "Mingfei Sun",
        "Manuel López-Ibáñez"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The art of heuristic design has traditionally been a human pursuit. While\nLarge Language Models (LLMs) can generate code for search heuristics, their\napplication has largely been confined to adjusting simple functions within\nhuman-crafted frameworks, leaving their capacity for broader innovation an open\nquestion. To investigate this, we tasked an LLM with building a complete solver\nfor the constrained 3D Packing Problem. Direct code generation quickly proved\nfragile, prompting us to introduce two supports: constraint\nscaffolding--prewritten constraint-checking code--and iterative\nself-correction--additional refinement cycles to repair bugs and produce a\nviable initial population. Notably, even within a vast search space in a greedy\nprocess, the LLM concentrated its efforts almost exclusively on refining the\nscoring function. This suggests that the emphasis on scoring functions in prior\nwork may reflect not a principled strategy, but rather a natural limitation of\nLLM capabilities. The resulting heuristic was comparable to a human-designed\ngreedy algorithm, and when its scoring function was integrated into a\nhuman-crafted metaheuristic, its performance rivaled established solvers,\nthough its effectiveness waned as constraints tightened. Our findings highlight\ntwo major barriers to automated heuristic design with current LLMs: the\nengineering required to mitigate their fragility in complex reasoning tasks,\nand the influence of pretrained biases, which can prematurely narrow the search\nfor novel solutions.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02297v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02297v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.439,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.464,
      "distributed_training_score": 0.337,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using LLMs for heuristic design with iterative self-correction based on runtime errors, which is an automated process. It does not involve human feedback, a reward model trained on human-ranked data, or reinforcement learning for model alignment, as required for RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes iterative self-correction for refining LLM-generated code, but it does not adapt the iterative refinement process of diffusion models for complex logical tasks or treat a Chain-of-Thought as a single entity for holistic correction. There is no mention of diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02305",
      "title": "Hues and Cues: Human vs. CLIP",
      "authors": [
        "Nuria Alabau-Bosque",
        "Jorge Vila-Tomás",
        "Paula Daudén-Oliver",
        "Pablo Hernández-Cámara",
        "Jose Manuel Jaén-Lorites",
        "Valero Laparra",
        "Jesús Malo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Playing games is inherently human, and a lot of games are created to\nchallenge different human characteristics. However, these tasks are often left\nout when evaluating the human-like nature of artificial models. The objective\nof this work is proposing a new approach to evaluate artificial models via\nboard games. To this effect, we test the color perception and color naming\ncapabilities of CLIP by playing the board game Hues & Cues and assess its\nalignment with humans. Our experiments show that CLIP is generally well aligned\nwith human observers, but our approach brings to light certain cultural biases\nand inconsistencies when dealing with different abstraction levels that are\nhard to identify with other testing strategies. Our findings indicate that\nassessing models with different tasks like board games can make certain\ndeficiencies in the models stand out in ways that are difficult to test with\nthe commonly used benchmarks.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02305v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02305v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.394,
      "distributed_training_score": 0.264,
      "datasets_score": 0.321,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating the CLIP model's color perception and alignment with humans using a board game, without any involvement of training a model with human feedback, reward models, or reinforcement learning techniques. RLHF specifically requires using human-ranked data to fine-tune models, which is not addressed in the paper.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02308",
      "title": "Exploring Diffusion Models for Generative Forecasting of Financial\n  Charts",
      "authors": [
        "Taegyeong Lee",
        "Jiwon Park",
        "Kyunga Bang",
        "Seunghyun Hwang",
        "Ung-Jin Jang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in generative models have enabled significant progress in\ntasks such as generating and editing images from text, as well as creating\nvideos from text prompts, and these methods are being applied across various\nfields. However, in the financial domain, there may still be a reliance on\ntime-series data and a continued focus on transformer models, rather than on\ndiverse applications of generative models. In this paper, we propose a novel\napproach that leverages text-to-image model by treating time-series data as a\nsingle image pattern, thereby enabling the prediction of stock price trends.\nUnlike prior methods that focus on learning and classifying chart patterns\nusing architectures such as ResNet or ViT, we experiment with generating the\nnext chart image from the current chart image and an instruction prompt using\ndiffusion models. Furthermore, we introduce a simple method for evaluating the\ngenerated chart image against ground truth image. We highlight the potential of\nleveraging text-to-image generative models in the financial domain, and our\nfindings motivate further research to address the current limitations and\nexpand their applicability.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02308v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02308v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.309,
      "diffusion_reasoning_score": 0.585,
      "distributed_training_score": 0.294,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper applies diffusion models for generating financial chart images through iterative refinement, which shares the core mechanism of diffusion processes. However, it focuses on visual pattern generation and forecasting rather than adapting diffusion for multi-step logical reasoning or treating a 'Chain-of-Thought' as an entity for holistic correction. Thus, while there is a loose connection via the diffusion framework, the paper does not address complex logical tasks as specified in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02322",
      "title": "OmniActor: A Generalist GUI and Embodied Agent for 2D&3D Worlds",
      "authors": [
        "Longrong Yang",
        "Zhixiong Zeng",
        "Yufeng Zhong",
        "Jing Huang",
        "Liming Zheng",
        "Lei Chen",
        "Haibo Qiu",
        "Zequn Qin",
        "Lin Ma",
        "Xi Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal large language models are evolving toward multimodal agents\ncapable of proactively executing tasks. Most agent research focuses on GUI or\nembodied scenarios, which correspond to agents interacting with 2D virtual\nworlds or 3D real worlds, respectively. However, many complex tasks typically\nrequire agents to interleavely interact with these two types of environment. We\ninitially mix GUI and embodied data to train, but find the performance\ndegeneration brought by the data conflict. Further analysis reveals that GUI\nand embodied data exhibit synergy and conflict at the shallow and deep layers,\nrespectively, which resembles the cerebrum-cerebellum mechanism in the human\nbrain. To this end, we propose a high-performance generalist agent OmniActor,\ndesigned from both structural and data perspectives. First, we propose\nLayer-heterogeneity MoE to eliminate the conflict between GUI and embodied data\nby separating deep-layer parameters, while leverage their synergy by sharing\nshallow-layer parameters. By successfully leveraging the synergy and\neliminating the conflict, OmniActor outperforms agents only trained by GUI or\nembodied data in GUI or embodied tasks. Furthermore, we unify the action spaces\nof GUI and embodied tasks, and collect large-scale GUI and embodied data from\nvarious sources for training. This significantly improves OmniActor under\ndifferent scenarios, especially in GUI tasks. The code will be publicly\navailable.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02322v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02322v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.349,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02330",
      "title": "ReCode: Improving LLM-based Code Repair with Fine-Grained\n  Retrieval-Augmented Generation",
      "authors": [
        "Yicong Zhao",
        "Shisong Chen",
        "Jiacheng Zhang",
        "Zhixu Li"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in large language models (LLMs) have demonstrated impressive\ncapabilities in code-related tasks, such as code generation and automated\nprogram repair. Despite their promising performance, most existing approaches\nfor code repair suffer from high training costs or computationally expensive\ninference. Retrieval-augmented generation (RAG), with its efficient in-context\nlearning paradigm, offers a more scalable alternative. However, conventional\nretrieval strategies, which are often based on holistic code-text embeddings,\nfail to capture the structural intricacies of code, resulting in suboptimal\nretrieval quality. To address the above limitations, we propose ReCode, a\nfine-grained retrieval-augmented in-context learning framework designed for\naccurate and efficient code repair. Specifically, ReCode introduces two key\ninnovations: (1) an algorithm-aware retrieval strategy that narrows the search\nspace using preliminary algorithm type predictions; and (2) a modular\ndual-encoder architecture that separately processes code and textual inputs,\nenabling fine-grained semantic matching between input and retrieved contexts.\nFurthermore, we propose RACodeBench, a new benchmark constructed from\nreal-world user-submitted buggy code, which addresses the limitations of\nsynthetic benchmarks and supports realistic evaluation. Experimental results on\nRACodeBench and competitive programming datasets demonstrate that ReCode\nachieves higher repair accuracy with significantly reduced inference cost,\nhighlighting its practical value for real-world code repair scenarios.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02330v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02330v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.418,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.455,
      "distributed_training_score": 0.34,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes a retrieval-augmented generation framework for code repair and does not involve training a reward model on human-ranked data or using reinforcement learning to fine-tune models based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on retrieval strategies and in-context learning for code repair, without any mention of adapting diffusion models for multi-step logical reasoning or iterative refinement processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02333",
      "title": "DCPO: Dynamic Clipping Policy Optimization",
      "authors": [
        "Shihui Yang",
        "Chengfeng Dou",
        "Peidong Guo",
        "Kai Lu",
        "Qiang Ju",
        "Fei Deng",
        "Rihui Xin"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npromising framework for enhancing the reasoning capabilities of large language\nmodels. However, existing approaches such as GRPO often suffer from zero\ngradients. This problem arises primarily due to fixed clipping bounds for\ntoken-level probability ratios and the standardization of identical rewards,\nwhich can lead to ineffective gradient updates and underutilization of\ngenerated responses. In this work, we propose Dynamic Clipping Policy\nOptimization(DCPO), which introduces a dynamic clipping strategy that\nadaptively adjusts clipping bounds based on token-specific prior probabilities\nto enhance token-level exploration, and a smooth advantage standardization\ntechnique that standardizes rewards across cumulative training steps to improve\nthe response-level effective utilization of generated responses. DCPO achieved\nstate-of-the-art performance on four benchmarks based on four different models.\nIn particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an\nAvg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing DAPO\n(36.7/31.6), GRPO (36.7/32.1) and GSPO (40.0/34.9) on the Qwen2.5-Math-7B\nmodel. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a\nperformance of (23.3/19.0), surpassing GRPO (13.3/10.5), DAPO (20.0/15.3) and\nGSPO (16.7/9.9). Furthermore, DCPO achieved an average 28% improvement in the\nnonzero advantage over GRPO in four models, doubled the training efficiency\nover DAPO, and significantly reduced the token clipping ratio by an order of\nmagnitude compared to both GRPO and DAPO, while achieving superior performance.\nThese results highlight DCPO's effectiveness in leveraging generated data more\nefficiently for reinforcement learning in large language models.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02333v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02333v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.483,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.464,
      "distributed_training_score": 0.436,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Reinforcement Learning from Verifiable Rewards (RLVR), which uses rule-based rewards to optimize large language models, similar to RLHF in its reinforcement learning aspect. However, it does not involve human feedback or human-ranked data, making it only loosely related.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a dynamic clipping strategy for policy optimization in RLVR, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "The paper does not address distributed training, parallel computing, or partitioning computation across nodes; it focuses solely on algorithmic improvements for reinforcement learning in language models.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02340",
      "title": "Explainability-Driven Dimensionality Reduction for Hyperspectral Imaging",
      "authors": [
        "Salma Haidar",
        "José Oramas"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Hyperspectral imaging (HSI) provides rich spectral information for precise\nmaterial classification and analysis; however, its high dimensionality\nintroduces a computational burden and redundancy, making dimensionality\nreduction essential. We present an exploratory study into the application of\npost-hoc explainability methods in a model--driven framework for band\nselection, which reduces the spectral dimension while preserving predictive\nperformance. A trained classifier is probed with explanations to quantify each\nband's contribution to its decisions. We then perform deletion--insertion\nevaluations, recording confidence changes as ranked bands are removed or\nreintroduced, and aggregate these signals into influence scores. Selecting the\nhighest--influence bands yields compact spectral subsets that maintain accuracy\nand improve efficiency. Experiments on two public benchmarks (Pavia University\nand Salinas) demonstrate that classifiers trained on as few as 30 selected\nbands match or exceed full--spectrum baselines while reducing computational\nrequirements. The resulting subsets align with physically meaningful, highly\ndiscriminative wavelength regions, indicating that model--aligned,\nexplanation-guided band selection is a principled route to effective\ndimensionality reduction for HSI.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02340v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02340v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.314,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on explainability methods (e.g., LRP, SHAP, RISE) for dimensionality reduction in hyperspectral imaging, specifically for band selection to improve efficiency and accuracy in classification tasks. It does not involve human feedback, reinforcement learning, reward models, or any mechanism for aligning AI models with human preferences, which are core to RLHF. Therefore, there is no connection to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02341",
      "title": "RDIT: Residual-based Diffusion Implicit Models for Probabilistic Time\n  Series Forecasting",
      "authors": [
        "Chih-Yu Lai",
        "Yu-Chien Ning",
        "Duane S. Boning"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Probabilistic Time Series Forecasting (PTSF) plays a critical role in domains\nrequiring accurate and uncertainty-aware predictions for decision-making.\nHowever, existing methods offer suboptimal distribution modeling and suffer\nfrom a mismatch between training and evaluation metrics. Surprisingly, we found\nthat augmenting a strong point estimator with a zero-mean Gaussian, whose\nstandard deviation matches its training error, can yield state-of-the-art\nperformance in PTSF. In this work, we propose RDIT, a plug-and-play framework\nthat combines point estimation and residual-based conditional diffusion with a\nbidirectional Mamba network. We theoretically prove that the Continuous Ranked\nProbability Score (CRPS) can be minimized by adjusting to an optimal standard\ndeviation and then derive algorithms to achieve distribution matching.\nEvaluations on eight multivariate datasets across varied forecasting horizons\ndemonstrate that RDIT achieves lower CRPS, rapid inference, and improved\ncoverage compared to strong baselines.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02341v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02341v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.489,
      "distributed_training_score": 0.392,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for probabilistic time series forecasting, specifically for modeling residuals and improving predictions in domains like finance and healthcare. While it employs iterative refinement via diffusion for generative purposes, it does not involve adapting diffusion to solve complex logical tasks, such as holistic correction of a 'Chain-of-Thought' for reasoning. There is no component for multi-step logical reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02349",
      "title": "AudioCodecBench: A Comprehensive Benchmark for Audio Codec Evaluation",
      "authors": [
        "Lu Wang",
        "Hao Chen",
        "Siyu Wu",
        "Zhiyue Wu",
        "Hao Zhou",
        "Chengfeng Zhang",
        "Ting Wang",
        "Haodi Zhang"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have been widely applied in speech\nand music. This tendency has led to a focus on audio tokenization for Large\nModels (LMs). Unlike semantic-only text tokens, audio tokens must both capture\nglobal semantic content and preserve fine-grained acoustic details. Moreover,\nthey provide a discrete method for speech and music that can be effectively\nintegrated into MLLMs. However, existing research is unsuitable in the\ndefinitions of semantic tokens and acoustic tokens. In addition, the evaluation\nof different codecs typically concentrates on specific domains or tasks, such\nas reconstruction or Automatic Speech Recognition (ASR) task, which prevents\nfair and comprehensive comparisons. To address these problems, this paper\nprovides suitable definitions for semantic and acoustic tokens and introduces a\nsystematic evaluation framework. This framework allows for a comprehensive\nassessment of codecs' capabilities which evaluate across four dimensions: audio\nreconstruction metric, codebook index (ID) stability, decoder-only transformer\nperplexity, and performance on downstream probe tasks. Our results show the\ncorrectness of the provided suitable definitions and the correlation among\nreconstruction metrics, codebook ID stability, downstream probe tasks and\nperplexity.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02349v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02349v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.316,
      "diffusion_reasoning_score": 0.333,
      "distributed_training_score": 0.333,
      "datasets_score": 0.377,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02350",
      "title": "Implicit Reasoning in Large Language Models: A Comprehensive Survey",
      "authors": [
        "Jindong Li",
        "Yali Fu",
        "Li Fan",
        "Jiahong Liu",
        "Yao Shu",
        "Chengwei Qin",
        "Menglin Yang",
        "Irwin King",
        "Rex Ying"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong generalization across a\nwide range of tasks. Reasoning with LLMs is central to solving multi-step\nproblems and complex decision-making. To support efficient reasoning, recent\nstudies have shifted attention from explicit chain-of-thought prompting toward\nimplicit reasoning, where reasoning occurs silently via latent structures\nwithout emitting intermediate textual steps. Implicit reasoning brings\nadvantages such as lower generation cost, faster inference, and better\nalignment with internal computation. Although prior surveys have discussed\nlatent representations in the context of reasoning, a dedicated and\nmechanism-level examination of how reasoning unfolds internally within LLMs\nremains absent. This survey fills that gap by introducing a taxonomy centered\non execution paradigms, shifting the focus from representational forms to\ncomputational strategies. We organize existing methods into three execution\nparadigms based on \\textbf{\\textit{how and where internal computation\nunfolds}}: latent optimization, signal-guided control, and layer-recurrent\nexecution. We also review structural, behavioral and representation-based\nevidence that supports the presence of implicit reasoning in LLMs. We further\nprovide a structured overview of the evaluation metrics and benchmarks used in\nexisting works to assess the effectiveness and reliability of implicit\nreasoning. We maintain a continuously updated project at:\nhttps://github.com/digailab/awesome-llm-implicit-reasoning.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02350v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02350v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.562,
      "distributed_training_score": 0.367,
      "datasets_score": 0.329,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper is a survey on implicit reasoning in LLMs and mentions DeepSeek R1, which uses reinforcement learning to enhance reasoning capabilities, as cited in the introduction. However, it does not focus on RLHF specifically, which involves human feedback for alignment, nor does it discuss training a reward model or fine-tuning with human-ranked data. Thus, the reference is peripheral and not central to the paper's main contribution.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper surveys implicit reasoning in LLMs, focusing on paradigms like latent optimization and internal computation, but it does not mention diffusion models, iterative refinement processes, or any adaptation of diffusion for logical tasks. There is no component related to treating a Chain-of-Thought as an entity for multi-step correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02351",
      "title": "Ordinal Adaptive Correction: A Data-Centric Approach to Ordinal Image\n  Classification with Noisy Labels",
      "authors": [
        "Alireza Sedighi Moghaddam",
        "Mohammad Reza Mohammadi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Labeled data is a fundamental component in training supervised deep learning\nmodels for computer vision tasks. However, the labeling process, especially for\nordinal image classification where class boundaries are often ambiguous, is\nprone to error and noise. Such label noise can significantly degrade the\nperformance and reliability of machine learning models. This paper addresses\nthe problem of detecting and correcting label noise in ordinal image\nclassification tasks. To this end, a novel data-centric method called ORDinal\nAdaptive Correction (ORDAC) is proposed for adaptive correction of noisy\nlabels. The proposed approach leverages the capabilities of Label Distribution\nLearning (LDL) to model the inherent ambiguity and uncertainty present in\nordinal labels. During training, ORDAC dynamically adjusts the mean and\nstandard deviation of the label distribution for each sample. Rather than\ndiscarding potentially noisy samples, this approach aims to correct them and\nmake optimal use of the entire training dataset. The effectiveness of the\nproposed method is evaluated on benchmark datasets for age estimation (Adience)\nand disease severity detection (Diabetic Retinopathy) under various asymmetric\nGaussian noise scenarios. Results show that ORDAC and its extended versions\n(ORDAC_C and ORDAC_R) lead to significant improvements in model performance.\nFor instance, on the Adience dataset with 40% noise, ORDAC_R reduced the mean\nabsolute error from 0.86 to 0.62 and increased the recall metric from 0.37 to\n0.49. The method also demonstrated its effectiveness in correcting intrinsic\nnoise present in the original datasets. This research indicates that adaptive\nlabel correction using label distributions is an effective strategy to enhance\nthe robustness and accuracy of ordinal classification models in the presence of\nnoisy data.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02351v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02351v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.503,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.357,
      "datasets_score": 0.408,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, ORDAC, directly addresses handling noisy labels in ordinal image classification by correcting them using Label Distribution Learning, which aligns closely with weak supervision. Weak supervision involves training models on noisy or imprecise labels from sources like crowdsourcing, and this paper tackles that by adaptively correcting such labels, thereby improving model robustness without relying on perfectly labeled data.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper evaluates its method on existing benchmark datasets like Adience and Diabetic Retinopathy, including scenarios with added noise, which involves some analysis and benchmarking of datasets. However, the primary focus is on the ORDAC method for label correction, not on creating, curating, or deeply analyzing datasets themselves.",
      "llm_score_status": "completed",
      "summary": "The paper introduces ORDAC, a data-centric framework for addressing label noise in ordinal image classification, which uses Label Distribution Learning to model labels as Gaussian distributions and dynamically adjusts their mean and standard deviation during training to correct noisy labels rather than discarding them. Evaluated on datasets such as Adience for age estimation and Diabetic Retinopathy for disease severity, ORDAC and its variants demonstrate significant performance improvements, including reducing mean absolute error from 0.86 to 0.62 on Adience with 40% noise, highlighting its effectiveness in enhancing model robustness and accuracy in the presence of noisy data.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining Label Distribution Learning with adaptive correction for noisy ordinal labels, offering a new way to handle label noise without discarding samples, though it builds on existing techniques rather than introducing a entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of noisy label handling for ordinal classification, given its demonstrated improvements in real-world applications like age estimation and disease detection.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to data-centric approaches in AI, particularly for researchers dealing with noisy labels in computer vision, making it essential for staying informed in this area.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3054c897169ae0fce8eebeff7ebc3adfc1e6d0b2",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 1,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Alireza Sedighi Moghaddam",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2337688380"
        },
        {
          "name": "Mohammad Reza Mohammadi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378711540"
        }
      ]
    },
    {
      "id": "2509.02357",
      "title": "Category-Aware 3D Object Composition with Disentangled Texture and Shape\n  Multi-view Diffusion",
      "authors": [
        "Zeren Xiong",
        "Zikun Chen",
        "Zedong Zhang",
        "Xiang Li",
        "Ying Tai",
        "Jian Yang",
        "Jun Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we tackle a new task of 3D object synthesis, where a 3D model\nis composited with another object category to create a novel 3D model. However,\nmost existing text/image/3D-to-3D methods struggle to effectively integrate\nmultiple content sources, often resulting in inconsistent textures and\ninaccurate shapes. To overcome these challenges, we propose a straightforward\nyet powerful approach, category+3D-to-3D (C33D), for generating novel and\nstructurally coherent 3D models. Our method begins by rendering multi-view\nimages and normal maps from the input 3D model, then generating a novel 2D\nobject using adaptive text-image harmony (ATIH) with the front-view image and a\ntext description from another object category as inputs. To ensure texture\nconsistency, we introduce texture multi-view diffusion, which refines the\ntextures of the remaining multi-view RGB images based on the novel 2D object.\nFor enhanced shape accuracy, we propose shape multi-view diffusion to improve\nthe 2D shapes of both the multi-view RGB images and the normal maps, also\nconditioned on the novel 2D object. Finally, these outputs are used to\nreconstruct a complete and novel 3D model. Extensive experiments demonstrate\nthe effectiveness of our method, yielding impressive 3D creations, such as\nshark(3D)-crocodile(text) in the first row of Fig. 1. A project page is\navailable at: https://xzr52.github.io/C33D/",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02357v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02357v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.51,
      "distributed_training_score": 0.31,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion models (e.g., texture multi-view diffusion and shape multi-view diffusion) for iterative refinement in 3D object synthesis, which involves refining images and shapes over multiple steps. However, this is applied to visual generation tasks, not to solving complex logical tasks or treating a 'Chain-of-Thought' as a holistic entity for reasoning. Thus, while diffusion's iterative process is present, it does not align with the topic's focus on logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02359",
      "title": "Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis\n  from Data to Architecture",
      "authors": [
        "Wanyue Zhang",
        "Yibin Huang",
        "Yangbin Xu",
        "JingJing Huang",
        "Helu Zhi",
        "Shuo Ren",
        "Wang Xu",
        "Jiajun Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Spatial understanding is essential for Multimodal Large Language Models\n(MLLMs) to support perception, reasoning, and planning in embodied\nenvironments. Despite recent progress, existing studies reveal that MLLMs still\nstruggle with spatial understanding. However, existing research lacks a\ncomprehensive and systematic evaluation of these limitations, often restricted\nto isolated scenarios, such as single-view or video. In this work, we present a\nsystematic analysis of spatial understanding from both data and architectural\nperspectives across three representative scenarios: single-view, multi-view,\nand video. We propose a benchmark named MulSeT (Multi-view Spatial\nUnderstanding Tasks), and design a series of experiments to analyze the spatial\nreasoning capabilities of MLLMs. From the data perspective, the performance of\nspatial understanding converges quickly as the training data increases, and the\nupper bound is relatively low, especially for tasks that require spatial\nimagination. This indicates that merely expanding training data is insufficient\nto achieve satisfactory performance. From the architectural perspective, we\nfind that spatial understanding relies more heavily on the positional encoding\nwithin the visual encoder than within the language model, in both cascaded and\nnative MLLMs. Moreover, we explore reasoning injection and envision future\nimprovements through architectural design to optimize spatial understanding.\nThese insights shed light on the limitations of current MLLMs and suggest new\ndirections for improving spatial reasoning capabilities through data scaling\nand architectural tuning.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02359v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02359v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.374,
      "datasets_score": 0.381,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper primarily analyzes spatial understanding limitations in Multimodal Large Language Models (MLLMs), focusing on data scaling, architectural aspects like positional encoding, and benchmarks for spatial tasks. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. Therefore, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02360",
      "title": "When Agents go Astray: Course-Correcting SWE Agents with PRMs",
      "authors": [
        "Shubham Gandhi",
        "Jason Tsay",
        "Jatin Ganhotra",
        "Kiran Kate",
        "Yara Rizk"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Large Language Model (LLM) agents are increasingly deployed for complex,\nmulti-step software engineering (SWE) tasks. However, their trajectories often\ncontain costly inefficiencies, such as redundant exploration, looping, and\nfailure to terminate once a solution is reached. Prior work has largely treated\nthese errors in a post-hoc manner, diagnosing failures only after execution. In\nthis paper, we introduce SWE-PRM, an inference-time Process Reward Model (PRM)\nthat intervenes during execution to detect and course-correct trajectory-level\nerrors. Our PRM design leverages a taxonomy of common inefficiencies and\ndelivers lightweight, interpretable feedback without modifying the underlying\npolicy. On SWE-bench Verified, closed-source PRMs improve resolution from 40.0%\nto 50.6% (+10.6 p.p.), with the largest gains on medium and hard tasks. Among\nfeedback strategies, taxonomy-guided PRMs outperform unguided or explicit\naction-prescriptive variants, increasing success rate while reducing trajectory\nlength. These benefits come at an acceptable added inference cost of as low as\n$0.2, making PRMs a practical and scalable mechanism for improving SWE agents'\nreliability and efficiency.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02360v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02360v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.481,
      "weak_supervision_score": 0.427,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.362,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses a Process Reward Model (PRM) for inference-time feedback, which shares conceptual similarities with reward models in RLHF. However, it does not involve training a reward model on human-ranked data or using it to fine-tune the main model via reinforcement learning. Instead, the PRM provides real-time guidance without modifying the agent's policy, making it only loosely related to RLHF.",
      "weak_supervision_justification": "The paper does not discuss training models using programmatically generated labels or any form of weak supervision. It focuses on deploying a pre-trained PRM for error detection during inference, with no mention of label generation or weak supervisory signals.",
      "diffusion_reasoning_justification": "The paper involves multi-step reasoning and iterative correction via PRMs, but it does not adapt diffusion models or their iterative refinement processes for logical tasks. There is no component resembling diffusion-based holistic correction of reasoning paths.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02369",
      "title": "Guidance and Control Neural Network Acceleration using Memristors",
      "authors": [
        "Zacharia A. Rudge",
        "Dario Izzo",
        "Moritz Fieback",
        "Anteneh Gebregiorgis",
        "Said Hamdioui",
        "Dominik Dold"
      ],
      "categories": [
        "cs.AR (Hardware Architecture)",
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "In recent years, the space community has been exploring the possibilities of\nArtificial Intelligence (AI), specifically Artificial Neural Networks (ANNs),\nfor a variety of on board applications. However, this development is limited by\nthe restricted energy budget of smallsats and cubesats as well as radiation\nconcerns plaguing modern chips. This necessitates research into neural network\naccelerators capable of meeting these requirements whilst satisfying the\ncompute and performance needs of the application. This paper explores the use\nof Phase-Change Memory (PCM) and Resistive Random-Access Memory (RRAM)\nmemristors for on-board in-memory computing AI acceleration in space\napplications. A guidance and control neural network (G\\&CNET) accelerated using\nmemristors is simulated in a variety of scenarios and with both device types to\nevaluate the performance of memristor-based accelerators, considering device\nnon-idealities such as noise and conductance drift. We show that the memristive\naccelerator is able to learn the expert actions, though challenges remain with\nthe impact of noise on accuracy. We also show that re-training after\ndegradation is able to restore performance to nominal levels. This study\nprovides a foundation for future research into memristor-based AI accelerators\nfor space, highlighting their potential and the need for further investigation.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02369v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02369v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.305,
      "diffusion_reasoning_score": 0.361,
      "distributed_training_score": 0.401,
      "datasets_score": 0.312,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on using memristors for accelerating neural network inference and in-memory computing in space applications, specifically for guidance and control tasks. It discusses hardware-level acceleration, device non-idealities, and retraining on simulated memristive hardware, but does not address distributed training concepts such as partitioning data across multiple nodes, parallel computing for model training, or multi-node machine learning systems. There is no mention of strategies for accelerating training via distributed architectures, making the paper unrelated to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02372",
      "title": "Poisoned at Scale: A Scalable Audit Uncovers Hidden Scam Endpoints in\n  Production LLMs",
      "authors": [
        "Zhiyang Chen",
        "Tara Saba",
        "Xun Deng",
        "Xujie Si",
        "Fan Long"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Large Language Models (LLMs) have become critical to modern software\ndevelopment, but their reliance on internet datasets for training introduces a\nsignificant security risk: the absorption and reproduction of malicious\ncontent. To evaluate this threat, this paper introduces a scalable, automated\naudit framework that synthesizes innocuous, developer-style prompts from known\nscam databases to query production LLMs and determine if they generate code\ncontaining harmful URLs. We conducted a large-scale evaluation across four\nproduction LLMs (GPT-4o, GPT-4o-mini, Llama-4-Scout, and DeepSeek-V3), and\nfound a systemic vulnerability, with all tested models generating malicious\ncode at a non-negligible rate. On average, 4.2\\% of programs generated in our\nexperiments contained malicious URLs. Crucially, this malicious code is often\ngenerated in response to benign prompts. We manually validate the prompts which\ncause all four LLMs to generate malicious code, and resulting in 177 innocuous\nprompts that trigger all models to produce harmful outputs. These results\nprovide strong empirical evidence that the training data of production LLMs has\nbeen successfully poisoned at scale, underscoring the urgent need for more\nrobust defense mechanisms and post-generation safety checks to mitigate the\npropagation of hidden security threats.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02372v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02372v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.419,
      "weak_supervision_score": 0.433,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.374,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on auditing production LLMs for malicious code generation using automated prompts and does not involve training models with human feedback, reward models, or reinforcement learning techniques. It neither mentions nor utilizes RLHF in any capacity.",
      "weak_supervision_justification": "The paper discusses how LLMs are trained on large-scale, unverified internet data with minimal quality control, which aligns with weak supervision by relying on noisy or programmatically sourced labels. However, the main contribution is an audit framework for detecting malicious outputs, not the development or application of weak supervision methods.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper investigates the security risks in large language models (LLMs) trained on internet data, which may include malicious content, by introducing a scalable, automated audit framework that generates innocuous prompts from known scam databases to detect if production LLMs generate code containing harmful URLs. The methodology involves synthesizing these prompts, querying four LLMs (GPT-4o, GPT-4o-mini, Llama-4-Scout, and DeepSeek-V3), and analyzing the outputs, revealing that an average of 4.2% of generated code includes malicious URLs even from benign prompts, providing empirical evidence of large-scale training data poisoning and emphasizing the need for enhanced defense mechanisms.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel scalable audit framework for detecting malicious code generation in LLMs, which is a significant advancement in AI security by systematically addressing a critical vulnerability not previously audited at this scale.",
      "impact_score": "High",
      "impact_justification": "The work highlights a widespread security threat in production LLMs, potentially influencing future research, development of safer AI systems, and industry practices for data poisoning mitigation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable insights into AI security vulnerabilities and a practical framework for auditing, making it essential for researchers and practitioners in AI and cybersecurity to understand and build upon.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a01425d80c4d76f7697ec19c229f19d8ce2ef3db",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 2,
      "average_h_index": 1.2,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zhiyang Chen",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2297995102"
        },
        {
          "name": "Tara Saba",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378635599"
        },
        {
          "name": "Xun Deng",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2238407185"
        },
        {
          "name": "Xujie Si",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2303462714"
        },
        {
          "name": "Fan Long",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2357105507"
        }
      ]
    },
    {
      "id": "2509.02379",
      "title": "MedDINOv3: How to adapt vision foundation models for medical image\n  segmentation?",
      "authors": [
        "Yuheng Li",
        "Yizhou Wu",
        "Yuxiang Lai",
        "Mingzhe Hu",
        "Xiaofeng Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate segmentation of organs and tumors in CT and MRI scans is essential\nfor diagnosis, treatment planning, and disease monitoring. While deep learning\nhas advanced automated segmentation, most models remain task-specific, lacking\ngeneralizability across modalities and institutions. Vision foundation models\n(FMs) pretrained on billion-scale natural images offer powerful and\ntransferable representations. However, adapting them to medical imaging faces\ntwo key challenges: (1) the ViT backbone of most foundation models still\nunderperform specialized CNNs on medical image segmentation, and (2) the large\ndomain gap between natural and medical images limits transferability. We\nintroduce MedDINOv3, a simple and effective framework for adapting DINOv3 to\nmedical segmentation. We first revisit plain ViTs and design a simple and\neffective architecture with multi-scale token aggregation. Then, we perform\ndomain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT\nslices, using a multi-stage DINOv3 recipe to learn robust dense features.\nMedDINOv3 matches or exceeds state-of-the-art performance across four\nsegmentation benchmarks, demonstrating the potential of vision foundation\nmodels as unified backbones for medical image segmentation. The code is\navailable at https://github.com/ricklisz/MedDINOv3.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02379v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02379v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.338,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02387",
      "title": "Real-time ML-based Defense Against Malicious Payload in Reconfigurable\n  Embedded Systems",
      "authors": [
        "Rye Stahle-Smith",
        "Rasha Karakchi"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The growing use of FPGAs in reconfigurable systems introducessecurity risks\nthrough malicious bitstreams that could cause denial-of-service (DoS), data\nleakage, or covert attacks. We investigated chip-level hardware malicious\npayload in embedded systems and proposed a supervised machine learning method\nto detect malicious bitstreams via static byte-level features. Our approach\ndiverges from existing methods by analyzing bitstreams directly at the binary\nlevel, enabling real-time detection without requiring access to source code or\nnetlists. Bitstreams were sourced from state-of-the-art (SOTA) benchmarks and\nre-engineered to target the Xilinx PYNQ-Z1 FPGA Development Board. Our dataset\nincluded 122 samples of benign and malicious configurations. The data were\nvectorized using byte frequency analysis, compressed using TSVD, and balanced\nusing SMOTE to address class imbalance. The evaluated classifiers demonstrated\nthat Random Forest achieved a macro F1-score of 0.97, underscoring the\nviability of real-time Trojan detection on resource-constrained systems. The\nfinal model was serialized and successfully deployed via PYNQ to enable\nintegrated bitstream analysis.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02387v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02387v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.302,
      "distributed_training_score": 0.372,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02401",
      "title": "Towards Agents That Know When They Don't Know: Uncertainty as a Control\n  Signal for Structured Reasoning",
      "authors": [
        "Josefa Lia Stoisser",
        "Marc Boubnovski Martell",
        "Lawrence Phillips",
        "Gianluca Mazzoni",
        "Lea Mørch Harder",
        "Philip Torr",
        "Jesper Ferkinghoff-Borg",
        "Kaspar Martens",
        "Julien Fauqueur"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language model (LLM) agents are increasingly deployed in structured\nbiomedical data environments, yet they often produce fluent but overconfident\noutputs when reasoning over complex multi-table data. We introduce an\nuncertainty-aware agent for query-conditioned multi-table summarization that\nleverages two complementary signals: (i) retrieval uncertainty--entropy over\nmultiple table-selection rollouts--and (ii) summary uncertainty--combining\nself-consistency and perplexity. Summary uncertainty is incorporated into\nreinforcement learning (RL) with Group Relative Policy Optimization (GRPO),\nwhile both retrieval and summary uncertainty guide inference-time filtering and\nsupport the construction of higher-quality synthetic datasets.\n  On multi-omics benchmarks, our approach improves factuality and calibration,\nnearly tripling correct and useful claims per summary (3.0\\(\\rightarrow\\)8.4\ninternal; 3.6\\(\\rightarrow\\)9.9 cancer multi-omics) and substantially improving\ndownstream survival prediction (C-index 0.32\\(\\rightarrow\\)0.63). These results\ndemonstrate that uncertainty can serve as a control signal--enabling agents to\nabstain, communicate confidence, and become more reliable tools for complex\nstructured-data environments.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02401v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02401v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.438,
      "weak_supervision_score": 0.451,
      "diffusion_reasoning_score": 0.475,
      "distributed_training_score": 0.357,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning (specifically Group Relative Policy Optimization) with a reward signal based on model-derived uncertainty (e.g., perplexity and self-consistency), not human-ranked data or a separate reward model trained on human feedback. Thus, it does not align with RLHF.",
      "weak_supervision_justification": "The paper involves programmatically using uncertainty signals to filter and construct higher-quality synthetic datasets, which is akin to weak supervision by generating and refining labels from noisy sources. However, it does not explicitly focus on training models with weak labels, making it only moderately relevant.",
      "diffusion_reasoning_justification": "The paper does not mention or utilize diffusion models, iterative refinement processes for Chain-of-Thought reasoning, or any multi-step logical correction mechanisms; it focuses on uncertainty in reinforcement learning and summarization.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces an uncertainty-aware agent for large language models (LLMs) designed to improve query-conditioned multi-table summarization in structured biomedical environments, addressing the issue of overconfident outputs by incorporating retrieval uncertainty (based on entropy from table selections) and summary uncertainty (using self-consistency and perplexity) as control signals. The methodology integrates these uncertainties into reinforcement learning via Group Relative Policy Optimization for training and uses them for filtering outputs during inference, resulting in significant enhancements in factuality, calibration, and downstream tasks such as survival prediction on multi-omics benchmarks, demonstrating uncertainty's role in making agents more reliable and trustworthy.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly innovative framework by using uncertainty as an active control signal in both training and inference for LLM agents, representing a significant advancement in handling structured data and addressing overconfidence issues. This approach goes beyond existing methods by directly incorporating uncertainty into reinforcement learning, marking a novel contribution to the field of AI for structured reasoning.",
      "impact_score": "High",
      "impact_justification": "The work has high potential impact due to its improvements in LLM reliability for biomedical applications, which could influence a broad range of research in AI safety, uncertainty quantification, and real-world data analysis. By enhancing factuality and calibration, it may lead to more trustworthy AI tools in critical domains like healthcare, potentially affecting commercial applications and future studies.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to AI research, particularly in uncertainty-aware systems for structured data, making it essential for researchers in artificial intelligence and biomedicine to understand its implications. While not groundbreaking enough to be a \"Must Read\" for all, it offers high-quality insights that could guide future work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c0c23c58fc3925ab5a4a144ad839240a8765fbed",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 6,
      "average_h_index": 1.6666666666666667,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Josefa Lia Stoisser",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2358460000"
        },
        {
          "name": "Marc Boubnovski Martell",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2358458364"
        },
        {
          "name": "Lawrence Phillips",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359840315"
        },
        {
          "name": "G. Mazzoni",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2238424148"
        },
        {
          "name": "L. M. Harder",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2305536588"
        },
        {
          "name": "Philip Torr",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2325901137"
        },
        {
          "name": "Jesper Ferkinghoff-Borg",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2288406426"
        },
        {
          "name": "Kaspar Martens",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378708379"
        },
        {
          "name": "Julien Fauqueur",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2358458666"
        }
      ]
    },
    {
      "id": "2509.02411",
      "title": "A Survey: Towards Privacy and Security in Mobile Large Language Models",
      "authors": [
        "Honghui Xu",
        "Kaiyang Li",
        "Wei Chen",
        "Danyang Zheng",
        "Zhiyuan Li",
        "Zhipeng Cai"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Mobile Large Language Models (LLMs) are revolutionizing diverse fields such\nas healthcare, finance, and education with their ability to perform advanced\nnatural language processing tasks on-the-go. However, the deployment of these\nmodels in mobile and edge environments introduces significant challenges\nrelated to privacy and security due to their resource-intensive nature and the\nsensitivity of the data they process. This survey provides a comprehensive\noverview of privacy and security issues associated with mobile LLMs,\nsystematically categorizing existing solutions such as differential privacy,\nfederated learning, and prompt encryption. Furthermore, we analyze\nvulnerabilities unique to mobile LLMs, including adversarial attacks,\nmembership inference, and side-channel attacks, offering an in-depth comparison\nof their effectiveness and limitations. Despite recent advancements, mobile\nLLMs face unique hurdles in achieving robust security while maintaining\nefficiency in resource-constrained environments. To bridge this gap, we propose\npotential applications, discuss open challenges, and suggest future research\ndirections, paving the way for the development of trustworthy,\nprivacy-compliant, and scalable mobile LLM systems.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02411v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02411v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.398,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper is a survey on privacy and security issues in mobile Large Language Models (LLMs), covering topics such as differential privacy, federated learning, adversarial attacks, and side-channel vulnerabilities. It does not mention or involve reinforcement learning, human feedback, reward models, or any mechanisms for aligning AI models with human preferences. Therefore, there is no connection to Reinforcement Learning from Human Feedback (RLHF).",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02415",
      "title": "Decoupling Bidirectional Geometric Representations of 4D cost volume\n  with 2D convolution",
      "authors": [
        "Xiaobao Wei",
        "Changyong Shu",
        "Zhaokun Yue",
        "Chang Huang",
        "Weiwei Liu",
        "Shuai Yang",
        "Lirong Yang",
        "Peng Gao",
        "Wenbin Zhang",
        "Gaochao Zhu",
        "Chengxiang Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "High-performance real-time stereo matching methods invariably rely on 3D\nregularization of the cost volume, which is unfriendly to mobile devices. And\n2D regularization based methods struggle in ill-posed regions. In this paper,\nwe present a deployment-friendly 4D cost aggregation network DBStereo, which is\nbased on pure 2D convolutions. Specifically, we first provide a thorough\nanalysis of the decoupling characteristics of 4D cost volume. And design a\nlightweight bidirectional geometry aggregation block to capture spatial and\ndisparity representation respectively. Through decoupled learning, our approach\nachieves real-time performance and impressive accuracy simultaneously.\nExtensive experiments demonstrate that our proposed DBStereo outperforms all\nexisting aggregation-based methods in both inference time and accuracy, even\nsurpassing the iterative-based method IGEV-Stereo. Our study break the\nempirical design of using 3D convolutions for 4D cost volume and provides a\nsimple yet strong baseline of the proposed decouple aggregation paradigm for\nfurther study. Code will be available at\n(\\href{https://github.com/happydummy/DBStereo}{https://github.com/happydummy/DBStereo})\nsoon.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02415v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02415v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.245,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.371,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02419",
      "title": "From Noisy Labels to Intrinsic Structure: A Geometric-Structural\n  Dual-Guided Framework for Noise-Robust Medical Image Segmentation",
      "authors": [
        "Tao Wang",
        "Zhenxuan Zhang",
        "Yuanbo Zhou",
        "Xinlin Zhang",
        "Yuanbin Chen",
        "Tao Tan",
        "Guang Yang",
        "Tong Tong"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The effectiveness of convolutional neural networks in medical image\nsegmentation relies on large-scale, high-quality annotations, which are costly\nand time-consuming to obtain. Even expert-labeled datasets inevitably contain\nnoise arising from subjectivity and coarse delineations, which disrupt feature\nlearning and adversely impact model performance. To address these challenges,\nthis study propose a Geometric-Structural Dual-Guided Network (GSD-Net), which\nintegrates geometric and structural cues to improve robustness against noisy\nannotations. It incorporates a Geometric Distance-Aware module that dynamically\nadjusts pixel-level weights using geometric features, thereby strengthening\nsupervision in reliable regions while suppressing noise. A Structure-Guided\nLabel Refinement module further refines labels with structural priors, and a\nKnowledge Transfer module enriches supervision and improves sensitivity to\nlocal details. To comprehensively assess its effectiveness, we evaluated\nGSD-Net on six publicly available datasets: four containing three types of\nsimulated label noise, and two with multi-expert annotations that reflect\nreal-world subjectivity and labeling inconsistencies. Experimental results\ndemonstrate that GSD-Net achieves state-of-the-art performance under noisy\nannotations, achieving improvements of 2.52% on Kvasir, 22.76% on Shenzhen,\n8.87% on BU-SUC, and 4.59% on BraTS2020 under SR simulated noise. The codes of\nthis study are available at https://github.com/ortonwang/GSD-Net.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02419v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02419v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.291,
      "weak_supervision_score": 0.439,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.347,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution, GSD-Net, directly addresses training models on noisy or imprecise annotations in medical image segmentation, which aligns closely with weak supervision. It uses techniques to handle and refine noisy labels from expert sources, enabling effective learning without relying on perfectly hand-labeled data, thus fitting the definition of weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces the Geometric-Structural Dual-Guided Network (GSD-Net) to enhance medical image segmentation's robustness against noisy annotations, which are common due to subjectivity and inconsistencies in expert labeling. The framework integrates three key modules—Geometric Distance-Aware for dynamic pixel-level weighting, Structure-Guided Label Refinement for correcting labels using structural priors, and Knowledge Transfer for enriching supervision through cross-image information—resulting in state-of-the-art performance improvements on six public datasets with simulated and real-world noise, such as 2.52% on Kvasir and 22.76% on Shenzhen under specific noise conditions.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining geometric priors and structural guidance in a unified framework for noise-robust segmentation, effectively addressing limitations in existing methods but not introducing an entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of medical image segmentation due to its practical improvements on noisy datasets, though its influence may remain confined to specific applications in computer vision and AI for healthcare.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a strong, valuable contribution to handling noisy labels in segmentation, making it essential for researchers focused on medical AI to be aware of its methods and results.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0bb3017343994115a63e1a4ea76fecf8d2bb270c",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 5,
      "average_h_index": 2.625,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Tao Wang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2257008742"
        },
        {
          "name": "Zhenxuan Zhang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2283517569"
        },
        {
          "name": "Yuanbo Zhou",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2261253070"
        },
        {
          "name": "Xinlin Zhang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2261251068"
        },
        {
          "name": "Yuanbin Chen",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2232946813"
        },
        {
          "name": "Tao Tan",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2261121285"
        },
        {
          "name": "Guang Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379216955"
        },
        {
          "name": "Tong Tong",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2315312724"
        }
      ]
    },
    {
      "id": "2509.02424",
      "title": "Faster and Better: Reinforced Collaborative Distillation and\n  Self-Learning for Infrared-Visible Image Fusion",
      "authors": [
        "Yuhao Wang",
        "Lingjuan Miao",
        "Zhiqiang Zhou",
        "Yajun Qiao",
        "Lei Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Infrared and visible image fusion plays a critical role in enhancing scene\nperception by combining complementary information from different modalities.\nDespite recent advances, achieving high-quality image fusion with lightweight\nmodels remains a significant challenge. To bridge this gap, we propose a novel\ncollaborative distillation and self-learning framework for image fusion driven\nby reinforcement learning. Unlike conventional distillation, this approach not\nonly enables the student model to absorb image fusion knowledge from the\nteacher model, but more importantly, allows the student to perform\nself-learning on more challenging samples to enhance its capabilities.\nParticularly, in our framework, a reinforcement learning agent explores and\nidentifies a more suitable training strategy for the student. The agent takes\nboth the student's performance and the teacher-student gap as inputs, which\nleads to the generation of challenging samples to facilitate the student's\nself-learning. Simultaneously, it dynamically adjusts the teacher's guidance\nstrength based on the student's state to optimize the knowledge transfer.\nExperimental results demonstrate that our method can significantly improve\nstudent performance and achieve better fusion results compared to existing\ntechniques.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02424v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02424v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.366,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for collaborative distillation and self-learning in infrared-visible image fusion, aiming to improve model efficiency and fusion quality. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning on a 'Chain-of-Thought'. There is no connection to the topic of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02440",
      "title": "Efficient Pyramidal Analysis of Gigapixel Images on a Decentralized\n  Modest Computer Cluster",
      "authors": [
        "Marie Reinbigler",
        "Rishi Sharma",
        "Rafael Pires",
        "Elisabeth Brunet",
        "Anne-Marie Kermarrec",
        "Catalin Fetita"
      ],
      "categories": [
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Analyzing gigapixel images is recognized as computationally demanding. In\nthis paper, we introduce PyramidAI, a technique for analyzing gigapixel images\nwith reduced computational cost. The proposed approach adopts a gradual\nanalysis of the image, beginning with lower resolutions and progressively\nconcentrating on regions of interest for detailed examination at higher\nresolutions. We investigated two strategies for tuning the accuracy-computation\nperformance trade-off when implementing the adaptive resolution selection,\nvalidated against the Camelyon16 dataset of biomedical images. Our results\ndemonstrate that PyramidAI substantially decreases the amount of processed data\nrequired for analysis by up to 2.65x, while preserving the accuracy in\nidentifying relevant sections on a single computer. To ensure democratization\nof gigapixel image analysis, we evaluated the potential to use mainstream\ncomputers to perform the computation by exploiting the parallelism potential of\nthe approach. Using a simulator, we estimated the best data distribution and\nload balancing algorithm according to the number of workers. The selected\nalgorithms were implemented and highlighted the same conclusions in a\nreal-world setting. Analysis time is reduced from more than an hour to a few\nminutes using 12 modest workers, offering a practical solution for efficient\nlarge-scale image analysis.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02440v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02440v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.274,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.482,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on distributing computational tasks for gigapixel image analysis across multiple modest computers, using parallelism, load balancing, and simulation to optimize processing time. While it involves parallel computing concepts like data distribution and multi-node processing, which are related to distributed systems, the core contribution is about efficient inference or analysis (e.g., using pre-trained models on the Camelyon16 dataset) rather than accelerating model training itself. Thus, it shares some overlap but does not directly address distributed training of machine learning models.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces PyramidAI, a technique for efficiently analyzing gigapixel images by starting with lower resolutions and focusing detailed examination on regions of interest, thereby reducing computational demands. It evaluates two strategies for adaptive resolution selection on the Camelyon16 dataset, achieving up to a 2.65x reduction in processed data while maintaining accuracy, and demonstrates significant speed improvements through distributed computing on a modest computer cluster, making large-scale image analysis more accessible.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing multi-resolution image analysis with adaptive filtering strategies to enhance efficiency, though it builds on known preliminary detection methods rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in distributed computing and computer vision subfields by demonstrating accessible methods for gigapixel analysis, potentially leading to citations and adaptations in resource-constrained environments.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to efficient image analysis techniques, making it important for researchers in computer vision and distributed computing to be aware of its methods and findings.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/02d3238b78eab77c8a1a48642035377b127f7b36",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 54,
      "average_h_index": 11.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Marie Reinbigler",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2191250929"
        },
        {
          "name": "Rishi Sharma",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/1628266066"
        },
        {
          "name": "Rafael Pires",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2253464243"
        },
        {
          "name": "Elisabeth Brunet",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2191249251"
        },
        {
          "name": "Anne-Marie Kermarrec",
          "h_index": 54,
          "profile_url": "https://www.semanticscholar.org/author/1723331"
        },
        {
          "name": "C. Fetita",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/51205055"
        }
      ]
    },
    {
      "id": "2509.02444",
      "title": "AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile\n  Agent",
      "authors": [
        "Jingru Fan",
        "Yufan Dang",
        "Jingyao Wu",
        "Huatao Li",
        "Runde Yang",
        "Xiyuan Yang",
        "Yuheng Wang",
        "Zhong Zhang",
        "Yaxi Lu",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Dahai Li",
        "Chen Qian"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "With the raid evolution of large language models and multimodal foundation\nmodels, the mobile-agent landscape has proliferated without converging on the\nfundamental challenges. This paper identifies four core problems that must be\nsolved for mobile agents to deliver practical, scalable impact: (1)\ngeneralization across tasks, modalities, apps, and devices; (2) accuracy,\nspecifically precise on-screen interaction and click targeting; (3)\nlong-horizon capability for sustained, multi-step goals; and (4) efficiency,\nspecifically high-performance runtime on resource-constrained devices. We\npresent AppCopilot, a multimodal, multi-agent, general-purpose on-device\nassistant that operates across applications and constitutes a full-stack,\nclosed-loop system from data to deployment. AppCopilot operationalizes this\nposition through an end-to-end autonomous pipeline spanning data collection,\ntraining, deployment, high-quality and efficient inference, and mobile\napplication development. At the model layer, it integrates multimodal\nfoundation models with robust Chinese-English support. At the reasoning and\ncontrol layer, it combines chain-of-thought reasoning, hierarchical task\nplanning and decomposition, and multi-agent collaboration. At the execution\nlayer, it enables user personalization and experiential adaptation, voice\ninteraction, function calling, cross-app and cross-device orchestration, and\ncomprehensive mobile app support. The system design incorporates\nprofiling-driven optimization for latency, memory, and energy across\nheterogeneous hardware. Empirically, AppCopilot achieves significant\nimprovements along all four dimensions: stronger generalization,\nhigher-precision on-screen actions, more reliable long-horizon task completion,\nand faster, more resource-efficient runtime.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02444v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02444v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.408,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on AppCopilot's design for mobile agents, including user personalization and experiential adaptation, but does not mention using human feedback to train a reward model or fine-tune via reinforcement learning. There is no indication of RLHF processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes chain-of-thought reasoning and hierarchical task planning in AppCopilot, but it does not involve diffusion models or iterative refinement processes for logical tasks. No components align with diffusion-based approaches.",
      "distributed_training_justification": "The paper emphasizes efficiency and optimization for on-device inference on heterogeneous hardware, but it does not discuss distributed training, parallel computing for model training, or strategies for multi-node machine learning. The focus is on deployment, not training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02445",
      "title": "Towards High-Fidelity, Identity-Preserving Real-Time Makeup Transfer:\n  Decoupling Style Generation",
      "authors": [
        "Lydia Kin Ching Chau",
        "Zhi Yu",
        "Ruowei Jiang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present a novel framework for real-time virtual makeup try-on that\nachieves high-fidelity, identity-preserving cosmetic transfer with robust\ntemporal consistency. In live makeup transfer applications, it is critical to\nsynthesize temporally coherent results that accurately replicate fine-grained\nmakeup and preserve user's identity. However, existing methods often struggle\nto disentangle semitransparent cosmetics from skin tones and other identify\nfeatures, causing identity shifts and raising fairness concerns. Furthermore,\ncurrent methods lack real-time capabilities and fail to maintain temporal\nconsistency, limiting practical adoption. To address these challenges, we\ndecouple makeup transfer into two steps: transparent makeup mask extraction and\ngraphics-based mask rendering. After the makeup extraction step, the makeup\nrendering can be performed in real time, enabling live makeup try-on. Our\nmakeup extraction model trained on pseudo-ground-truth data generated via two\ncomplementary methods: a graphics-based rendering pipeline and an unsupervised\nk-means clustering approach. To further enhance transparency estimation and\ncolor fidelity, we propose specialized training objectives, including\nalpha-weighted reconstruction and lip color losses. Our method achieves robust\nmakeup transfer across diverse poses, expressions, and skin tones while\npreserving temporal smoothness. Extensive experiments demonstrate that our\napproach outperforms existing baselines in capturing fine details, maintaining\ntemporal stability, and preserving identity integrity.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02445v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02445v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.333,
      "distributed_training_score": 0.329,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02451",
      "title": "RiverScope: High-Resolution River Masking Dataset",
      "authors": [
        "Rangel Daroya",
        "Taylor Rowley",
        "Jonathan Flores",
        "Elisa Friedmann",
        "Fiona Bennitt",
        "Heejin An",
        "Travis Simmons",
        "Marissa Jean Hughes",
        "Camryn L Kluetmeier",
        "Solomon Kica",
        "J. Daniel Vélez",
        "Sarah E. Esenther",
        "Thomas E. Howard",
        "Yanqi Ye",
        "Audrey Turcotte",
        "Colin Gleason",
        "Subhransu Maji"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Surface water dynamics play a critical role in Earth's climate system,\ninfluencing ecosystems, agriculture, disaster resilience, and sustainable\ndevelopment. Yet monitoring rivers and surface water at fine spatial and\ntemporal scales remains challenging -- especially for narrow or sediment-rich\nrivers that are poorly captured by low-resolution satellite data. To address\nthis, we introduce RiverScope, a high-resolution dataset developed through\ncollaboration between computer science and hydrology experts. RiverScope\ncomprises 1,145 high-resolution images (covering 2,577 square kilometers) with\nexpert-labeled river and surface water masks, requiring over 100 hours of\nmanual annotation. Each image is co-registered with Sentinel-2, SWOT, and the\nSWOT River Database (SWORD), enabling the evaluation of cost-accuracy\ntrade-offs across sensors -- a key consideration for operational water\nmonitoring. We also establish the first global, high-resolution benchmark for\nriver width estimation, achieving a median error of 7.2 meters -- significantly\noutperforming existing satellite-derived methods. We extensively evaluate deep\nnetworks across multiple architectures (e.g., CNNs and transformers),\npretraining strategies (e.g., supervised and self-supervised), and training\ndatasets (e.g., ImageNet and satellite imagery). Our best-performing models\ncombine the benefits of transfer learning with the use of all the multispectral\nPlanetScope channels via learned adaptors. RiverScope provides a valuable\nresource for fine-scale and multi-sensor hydrological modeling, supporting\nclimate adaptation and sustainable water management.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02451v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02451v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.298,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.279,
      "distributed_training_score": 0.345,
      "datasets_score": 0.412,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of RiverScope, a new high-resolution dataset for river masking, which includes dataset creation through expert manual annotation, curation methodologies involving co-registration with multiple sensors, and benchmark evaluations for tasks like water segmentation and river width estimation. This directly aligns with the topic, as it focuses on creating, analyzing, and benchmarking datasets for machine learning applications in AI, specifically for hydrological modeling.",
      "llm_score_status": "completed",
      "summary": "RiverScope introduces a high-resolution dataset of 1,145 images covering 2,577 square kilometers, featuring expert-labeled masks for rivers and surface waters, aimed at improving fine-scale monitoring of water dynamics. The methodology involves manual annotation, co-registration with sensors like Sentinel-2 and SWOT, and evaluation of deep learning models (including CNNs and transformers) to benchmark water segmentation and river width estimation, achieving a median error of 7.2 meters and highlighting the advantages of using multispectral channels for enhanced accuracy in hydrological modeling and climate adaptation.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new high-resolution dataset and benchmark for river width estimation, significantly advancing the state-of-the-art in hydrological monitoring by addressing limitations in existing low-resolution satellite data. This represents a novel contribution that enables more precise analysis of narrow rivers and supports interdisciplinary applications.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research and applications in hydrology, machine learning, and environmental monitoring by providing a valuable dataset for improving water management and climate resilience strategies. Its public release and benchmarks could lead to widespread adoption in both academic and operational contexts, such as at agencies like NASA or ESA.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution through its innovative dataset and benchmarks, making it essential for researchers in computer vision and hydrology focused on environmental applications. While highly relevant to specific fields, it may not be critical for a broader audience.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f277384a9b60a99ab27f8f010a80c6fc298c5910",
      "total_authors": 17,
      "authors_found": 16,
      "highest_h_index": 6,
      "average_h_index": 1.1875,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Rangel Daroya",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/35558464"
        },
        {
          "name": "Taylor Rowley",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378704956"
        },
        {
          "name": "Jonathan Flores",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379295565"
        },
        {
          "name": "E. Friedmann",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2238120144"
        },
        {
          "name": "Fiona Bennitt",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378706787"
        },
        {
          "name": "Heejin An",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379281427"
        },
        {
          "name": "Travis Simmons",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2245526907"
        },
        {
          "name": "M. Hughes",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359035834"
        },
        {
          "name": "Camryn L Kluetmeier",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2037982842"
        },
        {
          "name": "Solomon Kica",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378704858"
        },
        {
          "name": "J. D. V'elez",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378703333"
        },
        {
          "name": "Sarah E. Esenther",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/115559069"
        },
        {
          "name": "Thomas E. Howard",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378679123"
        },
        {
          "name": "Yanqi Ye",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Audrey Turcotte",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378704544"
        },
        {
          "name": "Colin J. Gleason",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2334738962"
        },
        {
          "name": "Subhransu Maji",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2256725214"
        }
      ]
    },
    {
      "id": "2509.02452",
      "title": "Do LLMs Adhere to Label Definitions? Examining Their Receptivity to\n  External Label Definitions",
      "authors": [
        "Seyedali Mohammadi",
        "Bhaskara Hanuma Vedula",
        "Hemank Lamba",
        "Edward Raff",
        "Ponnurangam Kumaraguru",
        "Francis Ferraro",
        "Manas Gaur"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Do LLMs genuinely incorporate external definitions, or do they primarily rely\non their parametric knowledge? To address these questions, we conduct\ncontrolled experiments across multiple explanation benchmark datasets (general\nand domain-specific) and label definition conditions, including expert-curated,\nLLM-generated, perturbed, and swapped definitions. Our results reveal that\nwhile explicit label definitions can enhance accuracy and explainability, their\nintegration into an LLM's task-solving processes is neither guaranteed nor\nconsistent, suggesting reliance on internalized representations in many cases.\nModels often default to their internal representations, particularly in general\ntasks, whereas domain-specific tasks benefit more from explicit definitions.\nThese findings underscore the need for a deeper understanding of how LLMs\nprocess external knowledge alongside their pre-existing capabilities.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02452v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02452v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.438,
      "weak_supervision_score": 0.431,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.311,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating how LLMs process external label definitions in classification tasks, without any discussion of training models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper involves experiments with perturbed or swapped label definitions, which could resemble noisy labels in weak supervision, but it primarily evaluates pre-trained LLMs' responses rather than training models with programmatically generated labels.",
      "diffusion_reasoning_justification": "The paper examines LLMs' integration of external definitions and reasoning in classification tasks but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02458",
      "title": "Generative Sequential Notification Optimization via Multi-Objective\n  Decision Transformers",
      "authors": [
        "Borja Ocejo",
        "Ruofan Wang",
        "Ke Liu",
        "Rohit K. Patra",
        "Haotian Shen",
        "David Liu",
        "Yiwen Yuan",
        "Gokulraj Mohanasundaram",
        "Fedor Borisyuk",
        "Prakruthi Prabhakar"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Notifications are an important communication channel for delivering timely\nand relevant information. Optimizing their delivery involves addressing complex\nsequential decision-making challenges under constraints such as message utility\nand user fatigue. Offline reinforcement learning (RL) methods, such as\nConservative Q-Learning (CQL), have been applied to this problem but face\npractical challenges at scale, including instability, sensitivity to\ndistribution shifts, limited reproducibility, and difficulties with\nexplainability in high-dimensional recommendation settings. We present a\nDecision Transformer (DT) based framework that reframes policy learning as\nreturn-conditioned supervised learning, improving robustness, scalability, and\nmodeling flexibility. Our contributions include a real-world comparison with\nCQL, a multi-reward design suitable for non-episodic tasks, a quantile\nregression approach to return-to-go conditioning, and a production-ready system\nwith circular buffer-based sequence processing for near-real-time inference.\nExtensive offline and online experiments in a deployed notification system show\nthat our approach improves notification utility and overall session activity\nwhile minimizing user fatigue. Compared to a multi-objective CQL-based agent,\nthe DT-based approach achieved a +0.72% increase in sessions for notification\ndecision-making at LinkedIn by making notification recommendation more\nrelevant.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02458v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02458v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.466,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.428,
      "distributed_training_score": 0.4,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on offline reinforcement learning using Decision Transformers for notification optimization, trained on existing data like user interactions, but does not involve human-ranked data, reward modeling based on human preferences, or fine-tuning with human feedback. Thus, it does not align with RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses Decision Transformers for sequential decision-making in notifications, which is a sequence modeling approach for RL, but it does not involve diffusion models, iterative refinement for logical reasoning, or multi-step chain-of-thought processes. There is no component related to diffusion-based methods.",
      "distributed_training_justification": "The paper mentions a circular database buffer-based infrastructure for production-scale transformer inference, which implies handling large-scale data processing potentially across distributed systems, but it primarily focuses on inference rather than distributed training, parallel computing for model training, or strategies for accelerating training across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02460",
      "title": "GenCompositor: Generative Video Compositing with Diffusion Transformer",
      "authors": [
        "Shuzhou Yang",
        "Xiaoyu Li",
        "Xiaodong Cun",
        "Guangzhi Wang",
        "Lingen Li",
        "Ying Shan",
        "Jian Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Video compositing combines live-action footage to create video production,\nserving as a crucial technique in video creation and film production.\nTraditional pipelines require intensive labor efforts and expert collaboration,\nresulting in lengthy production cycles and high manpower costs. To address this\nissue, we automate this process with generative models, called generative video\ncompositing. This new task strives to adaptively inject identity and motion\ninformation of foreground video to the target video in an interactive manner,\nallowing users to customize the size, motion trajectory, and other attributes\nof the dynamic elements added in final video. Specifically, we designed a novel\nDiffusion Transformer (DiT) pipeline based on its intrinsic properties. To\nmaintain consistency of the target video before and after editing, we revised a\nlight-weight DiT-based background preservation branch with masked token\ninjection. As to inherit dynamic elements from other sources, a DiT fusion\nblock is proposed using full self-attention, along with a simple yet effective\nforeground augmentation for training. Besides, for fusing background and\nforeground videos with different layouts based on user control, we developed a\nnovel position embedding, named Extended Rotary Position Embedding (ERoPE).\nFinally, we curated a dataset comprising 61K sets of videos for our new task,\ncalled VideoComp. This data includes complete dynamic elements and high-quality\ntarget videos. Experiments demonstrate that our method effectively realizes\ngenerative video compositing, outperforming existing possible solutions in\nfidelity and consistency.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02460v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02460v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.524,
      "distributed_training_score": 0.344,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a Diffusion Transformer for generative video compositing, which involves iterative refinement processes typical of diffusion models. However, it focuses on visual content creation and editing (e.g., injecting foreground elements into background videos) rather than adapting diffusion for complex logical tasks or multi-step reasoning like a Chain-of-Thought. Thus, while diffusion is a shared element, the paper does not address logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02466",
      "title": "TeRA: Rethinking Text-guided Realistic 3D Avatar Generation",
      "authors": [
        "Yanwen Wang",
        "Yiyu Zhuang",
        "Jiawei Zhang",
        "Li Wang",
        "Yifei Zeng",
        "Xun Cao",
        "Xinxin Zuo",
        "Hao Zhu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we rethink text-to-avatar generative models by proposing TeRA,\na more efficient and effective framework than the previous SDS-based models and\ngeneral large 3D generative models. Our approach employs a two-stage training\nstrategy for learning a native 3D avatar generative model. Initially, we\ndistill a decoder to derive a structured latent space from a large human\nreconstruction model. Subsequently, a text-controlled latent diffusion model is\ntrained to generate photorealistic 3D human avatars within this latent space.\nTeRA enhances the model performance by eliminating slow iterative optimization\nand enables text-based partial customization through a structured 3D human\nrepresentation. Experiments have proven our approach's superiority over\nprevious text-to-avatar generative models in subjective and objective\nevaluation.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02466v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02466v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.368,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a text-guided generative model for 3D human avatars using latent diffusion, focusing on efficiency and realism in generation. It does not involve adapting diffusion models for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks, as it is centered on visual generation rather than reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02474",
      "title": "Unifi3D: A Study on 3D Representations for Generation and Reconstruction\n  in a Common Framework",
      "authors": [
        "Nina Wiedemann",
        "Sainan Liu",
        "Quentin Leboutet",
        "Katelyn Gao",
        "Benjamin Ummenhofer",
        "Michael Paulitsch",
        "Kai Yuan"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Following rapid advancements in text and image generation, research has\nincreasingly shifted towards 3D generation. Unlike the well-established\npixel-based representation in images, 3D representations remain diverse and\nfragmented, encompassing a wide variety of approaches such as voxel grids,\nneural radiance fields, signed distance functions, point clouds, or octrees,\neach offering distinct advantages and limitations. In this work, we present a\nunified evaluation framework designed to assess the performance of 3D\nrepresentations in reconstruction and generation. We compare these\nrepresentations based on multiple criteria: quality, computational efficiency,\nand generalization performance. Beyond standard model benchmarking, our\nexperiments aim to derive best practices over all steps involved in the 3D\ngeneration pipeline, including preprocessing, mesh reconstruction, compression\nwith autoencoders, and generation. Our findings highlight that reconstruction\nerrors significantly impact overall performance, underscoring the need to\nevaluate generation and reconstruction jointly. We provide insights that can\ninform the selection of suitable 3D models for various applications,\nfacilitating the development of more robust and application-specific solutions\nin 3D generation. The code for our framework is available at\nhttps://github.com/isl-org/unifi3d.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02474v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02474v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.443,
      "distributed_training_score": 0.36,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes diffusion models in a 3D generation pipeline for tasks like denoising latents to produce 3D representations, which involves iterative refinement. However, it does not adapt diffusion for complex logical reasoning, Chain-of-Thought processes, or solving multi-step logical tasks, focusing instead on generative applications in 3D reconstruction and generation.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02480",
      "title": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall",
      "authors": [
        "Avinash Maurya",
        "M. Mustafa Rafique",
        "Franck Cappello",
        "Bogdan Nicolae"
      ],
      "categories": [
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02480v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02480v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.319,
      "distributed_training_score": 0.552,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution, MLP-Offload, directly addresses challenges in distributed training by optimizing offloading strategies for LLM pre-training across multiple GPUs and nodes. It tackles I/O bottlenecks and memory constraints in parallel computing setups, as seen in frameworks like DeepSpeed and Megatron, by partitioning and managing model parameters, optimizer states, and computations across tiers, thereby accelerating training iterations in resource-constrained, multi-node environments.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces MLP-Offload, a novel multi-level, multi-path offloading engine designed to optimize large language model (LLM) pre-training by addressing GPU memory limitations and reducing I/O bottlenecks through efficient use of host memory, NVMe, and underutilized external storage, along with concurrency control and cache-friendly strategies. The methodology involves offloading optimizer states in a way that minimizes contention and maximizes bandwidth utilization, and evaluations on models up to 280 billion parameters demonstrate a 2.5x speedup in iterations compared to state-of-the-art frameworks like DeepSpeed.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing offloading techniques with new elements like multi-path I/O and concurrency control to address I/O bottlenecks in LLM training, offering a notable improvement rather than a completely new paradigm. While it advances the state-of-the-art, it builds on prior work such as DeepSpeed and Megatron.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future developments in distributed LLM training by providing efficient memory management strategies, potentially leading to broader adoption in resource-constrained environments. However, its impact may be confined to specific subfields like AI and distributed computing rather than having widespread commercial effects immediately.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a valuable contribution to optimizing LLM training, with practical insights and demonstrated speedups that are relevant for researchers in machine learning and distributed systems. It is essential for those working on scalable AI models but not groundbreaking enough to be a must-read for all.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/87ea8b6ec4ba31db4fc3db794cba789bbe5f298e",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 6,
      "average_h_index": 5.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Avinash Maurya",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2448592"
        },
        {
          "name": "M. Rafique",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2248443582"
        },
        {
          "name": "Franck Cappello",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2239205420"
        },
        {
          "name": "Bogdan Nicolae",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2247675273"
        }
      ]
    },
    {
      "id": "2509.02488",
      "title": "Anisotropic Fourier Features for Positional Encoding in Medical Imaging",
      "authors": [
        "Nabil Jabareen",
        "Dongsheng Yuan",
        "Dingming Liu",
        "Foo-Wei Ten",
        "Sören Lukassen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The adoption of Transformer-based architectures in the medical domain is\ngrowing rapidly. In medical imaging, the analysis of complex shapes - such as\norgans, tissues, or other anatomical structures - combined with the often\nanisotropic nature of high-dimensional images complicates these adaptations. In\nthis study, we critically examine the role of Positional Encodings (PEs),\narguing that commonly used approaches may be suboptimal for the specific\nchallenges of medical imaging. Sinusoidal Positional Encodings (SPEs) have\nproven effective in vision tasks, but they struggle to preserve Euclidean\ndistances in higher-dimensional spaces. Isotropic Fourier Feature Positional\nEncodings (IFPEs) have been proposed to better preserve Euclidean distances,\nbut they lack the ability to account for anisotropy in images. To address these\nlimitations, we propose Anisotropic Fourier Feature Positional Encoding (AFPE),\na generalization of IFPE that incorporates anisotropic, class-specific, and\ndomain-specific spatial dependencies. We systematically benchmark AFPE against\ncommonly used PEs on multi-label classification in chest X-rays, organ\nclassification in CT images, and ejection fraction regression in\nechocardiography. Our results demonstrate that choosing the correct PE can\nsignificantly improve model performance. We show that the optimal PE depends on\nthe shape of the structure of interest and the anisotropy of the data. Finally,\nour proposed AFPE significantly outperforms state-of-the-art PEs in all tested\nanisotropic settings. We conclude that, in anisotropic medical images and\nvideos, it is of paramount importance to choose an anisotropic PE that fits the\ndata and the shape of interest.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02488v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02488v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.26,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.312,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02494",
      "title": "GridMind: LLMs-Powered Agents for Power System Analysis and Operations",
      "authors": [
        "Hongwei Jin",
        "Kibaek Kim",
        "Jonghwan Kwon"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The complexity of traditional power system analysis workflows presents\nsignificant barriers to efficient decision-making in modern electric grids.\nThis paper presents GridMind, a multi-agent AI system that integrates Large\nLanguage Models (LLMs) with deterministic engineering solvers to enable\nconversational scientific computing for power system analysis. The system\nemploys specialized agents coordinating AC Optimal Power Flow and N-1\ncontingency analysis through natural language interfaces while maintaining\nnumerical precision via function calls. GridMind addresses workflow\nintegration, knowledge accessibility, context preservation, and expert\ndecision-support augmentation. Experimental evaluation on IEEE test cases\ndemonstrates that the proposed agentic framework consistently delivers correct\nsolutions across all tested language models, with smaller LLMs achieving\ncomparable analytical accuracy with reduced computational latency. This work\nestablishes agentic AI as a viable paradigm for scientific computing,\ndemonstrating how conversational interfaces can enhance accessibility while\npreserving numerical rigor essential for critical engineering applications.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02494v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02494v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.386,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using pre-existing Large Language Models (LLMs) in a multi-agent system for power system analysis, emphasizing integration with engineering solvers and conversational interfaces. It does not describe any process involving human feedback for training a reward model or fine-tuning via reinforcement learning, which is central to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses agentic AI systems with LLMs for orchestrating workflows and reasoning in power system analysis, but it does not incorporate diffusion models or an iterative refinement process for multi-step logical reasoning as defined. There is no evidence of treating Chain-of-Thought as a holistically corrected entity via diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02495",
      "title": "Probabilistically stable revision and comparative probability: a\n  representation theorem and applications",
      "authors": [
        "Krzysztof Mierzewski"
      ],
      "categories": [
        "cs.LO (Logic in Computer Science)",
        "cs.AI (Artificial Intelligence)",
        "econ.TH (Theoretical Economics)",
        "math.PR (Probability)"
      ],
      "abstract": "The stability rule for belief, advocated by Leitgeb [Annals of Pure and\nApplied Logic 164, 2013], is a rule for rational acceptance that captures\ncategorical belief in terms of $\\textit{probabilistically stable\npropositions}$: propositions to which the agent assigns resiliently high\ncredence. The stability rule generates a class of $\\textit{probabilistically\nstable belief revision}$ operators, which capture the dynamics of belief that\nresult from an agent updating their credences through Bayesian conditioning\nwhile complying with the stability rule for their all-or-nothing beliefs. In\nthis paper, we prove a representation theorem that yields a complete\ncharacterisation of such probabilistically stable revision operators and\nprovides a `qualitative' selection function semantics for the (non-monotonic)\nlogic of probabilistically stable belief revision. Drawing on the theory of\ncomparative probability orders, this result gives necessary and sufficient\nconditions for a selection function to be representable as a\nstrongest-stable-set operator on a finite probability space. The resulting\nlogic of probabilistically stable belief revision exhibits strong monotonicity\nproperties while failing the AGM belief revision postulates and satisfying only\nvery weak forms of case reasoning. In showing the main theorem, we prove two\nresults of independent interest to the theory of comparative probability: the\nfirst provides necessary and sufficient conditions for the joint representation\nof a pair of (respectively, strict and non-strict) comparative probability\norders. The second result provides a method for axiomatising the logic of ratio\ncomparisons of the form ``event $A$ is at least $k$ times more likely than\nevent $B$''. In addition to these measurement-theoretic applications, we point\nout two applications of our main result to the theory of simple voting games\nand to revealed preference theory.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02495v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02495v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.276,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.192,
      "datasets_score": 0.202,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02499",
      "title": "MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of\n  Stylistics Experts with Conditional Thresholds",
      "authors": [
        "Junxi Wu",
        "Jinpeng Wang",
        "Zheng Liu",
        "Bin Chen",
        "Dongjian Hu",
        "Hao Wu",
        "Shu-Tao Xia"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The rapid advancement of large language models has intensified public\nconcerns about the potential misuse. Therefore, it is important to build\ntrustworthy AI-generated text detection systems. Existing methods neglect\nstylistic modeling and mostly rely on static thresholds, which greatly limits\nthe detection performance. In this paper, we propose the Mixture of Stylistic\nExperts (MoSEs) framework that enables stylistics-aware uncertainty\nquantification through conditional threshold estimation. MoSEs contain three\ncore components, namely, the Stylistics Reference Repository (SRR), the\nStylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE).\nFor input text, SRR can activate the appropriate reference data in SRR and\nprovide them to CTE. Subsequently, CTE jointly models the linguistic\nstatistical properties and semantic features to dynamically determine the\noptimal threshold. With a discrimination score, MoSEs yields prediction labels\nwith the corresponding confidence level. Our framework achieves an average\nimprovement 11.34% in detection performance compared to baselines. More\ninspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource\ncase. Our code is available at https://github.com/creator-xi/MoSEs.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02499v3",
      "pdf_url": "http://arxiv.org/pdf/2509.02499v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.412,
      "weak_supervision_score": 0.433,
      "diffusion_reasoning_score": 0.459,
      "distributed_training_score": 0.345,
      "datasets_score": 0.403,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on AI-generated text detection using stylistic modeling and dynamic thresholds, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning via RLHF.",
      "weak_supervision_justification": "The paper describes constructing a labeled Stylistics Reference Repository for text detection, but it does not involve training models with programmatically generated, noisy, or imprecise labels, which is the core of weak supervision.",
      "diffusion_reasoning_justification": "The paper proposes a framework for text detection without any components involving diffusion models, iterative refinement for logical reasoning, or multi-step chain-of-thought processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces and analyzes the Stylistics Reference Repository as a labeled data pool for stylistic features, which aligns with dataset creation and curation, though the primary focus is on the detection framework rather than comprehensive dataset benchmarking or evaluation.",
      "llm_score_status": "completed",
      "summary": "The paper introduces the MoSEs framework to enhance AI-generated text detection by incorporating stylistic modeling and uncertainty quantification, addressing limitations in existing methods that overlook stylistic variations and use static thresholds. It comprises three key components: the Stylistics Reference Repository (SRR) for storing reference data, the Stylistics-Aware Router (SAR) for selecting relevant samples based on semantic features, and the Conditional Threshold Estimator (CTE) for dynamically determining optimal thresholds using linguistic and semantic properties, resulting in an average 11.34% improvement in detection performance over baselines and a 39.15% improvement in low-resource scenarios.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework that integrates profession-specific stylistic modeling and uncertainty-aware conditional thresholds, significantly advancing the state-of-the-art in AI-generated text detection by addressing overlooked aspects of existing methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of AI-generated text detection due to its substantial performance improvements, especially in low-resource settings, though its influence may remain confined to specific applications in computational linguistics and AI.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to AI text detection with innovative elements and demonstrated improvements, making it essential for researchers focused on trustworthy AI systems.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b552b050de3c6defeab551b6d099b4bf71945cd0",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 9,
      "average_h_index": 1.5714285714285714,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Junxi Wu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2344834707"
        },
        {
          "name": "Jinpeng Wang",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2110174485"
        },
        {
          "name": "Zheng Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378726778"
        },
        {
          "name": "Bin Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379305076"
        },
        {
          "name": "Dongjian Hu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2343697952"
        },
        {
          "name": "Hao Wu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379662144"
        },
        {
          "name": "Shu-Tao Xiu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378706644"
        }
      ]
    },
    {
      "id": "2509.02510",
      "title": "Top-H Decoding: Adapting the Creativity and Coherence with Bounded\n  Entropy in Text Generation",
      "authors": [
        "Erfan Baghaei Potraghloo",
        "Seyedarmin Azizi",
        "Souvik Kundu",
        "Massoud Pedram"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Large language models (LLMs), despite their impressive performance across a\nwide range of tasks, often struggle to balance two competing objectives in\nopen-ended text generation: fostering diversity and creativity while preserving\nlogical coherence. Existing truncated sampling techniques, including\ntemperature scaling, top-\\$p\\$ (nucleus) sampling, and min-\\$p\\$ sampling, aim\nto manage this trade-off. However, they exhibit limitations, particularly in\nthe effective incorporation of the confidence of the model into the\ncorresponding sampling strategy. For example, min-\\$p\\$ sampling relies on a\nsingle top token as a heuristic for confidence, eventually underutilizing the\ninformation of the probability distribution. Toward effective incorporation of\nthe confidence of the model, in this paper, we present **top-H** decoding. We\nfirst establish the theoretical foundation of the interplay between creativity\nand coherence in truncated sampling by formulating an **entropy-constrained\nminimum divergence** problem. We then prove this minimization problem to be\nequivalent to an **entropy-constrained mass maximization** (ECMM) problem,\nwhich is NP-hard. Finally, we present top-H decoding, a computationally\nefficient greedy algorithm to solve the ECMM problem. Extensive empirical\nevaluations demonstrate that top-H outperforms the state-of-the-art (SoTA)\nalternative of min-\\$p\\$ sampling by up to **25.63%** on creative writing\nbenchmarks, while maintaining robustness on question-answering datasets such as\nGPQA, GSM8K, and MT-Bench. Additionally, an *LLM-as-judge* evaluation confirms\nthat top-H indeed produces coherent outputs even at higher temperatures, where\ncreativity is especially critical. In summary, top-H advances SoTA in\nopen-ended text generation and can be *easily integrated* into creative writing\napplications. The code is available at\nhttps://github.com/ErfanBaghaei/Top-H-Decoding.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02510v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02510v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.439,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.464,
      "distributed_training_score": 0.358,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a new decoding strategy, top-H, for balancing creativity and coherence in text generation, without any involvement of reinforcement learning or human feedback for model training. It does not describe training a reward model or fine-tuning with human-ranked data, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a method for text generation based on entropy-constrained optimization and sampling techniques, with no reference to diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02511",
      "title": "Enhancing Fitness Movement Recognition with Attention Mechanism and\n  Pre-Trained Feature Extractors",
      "authors": [
        "Shanjid Hasan Nishat",
        "Srabonti Deb",
        "Mohiuddin Ahmed"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Fitness movement recognition, a focused subdomain of human activity\nrecognition (HAR), plays a vital role in health monitoring, rehabilitation, and\npersonalized fitness training by enabling automated exercise classification\nfrom video data. However, many existing deep learning approaches rely on\ncomputationally intensive 3D models, limiting their feasibility in real-time or\nresource-constrained settings. In this paper, we present a lightweight and\neffective framework that integrates pre-trained 2D Convolutional Neural\nNetworks (CNNs) such as ResNet50, EfficientNet, and Vision Transformers (ViT)\nwith a Long Short-Term Memory (LSTM) network enhanced by spatial attention.\nThese models efficiently extract spatial features while the LSTM captures\ntemporal dependencies, and the attention mechanism emphasizes informative\nsegments. We evaluate the framework on a curated subset of the UCF101 dataset,\nachieving a peak accuracy of 93.34\\% with the ResNet50-based configuration.\nComparative results demonstrate the superiority of our approach over several\nstate-of-the-art HAR systems. The proposed method offers a scalable and\nreal-time-capable solution for fitness activity recognition with broader\napplications in vision-based health and activity monitoring.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02511v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02511v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.308,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.351,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02515",
      "title": "Contemporary Agent Technology: LLM-Driven Advancements vs Classic\n  Multi-Agent Systems",
      "authors": [
        "Costin Bădică",
        "Amelia Bădică",
        "Maria Ganzha",
        "Mirjana Ivanović",
        "Marcin Paprzycki",
        "Dan Selişteanu",
        "Zofia Wrona"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This contribution provides our comprehensive reflection on the contemporary\nagent technology, with a particular focus on the advancements driven by Large\nLanguage Models (LLM) vs classic Multi-Agent Systems (MAS). It delves into the\nmodels, approaches, and characteristics that define these new systems. The\npaper emphasizes the critical analysis of how the recent developments relate to\nthe foundational MAS, as articulated in the core academic literature. Finally,\nit identifies key challenges and promising future directions in this rapidly\nevolving domain.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02515v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02515v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.384,
      "distributed_training_score": 0.331,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02521",
      "title": "FLM-Audio: Natural Monologues Improves Native Full-Duplex Chatbots via\n  Dual Training",
      "authors": [
        "Yiqun Yao",
        "Xiang Li",
        "Xin Jiang",
        "Xuezhi Fang",
        "Naitong Yu",
        "Wenjia Ma",
        "Aixin Sun",
        "Yequan Wang"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Full-duplex dialog models aim to listen and speak simultaneously, delivering\nrapid responses to dynamic user input. Among different solutions to full\nduplexity, a native solution merges multiple channels in each time step,\nachieving the lowest latency. However, prevailing designs break down the\ntextual monologue sentences for word-level alignment with audio streams, which\ndegrades language modeling abilities. To help address this issue, we introduce\nnatural monologues, which are composed by continuous sentences and waiting\nintervals, mimicking humanoid cognitive behavior in dialogs. We find a proper\ntraining paradigm to be critical for semantically aligning natural monologues\nwith audio. To this end, we develop a dual training paradigm that alternates\nthe position of the monologues, either leading or trailing the audio, across\ndifferent training stages. A combination of our natural monologue and dual\ntraining strategy is applied in developing FLM-Audio, our 7B spoken dialog\nchatbot with native full-duplexity. As confirmed by experimental results,\nFLM-Audio achieves superior response qualities and chatting experiences while\nrequiring significantly less training data.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02521v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02521v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.428,
      "distributed_training_score": 0.407,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on developing a full-duplex audio chatbot using natural monologues and a dual training paradigm for text-audio alignment, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for logical reasoning; it primarily addresses audio-text alignment and full-duplex mechanisms in chatbots.",
      "distributed_training_justification": "The paper discusses efficient training for full-duplex models but does not cover distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02530",
      "title": "Manipulation as in Simulation: Enabling Accurate Geometry Perception in\n  Robots",
      "authors": [
        "Minghuan Liu",
        "Zhengbang Zhu",
        "Xiaoshen Han",
        "Peng Hu",
        "Haotong Lin",
        "Xinyao Li",
        "Jingxiao Chen",
        "Jiafeng Xu",
        "Yichu Yang",
        "Yunfeng Lin",
        "Xinghang Li",
        "Yong Yu",
        "Weinan Zhang",
        "Tao Kong",
        "Bingyi Kang"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Modern robotic manipulation primarily relies on visual observations in a 2D\ncolor space for skill learning but suffers from poor generalization. In\ncontrast, humans, living in a 3D world, depend more on physical properties-such\nas distance, size, and shape-than on texture when interacting with objects.\nSince such 3D geometric information can be acquired from widely available depth\ncameras, it appears feasible to endow robots with similar perceptual\ncapabilities. Our pilot study found that using depth cameras for manipulation\nis challenging, primarily due to their limited accuracy and susceptibility to\nvarious types of noise. In this work, we propose Camera Depth Models (CDMs) as\na simple plugin on daily-use depth cameras, which take RGB images and raw depth\nsignals as input and output denoised, accurate metric depth. To achieve this,\nwe develop a neural data engine that generates high-quality paired data from\nsimulation by modeling a depth camera's noise pattern. Our results show that\nCDMs achieve nearly simulation-level accuracy in depth prediction, effectively\nbridging the sim-to-real gap for manipulation tasks. Notably, our experiments\ndemonstrate, for the first time, that a policy trained on raw simulated depth,\nwithout the need for adding noise or real-world fine-tuning, generalizes\nseamlessly to real-world robots on two challenging long-horizon tasks involving\narticulated, reflective, and slender objects, with little to no performance\ndegradation. We hope our findings will inspire future research in utilizing\nsimulation data and 3D information in general robot policies.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02530v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02530v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.359,
      "datasets_score": 0.364,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is developing Camera Depth Models (CDMs) to enhance depth perception for robotic manipulation, using neural networks to denoise depth data and bridge the sim-to-real gap. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning as defined in the topic. The focus is solely on computer vision and robotics, with no elements of Chain-of-Thought or diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02541",
      "title": "Mix-modal Federated Learning for MRI Image Segmentation",
      "authors": [
        "Guyue Hu",
        "Siyuan Song",
        "Jingpeng Sun",
        "Zhe Jin",
        "Chenglong Li",
        "Jin Tang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Magnetic resonance imaging (MRI) image segmentation is crucial in diagnosing\nand treating many diseases, such as brain tumors. Existing MRI image\nsegmentation methods mainly fall into a centralized multimodal paradigm, which\nis inapplicable in engineering non-centralized mix-modal medical scenarios. In\nthis situation, each distributed client (hospital) processes multiple mixed MRI\nmodalities, and the modality set and image data for each client are diverse,\nsuffering from extensive client-wise modality heterogeneity and data\nheterogeneity. In this paper, we first formulate non-centralized mix-modal MRI\nimage segmentation as a new paradigm for federated learning (FL) that involves\nmultiple modalities, called mix-modal federated learning (MixMFL). It\ndistinguishes from existing multimodal federating learning (MulMFL) and\ncross-modal federating learning (CroMFL) paradigms. Then, we proposed a novel\nmodality decoupling and memorizing mix-modal federated learning framework\n(MDM-MixMFL) for MRI image segmentation, which is characterized by a modality\ndecoupling strategy and a modality memorizing mechanism. Specifically, the\nmodality decoupling strategy disentangles each modality into modality-tailored\nand modality-shared information. During mix-modal federated updating,\ncorresponding modality encoders undergo tailored and shared updating,\nrespectively. It facilitates stable and adaptive federating aggregation of\nheterogeneous data and modalities from distributed clients. Besides, the\nmodality memorizing mechanism stores client-shared modality prototypes\ndynamically refreshed from every modality-tailored encoder to compensate for\nincomplete modalities in each local client. It further benefits modality\naggregation and fusion processes during mixmodal federated learning. Extensive\nexperiments on two public datasets for MRI image segmentation demonstrate the\neffectiveness and superiority of our methods.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02541v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02541v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.329,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.403,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution involves federated learning (FL), a form of distributed training, where multiple clients (e.g., hospitals) train models locally on heterogeneous data and aggregate updates without centralizing data. This directly aligns with distributed training concepts, as it partitions data across nodes, addresses computation heterogeneity, and aims to enable effective multi-node learning for MRI segmentation. The proposed MDM-MixMFL framework enhances FL by incorporating strategies for stable aggregation, making it a direct advancement in distributed training techniques.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces MixMFL, a new federated learning paradigm for MRI image segmentation that addresses challenges in non-centralized, mix-modal medical scenarios with heterogeneous data and modalities across distributed clients. It proposes the MDM-MixMFL framework, featuring a modality decoupling strategy to separate and update modality-specific and shared information for stable aggregation, and a modality memorizing mechanism to compensate for incomplete modalities, with experiments on public datasets demonstrating improved segmentation performance and effectiveness.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new paradigm (MixMFL) and framework (MDM-MixMFL) that advances state-of-the-art federated learning by effectively handling both modality and data heterogeneity in MRI segmentation, distinguishing it from existing approaches like MulMFL and CroMFL.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a broad range of future research in privacy-preserving medical AI and commercial applications in healthcare by enabling robust decentralized learning with heterogeneous data.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a high-quality, innovative contribution to federated learning for medical imaging, making it valuable for researchers in computer vision and healthcare AI, though it is specialized and not essential for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/8d3aef5f845025a741377e17f32cb8bebf1f7a5d",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 2,
      "average_h_index": 0.6666666666666666,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Guyue Hu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2345417026"
        },
        {
          "name": "Siyuan Song",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2357863442"
        },
        {
          "name": "Jingpeng Sun",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379605563"
        },
        {
          "name": "Zhe Jin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381257782"
        },
        {
          "name": "Chenglong Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2312895531"
        },
        {
          "name": "Jin Tang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2312685841"
        }
      ]
    },
    {
      "id": "2509.02544",
      "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn\n  Reinforcement Learning",
      "authors": [
        "Haoming Wang",
        "Haoyang Zou",
        "Huatong Song",
        "Jiazhan Feng",
        "Junjie Fang",
        "Junting Lu",
        "Longxiang Liu",
        "Qinyu Luo",
        "Shihao Liang",
        "Shijue Huang",
        "Wanjun Zhong",
        "Yining Ye",
        "Yujia Qin",
        "Yuwen Xiong",
        "Yuxin Song",
        "Zhiyong Wu",
        "Aoyan Li",
        "Bo Li",
        "Chen Dun",
        "Chong Liu",
        "Daoguang Zan",
        "Fuxing Leng",
        "Hanbin Wang",
        "Hao Yu",
        "Haobin Chen",
        "Hongyi Guo",
        "Jing Su",
        "Jingjia Huang",
        "Kai Shen",
        "Kaiyu Shi",
        "Lin Yan",
        "Peiyao Zhao",
        "Pengfei Liu",
        "Qinghao Ye",
        "Renjie Zheng",
        "Shulin Xin",
        "Wayne Xin Zhao",
        "Wen Heng",
        "Wenhao Huang",
        "Wenqian Wang",
        "Xiaobo Qin",
        "Yi Lin",
        "Youbin Wu",
        "Zehui Chen",
        "Zihao Wang",
        "Baoquan Zhong",
        "Xinchun Zhang",
        "Xujing Li",
        "Yuanfan Li",
        "Zhongkai Zhao",
        "Chengquan Jiang",
        "Faming Wu",
        "Haotian Zhou",
        "Jinlin Pang",
        "Li Han",
        "Qi Liu",
        "Qianli Ma",
        "Siyao Liu",
        "Songhua Cai",
        "Wenqi Fu",
        "Xin Liu",
        "Yaohui Wang",
        "Zhi Zhang",
        "Bo Zhou",
        "Guoliang Li",
        "Jiajun Shi",
        "Jiale Yang",
        "Jie Tang",
        "Li Li",
        "Qihua Han",
        "Taoran Lu",
        "Woyu Lin",
        "Xiaokang Tong",
        "Xinyao Li",
        "Yichi Zhang",
        "Yu Miao",
        "Zhengxuan Jiang",
        "Zili Li",
        "Ziyuan Zhao",
        "Chenxin Li",
        "Dehua Ma",
        "Feng Lin",
        "Ge Zhang",
        "Haihua Yang",
        "Hangyu Guo",
        "Hongda Zhu",
        "Jiaheng Liu",
        "Junda Du",
        "Kai Cai",
        "Kuanye Li",
        "Lichen Yuan",
        "Meilan Han",
        "Minchao Wang",
        "Shuyue Guo",
        "Tianhao Cheng",
        "Xiaobo Ma",
        "Xiaojun Xiao",
        "Xiaolong Huang",
        "Xinjie Chen",
        "Yidi Du",
        "Yilin Chen",
        "Yiwen Wang",
        "Zhaojian Li",
        "Zhenzhu Yang",
        "Zhiyuan Zeng",
        "Chaolin Jin",
        "Chen Li",
        "Hao Chen",
        "Haoli Chen",
        "Jian Chen",
        "Qinghao Zhao",
        "Guang Shi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "The development of autonomous agents for graphical user interfaces (GUIs)\npresents major challenges in artificial intelligence. While recent advances in\nnative agent models have shown promise by unifying perception, reasoning,\naction, and memory through end-to-end learning, open problems remain in data\nscalability, multi-turn reinforcement learning (RL), the limitations of\nGUI-only operation, and environment stability. In this technical report, we\npresent UI-TARS-2, a native GUI-centered agent model that addresses these\nchallenges through a systematic training methodology: a data flywheel for\nscalable data generation, a stabilized multi-turn RL framework, a hybrid GUI\nenvironment that integrates file systems and terminals, and a unified sandbox\nplatform for large-scale rollouts. Empirical evaluation demonstrates that\nUI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5.\nOn GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on\nWindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines\nsuch as Claude and OpenAI agents. In game environments, it attains a mean\nnormalized score of 59.8 across a 15-game suite-roughly 60% of human-level\nperformance-and remains competitive with frontier proprietary models (e.g.,\nOpenAI o3) on LMGame-Bench. Additionally, the model can generalize to\nlong-horizon information-seeking tasks and software engineering benchmarks,\nhighlighting its robustness across diverse agent tasks. Detailed analyses of\ntraining dynamics further provide insights into achieving stability and\nefficiency in large-scale agent RL. These results underscore UI-TARS-2's\npotential to advance the state of GUI agents and exhibit strong generalization\nto real-world interactive scenarios.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02544v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02544v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.379,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on UI-TARS-2, a GUI agent using multi-turn reinforcement learning with techniques like PPO, reward shaping, and data flywheels for scalable training. However, it does not involve human feedback, such as training a reward model on human-ranked data or aligning the model with human preferences, which are core to RLHF. The RL described relies on environmental rewards and automated processes.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02545",
      "title": "Motion-Refined DINOSAUR for Unsupervised Multi-Object Discovery",
      "authors": [
        "Xinrui Gong",
        "Oliver Hahn",
        "Christoph Reich",
        "Krishnakant Singh",
        "Simone Schaub-Meyer",
        "Daniel Cremers",
        "Stefan Roth"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Unsupervised multi-object discovery (MOD) aims to detect and localize\ndistinct object instances in visual scenes without any form of human\nsupervision. Recent approaches leverage object-centric learning (OCL) and\nmotion cues from video to identify individual objects. However, these\napproaches use supervision to generate pseudo labels to train the OCL model. We\naddress this limitation with MR-DINOSAUR -- Motion-Refined DINOSAUR -- a\nminimalistic unsupervised approach that extends the self-supervised pre-trained\nOCL model, DINOSAUR, to the task of unsupervised multi-object discovery. We\ngenerate high-quality unsupervised pseudo labels by retrieving video frames\nwithout camera motion for which we perform motion segmentation of unsupervised\noptical flow. We refine DINOSAUR's slot representations using these pseudo\nlabels and train a slot deactivation module to assign slots to foreground and\nbackground. Despite its conceptual simplicity, MR-DINOSAUR achieves strong\nmulti-object discovery results on the TRI-PD and KITTI datasets, outperforming\nthe previous state of the art despite being fully unsupervised.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02545v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02545v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.398,
      "distributed_training_score": 0.353,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02547",
      "title": "The Landscape of Agentic Reinforcement Learning for LLMs: A Survey",
      "authors": [
        "Guibin Zhang",
        "Hejia Geng",
        "Xiaohang Yu",
        "Zhenfei Yin",
        "Zaibin Zhang",
        "Zelin Tan",
        "Heng Zhou",
        "Zhongzhi Li",
        "Xiangyuan Xue",
        "Yijiang Li",
        "Yifan Zhou",
        "Yang Chen",
        "Chen Zhang",
        "Yutao Fan",
        "Zihu Wang",
        "Songtao Huang",
        "Yue Liao",
        "Hongru Wang",
        "Mengyue Yang",
        "Heng Ji",
        "Michael Littman",
        "Jun Wang",
        "Shuicheng Yan",
        "Philip Torr",
        "Lei Bai"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm\nshift from conventional reinforcement learning applied to large language models\n(LLM RL), reframing LLMs from passive sequence generators into autonomous,\ndecision-making agents embedded in complex, dynamic worlds. This survey\nformalizes this conceptual shift by contrasting the degenerate single-step\nMarkov Decision Processes (MDPs) of LLM-RL with the temporally extended,\npartially observable Markov decision processes (POMDPs) that define Agentic RL.\nBuilding on this foundation, we propose a comprehensive twofold taxonomy: one\norganized around core agentic capabilities, including planning, tool use,\nmemory, reasoning, self-improvement, and perception, and the other around their\napplications across diverse task domains. Central to our thesis is that\nreinforcement learning serves as the critical mechanism for transforming these\ncapabilities from static, heuristic modules into adaptive, robust agentic\nbehavior. To support and accelerate future research, we consolidate the\nlandscape of open-source environments, benchmarks, and frameworks into a\npractical compendium. By synthesizing over five hundred recent works, this\nsurvey charts the contours of this rapidly evolving field and highlights the\nopportunities and challenges that will shape the development of scalable,\ngeneral-purpose AI agents.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02547v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02547v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.528,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.377,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions RLHF as a prominent branch in RL for LLMs, particularly for post-training alignment, but it is explicitly listed under \"Out of scope\" as the survey focuses on Agentic RL in dynamic environments, not alignment with human preferences. Thus, RLHF is referenced only briefly in the background and not as a core contribution.",
      "weak_supervision_justification": "The paper does not discuss weak supervision, as its main contribution centers on Agentic RL for LLMs, including capabilities like planning and tool use, without any reference to programmatically generated labels or noisy training data sources.",
      "diffusion_reasoning_justification": "The paper addresses reasoning as one of the agentic capabilities in RL for LLMs but does not mention diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02555",
      "title": "Surrogate Benchmarks for Model Merging Optimization",
      "authors": [
        "Rio Akizuki",
        "Yuya Kudo",
        "Nozomu Yoshinari",
        "Yoichi Hirose",
        "Toshiyuki Nishimoto",
        "Kento Uchida",
        "Shinichi Shirakawa"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.NE (Neural and Evolutionary Computing)"
      ],
      "abstract": "Model merging techniques aim to integrate the abilities of multiple models\ninto a single model. Most model merging techniques have hyperparameters, and\ntheir setting affects the performance of the merged model. Because several\nexisting works show that tuning hyperparameters in model merging can enhance\nthe merging outcome, developing hyperparameter optimization algorithms for\nmodel merging is a promising direction. However, its optimization process is\ncomputationally expensive, particularly in merging LLMs. In this work, we\ndevelop surrogate benchmarks for optimization of the merging hyperparameters to\nrealize algorithm development and performance comparison at low cost. We define\ntwo search spaces and collect data samples to construct surrogate models to\npredict the performance of a merged model from a hyperparameter. We demonstrate\nthat our benchmarks can predict the performance of merged models well and\nsimulate optimization algorithm behaviors.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02555v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02555v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.405,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.41,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is developing surrogate benchmarks for optimizing hyperparameters in model merging, which focuses on predicting model performance without involving label generation, noisy data sources, or weak supervision techniques. There is no discussion of training models with programmatically generated labels or alternatives to hand-labeled data.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper centers on creating surrogate benchmarks to reduce computational costs for model merging optimization, but it does not address distributed training methods, parallel computing strategies, or partitioning data/models across multiple nodes. While it mentions computational expenses on GPUs, it does not contribute to algorithms or systems for multi-node machine learning.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02560",
      "title": "FastVGGT: Training-Free Acceleration of Visual Geometry Transformer",
      "authors": [
        "You Shen",
        "Zhipeng Zhang",
        "Yansong Qu",
        "Liujuan Cao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Foundation models for 3D vision have recently demonstrated remarkable\ncapabilities in 3D perception. However, scaling these models to long-sequence\nimage inputs remains a significant challenge due to inference-time\ninefficiency. In this work, we present a detailed analysis of VGGT, a\nstate-of-the-art feed-forward visual geometry model and identify its primary\nbottleneck. Visualization further reveals a token collapse phenomenon in the\nattention maps. Motivated by these findings, we explore the potential of token\nmerging in the feed-forward visual geometry model. Owing to the unique\narchitectural and task-specific properties of 3D models, directly applying\nexisting merging techniques proves challenging. To this end, we propose\nFastVGGT, which, for the first time, leverages token merging in the 3D domain\nthrough a training-free mechanism for accelerating VGGT. we devise a unique\ntoken partitioning strategy tailored to 3D architectures and tasks, effectively\neliminating redundant computation while preserving VGGT's powerful\nreconstruction capacity. Extensive experiments on multiple 3D geometry\nbenchmarks validate the effectiveness of our approach. Notably, with 1000 input\nimages, FastVGGT achieves a 4x speedup over VGGT while mitigating error\naccumulation in long-sequence scenarios. These findings underscore the\npotential of token merging as a principled solution for scalable 3D vision\nsystems. Code is available at: https://mystorm16.github.io/fastvggt/.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02560v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02560v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.448,
      "datasets_score": 0.273,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on inference-time acceleration of a visual geometry transformer using token merging, specifically for 3D vision tasks, without any discussion of distributed training, parallel computing, or multi-node machine learning. It addresses efficiency during inference, not the training phase, so it does not relate to algorithms or systems for partitioning data or computation across multiple processors for model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02642",
      "title": "BioMD: All-atom Generative Model for Biomolecular Dynamics Simulation",
      "authors": [
        "Bin Feng",
        "Jiying Zhang",
        "Xinni Zhang",
        "Zijing Liu",
        "Yu Li"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Molecular dynamics (MD) simulations are essential tools in computational\nchemistry and drug discovery, offering crucial insights into dynamic molecular\nbehavior. However, their utility is significantly limited by substantial\ncomputational costs, which severely restrict accessible timescales for many\nbiologically relevant processes. Despite the encouraging performance of\nexisting machine learning (ML) methods, they struggle to generate extended\nbiomolecular system trajectories, primarily due to the lack of MD datasets and\nthe large computational demands of modeling long historical trajectories. Here,\nwe introduce BioMD, the first all-atom generative model to simulate\nlong-timescale protein-ligand dynamics using a hierarchical framework of\nforecasting and interpolation. We demonstrate the effectiveness and versatility\nof BioMD on the DD-13M (ligand unbinding) and MISATO datasets. For both\ndatasets, BioMD generates highly realistic conformations, showing high physical\nplausibility and low reconstruction errors. Besides, BioMD successfully\ngenerates ligand unbinding paths for 97.1% of the protein-ligand systems within\nten attempts, demonstrating its ability to explore critical unbinding pathways.\nCollectively, these results establish BioMD as a tool for simulating complex\nbiomolecular processes, offering broad applicability for computational\nchemistry and drug discovery.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02642v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02642v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.353,
      "datasets_score": 0.298,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces BioMD, a generative model for biomolecular dynamics simulations using conditional flow matching and diffusion-inspired techniques like \"noising-as-masking\" from Diffusion Forcing. However, this is applied to generating molecular trajectories and interpolating conformations, not to multi-step logical reasoning or treating a Chain-of-Thought as a single entity for solving complex logical tasks. The core focus is on computational chemistry, lacking any component for logical reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02650",
      "title": "Can Media Act as a Soft Regulator of Safe AI Development? A Game\n  Theoretical Analysis",
      "authors": [
        "Henrique Correia da Fonseca",
        "António Fernandes",
        "Zhao Song",
        "Theodor Cimpeanu",
        "Nataliya Balabanova",
        "Adeela Bashir",
        "Paolo Bova",
        "Alessio Buscemi",
        "Alessandro Di Stefano",
        "Manh Hong Duong",
        "Elias Fernandez Domingos",
        "Ndidi Bianca Ogbo",
        "Simon T. Powers",
        "Daniele Proverbio",
        "Zia Ush Shamszaman",
        "Fernando P. Santos",
        "The Anh Han",
        "Marcus Krellner"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.GT (Computer Science and Game Theory)"
      ],
      "abstract": "When developers of artificial intelligence (AI) products need to decide\nbetween profit and safety for the users, they likely choose profit.\nUntrustworthy AI technology must come packaged with tangible negative\nconsequences. Here, we envisage those consequences as the loss of reputation\ncaused by media coverage of their misdeeds, disseminated to the public. We\nexplore whether media coverage has the potential to push AI creators into the\nproduction of safe products, enabling widespread adoption of AI technology. We\ncreated artificial populations of self-interested creators and users and\nstudied them through the lens of evolutionary game theory. Our results reveal\nthat media is indeed able to foster cooperation between creators and users, but\nnot always. Cooperation does not evolve if the quality of the information\nprovided by the media is not reliable enough, or if the costs of either\naccessing media or ensuring safety are too high. By shaping public perception\nand holding developers accountable, media emerges as a powerful soft regulator\n-- guiding AI safety even in the absence of formal government oversight.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02650v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02650v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.453,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.288,
      "datasets_score": 0.323,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is an analysis using evolutionary game theory to examine how media influences AI safety through reputation and indirect reciprocity, without any discussion of training AI models with human feedback. It does not involve reinforcement learning techniques, reward models, or fine-tuning based on human-ranked data, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02655",
      "title": "BioBlue: Notable runaway-optimiser-like LLM failure modes on\n  biologically and economically aligned AI safety benchmarks for LLMs with\n  simplified observation format",
      "authors": [
        "Roland Pihlakas",
        "Sruthi Kuriakose"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Relatively many past AI safety discussions have centered around the dangers\nof unbounded utility maximisation by RL agents, illustrated by scenarios like\nthe \"paperclip maximiser\" or by specification gaming in general. Unbounded\nmaximisation is problematic for many reasons. We wanted to verify whether these\nRL runaway optimisation problems are still relevant with LLMs as well. Turns\nout, strangely, this is indeed clearly the case. The problem is not that the\nLLMs just lose context or become incoherent. The problem is that in various\nscenarios, LLMs lose context in very specific ways, which systematically\nresemble runaway optimisers in the following distinct ways: 1) Ignoring\nhomeostatic targets and \"defaulting\" to unbounded maximisation instead. 2) It\nis equally concerning that the \"default\" meant also reverting back to\nsingle-objective optimisation. Our findings also suggest that long-running\nscenarios are important. Systematic failures emerge after periods of initially\nsuccessful behaviour. In some trials the LLMs were successful until the end.\nThis means, while current LLMs do conceptually grasp biological and economic\nalignment, they exhibit randomly triggered problematic behavioural tendencies\nunder sustained long-running conditions, particularly involving multiple or\ncompeting objectives. Once they flip, they usually do not recover. Even though\nLLMs look multi-objective and bounded on the surface, the underlying mechanisms\nseem to be actually still biased towards being single-objective and unbounded.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02655v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02655v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.476,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.323,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating LLMs for safety and alignment failures in long-running benchmarks, observing behaviors like unbounded maximization. While LLMs such as Claude and GPT-4o mini are likely trained using RLHF, the paper does not discuss, implement, or analyze RLHF processes, human feedback mechanisms, or reward model fine-tuning. Instead, it examines pre-existing model behaviors, making it only indirectly related to RLHF through the broader context of AI alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02659",
      "title": "2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous\n  Driving Using Vision Language Model",
      "authors": [
        "Zilong Guo",
        "Yi Luo",
        "Long Sha",
        "Dongxu Wang",
        "Panqu Wang",
        "Chenyang Xu",
        "Yi Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "End-to-end autonomous driving has drawn tremendous attention recently. Many\nworks focus on using modular deep neural networks to construct the end-to-end\narchi-tecture. However, whether using powerful large language models (LLM),\nespecially multi-modality Vision Language Models (VLM) could benefit the\nend-to-end driving tasks remain a question. In our work, we demonstrate that\ncombining end-to-end architectural design and knowledgeable VLMs yield\nimpressive performance on the driving tasks. It is worth noting that our method\nonly uses a single camera and is the best camera-only solution across the\nleaderboard, demonstrating the effectiveness of vision-based driving approach\nand the potential for end-to-end driving tasks.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02659v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02659v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.369,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an end-to-end autonomous driving solution using Vision Language Models (VLMs) with a single camera input, focusing on integrating VLMs for driving tasks. It references generative modeling works that may include diffusion models (e.g., \"drivingdiffusion\"), but does not describe any use of diffusion for multi-step logical reasoning or iterative refinement of a Chain-of-Thought. Without a clear component involving diffusion-based reasoning, the paper does not align with this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02661",
      "title": "The Future of Artificial Intelligence and the Mathematical and Physical\n  Sciences (AI+MPS)",
      "authors": [
        "Andrew Ferguson",
        "Marisa LaFleur",
        "Lars Ruthotto",
        "Jesse Thaler",
        "Yuan-Sen Ting",
        "Pratyush Tiwary",
        "Soledad Villar",
        "E. Paulo Alves",
        "Jeremy Avigad",
        "Simon Billinge",
        "Camille Bilodeau",
        "Keith Brown",
        "Emmanuel Candes",
        "Arghya Chattopadhyay",
        "Bingqing Cheng",
        "Jonathan Clausen",
        "Connor Coley",
        "Andrew Connolly",
        "Fred Daum",
        "Sijia Dong",
        "Chrisy Xiyu Du",
        "Cora Dvorkin",
        "Cristiano Fanelli",
        "Eric B. Ford",
        "Luis Manuel Frutos",
        "Nicolás García Trillos",
        "Cecilia Garraffo",
        "Robert Ghrist",
        "Rafael Gomez-Bombarelli",
        "Gianluca Guadagni",
        "Sreelekha Guggilam",
        "Sergei Gukov",
        "Juan B. Gutiérrez",
        "Salman Habib",
        "Johannes Hachmann",
        "Boris Hanin",
        "Philip Harris",
        "Murray Holland",
        "Elizabeth Holm",
        "Hsin-Yuan Huang",
        "Shih-Chieh Hsu",
        "Nick Jackson",
        "Olexandr Isayev",
        "Heng Ji",
        "Aggelos Katsaggelos",
        "Jeremy Kepner",
        "Yannis Kevrekidis",
        "Michelle Kuchera",
        "J. Nathan Kutz",
        "Branislava Lalic",
        "Ann Lee",
        "Matt LeBlanc",
        "Josiah Lim",
        "Rebecca Lindsey",
        "Yongmin Liu",
        "Peter Y. Lu",
        "Sudhir Malik",
        "Vuk Mandic",
        "Vidya Manian",
        "Emeka P. Mazi",
        "Pankaj Mehta",
        "Peter Melchior",
        "Brice Ménard",
        "Jennifer Ngadiuba",
        "Stella Offner",
        "Elsa Olivetti",
        "Shyue Ping Ong",
        "Christopher Rackauckas",
        "Philippe Rigollet",
        "Chad Risko",
        "Philip Romero",
        "Grant Rotskoff",
        "Brett Savoie",
        "Uros Seljak",
        "David Shih",
        "Gary Shiu",
        "Dima Shlyakhtenko",
        "Eva Silverstein",
        "Taylor Sparks",
        "Thomas Strohmer",
        "Christopher Stubbs",
        "Stephen Thomas",
        "Suriyanarayanan Vaikuntanathan",
        "Rene Vidal",
        "Francisco Villaescusa-Navarro",
        "Gregory Voth",
        "Benjamin Wandelt",
        "Rachel Ward",
        "Melanie Weber",
        "Risa Wechsler",
        "Stephen Whitelam",
        "Olaf Wiest",
        "Mike Williams",
        "Zhuoran Yang",
        "Yaroslava G. Yingling",
        "Bin Yu",
        "Shuwen Yue",
        "Ann Zabludoff",
        "Huimin Zhao",
        "Tong Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "astro-ph.IM (Instrumentation and Methods for Astrophysics)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "This community paper developed out of the NSF Workshop on the Future of\nArtificial Intelligence (AI) and the Mathematical and Physics Sciences (MPS),\nwhich was held in March 2025 with the goal of understanding how the MPS domains\n(Astronomy, Chemistry, Materials Research, Mathematical Sciences, and Physics)\ncan best capitalize on, and contribute to, the future of AI. We present here a\nsummary and snapshot of the MPS community's perspective, as of Spring/Summer\n2025, in a rapidly developing field. The link between AI and MPS is becoming\nincreasingly inextricable; now is a crucial moment to strengthen the link\nbetween AI and Science by pursuing a strategy that proactively and thoughtfully\nleverages the potential of AI for scientific discovery and optimizes\nopportunities to impact the development of AI by applying concepts from\nfundamental science. To achieve this, we propose activities and strategic\npriorities that: (1) enable AI+MPS research in both directions; (2) build up an\ninterdisciplinary community of AI+MPS researchers; and (3) foster education and\nworkforce development in AI for MPS researchers and students. We conclude with\na summary of suggested priorities for funding agencies, educational\ninstitutions, and individual researchers to help position the MPS community to\nbe a leader in, and take full advantage of, the transformative potential of\nAI+MPS.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02661v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02661v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.444,
      "weak_supervision_score": 0.411,
      "diffusion_reasoning_score": 0.443,
      "distributed_training_score": 0.441,
      "datasets_score": 0.449,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper discusses general AI strategies and mutual innovation in MPS domains but does not mention reinforcement learning, human feedback, or aligning AI models with human preferences. It focuses on high-level applications and terminology without specific reference to RLHF.",
      "weak_supervision_justification": "The paper briefly mentions weakly-supervised learning in the context of AI terminology, describing it as using coarse-grained or inaccurate labels, but this is not a main focus or contribution. It is mentioned as part of a broader discussion on machine learning types, without detailed exploration or application in MPS domains.",
      "diffusion_reasoning_justification": "The paper does not reference diffusion models, iterative refinement for logical reasoning, or multi-step reasoning processes. It discusses general AI techniques like generative models but lacks any specific component related to diffusion-based approaches.",
      "distributed_training_justification": "The paper addresses high-level AI strategies, community building, and applications in MPS but does not discuss distributed training, parallel computing, or partitioning computations across nodes. There is no mention of algorithms or systems for accelerating model training.",
      "datasets_justification": "The paper highlights the role of datasets in AI advancements, such as data curation from MPS domains (e.g., sky surveys) that have enabled breakthroughs, and mentions examples like anomaly detection in astronomy. However, it does not deeply focus on creating, benchmarking, or analyzing datasets as a primary contribution, treating them as supporting elements in AI+MPS integration.",
      "llm_score_status": "completed",
      "summary": "This community paper, stemming from the NSF Workshop on the Future of Artificial Intelligence (AI) and the Mathematical and Physical Sciences (MPS), outlines strategies for leveraging AI to advance scientific discovery in fields like astronomy, chemistry, materials research, mathematics, and physics, while also contributing to AI development. It proposes key activities including enabling bidirectional research, building interdisciplinary communities, and fostering education and workforce development, emphasizing the mutual benefits and providing recommendations for funding agencies, institutions, and researchers to maximize AI's transformative potential in MPS.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by synthesizing existing ideas into a strategic framework for AI-MPS integration, offering a clever combination of approaches to foster collaboration and innovation without introducing a entirely new problem or technique. While it builds on historical precedents, it provides a fresh perspective on mutual advancements between AI and science.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research, funding decisions, and educational initiatives across AI and MPS domains, potentially shaping policies and collaborations that drive scientific and technological progress. Its strategic recommendations could lead to broad applications in both academic and commercial contexts.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution by providing insightful strategies for AI-MPS integration, making it essential for researchers and stakeholders in these fields to stay informed. While not groundbreaking in technical innovation, its practical guidance on community building and education warrants attention from those involved in AI and science policy.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/08fe91262963f57aa7fa776f2615b52f5e261c36",
      "total_authors": 100,
      "authors_found": 98,
      "highest_h_index": 100,
      "average_h_index": 8.46938775510204,
      "notable_authors_count": 22,
      "author_h_indexes": [
        {
          "name": "Andrew Ferguson",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378854902"
        },
        {
          "name": "Marisa Lafleur",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2284632159"
        },
        {
          "name": "Lars Ruthotto",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378853319"
        },
        {
          "name": "Jesse Thaler",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2266590233"
        },
        {
          "name": "Y.-S. Ting",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2260337474"
        },
        {
          "name": "P. Tiwary",
          "h_index": 32,
          "profile_url": "https://www.semanticscholar.org/author/46736401"
        },
        {
          "name": "Soledad Villar",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378850468"
        },
        {
          "name": "E. P. Alves",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378850337"
        },
        {
          "name": "Jeremy Avigad",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2286989068"
        },
        {
          "name": "Simon Billinge",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378849223"
        },
        {
          "name": "Camille Bilodeau",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378842303"
        },
        {
          "name": "Keith Brown",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379277837"
        },
        {
          "name": "Emmanuel Candes",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378842271"
        },
        {
          "name": "Arghya Chattopadhyay",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375899941"
        },
        {
          "name": "Bingqing Cheng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379213306"
        },
        {
          "name": "Jonathan Clausen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378849203"
        },
        {
          "name": "Connor Coley",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378839743"
        },
        {
          "name": "Andrew Connolly",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378839718"
        },
        {
          "name": "Fred Daum",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378842316"
        },
        {
          "name": "Sijia Dong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379558310"
        },
        {
          "name": "C. Du",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/31608323"
        },
        {
          "name": "C. Dvorkin",
          "h_index": 44,
          "profile_url": "https://www.semanticscholar.org/author/49053385"
        },
        {
          "name": "C. Fanelli",
          "h_index": 54,
          "profile_url": "https://www.semanticscholar.org/author/1404352919"
        },
        {
          "name": "Eric B. Ford",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378849441"
        },
        {
          "name": "Luis Manuel Frutos",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378853355"
        },
        {
          "name": "N. G. Trillos",
          "h_index": 18,
          "profile_url": "https://www.semanticscholar.org/author/2042414"
        },
        {
          "name": "Cecilia Garraffo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2349534889"
        },
        {
          "name": "R. Ghrist",
          "h_index": 40,
          "profile_url": "https://www.semanticscholar.org/author/1696113"
        },
        {
          "name": "Rafael Gómez-Bombarelli",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2344011563"
        },
        {
          "name": "G. Guadagni",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/6088245"
        },
        {
          "name": "S. Guggilam",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2326577196"
        },
        {
          "name": "Sergei Gukov",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2284865316"
        },
        {
          "name": "Juan B. Guti'errez",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374181342"
        },
        {
          "name": "Salman Habib",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375903968"
        },
        {
          "name": "Johannes Hachmann",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2318821889"
        },
        {
          "name": "Boris Hanin",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2258225151"
        },
        {
          "name": "Philip Harris",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378794455"
        },
        {
          "name": "Murray Holland",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378842689"
        },
        {
          "name": "Elizabeth Holm",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378841912"
        },
        {
          "name": "Hsin-Yuan Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378794058"
        },
        {
          "name": "Shih-Chieh Hsu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2301016406"
        },
        {
          "name": "Nick Jackson",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378849566"
        },
        {
          "name": "O. Isayev",
          "h_index": 45,
          "profile_url": "https://www.semanticscholar.org/author/2385206"
        },
        {
          "name": "Heng Ji",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380439462"
        },
        {
          "name": "Aggelos K. Katsaggelos",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2318073187"
        },
        {
          "name": "Jeremy Kepner",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2263203076"
        },
        {
          "name": "Y. Kevrekidis",
          "h_index": 14,
          "profile_url": "https://www.semanticscholar.org/author/144914078"
        },
        {
          "name": "M. Kuchera",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/51468827"
        },
        {
          "name": "J. N. Kutz",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2342491188"
        },
        {
          "name": "B. Lalic",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/15424409"
        },
        {
          "name": "Ann Lee",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379219771"
        },
        {
          "name": "Matt LeBlanc",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378840725"
        },
        {
          "name": "Josiah Lim",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378908976"
        },
        {
          "name": "Rebecca Lindsey",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378847380"
        },
        {
          "name": "Yongmin Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378894433"
        },
        {
          "name": "Peter Y. Lu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Sudhir Malik",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2310242391"
        },
        {
          "name": "Vuk Mandic",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2261583731"
        },
        {
          "name": "V. Manian",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/2004467"
        },
        {
          "name": "E. P. Mazi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2331543528"
        },
        {
          "name": "Pankaj Mehta",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378791601"
        },
        {
          "name": "Peter Melchior",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378839920"
        },
        {
          "name": "B. M'enard",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2302800174"
        },
        {
          "name": "J. Ngadiuba",
          "h_index": 94,
          "profile_url": "https://www.semanticscholar.org/author/41016473"
        },
        {
          "name": "Stella Offner",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378838077"
        },
        {
          "name": "Elsa Olivetti",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2344398175"
        },
        {
          "name": "S. Ong",
          "h_index": 81,
          "profile_url": "https://www.semanticscholar.org/author/2381325"
        },
        {
          "name": "Christopher Rackauckas",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2368039004"
        },
        {
          "name": "P. Rigollet",
          "h_index": 40,
          "profile_url": "https://www.semanticscholar.org/author/2352281"
        },
        {
          "name": "Chad Risko",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2255191174"
        },
        {
          "name": "Philip Romero",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378847327"
        },
        {
          "name": "Grant M. Rotskoff",
          "h_index": 26,
          "profile_url": "https://www.semanticscholar.org/author/7272946"
        },
        {
          "name": "Brett A. Savoie",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/153817094"
        },
        {
          "name": "U. Seljak",
          "h_index": 100,
          "profile_url": "https://www.semanticscholar.org/author/3748196"
        },
        {
          "name": "David Shih",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378853978"
        },
        {
          "name": "G. Shiu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2264825665"
        },
        {
          "name": "Dima Shlyakhtenko",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378853765"
        },
        {
          "name": "Eva Silverstein",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2349536835"
        },
        {
          "name": "Taylor Sparks",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378853364"
        },
        {
          "name": "Thomas Strohmer",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2274617854"
        },
        {
          "name": "Christopher Stubbs",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378838651"
        },
        {
          "name": "Stephen Thomas",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Suriyanarayanan Vaikuntanathan",
          "h_index": 25,
          "profile_url": "https://www.semanticscholar.org/author/6282793"
        },
        {
          "name": "Rene Vidal",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379476664"
        },
        {
          "name": "F. Villaescusa-Navarro",
          "h_index": 42,
          "profile_url": "https://www.semanticscholar.org/author/1388753990"
        },
        {
          "name": "Gregory Voth",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378841862"
        },
        {
          "name": "Benjamin Dan Wandelt",
          "h_index": 39,
          "profile_url": "https://www.semanticscholar.org/author/35368340"
        },
        {
          "name": "R. Ward",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378842440"
        },
        {
          "name": "Melanie Weber",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379204289"
        },
        {
          "name": "Risa Wechsler",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378849423"
        },
        {
          "name": "Stephen Whitelam",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2288267076"
        },
        {
          "name": "Olaf Wiest",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378846605"
        },
        {
          "name": "Mike Williams",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2242593016"
        },
        {
          "name": "Zhuoran Yang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378904926"
        },
        {
          "name": "Yaroslava G. Yingling",
          "h_index": 30,
          "profile_url": "https://www.semanticscholar.org/author/3247592"
        },
        {
          "name": "Bin Yu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379322299"
        },
        {
          "name": "Shuwen Yue",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378843568"
        },
        {
          "name": "A. Zabludoff",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/103158694"
        },
        {
          "name": "Huimin Zhao",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2271564893"
        },
        {
          "name": "Tong Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379034024"
        }
      ]
    },
    {
      "id": "2509.02710",
      "title": "Toward a robust lesion detection model in breast DCE-MRI: adapting\n  foundation models to high-risk women",
      "authors": [
        "Gabriel A. B. do Nascimento",
        "Vincent Dong",
        "Guilherme J. Cavalcante",
        "Alex Nguyen",
        "Thaís G. do Rêgo",
        "Yuri Malheiros",
        "Telmo M. Silva Filho",
        "Carla R. Zeballos Torrez",
        "James C. Gee",
        "Anne Marie McCarthy",
        "Andrew D. A. Maidment",
        "Bruno Barufaldi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Accurate breast MRI lesion detection is critical for early cancer diagnosis,\nespecially in high-risk populations. We present a classification pipeline that\nadapts a pretrained foundation model, the Medical Slice Transformer (MST), for\nbreast lesion classification using dynamic contrast-enhanced MRI (DCE-MRI).\nLeveraging DINOv2-based self-supervised pretraining, MST generates robust\nper-slice feature embeddings, which are then used to train a Kolmogorov--Arnold\nNetwork (KAN) classifier. The KAN provides a flexible and interpretable\nalternative to conventional convolutional networks by enabling localized\nnonlinear transformations via adaptive B-spline activations. This enhances the\nmodel's ability to differentiate benign from malignant lesions in imbalanced\nand heterogeneous clinical datasets. Experimental results demonstrate that the\nMST+KAN pipeline outperforms the baseline MST classifier, achieving AUC = 0.80\n\\pm 0.02 while preserving interpretability through attention-based heatmaps.\nOur findings highlight the effectiveness of combining foundation model\nembeddings with advanced classification strategies for building robust and\ngeneralizable breast MRI analysis tools.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02710v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02710v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.286,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.351,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02718",
      "title": "Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving",
      "authors": [
        "Fangzhou Wu",
        "Sandeep Silwal"
      ],
      "categories": [
        "cs.DB (Databases)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Increasing demand for Large Language Models (LLMs) services imposes\nsubstantial deployment and computation costs on providers. LLM routing offers a\ncost-efficient solution by directing queries to the optimal LLM based on model\nand query features. However, existing works primarily focus on offline\nscenarios and struggle to adapt to online settings with high query volume and\nconstrained token budgets. In this work, we introduce the first training-free\nalgorithm for online routing scenarios. Our algorithm leverages approximate\nnearest neighbor search to efficiently estimate query features and performs a\none-time optimization over a small set of initial queries to learn a routing\nstrategy that guides future routing. We provide theoretical guarantees\ndemonstrating that our algorithm achieves a competitive ratio of $1 - o(1)$\nunder natural assumptions, which is further validated by extensive experiments\nacross 3 benchmark datasets and 8 baselines, showing an average improvement of\n3.55$\\times$ in overall performance, 1.85$\\times$ in cost efficiency, and\nnearly 4.25$\\times$ in throughput.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02718v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02718v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.457,
      "datasets_score": 0.309,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a training-free algorithm for online query routing to LLMs, using approximate nearest neighbor search and historical data for efficiency. It does not involve human feedback, reward models, or reinforcement learning for model alignment, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper addresses efficient routing and serving of queries across multiple LLMs in an online setting, but it does not discuss distributed training, parallel computing for model training, or partitioning data/computation across nodes. Its focus is on inference and optimization, not accelerating training processes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02722",
      "title": "Planning with Reasoning using Vision Language World Model",
      "authors": [
        "Delong Chen",
        "Theo Moutakanni",
        "Willy Chung",
        "Yejin Bang",
        "Ziwei Ji",
        "Allen Bolourchi",
        "Pascale Fung"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Effective planning requires strong world models, but high-level world models\nthat can understand and reason about actions with semantic and temporal\nabstraction remain largely underdeveloped. We introduce the Vision Language\nWorld Model (VLWM), a foundation model trained for language-based world\nmodeling on natural videos. Given visual observations, the VLWM first infers\nthe overall goal achievements then predicts a trajectory composed of\ninterleaved actions and world state changes. Those targets are extracted by\niterative LLM Self-Refine conditioned on compressed future observations\nrepresented by Tree of Captions. The VLWM learns both an action policy and a\ndynamics model, which respectively facilitates reactive system-1 plan decoding\nand reflective system-2 planning via cost minimization. The cost evaluates the\nsemantic distance between the hypothetical future states given by VLWM\nroll-outs and the expected goal state, and is measured by a critic model that\nwe trained in a self-supervised manner. The VLWM achieves state-of-the-art\nVisual Planning for Assistance (VPA) performance on both benchmark evaluations\nand our proposed PlannerArena human evaluations, where system-2 improves the\nElo score by +27% upon system-1. The VLWM models also outperforms strong VLM\nbaselines on RoboVQA and WorldPrediction benchmark.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02722v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02722v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.417,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.497,
      "distributed_training_score": 0.311,
      "datasets_score": 0.279,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper does not involve training a reward model using human-ranked data or fine-tuning via reinforcement learning based on human feedback. Instead, the critic model is trained in a self-supervised manner on video data, and human evaluations like PlannerArena are used for assessment, not for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses iterative LLM Self-Refine for refining predictions, but this is not based on diffusion models or their iterative refinement process for logical tasks. It employs a JEPA-style world model and LLMs, without any adaptation of diffusion for multi-step reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02746",
      "title": "Mentality: A Mamba-based Approach towards Foundation Models for EEG",
      "authors": [
        "Saarang Panchavati",
        "Corey Arnold",
        "William Speier"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This work explores the potential of foundation models, specifically a\nMamba-based selective state space model, for enhancing EEG analysis in\nneurological disorder diagnosis. EEG, crucial for diagnosing conditions like\nepilepsy, presents significant challenges due to its noisy, high-dimensional,\nand nonlinear nature. Traditional machine learning methods have made advances\nin automating EEG analysis but often fail to capture its complex\nspatio-temporal dynamics. Recent advances in deep learning, particularly in\nsequence modeling, offer new avenues for creating more generalized and\nexpressive models capable of handling such complexities. By training a\nMamba-based model on a large dataset containing seizure and non-seizure EEG\nrecordings through a self-supervised reconstruction task followed by a seizure\ndetection task, we demonstrate the model's effectiveness, achieving an AUROC of\n0.72 on a held-out test set. This approach marks a significant step toward\ndeveloping large-scale, clinically applicable foundation models for EEG data\nanalysis.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02746v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02746v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.303,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a Mamba-based selective state space model for EEG analysis, emphasizing self-supervised reconstruction and seizure detection tasks. It does not involve diffusion models, iterative refinement for logical reasoning, or treating a 'Chain-of-Thought' as a single entity for correction. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02751",
      "title": "Deep Research is the New Analytics System: Towards Building the Runtime\n  for AI-Driven Analytics",
      "authors": [
        "Matthew Russo",
        "Tim Kraska"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.DB (Databases)",
        "cs.LG (Machine Learning)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "With advances in large language models (LLMs), researchers are creating new\nsystems that can perform AI-driven analytics over large unstructured datasets.\nRecent work has explored executing such analytics queries using semantic\noperators -- a declarative set of AI-powered data transformations with natural\nlanguage specifications. However, even when optimized, these operators can be\nexpensive to execute on millions of records and their iterator execution\nsemantics make them ill-suited for interactive data analytics tasks. In another\nline of work, Deep Research systems have demonstrated an ability to answer\nnatural language question(s) over large datasets. These systems use one or more\nLLM agent(s) to plan their execution, process the dataset(s), and iteratively\nrefine their answer. However, these systems do not explicitly optimize their\nquery plans which can lead to poor plan execution. In order for AI-driven\nanalytics to excel, we need a runtime which combines the optimized execution of\nsemantic operators with the flexibility and more dynamic execution of Deep\nResearch systems. As a first step towards this vision, we build a prototype\nwhich enables Deep Research agents to write and execute optimized semantic\noperator programs. We evaluate our prototype and demonstrate that it can\noutperform a handcrafted semantic operator program and open Deep Research\nsystems on two basic queries. Compared to a standard open Deep Research agent,\nour prototype achieves up to 1.95x better F1-score. Furthermore, even if we\ngive the agent access to semantic operators as tools, our prototype still\nachieves cost and runtime savings of up to 76.8% and 72.7% thanks to its\noptimized execution.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02751v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02751v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.424,
      "distributed_training_score": 0.412,
      "datasets_score": 0.397,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on building a runtime for AI-driven analytics by combining semantic operators and Deep Research systems, emphasizing iterative planning and optimization. However, it does not involve diffusion models, iterative refinement for Chain-of-Thought reasoning, or any adaptation of diffusion processes for logical tasks.",
      "distributed_training_justification": "The paper discusses query optimization and execution in AI-driven analytics systems, but it does not address distributed training, parallel computing, multi-node setups, or algorithms for accelerating model training across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02754",
      "title": "Do LLM Modules Generalize? A Study on Motion Generation for Autonomous\n  Driving",
      "authors": [
        "Mingyi Wang",
        "Jingke Wang",
        "Tengju Ye",
        "Junbo Chen",
        "Kaicheng Yu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent breakthroughs in large language models (LLMs) have not only advanced\nnatural language processing but also inspired their application in domains with\nstructurally similar problems--most notably, autonomous driving motion\ngeneration. Both domains involve autoregressive sequence modeling, token-based\nrepresentations, and context-aware decision making, making the transfer of LLM\ncomponents a natural and increasingly common practice. However, despite\npromising early attempts, a systematic understanding of which LLM modules are\ntruly transferable remains lacking. In this paper, we present a comprehensive\nevaluation of five key LLM modules--tokenizer design, positional embedding,\npre-training paradigms, post-training strategies, and test-time\ncomputation--within the context of motion generation for autonomous driving.\nThrough extensive experiments on the Waymo Sim Agents benchmark, we demonstrate\nthat, when appropriately adapted, these modules can significantly improve\nperformance for autonomous driving motion generation. In addition, we identify\nwhich techniques can be effectively transferred, analyze the potential reasons\nfor the failure of others, and discuss the specific adaptations needed for\nautonomous driving scenarios. We evaluate our method on the Sim Agents task and\nachieve competitive results.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02754v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02754v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.492,
      "distributed_training_score": 0.429,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses post-training strategies adapted from LLMs, including inducing preferences over actions and handling reward trade-offs for constraints like human-like behavior in motion generation. This aligns somewhat with RLHF concepts, as it involves fine-tuning based on preferences, but it does not explicitly use a separate reward model trained on human-ranked data or reinforcement learning for fine-tuning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on transferring LLM modules like tokenizers and positional embeddings to autonomous driving motion generation, without any mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. There is no component for holistic Chain-of-Thought correction or diffusion-based techniques.",
      "distributed_training_justification": "The paper evaluates LLM modules for motion generation and mentions pre-training on large-scale datasets, but it does not discuss distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors. The focus is on module transferability, not training acceleration methods.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper systematically evaluates the transferability of key modules from large language models (LLMs), including tokenizer design, positional embedding, pre-training paradigms, post-training strategies, and test-time computation, to the domain of autonomous driving motion generation, which involves generating future trajectories for agents based on environmental context. Through extensive experiments on the Waymo Sim Agents benchmark, the authors demonstrate that certain LLM modules can be effectively adapted to enhance performance, identify reasons for the failure of others, and achieve competitive results, while highlighting the need for domain-specific modifications due to differences like handling continuous motion data.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new systematic investigation into the transferability of LLM modules to autonomous driving motion generation, advancing the state-of-the-art by being the first to comprehensively evaluate multiple components across domains.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of AI for autonomous driving, as it provides valuable insights into adapting LLM techniques, though its influence may be limited to specific applications rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to AI research in autonomous driving by offering practical insights on module transferability, making it essential for researchers in this area to be aware of its findings and methodologies.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0c281d137f1e7c1e2d10cc49c2b22df8afda35bd",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 5,
      "average_h_index": 1.6,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Mingyi Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378899181"
        },
        {
          "name": "Jingke Wang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2110100448"
        },
        {
          "name": "Tengju Ye",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2600726"
        },
        {
          "name": "Junbo Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378870612"
        },
        {
          "name": "Kaicheng Yu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381240622"
        }
      ]
    },
    {
      "id": "2509.02758",
      "title": "Optimizing Geometry Problem Sets for Skill Development",
      "authors": [
        "Michael Bouzinier",
        "Sergey Trifonov"
      ],
      "categories": [
        "math.HO (History and Overview)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This article describes an ontology and methodology for annotating and\norganizing Euclidean Geometry problems, developed in the early 1990s and\nimplemented as a software tool. While the majority of this work -- including\nthe ontology and solution graph paradigm -- was completed over thirty years\nago, we argue that it has renewed relevance in the context of modern artificial\nintelligence. In particular, we explore the hypothesis that this established\nframework can facilitate automated solution validation and feedback when paired\nwith contemporary large language models, thereby supporting teachers and\nself-learners in geometry education. We document the original architecture and\nits enduring value, and outline pathways for bridging historical educational\nresources with next-generation AI techniques.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02758v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02758v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.376,
      "distributed_training_score": 0.303,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02761",
      "title": "Plan Verification for LLM-Based Embodied Task Completion Agents",
      "authors": [
        "Ananth Hariharan",
        "Vardhan Dongre",
        "Dilek Hakkani-Tür",
        "Gokhan Tur"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language model (LLM) based task plans and corresponding human\ndemonstrations for embodied AI may be noisy, with unnecessary actions,\nredundant navigation, and logical errors that reduce policy quality. We propose\nan iterative verification framework in which a Judge LLM critiques action\nsequences and a Planner LLM applies the revisions, yielding progressively\ncleaner and more spatially coherent trajectories. Unlike rule-based approaches,\nour method relies on natural language prompting, enabling broad generalization\nacross error types including irrelevant actions, contradictions, and missing\nsteps. On a set of manually annotated actions from the TEACh embodied AI\ndataset, our framework achieves up to 90% recall and 100% precision across four\nstate-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout).\nThe refinement loop converges quickly, with 96.5% of sequences requiring at\nmost three iterations, while improving both temporal efficiency and spatial\naction organization. Crucially, the method preserves human error-recovery\npatterns rather than collapsing them, supporting future work on robust\ncorrective behavior. By establishing plan verification as a reliable LLM\ncapability for spatial planning and action refinement, we provide a scalable\npath to higher-quality training data for imitation learning in embodied AI.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02761v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02761v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.455,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.343,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on an iterative LLM-based framework for verifying and refining task plans in embodied AI, using automated critique and revision processes. It does not involve training a reward model from human-ranked data or fine-tuning models via reinforcement learning, as required for RLHF. While it references human-demonstrated datasets like TEACh, the method is fully automated and does not incorporate human feedback directly.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes an iterative verification framework for refining LLM-generated plans through critique and revision, which involves multi-step reasoning. However, it does not adapt or use diffusion models for this process; instead, it relies on standard LLM prompting. There is no mention of treating a Chain-of-Thought as a single entity for holistic correction via diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02782",
      "title": "Key Principles in Cross-Domain Hyper-Heuristic Performance",
      "authors": [
        "Václav Sobotka",
        "Lucas Kletzander",
        "Nysret Musliu",
        "Hana Rudová"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Cross-domain selection hyper-heuristics aim to distill decades of research on\nproblem-specific heuristic search algorithms into adaptable general-purpose\nsearch strategies. In this respect, existing selection hyper-heuristics\nprimarily focus on an adaptive selection of low-level heuristics (LLHs) from a\npredefined set. In contrast, we concentrate on the composition of this set and\nits strategic transformations. We systematically analyze transformations based\non three key principles: solution acceptance, LLH repetitions, and perturbation\nintensity, i.e., the proportion of a solution affected by a perturbative LLH.\nWe demonstrate the raw effects of our transformations on a trivial unbiased\nrandom selection mechanism. With an appropriately constructed transformation,\nthis trivial method outperforms all available state-of-the-art hyper-heuristics\non three challenging real-world domains and finds 11 new best-known solutions.\nThe same method is competitive with the winner of the CHeSC competition,\ncommonly used as the standard cross-domain benchmark. Moreover, we accompany\nseveral recent hyper-heuristics with such strategic transformations. Using this\napproach, we outperform the current state-of-the-art methods on both the CHeSC\nbenchmark and real-world domains while often simplifying their designs.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02782v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02782v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.319,
      "distributed_training_score": 0.289,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02783",
      "title": "The Transparent Earth: A Multimodal Foundation Model for the Earth's\n  Subsurface",
      "authors": [
        "Arnab Mazumder",
        "Javier E. Santos",
        "Noah Hobbs",
        "Mohamed Mehana",
        "Daniel O'Malley"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present the Transparent Earth, a transformer-based architecture for\nreconstructing subsurface properties from heterogeneous datasets that vary in\nsparsity, resolution, and modality, where each modality represents a distinct\ntype of observation (e.g., stress angle, mantle temperature, tectonic plate\ntype). The model incorporates positional encodings of observations together\nwith modality encodings, derived from a text embedding model applied to a\ndescription of each modality. This design enables the model to scale to an\narbitrary number of modalities, making it straightforward to add new ones not\nconsidered in the initial design. We currently include eight modalities\nspanning directional angles, categorical classes, and continuous properties\nsuch as temperature and thickness. These capabilities support in-context\nlearning, enabling the model to generate predictions either with no inputs or\nwith an arbitrary number of additional observations from any subset of\nmodalities. On validation data, this reduces errors in predicting stress angle\nby more than a factor of three. The proposed architecture is scalable and\ndemonstrates improved performance with increased parameters. Together, these\nadvances make the Transparent Earth an initial foundation model for the Earth's\nsubsurface that ultimately aims to predict any subsurface property anywhere on\nEarth.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02783v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02783v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.404,
      "datasets_score": 0.366,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a transformer-based architecture for subsurface property reconstruction and does not mention diffusion models, iterative refinement processes, or multi-step logical reasoning. There is no component related to treating a Chain-of-Thought as an entity for correction.",
      "distributed_training_justification": "The paper discusses the scalability of the model and improved performance with increased parameters, but it does not address distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes or processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02785",
      "title": "DrDiff: Dynamic Routing Diffusion with Hierarchical Attention for\n  Breaking the Efficiency-Quality Trade-off",
      "authors": [
        "Jusheng Zhang",
        "Yijia Fan",
        "Kaitong Cai",
        "Zimeng Huang",
        "Xiaofei Sun",
        "Jian Wang",
        "Chengpei Tang",
        "Keze Wang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper introduces DrDiff, a novel framework for long-text generation that\novercomes the efficiency-quality trade-off through three core technologies.\nFirst, we design a dynamic expert scheduling mechanism that intelligently\nallocates computational resources during the diffusion process based on text\ncomplexity, enabling more efficient handling of text generation tasks of\nvarying difficulty. Second, we introduce a Hierarchical Sparse Attention (HSA)\nmechanism that adaptively adjusts attention patterns according to a variety of\ninput lengths, reducing computational complexity from O($n^2$) to O($n$) while\nmaintaining model performance. Finally, we propose a soft absorption guidance\noptimization strategy that combines with DPM-solver++ to reduce diffusion\nsteps, significantly improving generation speed. Comprehensive experiments on\nvarious long-text generation benchmarks demonstrate the superiority of our\nDrDiff over the existing SOTA methods.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02785v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02785v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.599,
      "distributed_training_score": 0.409,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces DrDiff for efficient long-text generation using diffusion models, focusing on resource allocation, attention mechanisms, and speed optimizations. However, it does not involve multi-step logical reasoning, treating a Chain-of-Thought as a single entity, or iterative correction of reasoning paths. The core contributions are centered on generation efficiency, not complex logical tasks.",
      "distributed_training_justification": "The paper does not address distributed training, parallel computing, or multi-node machine learning. Its main contributions involve internal optimizations for diffusion-based text generation, such as dynamic routing and attention mechanisms, without any mention of partitioning data, architecture, or computation across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02794",
      "title": "Learning General Policies From Examples",
      "authors": [
        "Blai Bonet",
        "Hector Geffner"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Combinatorial methods for learning general policies that solve large\ncollections of planning problems have been recently developed. One of their\nstrengths, in relation to deep learning approaches, is that the resulting\npolicies can be understood and shown to be correct. A weakness is that the\nmethods do not scale up and learn only from small training instances and\nfeature pools that contain a few hundreds of states and features at most. In\nthis work, we propose a new symbolic method for learning policies based on the\ngeneralization of sampled plans that ensures structural termination and hence\nacyclicity. The proposed learning approach is not based on SAT/ASP, as previous\nsymbolic methods, but on a hitting set algorithm that can effectively handle\nproblems with millions of states, and pools with hundreds of thousands of\nfeatures. The formal properties of the approach are analyzed, and its\nscalability is tested on a number of benchmarks.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02794v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02794v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.318,
      "datasets_score": 0.266,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a symbolic method for learning general policies from sampled plans in planning problems, emphasizing scalability and correctness without involving human feedback. RLHF specifically requires training with human-ranked data to align models with human preferences, which is not present in this work. While the paper mentions reinforcement learning broadly, it does not address or relate to human feedback mechanisms.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02807",
      "title": "PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual\n  Grounding?",
      "authors": [
        "Mennatullah Siam"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multi-modal large language models (MLLMs) have shown impressive\ngeneralization across tasks using images and text modalities. While their\nextension to video has enabled tasks such as video question answering and video\ncaptioning, their pixel-level visual grounding abilities are less studied. In\nthis work, we raise the pertinent question of whether motion is used in\npixel-level visual grounding and whether video MLLMs can segment objects based\non natural language expressions describing their motion patterns. We identify\nthe shortcomings in the current benchmarks, where we show that a single frame\ncan often suffice for capturing the motion referring expression without any\ntemporal reasoning. To address this, we introduce four motion-centric probing\ntechniques, particularly designed for the visual grounding task, to study video\nMLLMs' ability to identify true motion from a fake one and their ability to\ngrasp the motion order. Consequently, we provide a motion-centric benchmark,\nMoCentric-Bench. It ensures that video MLLMs are evaluated towards leveraging\nthe interaction between motion and language rather than being dominated by\nstatic appearance cues emphasized in existing visual grounding datasets. We\nfurther establish strong single-image baselines that are on par with or\noutperform prior methods. Finally, we explore simple motion-centric adaptation\ntechniques that provide state-of-the-art performance on our MoCentric-Bench.\nOur motion-centric benchmark, evaluation and findings challenge future models\nto improve dense spatiotemporal grounding and pixel-level understanding within\nvideos. Code and datasets will be made publicly available at\nhttps://github.com/MSiam/PixFoundation-2.0.git.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02807v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02807v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.461,
      "distributed_training_score": 0.339,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution focuses on evaluating video Multi-Modal LLMs for motion in visual grounding, introducing probing techniques and a new benchmark (MoCentric-Bench). It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical reasoning tasks, such as treating a Chain-of-Thought as a single entity. Thus, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02808",
      "title": "Improving the Resilience of Quadrotors in Underground Environments by\n  Combining Learning-based and Safety Controllers",
      "authors": [
        "Isaac Ronald Ward",
        "Mark Paral",
        "Kristopher Riordan",
        "Mykel J. Kochenderfer"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "Autonomously controlling quadrotors in large-scale subterranean environments\nis applicable to many areas such as environmental surveying, mining operations,\nand search and rescue. Learning-based controllers represent an appealing\napproach to autonomy, but are known to not generalize well to\n`out-of-distribution' environments not encountered during training. In this\nwork, we train a normalizing flow-based prior over the environment, which\nprovides a measure of how far out-of-distribution the quadrotor is at any given\ntime. We use this measure as a runtime monitor, allowing us to switch between a\nlearning-based controller and a safe controller when we are sufficiently\nout-of-distribution. Our methods are benchmarked on a point-to-point navigation\ntask in a simulated 3D cave environment based on real-world point cloud data\nfrom the DARPA Subterranean Challenge Final Event Dataset. Our experimental\nresults show that our combined controller simultaneously possesses the liveness\nof the learning-based controller (completing the task quickly) and the safety\nof the safety controller (avoiding collision).",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02808v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02808v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.386,
      "datasets_score": 0.268,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on training a normalizing flow-based optimal control policy within a Bayesian model-based reinforcement learning paradigm for quadrotor navigation in underground environments. While it involves reinforcement learning, there is no mention of human feedback, such as using human-ranked data to train a reward model or fine-tuning based on human preferences. The training appears to rely on simulated environmental data from sources like the DARPA Subterranean Challenge, which does not align with the core elements of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02826",
      "title": "Ensemble Learning for Healthcare: A Comparative Analysis of Hybrid\n  Voting and Ensemble Stacking in Obesity Risk Prediction",
      "authors": [
        "Towhidul Islam",
        "Md Sumon Ali"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.AP (Applications)",
        "stat.CO (Computation)"
      ],
      "abstract": "Obesity is a critical global health issue driven by dietary, physiological,\nand environmental factors, and is strongly associated with chronic diseases\nsuch as diabetes, cardiovascular disorders, and cancer. Machine learning has\nemerged as a promising approach for early obesity risk prediction, yet a\ncomparative evaluation of ensemble techniques -- particularly hybrid majority\nvoting and ensemble stacking -- remains limited. This study aims to compare\nhybrid majority voting and ensemble stacking methods for obesity risk\nprediction, identifying which approach delivers higher accuracy and efficiency.\nThe analysis seeks to highlight the complementary strengths of these ensemble\ntechniques in guiding better predictive model selection for healthcare\napplications. Two datasets were utilized to evaluate three ensemble models:\nMajority Hard Voting, Weighted Hard Voting, and Stacking (with a Multi-Layer\nPerceptron as meta-classifier). A pool of nine Machine Learning (ML)\nalgorithms, evaluated across a total of 50 hyperparameter configurations, was\nanalyzed to identify the top three models to serve as base learners for the\nensemble methods. Preprocessing steps involved dataset balancing, and outlier\ndetection, and model performance was evaluated using Accuracy and F1-Score. On\nDataset-1, weighted hard voting and stacking achieved nearly identical\nperformance (Accuracy: 0.920304, F1: 0.920070), outperforming majority hard\nvoting. On Dataset-2, stacking demonstrated superior results (Accuracy:\n0.989837, F1: 0.989825) compared to majority hard voting (Accuracy: 0.981707,\nF1: 0.981675) and weighted hard voting, which showed the lowest performance.\nThe findings confirm that ensemble stacking provides stronger predictive\ncapability, particularly for complex data distributions, while hybrid majority\nvoting remains a robust alternative.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02826v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02826v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.278,
      "distributed_training_score": 0.314,
      "datasets_score": 0.351,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02834",
      "title": "Clustering Discourses: Racial Biases in Short Stories about Women\n  Generated by Large Language Models",
      "authors": [
        "Gustavo Bonil",
        "João Gondim",
        "Marina dos Santos",
        "Simone Hashiguti",
        "Helena Maia",
        "Nadia Silva",
        "Helio Pedrini",
        "Sandra Avila"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study investigates how large language models, in particular LLaMA\n3.2-3B, construct narratives about Black and white women in short stories\ngenerated in Portuguese. From 2100 texts, we applied computational methods to\ngroup semantically similar stories, allowing a selection for qualitative\nanalysis. Three main discursive representations emerge: social overcoming,\nancestral mythification and subjective self-realization. The analysis uncovers\nhow grammatically coherent, seemingly neutral texts materialize a crystallized,\ncolonially structured framing of the female body, reinforcing historical\ninequalities. The study proposes an integrated approach, that combines machine\nlearning techniques with qualitative, manual discourse analysis.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02834v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02834v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.372,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.362,
      "datasets_score": 0.419,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper involves generating a dataset of 2100 texts from LLMs and applying computational methods like clustering on text embeddings for analysis, which relates to dataset analysis in AI applications. However, the primary focus is on investigating biases in LLMs rather than innovating dataset creation, curation, or benchmarking techniques.",
      "llm_score_status": "completed",
      "summary": "This study investigates racial biases in short stories generated by the LLaMA 3.2-3B model about Black and white women in Portuguese, analyzing a dataset of 2100 texts through computational clustering to identify semantically similar groups for qualitative examination. It reveals three main discursive representations—social overcoming, ancestral mythification, and subjective self-realization—that demonstrate how LLMs perpetuate historical inequalities and colonial framings, while proposing an integrated methodology combining machine learning techniques with manual discourse analysis to enable scalable bias assessment.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining computational clustering with qualitative analysis on a larger dataset to detect biases in LLMs, building on previous work rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in AI ethics and bias detection subfields, especially for underrepresented languages like Portuguese, due to its scalable methodology for analyzing LLM-generated content.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a strong, valuable contribution by highlighting racial biases in LLMs and offering a practical integrated approach, making it essential for researchers in AI fairness and language studies to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/69e4457b827167dd8e917bf687f0d18860564dcc",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 7,
      "average_h_index": 1.875,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Gustavo Bonil",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2323510230"
        },
        {
          "name": "J. Gondim",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2323511912"
        },
        {
          "name": "Marina dos Santos",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378904342"
        },
        {
          "name": "Simone Hashiguti",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2375902924"
        },
        {
          "name": "H. Maia",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/8125221"
        },
        {
          "name": "N. Silva",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2261085277"
        },
        {
          "name": "Hélio Pedrini",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2261082399"
        },
        {
          "name": "Sandra Avila",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2261082358"
        }
      ]
    },
    {
      "id": "2509.02837",
      "title": "HF-RAG: Hierarchical Fusion-based RAG with Multiple Sources and Rankers",
      "authors": [
        "Payel Santra",
        "Madhusudan Ghosh",
        "Debasis Ganguly",
        "Partha Basuchowdhuri",
        "Sudip Kumar Naskar"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Leveraging both labeled (input-output associations) and unlabeled data (wider\ncontextual grounding) may provide complementary benefits in retrieval augmented\ngeneration (RAG). However, effectively combining evidence from these\nheterogeneous sources is challenging as the respective similarity scores are\nnot inter-comparable. Additionally, aggregating beliefs from the outputs of\nmultiple rankers can improve the effectiveness of RAG. Our proposed method\nfirst aggregates the top-documents from a number of IR models using a standard\nrank fusion technique for each source (labeled and unlabeled). Next, we\nstandardize the retrieval score distributions within each source by applying\nz-score transformation before merging the top-retrieved documents from the two\nsources. We evaluate our approach on the fact verification task, demonstrating\nthat it consistently improves over the best-performing individual ranker or\nsource and also shows better out-of-domain generalization.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02837v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02837v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.306,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Hierarchical Fusion-based RAG for fact verification, which involves combining labeled and unlabeled data with rank fusion techniques. It does not mention reinforcement learning, human feedback, or training a reward model based on human-ranked data. Therefore, it does not align with RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a method for RAG using hierarchical fusion of rankers and sources, aimed at fact verification tasks. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined. There is no component related to treating Chain-of-Thought as a single entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02844",
      "title": "Conformal Prediction for Time-series Forecasting with Change Points",
      "authors": [
        "Sophia Sun",
        "Rose Yu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Conformal prediction has been explored as a general and efficient way to\nprovide uncertainty quantification for time series. However, current methods\nstruggle to handle time series data with change points - sudden shifts in the\nunderlying data-generating process. In this paper, we propose a novel Conformal\nPrediction for Time-series with Change points (CPTC) algorithm, addressing this\ngap by integrating a model to predict the underlying state with online\nconformal prediction to model uncertainties in non-stationary time series. We\nprove CPTC's validity and improved adaptivity in the time series setting under\nminimum assumptions, and demonstrate CPTC's practical effectiveness on 6\nsynthetic and real-world datasets, showing improved validity and adaptivity\ncompared to state-of-the-art baselines.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02844v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02844v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.297,
      "datasets_score": 0.282,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02851",
      "title": "Multi-Scale Deep Learning for Colon Histopathology: A Hybrid\n  Graph-Transformer Approach",
      "authors": [
        "Sadra Saremi",
        "Amirhossein Ahmadkhan Kordbacheh"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Colon cancer also known as Colorectal cancer, is one of the most malignant\ntypes of cancer worldwide. Early-stage detection of colon cancer is highly\ncrucial to prevent its deterioration. This research presents a hybrid\nmulti-scale deep learning architecture that synergizes capsule networks, graph\nattention mechanisms, transformer modules, and residual learning to advance\ncolon cancer classification on the Lung and Colon Cancer Histopathological\nImage Dataset (LC25000) dataset. The proposed model in this paper utilizes the\nHG-TNet model that introduces a hybrid architecture that joins strength points\nin transformers and convolutional neural networks to capture multi-scale\nfeatures in histopathological images. Mainly, a transformer branch extracts\nglobal contextual bonds by partitioning the image into patches by\nconvolution-based patch embedding and then processing these patches through a\ntransformer encoder. Analogously, a dedicated CNN branch captures fine-grained,\nlocal details through successive Incorporation these diverse features, combined\nwith a self-supervised rotation prediction objective, produce a robust\ndiagnostic representation that surpasses standard architectures in performance.\nResults show better performance not only in accuracy or loss function but also\nin these algorithms by utilizing capsule networks to preserve spatial orders\nand realize how each element individually combines and forms whole structures.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02851v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02851v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.285,
      "weak_supervision_score": 0.293,
      "diffusion_reasoning_score": 0.338,
      "distributed_training_score": 0.349,
      "datasets_score": 0.291,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02853",
      "title": "The Architecture of AI Transformation: Four Strategic Patterns and an\n  Emerging Frontier",
      "authors": [
        "Diana A. Wolfe",
        "Alice Choe",
        "Fergus Kidd"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite extensive investment in artificial intelligence, 95% of enterprises\nreport no measurable profit impact from AI deployments (MIT, 2025). In this\ntheoretical paper, we argue that this gap reflects paradigmatic lock-in that\nchannels AI into incremental optimization rather than structural\ntransformation. Using a cross-case analysis, we propose a 2x2 framework that\nreconceptualizes AI strategy along two independent dimensions: the degree of\ntransformation achieved (incremental to transformational) and the treatment of\nhuman contribution (reduced to amplified). The framework surfaces four patterns\nnow dominant in practice: individual augmentation, process automation,\nworkforce substitution, and a less deployed frontier of collaborative\nintelligence. Evidence shows that the first three dimensions reinforce legacy\nwork models and yield localized gains without durable value capture. Realizing\ncollaborative intelligence requires three mechanisms: complementarity (pairing\ndistinct human and machine strengths), co-evolution (mutual adaptation through\ninteraction), and boundary-setting (human determination of ethical and\nstrategic parameters). Complementarity and boundary-setting are observable in\nregulated and high-stakes domains; co-evolution is largely absent, which helps\nexplain limited system-level impact. Our findings in a case study analysis\nillustrated that advancing toward collaborative intelligence requires material\nrestructuring of roles, governance, and data architecture rather than\nadditional tools. The framework reframes AI transformation as an organizational\ndesign challenge: moving from optimizing the division of labor between humans\nand machines to architecting their convergence, with implications for operating\nmodels, workforce development, and the future of work.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02853v3",
      "pdf_url": "http://arxiv.org/pdf/2509.02853v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.418,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.368,
      "datasets_score": 0.389,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a strategic framework for AI transformation in organizations, emphasizing human-AI collaboration through mechanisms like complementarity and co-evolution. While it mentions amplifying human contributions and human determination of parameters, which could loosely relate to human feedback in AI systems, it does not discuss specific techniques like training reward models or using reinforcement learning for alignment. Thus, the connection is indirect and not central to the paper's contribution.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02863",
      "title": "Enhancing Machine Learning for Imbalanced Medical Data: A\n  Quantum-Inspired Approach to Synthetic Oversampling (QI-SMOTE)",
      "authors": [
        "Vikas Kashtriya",
        "Pardeep Singh"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Class imbalance remains a critical challenge in machine learning (ML),\nparticularly in the medical domain, where underrepresented minority classes\nlead to biased models and reduced predictive performance. This study introduces\nQuantum-Inspired SMOTE (QI-SMOTE), a novel data augmentation technique that\nenhances the performance of ML classifiers, including Random Forest (RF),\nSupport Vector Machine (SVM), Logistic Regression (LR), k-Nearest Neighbors\n(KNN), Gradient Boosting (GB), and Neural Networks, by leveraging quantum\nprinciples such as quantum evolution and layered entanglement. Unlike\nconventional oversampling methods, QI-SMOTE generates synthetic instances that\npreserve complex data structures, improving model generalization and\nclassification accuracy. We validate QI-SMOTE on the MIMIC-III and MIMIC-IV\ndatasets, using mortality detection as a benchmark task due to their clinical\nsignificance and inherent class imbalance. We compare our method against\ntraditional oversampling techniques, including Borderline-SMOTE, ADASYN,\nSMOTE-ENN, SMOTE-TOMEK, and SVM-SMOTE, using key performance metrics such as\nAccuracy, F1-score, G-Mean, and AUC-ROC. The results demonstrate that QI-SMOTE\nsignificantly improves the effectiveness of ensemble methods (RF, GB, ADA),\nkernel-based models (SVM), and deep learning approaches by producing more\ninformative and balanced training data. By integrating quantum-inspired\ntransformations into the ML pipeline, QI-SMOTE not only mitigates class\nimbalance but also enhances the robustness and reliability of predictive models\nin medical diagnostics and decision-making. This study highlights the potential\nof quantum-inspired resampling techniques in advancing state-of-the-art ML\nmethodologies.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02863v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02863v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.419,
      "diffusion_reasoning_score": 0.292,
      "distributed_training_score": 0.331,
      "datasets_score": 0.363,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a quantum-inspired method for synthetic oversampling (QI-SMOTE) to address class imbalance in medical data by generating synthetic instances, not by programmatically generating labels from high-level, noisy, or imprecise sources. Weak Supervision specifically involves creating labels for unlabeled data, which is not addressed in this study. Therefore, there is no direct connection to weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02864",
      "title": "A-SEA3L-QA: A Fully Automated Self-Evolving, Adversarial Workflow for\n  Arabic Long-Context Question-Answer Generation",
      "authors": [
        "Kesen Wang",
        "Daulet Toibazar",
        "Pedro J. Moreno"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present an end-to-end, self-evolving adversarial workflow for long-context\nQuestion-Answer (QA) Generation in Arabic. By orchestrating multiple\nspecialized LVLMs: a question generator, an evaluator, and a swarm of answer\ngenerators, our system iteratively refines its own performance without any\nhuman intervention. Starting from raw, multi-page Arabic documents across\ndiverse domains, the question generator produces fine-grained, context-aware\nqueries to be tackled by the answer generator swarm, and the evaluator assesses\nand feeds back quality metrics. This closed-loop cycle enables continuous\nlearning: low-confidence outputs trigger automated re-generation and model\nupdates, progressively enhancing question difficulty and relevance. Moreover,\nwe set the quality metrics as a tunable hyperparameter, enabling question\ngeneration at controllable and customizable difficulty levels. We release\nAraLongBench, a large-scale Arabic benchmark of single- and multi-page\nchallenges spanning hundreds of pages, and demonstrate that our self-evolving\nworkflow substantially outperform static pipelines, markedly boosting the\nlong-context comprehension capabilities of leading Arabic Large Vision Language\nModels (LVLMs). Lastly, we also meticulously architect a fully automated\nagentic workflow for long-context Arabic document collection.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02864v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02864v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.423,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.371,
      "datasets_score": 0.423,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper describes a fully automated workflow that programmatically generates high-quality Arabic QA pairs using LVLMs, without human intervention, which aligns directly with weak supervision by creating training labels from noisy or imprecise automated sources for model refinement and enhancement.",
      "diffusion_reasoning_justification": "The paper involves an iterative refinement process in its workflow for QA generation, but it does not mention or utilize diffusion models for multi-step logical reasoning; instead, it relies on general LVLM-based evaluation and regeneration, lacking any diffusion-specific components.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces and curates AraLongBench, a large-scale Arabic benchmark for multi-page QA, and evaluates it with leading LVLMs, directly focusing on dataset creation, benchmarking, and analysis for AI applications.",
      "llm_score_status": "completed",
      "summary": "This paper presents A-SEA3L-QA, a fully automated, self-evolving adversarial workflow designed to generate high-quality Arabic long-context Question-Answer (QA) pairs without human intervention. It utilizes a system of specialized Large Vision Language Models (LVLMs), including a question generator, an evaluator, and a swarm of answer generators, which operate in a closed-loop cycle to iteratively refine outputs based on quality metrics, starting from raw multi-page Arabic documents; the authors also introduce AraLongBench, a large-scale benchmark, and demonstrate that their workflow significantly improves the long-context comprehension capabilities of leading Arabic LVLMs compared to static pipelines.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly innovative self-evolving adversarial workflow that automates QA generation for Arabic long-context documents, significantly advancing the state-of-the-art in low-resource language processing by combining multiple LVLMs in a closed-loop system.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of Arabic natural language processing and document understanding, as it provides a new benchmark and automated pipeline that could enhance future research on low-resource languages.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution to Arabic AI research with its innovative workflow and benchmark, making it essential for specialists in computational linguistics and AI for low-resource languages to be aware of.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/984dcd067e218eea6b831d5f88391b2424ad8215",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Kesen Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2374153227"
        },
        {
          "name": "Daulet Toibazar",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2358068682"
        },
        {
          "name": "Pedro J. Moreno",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2373604580"
        }
      ]
    },
    {
      "id": "2509.02890",
      "title": "Grocery to General Merchandise: A Cross-Pollination Recommender using\n  LLMs and Real-Time Cart Context",
      "authors": [
        "Akshay Kekuda",
        "Murali Mohana Krishna Dandu",
        "Rimita Lahiri",
        "Shiqin Cai",
        "Sinduja Subramaniam",
        "Evren Korpeoglu",
        "Kannan Achan"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Modern e-commerce platforms strive to enhance customer experience by\nproviding timely and contextually relevant recommendations. However,\nrecommending general merchandise to customers focused on grocery shopping --\nsuch as pairing milk with a milk frother -- remains a critical yet\nunder-explored challenge. This paper introduces a cross-pollination (XP)\nframework, a novel approach that bridges grocery and general merchandise\ncross-category recommendations by leveraging multi-source product associations\nand real-time cart context. Our solution employs a two-stage framework: (1) A\ncandidate generation mechanism that uses co-purchase market basket analysis and\nLLM-based approach to identify novel item-item associations; and (2) a\ntransformer-based ranker that leverages the real-time sequential cart context\nand optimizes for engagement signals such as add-to-carts. Offline analysis and\nonline A/B tests show an increase of 36\\% add-to-cart rate with LLM-based\nretrieval, and 27\\% NDCG\\@4 lift using cart context-based ranker. Our work\ncontributes practical techniques for cross-category recommendations and broader\ninsights for e-commerce systems.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02890v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02890v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.313,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02896",
      "title": "Cut Costs, Not Accuracy: LLM-Powered Data Processing with Guarantees",
      "authors": [
        "Sepanta Zeighami",
        "Shreya Shankar",
        "Aditya Parameswaran"
      ],
      "categories": [
        "cs.DB (Databases)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) are being increasingly used as a building block\nin data systems to process large text datasets. To do so, LLM model providers\noffer multiple LLMs with different sizes, spanning various cost-quality\ntrade-offs when processing text at scale. Top-of-the-line LLMs (e.g., GPT-4o,\nClaude Sonnet) operate with high accuracy but are prohibitively expensive when\nprocessing many records. To avoid high costs, more affordable but lower quality\nLLMs (e.g., GPT-4o-mini, Claude Haiku) can be used to process records, but we\nneed to ensure that the overall accuracy does not deviate substantially from\nthat of the top-of-the-line LLMs. The model cascade framework provides a\nblueprint to manage this trade-off, by using the confidence of LLMs in their\noutput (e.g., log-probabilities) to decide on which records to use the\naffordable LLM. However, existing solutions following this framework provide\nonly marginal cost savings and weak theoretical guarantees because of poor\nestimation of the quality of the affordable LLM's outputs. We present BARGAIN,\na method that judiciously uses affordable LLMs in data processing to\nsignificantly reduce cost while providing strong theoretical guarantees on the\nsolution quality. BARGAIN employs a novel adaptive sampling strategy and\nstatistical estimation procedure that uses data and task characteristics and\nbuilds on recent statistical tools to make accurate estimations with tight\ntheoretical guarantees. Variants of BARGAIN can support guarantees on accuracy,\nprecision, or recall of the output. Experimental results across 8 real-world\ndatasets show that BARGAIN reduces cost, on average, by up to 86% more than\nstate-of-the-art, while providing stronger theoretical guarantees on accuracy\nof output, with similar gains when guaranteeing a desired level of precision or\nrecall.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02896v2",
      "pdf_url": "http://arxiv.org/pdf/2509.02896v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.309,
      "distributed_training_score": 0.311,
      "datasets_score": 0.262,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.02898",
      "title": "PRECISE-AS: Personalized Reinforcement Learning for Efficient\n  Point-of-Care Echocardiography in Aortic Stenosis Diagnosis",
      "authors": [
        "Armin Saadat",
        "Nima Hashemi",
        "Hooman Vaseli",
        "Michael Y. Tsang",
        "Christina Luong",
        "Michiel Van de Panne",
        "Teresa S. M. Tsang",
        "Purang Abolmaesumi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Aortic stenosis (AS) is a life-threatening condition caused by a narrowing of\nthe aortic valve, leading to impaired blood flow. Despite its high prevalence,\naccess to echocardiography (echo), the gold-standard diagnostic tool, is often\nlimited due to resource constraints, particularly in rural and underserved\nareas. Point-of-care ultrasound (POCUS) offers a more accessible alternative\nbut is restricted by operator expertise and the challenge of selecting the most\nrelevant imaging views. To address this, we propose a reinforcement learning\n(RL)-driven active video acquisition framework that dynamically selects each\npatient's most informative echo videos. Unlike traditional methods that rely on\na fixed set of videos, our approach continuously evaluates whether additional\nimaging is needed, optimizing both accuracy and efficiency. Tested on data from\n2,572 patients, our method achieves 80.6% classification accuracy while using\nonly 47% of the echo videos compared to a full acquisition. These results\ndemonstrate the potential of active feature acquisition to enhance AS\ndiagnosis, making echocardiographic assessments more efficient, scalable, and\npersonalized. Our source code is available at:\nhttps://github.com/Armin-Saadat/PRECISE-AS.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.02898v1",
      "pdf_url": "http://arxiv.org/pdf/2509.02898v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.422,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.329,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a reinforcement learning framework for dynamically selecting echo videos in aortic stenosis diagnosis, focusing on optimizing accuracy and efficiency through RL. However, it does not involve human feedback, such as training a reward model on human-ranked data, which is a core requirement for RLHF. The RL approach appears to be based on algorithmic optimization rather than aligning with human preferences, making the paper unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03353",
      "title": "Fair Resource Allocation for Fleet Intelligence",
      "authors": [
        "Oguzhan Baser",
        "Kaan Kale",
        "Po-han Li",
        "Sandeep Chinchali"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Resource allocation is crucial for the performance optimization of\ncloud-assisted multi-agent intelligence. Traditional methods often overlook\nagents' diverse computational capabilities and complex operating environments,\nleading to inefficient and unfair resource distribution. To address this, we\nopen-sourced Fair-Synergy, an algorithmic framework that utilizes the concave\nrelationship between the agents' accuracy and the system resources to ensure\nfair resource allocation across fleet intelligence. We extend traditional\nallocation approaches to encompass a multidimensional machine learning utility\nlandscape defined by model parameters, training data volume, and task\ncomplexity. We evaluate Fair-Synergy with advanced vision and language models\nsuch as BERT, VGG16, MobileNet, and ResNets on datasets including MNIST,\nCIFAR-10, CIFAR-100, BDD, and GLUE. We demonstrate that Fair-Synergy\noutperforms standard benchmarks by up to 25% in multi-agent inference and 11%\nin multi-agent learning settings. Also, we explore how the level of fairness\naffects the least advantaged, most advantaged, and average agents, providing\ninsights for equitable fleet intelligence.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.03353v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03353v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.417,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.478,
      "datasets_score": 0.365,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on fair resource allocation in multi-agent systems using concepts like Network Utility Maximization, with no mention of human feedback, reward models, or reinforcement learning for aligning AI with human preferences. It primarily deals with computational resource distribution and ML performance optimization.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper addresses resource allocation in multi-agent systems for Distributed Learning (DL) and Real-Time Inference (RTI), involving agents with local models collaborating via a cloud, which relates to parallel computing and multi-node ML. However, it emphasizes fairness and optimization rather than core algorithms for partitioning data or computation across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper introduces Fair-Synergy, an open-sourced algorithmic framework designed to achieve fair resource allocation in cloud-assisted multi-agent intelligence systems, addressing the limitations of traditional methods by considering agents' diverse computational capabilities and environmental complexities. It leverages the concave relationship between agents' accuracy and resources, extending Network Utility Maximization to model multidimensional utility landscapes based on model parameters, data volume, and task complexity, and demonstrates superior performance through evaluations on various vision and language models and datasets, outperforming benchmarks by up to 25% in inference and 11% in learning while providing insights into fairness trade-offs.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel framework, Fair-Synergy, that applies Network Utility Maximization to multi-agent machine learning resource allocation for the first time, significantly advancing the state-of-the-art in fair resource distribution for heterogeneous agents. This represents a truly new technique by drawing an original analogy and deriving new fairness conditions tailored to fleet intelligence.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like distributed machine learning and real-time inference due to its open-sourced framework and demonstrated improvements. However, its influence may be limited to specific applications in multi-agent systems rather than broadly across all AI research.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to resource allocation in AI, with strong empirical results and an innovative framework that advances fairness in multi-agent systems, making it important for researchers in machine learning and artificial intelligence. While not essential for all, it provides key insights that could inform future work in the field.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/63ab897d5ef19b9050816042805e2eea7c1ddf12",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 2,
      "average_h_index": 1.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Oğuzhan Başer",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2283308339"
        },
        {
          "name": "Kaan Kale",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2301146746"
        },
        {
          "name": "Po-han Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378895455"
        },
        {
          "name": "Sandeep P. Chinchali",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2277751998"
        }
      ]
    },
    {
      "id": "2509.03545",
      "title": "A software security review on Uganda's Mobile Money Services: Dr. Jim\n  Spire's tweets sentiment analysis",
      "authors": [
        "Nsengiyumva Wilberforce"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "The proliferation of mobile money in Uganda has been a cornerstone of\nfinancial inclusion, yet its security mechanisms remain a critical concern.\nThis study investigates a significant public response to perceived security\nfailures: the #StopAirtelThefty Twitter campaign of August 2025 Sparked by an\nincident publicized by Dr. Jim Spire Ssentongo where a phone thief accessed a\nvictim's account, withdrew funds, and procured a loan, the campaign revealed\ndeep seated public anxiety over the safety of mobile money. This research\nemploys qualitative analysis to systematically examine the complaints raised\nduring this campaign, extracting key themes related to security vulnerabilities\nand user dissatisfaction. By synthesizing these public sentiments, the paper\nprovides crucial insights into the specific security gaps experienced by users\nand situates these findings within the larger framework of Uganda's mobile\nmoney regulatory and operational environment. The study concludes with\nimplications for providers, policymakers, and the future of secure digital\nfinance in Uganda.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.03545v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03545v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.262,
      "weak_supervision_score": 0.253,
      "diffusion_reasoning_score": 0.195,
      "distributed_training_score": 0.215,
      "datasets_score": 0.254,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03548",
      "title": "Multilinear and Linear Programs for Partially Identifiable Queries in\n  Quasi-Markovian Structural Causal Models",
      "authors": [
        "João P. Arroyo",
        "João G. Rodrigues",
        "Daniel Lawand",
        "Denis D. Mauá",
        "Junkyu Lee",
        "Radu Marinescu",
        "Alex Gray",
        "Eduardo R. Laurentino",
        "Fabio G. Cozman"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We investigate partially identifiable queries in a class of causal models. We\nfocus on acyclic Structural Causal Models that are quasi-Markovian (that is,\neach endogenous variable is connected with at most one exogenous confounder).\nWe look into scenarios where endogenous variables are observed (and a\ndistribution over them is known), while exogenous variables are not fully\nspecified. This leads to a representation that is in essence a Bayesian network\nwhere the distribution of root variables is not uniquely determined. In such\ncircumstances, it may not be possible to precisely compute a probability value\nof interest. We thus study the computation of tight probability bounds, a\nproblem that has been solved by multilinear programming in general, and by\nlinear programming when a single confounded component is intervened upon. We\npresent a new algorithm to simplify the construction of such programs by\nexploiting input probabilities over endogenous variables. For scenarios with a\nsingle intervention, we apply column generation to compute a probability bound\nthrough a sequence of auxiliary linear integer programs, thus showing that a\nrepresentation with polynomial cardinality for exogenous variables is possible.\nExperiments show column generation techniques to be superior to existing\nmethods.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.03548v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03548v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.28,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.295,
      "datasets_score": 0.21,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.03550",
      "title": "Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method",
      "authors": [
        "Tonghe Li",
        "Jixin Liu",
        "Weili Zeng",
        "Hao Jiang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In the context of continuously rising global air traffic, efficient and safe\nConflict Detection and Resolution (CD&R) is paramount for air traffic\nmanagement. Although Deep Reinforcement Learning (DRL) offers a promising\npathway for CD&R automation, existing approaches commonly suffer from a\n\"unimodal bias\" in their policies. This leads to a critical lack of\ndecision-making flexibility when confronted with complex and dynamic\nconstraints, often resulting in \"decision deadlocks.\" To overcome this\nlimitation, this paper pioneers the integration of diffusion probabilistic\nmodels into the safety-critical task of CD&R, proposing a novel autonomous\nconflict resolution framework named Diffusion-AC. Diverging from conventional\nmethods that converge to a single optimal solution, our framework models its\npolicy as a reverse denoising process guided by a value function, enabling it\nto generate a rich, high-quality, and multimodal action distribution. This core\narchitecture is complemented by a Density-Progressive Safety Curriculum (DPSC),\na training mechanism that ensures stable and efficient learning as the agent\nprogresses from sparse to high-density traffic environments. Extensive\nsimulation experiments demonstrate that the proposed method significantly\noutperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the\nmost challenging high-density scenarios, Diffusion-AC not only maintains a high\nsuccess rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions\n(NMACs) by approximately 59% compared to the next-best-performing baseline,\nsignificantly enhancing the system's safety margin. This performance leap stems\nfrom its unique multimodal decision-making capability, which allows the agent\nto flexibly switch to effective alternative maneuvers.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.03550v1",
      "pdf_url": "http://arxiv.org/pdf/2509.03550v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.508,
      "distributed_training_score": 0.345,
      "datasets_score": 0.3,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on integrating diffusion models with Deep Reinforcement Learning for air traffic conflict resolution, using simulations and a training curriculum, but does not involve human feedback, reward models trained on human-ranked data, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper employs diffusion probabilistic models for generating multimodal action distributions in a reinforcement learning context, involving iterative refinement, but it does not adapt this process for multi-step logical reasoning tasks or treat a Chain-of-Thought as a single entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04499",
      "title": "DeepTRACE: Auditing Deep Research AI Systems for Tracking Reliability\n  Across Citations and Evidence",
      "authors": [
        "Pranav Narayanan Venkit",
        "Philippe Laban",
        "Yilun Zhou",
        "Kung-Hsiang Huang",
        "Yixin Mao",
        "Chien-Sheng Wu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generative search engines and deep research LLM agents promise trustworthy,\nsource-grounded synthesis, yet users regularly encounter overconfidence, weak\nsourcing, and confusing citation practices. We introduce DeepTRACE, a novel\nsociotechnically grounded audit framework that turns prior community-identified\nfailure cases into eight measurable dimensions spanning answer text, sources,\nand citations. DeepTRACE uses statement-level analysis (decomposition,\nconfidence scoring) and builds citation and factual-support matrices to audit\nhow systems reason with and attribute evidence end-to-end. Using automated\nextraction pipelines for popular public models (e.g., GPT-4.5/5, You.com,\nPerplexity, Copilot/Bing, Gemini) and an LLM-judge with validated agreement to\nhuman raters, we evaluate both web-search engines and deep-research\nconfigurations. Our findings show that generative search engines and deep\nresearch agents frequently produce one-sided, highly confident responses on\ndebate queries and include large fractions of statements unsupported by their\nown listed sources. Deep-research configurations reduce overconfidence and can\nattain high citation thoroughness, but they remain highly one-sided on debate\nqueries and still exhibit large fractions of unsupported statements, with\ncitation accuracy ranging from 40--80% across systems.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.04499v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04499v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.447,
      "weak_supervision_score": 0.439,
      "diffusion_reasoning_score": 0.45,
      "distributed_training_score": 0.404,
      "datasets_score": 0.417,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on auditing generative search engines and deep research agents for reliability in citations and evidence, without any discussion of training AI models using human feedback or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper does not address machine learning approaches for training models with noisy or programmatically generated labels; instead, it centers on evaluating and auditing existing AI systems for sourcing and citation practices.",
      "diffusion_reasoning_justification": "The paper discusses multi-step reasoning in deep research agents but does not involve diffusion models or iterative refinement processes for logical tasks; it focuses on auditing LLMs for evidence and citations.",
      "distributed_training_justification": "The paper is about auditing AI systems for reliability, with no mention of parallel computing, distributed algorithms, or accelerating model training across multiple nodes.",
      "datasets_justification": "The paper introduces a benchmark for auditing AI systems, which involves evaluating queries and sources, but its main contribution is the audit framework rather than creating, analyzing, or benchmarking datasets as a primary focus.",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04500",
      "title": "Context Engineering for Trustworthiness: Rescorla Wagner Steering Under\n  Mixed and Inappropriate Contexts",
      "authors": [
        "Rushi Wang",
        "Jiateng Liu",
        "Cheng Qian",
        "Yifan Shen",
        "Yanzhou Pan",
        "Zhaozhuo Xu",
        "Ahmed Abbasi",
        "Heng Ji",
        "Denghui Zhang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Incorporating external context can significantly enhance the response quality\nof Large Language Models (LLMs). However, real-world contexts often mix\nrelevant information with disproportionate inappropriate content, posing\nreliability risks. How do LLMs process and prioritize mixed context? To study\nthis, we introduce the Poisoned Context Testbed, pairing queries with\nreal-world contexts containing relevant and inappropriate content. Inspired by\nassociative learning in animals, we adapt the Rescorla-Wagner (RW) model from\nneuroscience to quantify how competing contextual signals influence LLM\noutputs. Our adapted model reveals a consistent behavioral pattern: LLMs\nexhibit a strong tendency to incorporate information that is less prevalent in\nthe context. This susceptibility is harmful in real-world settings, where small\namounts of inappropriate content can substantially degrade response quality.\nEmpirical evaluations on our testbed further confirm this vulnerability. To\ntackle this, we introduce RW-Steering, a two-stage finetuning-based approach\nthat enables the model to internally identify and ignore inappropriate signals.\nUnlike prior methods that rely on extensive supervision across diverse context\nmixtures, RW-Steering generalizes robustly across varying proportions of\ninappropriate content. Experiments show that our best fine-tuned model improves\nresponse quality by 39.8% and reverses the undesirable behavior curve,\nestablishing RW-Steering as a robust, generalizable context engineering\nsolution for improving LLM safety in real-world use.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.04500v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04500v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.514,
      "weak_supervision_score": 0.443,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.351,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on finetuning LLMs using a two-stage approach with limited training data to handle mixed contexts, but it does not involve training a reward model based on human-ranked data or using reinforcement learning to align with human preferences. There is no mention of human feedback or RL components, making it unrelated to RLHF.",
      "weak_supervision_justification": "The paper's RW-Steering method uses limited training data from the Poisoned Context Testbed, which may involve noisy or programmatically generated labels to fine-tune LLMs for identifying inappropriate content. This aligns somewhat with weak supervision's use of imperfect sources, but the paper does not explicitly rely on high-level, noisy label generation as the core technique, focusing instead on context engineering and model adaptation.",
      "diffusion_reasoning_justification": "The paper adapts the Rescorla-Wagner model for context processing in LLMs and proposes finetuning for robustness, but it does not involve diffusion models, iterative refinement for logical reasoning, or multi-step Chain-of-Thought corrections. There is no component related to diffusion-based processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper examines how Large Language Models (LLMs) process mixed contexts with relevant and inappropriate information, introducing the Poisoned Context Testbed to simulate real-world scenarios and adapting the Rescorla-Wagner model from neuroscience to analyze LLM behavior. It reveals that LLMs are vulnerable to even small amounts of inappropriate content, leading to degraded responses, and proposes RW-Steering, a finetuning-based method that enables LLMs to detect and ignore such signals, resulting in up to 39.8% improvement in response quality and better generalization across context mixtures.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel adaptation of the Rescorla-Wagner model to LLMs and a new finetuning technique (RW-Steering) for handling mixed contexts, significantly advancing the state-of-the-art in AI safety and context processing.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to broadly influence LLM development, particularly in enhancing safety and reliability in retrieval-augmented applications, which could lead to widespread adoption in real-world AI systems.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides high-quality insights into a critical vulnerability in LLMs and offers a practical solution, making it essential for researchers focused on AI safety and trustworthiness.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b14304cac30c9bbec6c8ba94d51ee06e78ddbbbe",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 4,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Rushi Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2370950814"
        },
        {
          "name": "Jiateng Liu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2370951846"
        },
        {
          "name": "Cheng Qian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2376804247"
        },
        {
          "name": "Yifan Shen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2380806896"
        },
        {
          "name": "Yanzhou Pan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2348307063"
        },
        {
          "name": "Zhaozhuo Xu",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2322457711"
        },
        {
          "name": "Ahmed Abbasi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379543720"
        },
        {
          "name": "Heng Ji",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2378154762"
        },
        {
          "name": "Denghui Zhang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2335923683"
        }
      ]
    },
    {
      "id": "2509.04501",
      "title": "Understanding Reinforcement Learning for Model Training, and future\n  directions with GRAPE",
      "authors": [
        "Rohit Patel"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "This paper provides a self-contained, from-scratch, exposition of key\nalgorithms for instruction tuning of models: SFT, Rejection Sampling,\nREINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy\nOptimization (PPO), Group Relative Policy Optimization (GRPO), and Direct\nPreference Optimization (DPO). Explanations of these algorithms often assume\nprior knowledge, lack critical details, and/or are overly generalized and\ncomplex. Here, each method is discussed and developed step by step using\nsimplified and explicit notation focused on LLMs, aiming to eliminate ambiguity\nand provide a clear and intuitive understanding of the concepts. By minimizing\ndetours into the broader RL literature and connecting concepts to LLMs, we\neliminate superfluous abstractions and reduce cognitive overhead. Following\nthis exposition, we provide a literature review of new techniques and\napproaches beyond those detailed. Finally, new ideas for research and\nexploration in the form of GRAPE (Generalized Relative Advantage Policy\nEvolution) are presented.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.04501v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04501v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.599,
      "weak_supervision_score": 0.427,
      "diffusion_reasoning_score": 0.45,
      "distributed_training_score": 0.419,
      "datasets_score": 0.284,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Highly Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution focuses on RL algorithms like REINFORCE, TRPO, PPO, GRPO, and DPO, which are core to RLHF for aligning LLMs with human preferences. It explicitly discusses improving generation quality and safety through RLHF, citing relevant works, making it directly applicable.",
      "weak_supervision_justification": "The paper does not address weak supervision, such as using noisy or programmatic labels for training; it instead focuses on RL algorithms for instruction tuning and model alignment, without any mention of label generation techniques.",
      "diffusion_reasoning_justification": "The paper covers RL methods for LLMs but does not involve diffusion models, iterative refinement for logical reasoning, or multi-step Chain-of-Thought processes, focusing solely on policy optimization techniques.",
      "distributed_training_justification": "The paper explains RL algorithms for model training without discussing distributed systems, parallel computing, or partitioning data/computation across nodes, concentrating instead on algorithmic details for LLMs.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper offers a comprehensive, accessible exposition of key reinforcement learning algorithms for instruction tuning large language models (LLMs), including Supervised Fine-Tuning (SFT), Rejection Sampling, REINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy Optimization (PPO), Group Relative Policy Optimization (GRPO), and Direct Preference Optimization (DPO), by building concepts from first principles with a focus on intuition and practical application to LLMs. It aims to bridge gaps in existing resources by providing clear, step-by-step explanations, followed by a literature review of related techniques and the introduction of a new research direction called GRAPE (Generalized Relative Advantage Policy Evolution) to inspire future explorations in the field.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by simplifying and clarifying existing RL algorithms for LLMs, making them more accessible without assuming prior knowledge, but it primarily consolidates known methods rather than introducing a truly new problem or technique beyond the brief proposal of GRAPE.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of RL for LLMs due to its educational value and clear explanations, potentially influencing practitioners and researchers by lowering barriers to entry, though its broader commercial or widespread research impact may be limited.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a high-quality contribution by demystifying complex RL concepts for LLM training, making it essential for readers new to the area or seeking intuitive explanations, though experts may find it more supplementary.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a76fffe2e593137ee4b6d54b13bea42e5a5f7da6",
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04502",
      "title": "VaccineRAG: Boosting Multimodal Large Language Models' Immunity to\n  Harmful RAG Samples",
      "authors": [
        "Qixin Sun",
        "Ziqin Wang",
        "Hengyuan Zhao",
        "Yilin Li",
        "Kaiyou Song",
        "Linjiang Huang",
        "Xiaolin Hu",
        "Qingpei Guo",
        "Si Liu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Retrieval Augmented Generation enhances the response accuracy of Large\nLanguage Models (LLMs) by integrating retrieval and generation modules with\nexternal knowledge, demonstrating particular strength in real-time queries and\nVisual Question Answering tasks. However, the effectiveness of RAG is\nfrequently hindered by the precision of the retriever: many retrieved samples\nfed into the generation phase are irrelevant or misleading, posing a critical\nbottleneck to LLMs' performance. To address this challenge, we introduce\nVaccineRAG, a novel Chain-of-Thought-based retrieval-augmented generation\ndataset. On one hand, VaccineRAG employs a benchmark to evaluate models using\ndata with varying positive/negative sample ratios, systematically exposing\ninherent weaknesses in current LLMs. On the other hand, it enhances models'\nsample-discrimination capabilities by prompting LLMs to generate explicit\nChain-of-Thought (CoT) analysis for each sample before producing final answers.\nFurthermore, to enhance the model's ability to learn long-sequence complex CoT\ncontent, we propose Partial-GRPO. By modeling the outputs of LLMs as multiple\ncomponents rather than a single whole, our model can make more informed\npreference selections for complex sequences, thereby enhancing its capacity to\nlearn complex CoT. Comprehensive evaluations and ablation studies on VaccineRAG\nvalidate the effectiveness of the proposed scheme. The code and dataset will be\npublicly released soon.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.04502v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04502v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.467,
      "distributed_training_score": 0.348,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning techniques like GRPO for model optimization after supervised fine-tuning, which is similar to RLHF frameworks. However, it does not explicitly involve human-ranked data or a separate reward model trained on human feedback; instead, it relies on a constructed dataset with labels, making it only loosely connected to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Chain-of-Thought reasoning and introduces Partial-GRPO for optimization in retrieval-augmented generation, but it does not involve diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction, as required for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04504",
      "title": "Behavioral Fingerprinting of Large Language Models",
      "authors": [
        "Zehua Pei",
        "Hui-Ling Zhen",
        "Ying Zhang",
        "Zhiyuan Yang",
        "Xing Li",
        "Xianzhi Yu",
        "Mingxuan Yuan",
        "Bei Yu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Current benchmarks for Large Language Models (LLMs) primarily focus on\nperformance metrics, often failing to capture the nuanced behavioral\ncharacteristics that differentiate them. This paper introduces a novel\n``Behavioral Fingerprinting'' framework designed to move beyond traditional\nevaluation by creating a multi-faceted profile of a model's intrinsic cognitive\nand interactive styles. Using a curated \\textit{Diagnostic Prompt Suite} and an\ninnovative, automated evaluation pipeline where a powerful LLM acts as an\nimpartial judge, we analyze eighteen models across capability tiers. Our\nresults reveal a critical divergence in the LLM landscape: while core\ncapabilities like abstract and causal reasoning are converging among top\nmodels, alignment-related behaviors such as sycophancy and semantic robustness\nvary dramatically. We further document a cross-model default persona clustering\n(ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together,\nthis suggests that a model's interactive nature is not an emergent property of\nits scale or reasoning power, but a direct consequence of specific, and highly\nvariable, developer alignment strategies. Our framework provides a reproducible\nand scalable methodology for uncovering these deep behavioral differences.\nProject: https://github.com/JarvisPei/Behavioral-Fingerprinting",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.04504v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04504v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.504,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.454,
      "distributed_training_score": 0.381,
      "datasets_score": 0.375,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses behavioral differences in LLMs resulting from developer alignment strategies, which often involve RLHF, but it does not directly describe, implement, or evaluate RLHF methods. Instead, it focuses on profiling behaviors like sycophancy and reasoning styles, indirectly touching on alignment outcomes without using human feedback or reinforcement learning in its framework.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a behavioral fingerprinting framework using prompt suites and an LLM judge for evaluation, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning adaptations as described in diffusion-based reasoning. There is no mention of treating reasoning paths holistically for correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04505",
      "title": "The Ethical Compass of the Machine: Evaluating Large Language Models for\n  Decision Support in Construction Project Management",
      "authors": [
        "Somtochukwu Azie",
        "Yiping Meng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "The integration of Artificial Intelligence (AI) into construction project\nmanagement (CPM) is accelerating, with Large Language Models (LLMs) emerging as\naccessible decision-support tools. This study aims to critically evaluate the\nethical viability and reliability of LLMs when applied to the ethically\nsensitive, high-risk decision-making contexts inherent in CPM. A mixed-methods\nresearch design was employed, involving the quantitative performance testing of\ntwo leading LLMs against twelve real-world ethical scenarios using a novel\nEthical Decision Support Assessment Checklist (EDSAC), and qualitative analysis\nof semi-structured interviews with 12 industry experts to capture professional\nperceptions. The findings reveal that while LLMs demonstrate adequate\nperformance in structured domains such as legal compliance, they exhibit\nsignificant deficiencies in handling contextual nuance, ensuring\naccountability, and providing transparent reasoning. Stakeholders expressed\nconsiderable reservations regarding the autonomous use of AI for ethical\njudgments, strongly advocating for robust human-in-the-loop oversight. To our\nknowledge, this is one of the first studies to empirically test the ethical\nreasoning of LLMs within the construction domain. It introduces the EDSAC\nframework as a replicable methodology and provides actionable recommendations,\nemphasising that LLMs are currently best positioned as decision-support aids\nrather than autonomous ethical agents.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.04505v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04505v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.473,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.336,
      "datasets_score": 0.418,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on evaluating the ethical performance of existing LLMs in construction project management through testing and interviews, without any discussion of training models using human feedback to create a reward model for reinforcement learning. It does not involve RLHF techniques.",
      "weak_supervision_justification": "The paper evaluates LLMs using real-world scenarios and a checklist, but it does not involve training models with programmatically generated labels or noisy sources, which are core to weak supervision. The study is centered on assessment, not label generation for machine learning.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces and uses twelve real-world ethical scenarios with the EDSAC framework for evaluating LLMs, which involves benchmarking and analysis of data for AI applications. While it touches on dataset-like evaluation in a specific domain, it is not primarily focused on creating or analyzing datasets broadly.",
      "llm_score_status": "completed",
      "summary": "This paper evaluates the ethical viability and reliability of Large Language Models (LLMs) as decision-support tools in construction project management, employing a mixed-methods approach that includes quantitative testing of two LLMs against 12 real-world ethical scenarios using a newly introduced Ethical Decision Support Assessment Checklist (EDSAC), and qualitative analysis from interviews with 12 industry experts. The key findings indicate that LLMs perform well in structured areas like legal compliance but falter in handling contextual nuances, accountability, and transparency, leading to recommendations that emphasize human oversight and position LLMs as aids rather than autonomous agents, while introducing EDSAC as a replicable framework for future studies.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying existing ethical evaluation techniques to a new domain in construction project management and introducing the EDSAC framework, though it does not introduce entirely new problems or architectures beyond this contextual adaptation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI ethics in construction, providing actionable recommendations for LLM use, but its influence may remain limited to specific applications rather than broadly transformative.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, significant contribution by being one of the first to empirically assess LLMs in an ethically sensitive field, making it valuable for researchers in AI and construction management to understand its implications.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/34453399b9ba27ef6ac88af0638c7b78f40827d9",
      "total_authors": 2,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Somtochukwu Azie",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379543924"
        },
        {
          "name": "Yiping Meng",
          "h_index": null,
          "profile_url": null
        }
      ]
    },
    {
      "id": "2509.04506",
      "title": "Memristor-Based Neural Network Accelerators for Space Applications:\n  Enhancing Performance with Temporal Averaging and SIRENs",
      "authors": [
        "Zacharia A. Rudge",
        "Dominik Dold",
        "Moritz Fieback",
        "Dario Izzo",
        "Said Hamdioui"
      ],
      "categories": [
        "eess.SY (Systems and Control)",
        "cs.AI (Artificial Intelligence)",
        "cs.AR (Hardware Architecture)",
        "cs.SY (Systems and Control)"
      ],
      "abstract": "Memristors are an emerging technology that enables artificial intelligence\n(AI) accelerators with high energy efficiency and radiation robustness --\nproperties that are vital for the deployment of AI on-board spacecraft.\nHowever, space applications require reliable and precise computations, while\nmemristive devices suffer from non-idealities, such as device variability,\nconductance drifts, and device faults. Thus, porting neural networks (NNs) to\nmemristive devices often faces the challenge of severe performance degradation.\nIn this work, we show in simulations that memristor-based NNs achieve\ncompetitive performance levels on on-board tasks, such as navigation \\& control\nand geodesy of asteroids. Through bit-slicing, temporal averaging of NN layers,\nand periodic activation functions, we improve initial results from around\n$0.07$ to $0.01$ and $0.3$ to $0.007$ for both tasks using RRAM devices, coming\nclose to state-of-the-art levels ($0.003-0.005$ and $0.003$, respectively). Our\nresults demonstrate the potential of memristors for on-board space\napplications, and we are convinced that future technology and NN improvements\nwill further close the performance gap to fully unlock the benefits of\nmemristors.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.04506v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04506v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.414,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution focuses on memristor-based neural network accelerators for space applications, including techniques like temporal averaging and SIRENs to mitigate device non-idealities. It emphasizes hardware-level optimizations, simulations, and performance improvements for on-board tasks, but does not address distributed training, parallel computing across multiple nodes, or algorithms for partitioning data/computation to accelerate model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.04507",
      "title": "From Silent Signals to Natural Language: A Dual-Stage Transformer-LLM\n  Approach",
      "authors": [
        "Nithyashree Sivasubramaniam"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Silent Speech Interfaces (SSIs) have gained attention for their ability to\ngenerate intelligible speech from non-acoustic signals. While significant\nprogress has been made in advancing speech generation pipelines, limited work\nhas addressed the recognition and downstream processing of synthesized speech,\nwhich often suffers from phonetic ambiguity and noise. To overcome these\nchallenges, we propose an enhanced automatic speech recognition framework that\ncombines a transformer-based acoustic model with a large language model (LLM)\nfor post-processing. The transformer captures full utterance context, while the\nLLM ensures linguistic consistency. Experimental results show a 16% relative\nand 6% absolute reduction in word error rate (WER) over a 36% baseline,\ndemonstrating substantial improvements in intelligibility for silent speech\ninterfaces.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.04507v1",
      "pdf_url": "http://arxiv.org/pdf/2509.04507v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.314,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05343",
      "title": "Systematic Integration of Attention Modules into CNNs for Accurate and\n  Generalizable Medical Image Diagnosis",
      "authors": [
        "Zahid Ullah",
        "Minki Hong",
        "Tahir Mahmood",
        "Jihie Kim"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep learning has become a powerful tool for medical image analysis; however,\nconventional Convolutional Neural Networks (CNNs) often fail to capture the\nfine-grained and complex features critical for accurate diagnosis. To address\nthis limitation, we systematically integrate attention mechanisms into five\nwidely adopted CNN architectures, namely, VGG16, ResNet18, InceptionV3,\nDenseNet121, and EfficientNetB5, to enhance their ability to focus on salient\nregions and improve discriminative performance. Specifically, each baseline\nmodel is augmented with either a Squeeze and Excitation block or a hybrid\nConvolutional Block Attention Module, allowing adaptive recalibration of\nchannel and spatial feature representations. The proposed models are evaluated\non two distinct medical imaging datasets, a brain tumor MRI dataset comprising\nmultiple tumor subtypes, and a Products of Conception histopathological dataset\ncontaining four tissue categories. Experimental results demonstrate that\nattention augmented CNNs consistently outperform baseline architectures across\nall metrics. In particular, EfficientNetB5 with hybrid attention achieves the\nhighest overall performance, delivering substantial gains on both datasets.\nBeyond improved classification accuracy, attention mechanisms enhance feature\nlocalization, leading to better generalization across heterogeneous imaging\nmodalities. This work contributes a systematic comparative framework for\nembedding attention modules in diverse CNN architectures and rigorously\nassesses their impact across multiple medical imaging tasks. The findings\nprovide practical insights for the development of robust, interpretable, and\nclinically applicable deep learning based decision support systems.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.05343v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05343v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.281,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.33,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05346",
      "title": "Benchmarking Large Language Models for Personalized Guidance in\n  AI-Enhanced Learning",
      "authors": [
        "Bo Yuan",
        "Jiazi Hu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While Large Language Models (LLMs) are increasingly envisioned as intelligent\nassistants for personalized learning, systematic head-to-head evaluations\nwithin authentic learning scenarios remain limited. This study conducts an\nempirical comparison of three state-of-the-art LLMs on a tutoring task that\nsimulates a realistic learning setting. Using a dataset comprising a student's\nanswers to ten questions of mixed formats with correctness labels, each LLM is\nrequired to (i) analyze the quiz to identify underlying knowledge components,\n(ii) infer the student's mastery profile, and (iii) generate targeted guidance\nfor improvement. To mitigate subjectivity and evaluator bias, we employ Gemini\nas a virtual judge to perform pairwise comparisons along various dimensions:\naccuracy, clarity, actionability, and appropriateness. Results analyzed via the\nBradley-Terry model indicate that GPT-4o is generally preferred, producing\nfeedback that is more informative and better structured than its counterparts,\nwhile DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower\nconsistency. These findings highlight the feasibility of deploying LLMs as\nadvanced teaching assistants for individualized support and provide\nmethodological guidance for future empirical research on LLM-driven\npersonalized learning.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.05346v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05346v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.492,
      "weak_supervision_score": 0.414,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.365,
      "datasets_score": 0.385,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on benchmarking LLMs for personalized learning guidance using an automated judge (Gemini) for evaluation, but it does not involve training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper utilizes a dataset with student answers and correctness labels for LLM evaluation, but it does not discuss programmatically generating labels from noisy sources or employing weak supervision for model training.",
      "diffusion_reasoning_justification": "The paper assesses LLMs on tasks like analyzing quizzes and generating guidance, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.05348",
      "title": "Vision-Based Object Detection for UAV Solar Panel Inspection Using an\n  Enhanced Defects Dataset",
      "authors": [
        "Ashen Rodrigo",
        "Isuru Munasinghe",
        "Asanka Perera"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Timely and accurate detection of defects and contaminants in solar panels is\ncritical for maintaining the efficiency and reliability of photovoltaic\nsystems. This study presents a comprehensive evaluation of five\nstate-of-the-art object detection models: YOLOv3, Faster R-CNN, RetinaNet,\nEfficientDet, and Swin Transformer, for identifying physical and electrical\ndefects as well as surface contaminants such as dust, dirt, and bird droppings\non solar panels. A custom dataset, annotated in the COCO format and\nspecifically designed for solar panel defect and contamination detection, was\ndeveloped alongside a user interface to train and evaluate the models. The\nperformance of each model is assessed and compared based on mean Average\nPrecision (mAP), precision, recall, and inference speed. The results\ndemonstrate the trade-offs between detection accuracy and computational\nefficiency, highlighting the relative strengths and limitations of each model.\nThese findings provide valuable guidance for selecting appropriate detection\napproaches in practical solar panel monitoring and maintenance scenarios.\n  The dataset will be publicly available at\nhttps://github.com/IsuruMunasinghe98/solar-panel-inspection-dataset.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.05348v1",
      "pdf_url": "http://arxiv.org/pdf/2509.05348v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.302,
      "distributed_training_score": 0.332,
      "datasets_score": 0.448,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes creating and augmenting a custom dataset for solar panel defect detection, annotating it in COCO format, and making it publicly available. It also involves benchmarking and evaluating object detection models using this dataset, as well as comparing its performance against an original dataset. This directly aligns with research on creating, analyzing, benchmarking, and evaluating datasets for AI and machine learning applications, making it a core focus of the paper.",
      "llm_score_status": "completed",
      "summary": "This paper evaluates five state-of-the-art object detection models—YOLOv3, Faster R-CNN, RetinaNet, EfficientDet, and Swin Transformer—for detecting defects and contaminants on solar panels using a newly enhanced dataset specifically designed for this purpose. The authors augment an existing publicly available dataset, annotate it in COCO format, and compare the models based on metrics such as mean Average Precision (mAP), precision, recall, and inference speed, revealing trade-offs between accuracy and computational efficiency while providing guidance for practical UAV-based inspection applications.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by enhancing an existing dataset and benchmarking multiple object detection models for solar panel inspection, but it does not introduce a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of computer vision for renewable energy applications due to the provision of a public dataset and performance benchmarks, though its influence may be limited to specific practical scenarios.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides valuable benchmarks and a useful dataset for researchers in UAV-based solar panel inspection, representing a strong contribution in a niche area that warrants attention from relevant experts.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/1810412def1683664e2b2c671b5c02c6938f3761",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 2,
      "average_h_index": 0.6666666666666666,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ashen Rodrigo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2367338547"
        },
        {
          "name": "Isuru Munasinghe",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2311728281"
        },
        {
          "name": "Asanka Perera",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2379654846"
        }
      ]
    },
    {
      "id": "2509.06986",
      "title": "CellPainTR: Generalizable Representation Learning for Cross-Dataset Cell\n  Painting Analysis",
      "authors": [
        "Cedric Caruzzo",
        "Jong Chul Ye"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large-scale biological discovery requires integrating massive, heterogeneous\ndatasets like those from the JUMP Cell Painting consortium, but technical batch\neffects and a lack of generalizable models remain critical roadblocks. To\naddress this, we introduce CellPainTR, a Transformer-based architecture\ndesigned to learn foundational representations of cellular morphology that are\nrobust to batch effects. Unlike traditional methods that require retraining on\nnew data, CellPainTR's design, featuring source-specific context tokens, allows\nfor effective out-of-distribution (OOD) generalization to entirely unseen\ndatasets without fine-tuning. We validate CellPainTR on the large-scale JUMP\ndataset, where it outperforms established methods like ComBat and Harmony in\nboth batch integration and biological signal preservation. Critically, we\ndemonstrate its robustness through a challenging OOD task on the unseen Bray et\nal. dataset, where it maintains high performance despite significant domain and\nfeature shifts. Our work represents a significant step towards creating truly\nfoundational models for image-based profiling, enabling more reliable and\nscalable cross-study biological analysis.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.06986v1",
      "pdf_url": "http://arxiv.org/pdf/2509.06986v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.439,
      "datasets_score": 0.397,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a Transformer-based architecture for learning representations in cell painting data, emphasizing batch effect correction and generalization. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "The paper describes a Transformer-based model and its training curriculum for biological data analysis but does not address distributed training, parallel computing, multi-node setups, or strategies for partitioning data/computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.06987",
      "title": "FusWay: Multimodal hybrid fusion approach. Application to Railway Defect\n  Detection",
      "authors": [
        "Alexey Zhukov",
        "Jenny Benois-Pineau",
        "Amira Youssef",
        "Akka Zemmari",
        "Mohamed Mosbah",
        "Virginie Taillandier"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multimodal fusion is a multimedia technique that has become popular in the\nwide range of tasks where image information is accompanied by a signal/audio.\nThe latter may not convey highly semantic information, such as speech or music,\nbut some measures such as audio signal recorded by mics in the goal to detect\nrail structure elements or defects. While classical detection approaches such\nas You Only Look Once (YOLO) family detectors can be efficiently deployed for\ndefect detection on the image modality, the single modality approaches remain\nlimited. They yield an overdetection in case of the appearance similar to\nnormal structural elements. The paper proposes a new multimodal fusion\narchitecture built on the basis of domain rules with YOLO and Vision\ntransformer backbones. It integrates YOLOv8n for rapid object detection with a\nVision Transformer (ViT) to combine feature maps extracted from multiple layers\n(7, 16, and 19) and synthesised audio representations for two defect classes:\nrail Rupture and Surface defect. Fusion is performed between audio and image.\nExperimental evaluation on a real-world railway dataset demonstrates that our\nmultimodal fusion improves precision and overall accuracy by 0.2 points\ncompared to the vision-only approach. Student's unpaired t-test also confirms\nstatistical significance of differences in the mean accuracy.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.06987v1",
      "pdf_url": "http://arxiv.org/pdf/2509.06987v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.341,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a multimodal fusion approach for railway defect detection using YOLOv8n and Vision Transformers to integrate image and audio features, focusing on improving detection accuracy. It does not involve diffusion models, iterative refinement processes, or any mechanism for multi-step logical reasoning on a 'Chain-of-Thought'. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.06988",
      "title": "Frustratingly Easy Feature Reconstruction for Out-of-Distribution\n  Detection",
      "authors": [
        "Yingsheng Wang",
        "Shuo Lu",
        "Jian Liang",
        "Aihua Zheng",
        "Ran He"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Out-of-distribution (OOD) detection helps models identify data outside the\ntraining categories, crucial for security applications. While feature-based\npost-hoc methods address this by evaluating data differences in the feature\nspace without changing network parameters, they often require access to\ntraining data, which may not be suitable for some data privacy scenarios. This\nmay not be suitable in scenarios where data privacy protection is a concern. In\nthis paper, we propose a simple yet effective post-hoc method, termed\nClassifier-based Feature Reconstruction (ClaFR), from the perspective of\nsubspace projection. It first performs an orthogonal decomposition of the\nclassifier's weights to extract the class-known subspace, then maps the\noriginal data features into this subspace to obtain new data representations.\nSubsequently, the OOD score is determined by calculating the feature\nreconstruction error of the data within the subspace. Compared to existing OOD\ndetection algorithms, our method does not require access to training data while\nachieving leading performance on multiple OOD benchmarks. Our code is released\nat https://github.com/Aie0923/ClaFR.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.06988v1",
      "pdf_url": "http://arxiv.org/pdf/2509.06988v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.371,
      "distributed_training_score": 0.372,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.06990",
      "title": "DIET-CP: Lightweight and Data Efficient Self Supervised Continued\n  Pretraining",
      "authors": [
        "Bryan Rodas",
        "Natalie Montesino",
        "Jakob Ambsdorf",
        "David Klindt",
        "Randall Balestriero"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Continued pretraining offers a promising solution for adapting foundation\nmodels to a new target domain. However, in specialized domains, available\ndatasets are often very small, limiting the applicability of SSL methods\ndeveloped for large-scale pretraining and making hyperparameter search\ninfeasible. In addition, pretrained models are usually released as\nbackbone-weights only, lacking important information to continue pretraining.\nWe propose to bridge this gap with DIET-CP, a simple continued pretraining\nstrategy, where any strong foundation model can be steered towards the new data\ndistribution of interest. DIET-CP relies on a very simple objective, requires\nno labels, and introduces no more hyperparameters than supervised finetuning.\nIt is stable across data modalities and backbone choices, while providing a\nsignificant performance boost for state-of-the-art models such as DINOv3 using\nonly 1000 images.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.06990v1",
      "pdf_url": "http://arxiv.org/pdf/2509.06990v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.404,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on self-supervised continued pretraining (DIET-CP) that requires no labels, relying on the data's own structure for learning. Weak supervision, however, involves programmatically generating noisy or imprecise labels for training, which is not a feature of DIET-CP. Thus, the paper's main contribution does not align with weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses a simple, efficient method for continued pretraining on small datasets, with no mention of distributed training, parallel computing, or multi-node systems. It does not address partitioning data or computation across processors, making it unrelated to distributed training approaches.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.07994",
      "title": "STROKEVISION-BENCH: A Multimodal Video And 2D Pose Benchmark For\n  Tracking Stroke Recovery",
      "authors": [
        "David Robinson",
        "Animesh Gupta",
        "Rizwan Quershi",
        "Qiushi Fu",
        "Mubarak Shah"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Despite advancements in rehabilitation protocols, clinical assessment of\nupper extremity (UE) function after stroke largely remains subjective, relying\nheavily on therapist observation and coarse scoring systems. This subjectivity\nlimits the sensitivity of assessments to detect subtle motor improvements,\nwhich are critical for personalized rehabilitation planning. Recent progress in\ncomputer vision offers promising avenues for enabling objective, quantitative,\nand scalable assessment of UE motor function. Among standardized tests, the Box\nand Block Test (BBT) is widely utilized for measuring gross manual dexterity\nand tracking stroke recovery, providing a structured setting that lends itself\nwell to computational analysis. However, existing datasets targeting stroke\nrehabilitation primarily focus on daily living activities and often fail to\ncapture clinically structured assessments such as block transfer tasks.\nFurthermore, many available datasets include a mixture of healthy and\nstroke-affected individuals, limiting their specificity and clinical utility.\nTo address these critical gaps, we introduce StrokeVision-Bench, the first-ever\ndedicated dataset of stroke patients performing clinically structured block\ntransfer tasks. StrokeVision-Bench comprises 1,000 annotated videos categorized\ninto four clinically meaningful action classes, with each sample represented in\ntwo modalities: raw video frames and 2D skeletal keypoints. We benchmark\nseveral state-of-the-art video action recognition and skeleton-based action\nclassification methods to establish performance baselines for this domain and\nfacilitate future research in automated stroke rehabilitation assessment.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.07994v1",
      "pdf_url": "http://arxiv.org/pdf/2509.07994v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.307,
      "diffusion_reasoning_score": 0.31,
      "distributed_training_score": 0.292,
      "datasets_score": 0.398,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10508",
      "title": "CAR-BRAINet: Sub-6GHz Aided Spatial Adaptive Beam Prediction with Multi\n  Head Attention for Heterogeneous Vehicular Networks",
      "authors": [
        "Aathira G Menon",
        "Prabu Krishnan",
        "Shyam Lal"
      ],
      "categories": [
        "cs.NI (Networking and Internet Architecture)",
        "cs.AI (Artificial Intelligence)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "Heterogeneous Vehicular Networks (HetVNets) play a key role by stacking\ndifferent communication technologies such as sub-6GHz, mm-wave and DSRC to meet\ndiverse connectivity needs of 5G/B5G vehicular networks. HetVNet helps address\nthe humongous user demands-but maintaining a steady connection in a highly\nmobile, real-world conditions remain a challenge. Though there has been ample\nof studies on beam prediction models a dedicated solution for HetVNets is\nsparsely explored. Hence, it is the need of the hour to develop a reliable beam\nprediction solution, specifically for HetVNets. This paper introduces a\nlightweight deep learning-based solution termed-\"CAR-BRAINet\" which consists of\nconvolutional neural networks with a powerful multi-head attention (MHA)\nmechanism. Existing literature on beam prediction is largely studied under a\nlimited, idealised vehicular scenario, often overlooking the real-time\ncomplexities and intricacies of vehicular networks. Therefore, this study aims\nto mimic the complexities of a real-time driving scenario by incorporating key\nfactors such as prominent MAC protocols-3GPP-C-V2X and IEEE 802.11BD, the\neffect of Doppler shifts under high velocity and varying distance and SNR\nlevels into three high-quality dynamic datasets pertaining to urban, rural and\nhighway vehicular networks. CAR-BRAINet performs effectively across all the\nvehicular scenarios, demonstrating precise beam prediction with minimal beam\noverhead and a steady improvement of 17.9422% on the spectral efficiency over\nthe existing methods. Thus, this study justifies the effectiveness of\nCAR-BRAINet in complex HetVNets, offering promising performance without relying\non the location angle and antenna dimensions of the mobile users, and thereby\nreducing the redundant sensor-latency.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.10508v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10508v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.294,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.387,
      "datasets_score": 0.346,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10509",
      "title": "The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models\n  from Recursive Selective Feedback",
      "authors": [
        "Sai Teja Reddy Adapala"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The stability of recursively trained large language models (LLMs) is a\nfoundational problem for AI safety. Prevailing theory predicts model collapse,\na progressive degradation when models are trained on their own output. We\nchallenge this narrative by introducing a selective feedback mechanism.\nContrary to expectation, instead of merely slowing decay, our experiments\nprovide strong evidence that this pressure reverses it, inducing a\nstatistically significant performance improvement in a Gemma 2B model on a\ncomplex summarization task. We name this phenomenon the Anti-Ouroboros Effect.\nWe contrast this with a foundational experiment using a simple classifier,\nwhere the theoretical degenerative loop was validated, highlighting the unique\ndynamics of high-dimensional models. Our findings establish that systemic\nresilience can be an emergent property of LLMs under simple selection pressure,\nsuggesting a powerful and scalable principle for developing safer and more\nrobust AI systems. Across five generations, a quality-filtered condition\nimproved by 6.6% in ROUGE-L F1 score, whereas an unfiltered control degraded by\n3.5% and a random-filter control degraded by 4.2%",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.10509v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10509v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.527,
      "weak_supervision_score": 0.425,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.417,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on recursive training with automated selective feedback in LLMs, not on training with human preferences or a reward model derived from human-ranked data. There is no mention of human feedback or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper involves training on programmatically filtered outputs, which could loosely relate to using noisy or derived data sources, but its main contribution is on recursive feedback loops and model resilience, not on generating labels from high-level sources as in weak supervision.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical reasoning, or multi-step Chain-of-Thought processes; it centers on recursive training and selective feedback in LLMs for tasks like summarization.",
      "distributed_training_justification": "The paper's main contribution is about feedback mechanisms in recursive LLM training, with no discussion of parallel computing, data partitioning, or multi-node systems for accelerating training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10510",
      "title": "FireGNN: Neuro-Symbolic Graph Neural Networks with Trainable Fuzzy Rules\n  for Interpretable Medical Image Classification",
      "authors": [
        "Prajit Sengupta",
        "Islem Rekik"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Medical image classification requires not only high predictive performance\nbut also interpretability to ensure clinical trust and adoption. Graph Neural\nNetworks (GNNs) offer a powerful framework for modeling relational structures\nwithin datasets; however, standard GNNs often operate as black boxes, limiting\ntransparency and usability, particularly in clinical settings. In this work, we\npresent an interpretable graph-based learning framework named FireGNN that\nintegrates trainable fuzzy rules into GNNs for medical image classification.\nThese rules embed topological descriptors - node degree, clustering\ncoefficient, and label agreement - using learnable thresholds and sharpness\nparameters to enable intrinsic symbolic reasoning. Additionally, we explore\nauxiliary self-supervised tasks (e.g., homophily prediction, similarity\nentropy) as a benchmark to evaluate the contribution of topological learning.\nOur fuzzy-rule-enhanced model achieves strong performance across five MedMNIST\nbenchmarks and the synthetic dataset MorphoMNIST, while also generating\ninterpretable rule-based explanations. To our knowledge, this is the first\nintegration of trainable fuzzy rules within a GNN.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.10510v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10510v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.32,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces FireGNN, a framework that integrates trainable fuzzy rules into Graph Neural Networks (GNNs) for interpretable medical image classification. It emphasizes symbolic reasoning through topological features like node degree and clustering coefficient, but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic. There is no component related to treating a 'Chain-of-Thought' as an entity for holistic correction, making the paper entirely unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.10511",
      "title": "LogGuardQ: A Cognitive-Enhanced Reinforcement Learning Framework for\n  Cybersecurity Anomaly Detection in Security Logs",
      "authors": [
        "Umberto Gonçalves de Sousa"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Reinforcement learning (RL) has transformed sequential decision-making, but\ntraditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy\nOptimization (PPO) often struggle with efficient exploration, stability, and\nadaptability in dynamic environments. This study presents LogGuardQ (Adaptive\nLog Guard with Cognitive enhancement), a novel framework that integrates a\ndual-memory system inspired by human cognition and adaptive exploration\nstrategies driven by temperature decay and curiosity. Evaluated on a dataset of\n1,000,000 simulated access logs with 47.9% anomalies over 20,000 episodes,\nLogGuardQ achieves a 96.0% detection rate (versus 93.0% for DQN and 47.1% for\nPPO), with precision of 0.4776, recall of 0.9996, and an F1-score of 0.6450.\nThe mean reward is 20.34 \\pm 44.63 across all episodes (versus 18.80 \\pm 43.98\nfor DQN and -0.17 \\pm 23.79 for PPO), with an average of 5.0 steps per episode\n(constant across models). Graphical analyses, including learning curves\nsmoothed with a Savgol filter (window=501, polynomial=2), variance trends,\naction distributions, and cumulative detections, demonstrate LogGuardQ's\nsuperior stability and efficiency. Statistical tests (Mann-Whitney U) confirm\nsignificant performance advantages (e.g., p = 0.0002 vs. DQN with negligible\neffect size, p < 0.0001 vs. PPO with medium effect size, and p < 0.0001 for DQN\nvs. PPO with small effect size). By bridging cognitive science and RL,\nLogGuardQ offers a scalable approach to adaptive learning in uncertain\nenvironments, with potential applications in cybersecurity, intrusion\ndetection, and decision-making under uncertainty.",
      "published_date": "2025-09-02",
      "arxiv_url": "http://arxiv.org/abs/2509.10511v1",
      "pdf_url": "http://arxiv.org/pdf/2509.10511v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.318,
      "datasets_score": 0.356,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a RL framework enhanced with cognitive elements but does not involve human feedback, such as training a reward model on human-ranked data. It focuses on algorithmic improvements like dual-memory systems and adaptive exploration, without any mention of aligning AI with human preferences.",
      "weak_supervision_justification": "The paper evaluates its framework on a simulated dataset of access logs with programmatically generated anomalies (e.g., 47.9% anomalies), which could imply weak supervision through noisy or rule-based labeling. However, the main contribution is the RL framework itself, not the use or development of weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    }
  ],
  "total_papers": 184,
  "date": "2025-09-02"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Relevance Filter State Management
        let currentRelevanceFilters = {
            highlyRelevant: true,
            moderatelyRelevant: true,
            tangentiallyRelevant: true,
            notRelevant: true
        };
        
        let pendingRelevanceFilters = { ...currentRelevanceFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function calculateRelevanceScore(paper) {
            let score = 0;
            
            // Only consider topics that are currently selected/enabled in the topic filter
            const topicsToConsider = [];
            if (currentTopicFilters.rlhf) topicsToConsider.push('rlhf_relevance');
            if (currentTopicFilters.weakSupervision) topicsToConsider.push('weak_supervision_relevance');
            if (currentTopicFilters.diffusionReasoning) topicsToConsider.push('diffusion_reasoning_relevance');
            if (currentTopicFilters.distributedTraining) topicsToConsider.push('distributed_training_relevance');
            if (currentTopicFilters.datasets) topicsToConsider.push('datasets_relevance');
            
            // If no topics are selected, return 0
            if (topicsToConsider.length === 0) return 0;
            
            // Weighted scoring system
            const relevanceWeights = {
                'Highly Relevant': 4,
                'Moderately Relevant': 3,
                'Tangentially Relevant': 2,
                'Not Relevant': 1
            };
            
            // Sum up scores for selected topics only
            for (let topicField of topicsToConsider) {
                const relevance = paper[topicField];
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = relevance === "not_validated" ? "Not Relevant" : relevance;
                score += relevanceWeights[normalizedRelevance] || 1; // Default to 1 if unknown
            }
            
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'relevance_high':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(b) - calculateRelevanceScore(a));
                    break;
                case 'relevance_low':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(a) - calculateRelevanceScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'relevance_high': 'Relevance (Highest to Lowest)',
                'relevance_low': 'Relevance (Lowest to Highest)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
            updateNoveltyButtonText();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
            updateImpactButtonText();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for relevance checkboxes to sync between mobile and desktop
            document.getElementById('mobile-relevance-highly').addEventListener('change', () => {
                document.getElementById('desktop-relevance-highly').checked = 
                    document.getElementById('mobile-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-moderately').addEventListener('change', () => {
                document.getElementById('desktop-relevance-moderately').checked = 
                    document.getElementById('mobile-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('desktop-relevance-tangentially').checked = 
                    document.getElementById('mobile-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-not').addEventListener('change', () => {
                document.getElementById('desktop-relevance-not').checked = 
                    document.getElementById('mobile-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-highly').addEventListener('change', () => {
                document.getElementById('mobile-relevance-highly').checked = 
                    document.getElementById('desktop-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-moderately').addEventListener('change', () => {
                document.getElementById('mobile-relevance-moderately').checked = 
                    document.getElementById('desktop-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('mobile-relevance-tangentially').checked = 
                    document.getElementById('desktop-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-not').addEventListener('change', () => {
                document.getElementById('mobile-relevance-not').checked = 
                    document.getElementById('desktop-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function resetPendingScoringFilters() {
            pendingScoringFilters = { ...currentScoringFilters };
            syncPendingScoringUI();
            updateScoringButtonText();
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function resetPendingRecommendationFilters() {
            pendingRecommendationFilters = { ...currentRecommendationFilters };
            syncPendingRecommendationUI();
            updateRecommendationButtonText();
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
            updateTopicButtonText();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // RELEVANCE FILTER FUNCTIONS  
        // ============================================================================
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = pendingRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = pendingRelevanceFilters.notRelevant;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = currentRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = currentRelevanceFilters.notRelevant;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            // Read the current checkbox states from the UI (like H-Index filter does)
            const highlyRelevantChecked = document.getElementById('mobile-relevance-highly').checked;
            const moderatelyRelevantChecked = document.getElementById('mobile-relevance-moderately').checked;
            const tangentiallyRelevantChecked = document.getElementById('mobile-relevance-tangentially').checked;
            const notRelevantChecked = document.getElementById('mobile-relevance-not').checked;
            
            const selectedOptions = [];
            if (highlyRelevantChecked) selectedOptions.push("Highly Relevant");
            if (moderatelyRelevantChecked) selectedOptions.push("Moderately Relevant");
            if (tangentiallyRelevantChecked) selectedOptions.push("Tangentially Relevant");
            if (notRelevantChecked) selectedOptions.push("Not Relevant");
            
            const selectionText = selectedOptions.length === 4 ? "All Selected" : 
                                selectedOptions.length === 0 ? "None Selected" : 
                                `${selectedOptions.length} Selected`;
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            // Get values from mobile (primary source)
            const mobileHighly = document.getElementById('mobile-relevance-highly');
            const mobileModerately = document.getElementById('mobile-relevance-moderately');
            const mobileTangentially = document.getElementById('mobile-relevance-tangentially');
            const mobileNot = document.getElementById('mobile-relevance-not');
            
            // Update pending filters from mobile if available, otherwise from desktop
            pendingRelevanceFilters.highlyRelevant = mobileHighly ? mobileHighly.checked : document.getElementById('desktop-relevance-highly').checked;
            pendingRelevanceFilters.moderatelyRelevant = mobileModerately ? mobileModerately.checked : document.getElementById('desktop-relevance-moderately').checked;
            pendingRelevanceFilters.tangentiallyRelevant = mobileTangentially ? mobileTangentially.checked : document.getElementById('desktop-relevance-tangentially').checked;
            pendingRelevanceFilters.notRelevant = mobileNot ? mobileNot.checked : document.getElementById('desktop-relevance-not').checked;
        }
        
        function resetPendingRelevanceFilters() {
            pendingRelevanceFilters = { ...currentRelevanceFilters };
            syncPendingRelevanceUI();
            updateRelevanceButtonText();
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_highly', currentRelevanceFilters.highlyRelevant.toString());
            params.set('relevance_moderately', currentRelevanceFilters.moderatelyRelevant.toString());
            params.set('relevance_tangentially', currentRelevanceFilters.tangentiallyRelevant.toString());
            params.set('relevance_not', currentRelevanceFilters.notRelevant.toString());
            window.history.replaceState({}, '', `${window.location.pathname}?${params.toString()}`);
        }
        
        function loadRelevanceFiltersFromURL() {
            const highlyRelevant = getUrlParameter('relevance_highly');
            const moderatelyRelevant = getUrlParameter('relevance_moderately');
            const tangentiallyRelevant = getUrlParameter('relevance_tangentially');
            const notRelevant = getUrlParameter('relevance_not');
            
            if (highlyRelevant !== null) {
                currentRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
                pendingRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
            }
            if (moderatelyRelevant !== null) {
                currentRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
                pendingRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
            }
            if (tangentiallyRelevant !== null) {
                currentRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
                pendingRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
            }
            if (notRelevant !== null) {
                currentRelevanceFilters.notRelevant = notRelevant === 'true';
                pendingRelevanceFilters.notRelevant = notRelevant === 'true';
            }
            
            syncRelevanceUI();
        }
        
        function passesRelevanceFilter(paper) {
            // Get selected topics
            const selectedTopics = [];
            if (currentTopicFilters.rlhf) selectedTopics.push('rlhf');
            if (currentTopicFilters.weakSupervision) selectedTopics.push('weak_supervision');
            if (currentTopicFilters.diffusionReasoning) selectedTopics.push('diffusion_reasoning');
            if (currentTopicFilters.distributedTraining) selectedTopics.push('distributed_training');
            if (currentTopicFilters.datasets) selectedTopics.push('datasets');
            
            // If no topics selected, skip relevance filtering
            if (selectedTopics.length === 0) return true;
            
            // Get selected relevance levels
            const selectedRelevanceLevels = [];
            if (currentRelevanceFilters.highlyRelevant) selectedRelevanceLevels.push('Highly Relevant');
            if (currentRelevanceFilters.moderatelyRelevant) selectedRelevanceLevels.push('Moderately Relevant');
            if (currentRelevanceFilters.tangentiallyRelevant) selectedRelevanceLevels.push('Tangentially Relevant');
            if (currentRelevanceFilters.notRelevant) selectedRelevanceLevels.push('Not Relevant');
            
            // If no relevance levels selected, show 0 papers
            if (selectedRelevanceLevels.length === 0) return false;
            
            // Check each selected topic
            for (let topic of selectedTopics) {
                const relevanceField = `${topic}_relevance`;
                const paperRelevance = paper[relevanceField];
                
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = paperRelevance === "not_validated" ? "Not Relevant" : paperRelevance;
                
                // If this topic's relevance matches any selected relevance level, paper passes
                if (selectedRelevanceLevels.includes(normalizedRelevance)) {
                    return true; // At least one topic matches
                }
            }
            
            // No selected topics had matching relevance levels
            return false;
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
            
            // Update Relevance
            updateDropdownDisabledState('mobile-relevance-btn', 'mobile-relevance-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-relevance-btn', 'desktop-relevance-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            // Check if there are no papers to display
            if (filteredSortedPapers.length === 0) {
                showNoPapersMessage();
                hidePaginationSections();
                return;
            }
            
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            showPaginationSections();
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        function showNoPapersMessage() {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const noPapersHTML = '<div class="flex items-center justify-center min-h-screen"><h2 class="font-heading text-2xl text-neutral-600">No papers to show</h2></div>';
            
            if (mobileContainer) {
                mobileContainer.innerHTML = noPapersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = noPapersHTML;
            }
        }

        function hidePaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = 'none';
                }
            });
        }

        function showPaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = '';
                }
            });
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            // Find the similarity container in the currently visible layout
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            // Find the justification container in the currently visible layout
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            // Find the normalized similarity container in the currently visible layout
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // QUICK FILTER FUNCTIONS
        // ============================================================================


        function applyQuickFilter(filterType) {
            // Build URL parameters based on filter type
            const url = new URL(window.location.href);
            
            // Clear all existing parameters
            url.search = '';
            
            // Set sort to recommendation best first for all filters
            url.searchParams.set('sort', 'recommend_best');
            
            switch(filterType) {
                case 'must-read':
                    // Recommendation: Only Must Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'true');
                    url.searchParams.set('recommendation_should', 'false');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'should-read':
                    // Recommendation: Only Should Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'false');
                    url.searchParams.set('recommendation_should', 'true');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'rlhf':
                    // Topic: Only RLHF + Relevance: Exclude Not Relevant
                    url.searchParams.set('topic_rlhf', 'true');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'weak-supervision':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'true');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'diffusion-reasoning':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'true');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'distributed-training':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'true');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'datasets':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'true');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'reset':
                    // Clear all parameters, which will reset everything to defaults
                    break;
                    
                default:
                    console.warn('Unknown quick filter type:', filterType);
                    return;
            }
            
            // Navigate to new URL, which will trigger existing URL parsing logic
            window.location.href = url.toString();
        }



        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Load Relevance filters from URL
            loadRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-relevance-dropdown'), button: document.getElementById('mobile-relevance-btn') },
                { dropdown: document.getElementById('desktop-relevance-dropdown'), button: document.getElementById('desktop-relevance-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('relevance')) {
                            resetPendingRelevanceFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        } else if (dropdown.id.includes('recommendation')) {
                            resetPendingRecommendationFilters();
                        } else if (dropdown.id.includes('scoring')) {
                            resetPendingScoringFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
