<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Feed -- 21 September 2025</title>
    
    <!-- Favicon and Apple Touch Icons -->
    <link rel="icon" type="image/png" href="/favicon.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-precomposed.png">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Custom Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        'heading': ['Space Grotesk', 'Inter', 'system-ui', 'sans-serif'],
                        'body': ['Space Mono', 'Fira Code', 'Consolas', 'monospace'],
                    },
                    
                    fontSize: {
                            // 4px increments with responsive scaling
                            'xs': 'clamp(0.5rem, 1vw, 0.625rem)',     // 8-10px
                            'sm': 'clamp(0.625rem, 1.2vw, 0.75rem)',  // 10-12px
                            'md': 'clamp(0.75rem, 1.4vw, 0.875rem)',  // 12-14px
                            'lg': 'clamp(0.875rem, 1.6vw, 1rem)',     // 14-16px
                            'xl': 'clamp(1rem, 1.8vw, 1.125rem)',     // 16-18px
                            '2xl': 'clamp(1.125rem, 2vw, 1.25rem)',   // 18-20px
                            '3xl': 'clamp(1.25rem, 2.2vw, 1.375rem)', // 20-22px
                            '4xl': 'clamp(1.375rem, 2.4vw, 1.5rem)',  // 22-24px
                            '5xl': 'clamp(1.5rem, 2.6vw, 1.625rem)',  // 24-26px
                            '6xl': 'clamp(1.625rem, 2.8vw, 1.75rem)', // 26-28px
                            '7xl': 'clamp(1.75rem, 3vw, 1.875rem)',   // 28-30px
                            '8xl': 'clamp(1.875rem, 3.2vw, 2rem)',    // 30-32px
                            '9xl': 'clamp(2rem, 3.4vw, 2.125rem)',    // 32-34px
                        },

                    colors: {
                        neutral: {
                            10: '#f5f2e7',
                            20: '#e5e5e5',
                            40: '#a3a3a3',
                            60: '#525252',
                            70: '#404040',
                            90: '#171717',
                            100: '#f5f2e7',
                            200: '#dad7cd',
                            300: '#bebcb3',
                            400: '#a2a199',
                            500: '#86857f',
                            600: '#6b6a65',
                            700: '#4f4e4b',
                            900: '#171717',
                        },
                        // Status colors with 70% opacity
                        status: {
                            green: 'rgba(22, 104, 52, 0.7)',     // #166834 with 70% opacity
                            blue: 'rgba(40, 100, 156, 0.7)',     // #28649C with 70% opacity
                            orange: 'rgba(234, 147, 0, 0.7)',    // #EA9300 with 70% opacity
                            red: 'rgba(129, 12, 12, 0.7)',       // #810C0C with 70% opacity
                        },
                        bar: {
                            raw: 'rgba(107, 106, 101, 0.7)',       // #6B6A65 with 70% opacity
                            normalized: 'rgba(107, 106, 101, 0.7)' // #6B6A65 with 70% opacity
                        }
                    },
                    
                    spacing: {
                        '2xs': 'clamp(0.125rem, 0.5vw, 0.25rem)', // 2-4px
                        'xs': 'clamp(0.25rem, 1vw, 0.5rem)',    // 4-8px
                        'sm': 'clamp(0.5rem, 1.5vw, 0.75rem)',  // 8-12px
                        'md': 'clamp(0.75rem, 2vw, 1rem)',      // 12-16px
                        'lg': 'clamp(1rem, 2.5vw, 1.5rem)',     // 16-24px
                        'xl': 'clamp(1.5rem, 3vw, 2rem)',       // 24-32px
                        '2xl': 'clamp(2rem, 4vw, 3rem)',        // 32-48px
                        '3xl': 'clamp(3rem, 6vw, 4rem)',        // 48-64px
                        '4xl': 'clamp(4rem, 8vw, 5rem)',        // 64-80px
                        '5xl': 'clamp(5rem, 10vw, 6rem)',       // 80-96px
                        '6xl': 'clamp(6rem, 12vw, 7rem)',       // 96-112px
                        
                        // Mobile-specific spacing
                        'mobile-header': '5px',                  // 5px for mobile header padding
                        
                        // Card-specific spacing
                        'card-gap': '20px',                      // 20px gap for card info grid
                        
                        // Tag-specific spacing
                        'tag-x': '8px',                          // 8px horizontal padding for tags
                        'tag-y': '4px',                          // 4px vertical padding for tags
                    },
                    
                    screens: {
                        'mobile': '480px',
                        'tablet': '768px',
                        'desktop': '1024px',
                        'wide': '1440px',
                    },
                }
            }
        }
    </script>
    
    <!-- Custom CSS for additional styles -->
    <style>
        /* Focus states */
        .nav-button:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-square:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow:focus-visible {
            outline: 2px solid #86857f;
            outline-offset: 2px;
        }
        
        .pagination-arrow {
            transition: background-color 0.2s ease, opacity 0.2s ease;
        }
        
        .pagination-arrow.disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .pagination-arrow.disabled:hover {
            background-color: transparent;
        }
        
        /* Fix for multiline text padding in author and category spans */
        .metadata-tag {
            box-decoration-break: clone;
            -webkit-box-decoration-break: clone;
        }
        
        /* Sidebar styling */
        #mobile-sidebar {
            backdrop-filter: blur(4px);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #mobile-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        #desktop-sidebar {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            /* Move scrollbar to left side */
            direction: rtl;
        }
        
        #desktop-sidebar > div {
            /* Reset text direction inside sidebar */
            direction: ltr;
        }
        
        /* Mobile main container transition */
        #mobile-main-container {
            transition: transform 300ms ease-in-out;
        }
        
        /* Prevent scrolling when any sidebar is open */
        body.no-scroll {
            overflow: hidden;
        }
        
        /* Dropdown positioning */
        .dropdown-up {
            bottom: 100% !important;
            top: auto !important;
        }
        
        .dropdown-down {
            top: 100% !important;
            bottom: auto !important;
        }
        
        /* Mobile active states */
        @media (hover: none) {
            /* Remove button animation */
        }
        
        /* Font fallbacks */
        .font-mono {
            font-family: 'Space Mono', 'Fira Code', 'Consolas', monospace;
        }
        
        /* Paper title link styling */
        .paper-title-link {
            color: inherit;
            text-decoration: none;
            transition: text-decoration 0.2s ease;
        }
        
        .paper-title-link:hover {
            text-decoration: underline;
        }
        
        /* Abstract text styling */
        .abstract-text {
            transition: all 0.3s ease-in-out;
        }

        /* KaTeX styling customization */
        .katex {
            font-size: 1em !important;
            line-height: inherit !important;
        }
        
        /* Inline math styling */
        .katex-display {
            margin: 0.5em 0 !important;
            text-align: left !important;
        }
        
        /* Make KaTeX blend with our color scheme */
        .katex .base {
            color: inherit;
        }
        
        /* Ensure KaTeX math doesn't break responsive design */
        .katex-display > .katex {
            max-width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
        }
        
        /* Prevent double tap to zoom on mobile - Global */
        * {
            touch-action: manipulation;
        }
        
        /* Prevent double tap to zoom on mobile - Specific elements (keeping for compatibility) */
        .button, .interactive-element {
            touch-action: manipulation;
        }
        
        /* Research Feed button hover effect */
        .research-feed-button:hover {
            background-color: #4f4e4b !important; /* bg-neutral-700 */
            color: #f5f2e7 !important; /* text-neutral-10 */
        }
        
        /* Custom checkbox styling */
        .custom-checkbox {
            position: relative;
            display: inline-flex;
            align-items: center;
        }
        
        .custom-checkbox input[type="checkbox"] {
            opacity: 0;
            position: absolute;
            width: 0;
            height: 0;
        }
        
        .custom-checkbox label {
            display: inline-block;
            width: 20px;
            height: 20px;
            background-color: #86857f; /* bg-neutral-500 */
            border: 2px solid #f5f2e7; /* border-neutral-100 */
            border-radius: 4px; /* rounded corners */
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            flex-shrink: 0;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label {
            background-color: #f5f2e7; /* bg-neutral-100 */
            border-color: #f5f2e7;
        }
        
        .custom-checkbox input[type="checkbox"]:checked + label::after {
            content: '';
            position: absolute;
            left: 6px;
            top: 2px;
            width: 6px;
            height: 10px;
            border: solid #86857f;
            border-width: 0 2px 2px 0;
            transform: rotate(45deg);
        }
        
        /* H-Index range section styling */
        .hindex-range-section {
            transition: opacity 0.2s ease;
        }
        
        .hindex-range-section.disabled {
            opacity: 0.5;
        }
        
        .hindex-range-section.disabled input {
            cursor: not-allowed !important;
        }
        
        .hindex-range-section.disabled input:hover {
            background-color: #6b6a65 !important; /* Keep original bg when disabled */
        }
        
        /* Override any Tailwind hover effects on disabled inputs */
        .hindex-range-section.disabled input.bg-neutral-600:hover {
            background-color: #6b6a65 !important;
        }
        
        /* Advanced filter dropdowns disabled state */
        .opacity-50 {
            opacity: 0.5 !important;
        }
        
        .cursor-not-allowed {
            cursor: not-allowed !important;
        }
    </style>
</head>

<body class="bg-neutral-100 min-h-screen">
    <!-- Mobile Layout (visible < 768px) -->
    <div class="flex flex-col tablet:hidden" id="mobile-main-container">
        <!-- Mobile Header -->
        <header class="bg-neutral-100 w-full flex items-center px-xs pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="mobile-menu-btn" class="absolute top-1/4 left-xs transform -translate-y-1/2 z-10 nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Open Menu" onclick="toggleMobileMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-lg mb-md" id="page-title-mobile">
                    Papers Published on 21 September 2025
                </h1>
                
                <!-- Mobile Pagination -->
                <div class="flex items-center gap-sm mb-md">
                    <!-- Previous Arrow -->
                    <button id="mobile-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
                
                <!-- Mobile Paper Count -->
                <p id="mobile-main-paper-count" class="text-neutral-60 font-heading font-bold text-lg">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Mobile Content Area -->
        <main class="bg-neutral-100 min-h-screen">
            <div class="max-w-[500px] mx-auto">
                <!-- Mobile Papers Grid -->
                <div class="flex flex-col gap-3xl" id="mobile-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Mobile Footer -->
        <footer class="py-xl px-lg bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Mobile Footer Pagination -->
                <div class="flex items-center gap-sm">
                    <!-- Previous Arrow -->
                    <button id="mobile-footer-prev-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-sm">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex gap-sm" id="mobile-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="mobile-footer-next-btn" class="pagination-arrow w-8 h-8 bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-sm">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>
    
    <!-- Mobile Sidebar -->
    <div id="mobile-sidebar" class="fixed inset-y-0 left-0 z-50 tablet:hidden bg-neutral-100 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 100vw; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Mobile Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="mobile-close-btn" class="nav-button w-12 h-12 bg-transparent flex items-center justify-center button" aria-label="Close Menu" onclick="closeMobileMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Mobile Sidebar Content -->
            <div class="flex-1 pt-sm px-lg pb-6xl">
                <div class="flex flex-col gap-lg gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent text-left">
                        <span class="text-neutral-70 font-heading font-bold text-2xl" id="mobile-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="mobile-quick-must-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="mobile-quick-should-read" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="mobile-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="mobile-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="mobile-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="mobile-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="mobile-quick-datasets" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="mobile-quick-reset" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-sm">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-scoring-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-has" checked>
                                            <label for="mobile-scoring-has"></label>
                                        </div>
                                        <label for="mobile-scoring-has" class="text-neutral-10 text-xl font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-scoring-no" checked>
                                            <label for="mobile-scoring-no"></label>
                                        </div>
                                        <label for="mobile-scoring-no" class="text-neutral-10 text-xl font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-must" checked>
                                            <label for="mobile-recommendation-must"></label>
                                        </div>
                                        <label for="mobile-recommendation-must" class="text-neutral-10 text-xl font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-should" checked>
                                            <label for="mobile-recommendation-should"></label>
                                        </div>
                                        <label for="mobile-recommendation-should" class="text-neutral-10 text-xl font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-skip" checked>
                                            <label for="mobile-recommendation-skip"></label>
                                        </div>
                                        <label for="mobile-recommendation-skip" class="text-neutral-10 text-xl font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-recommendation-ignore" checked>
                                            <label for="mobile-recommendation-ignore"></label>
                                        </div>
                                        <label for="mobile-recommendation-ignore" class="text-neutral-10 text-xl font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-novelty-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-high" checked>
                                            <label for="mobile-novelty-high"></label>
                                        </div>
                                        <label for="mobile-novelty-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-moderate" checked>
                                            <label for="mobile-novelty-moderate"></label>
                                        </div>
                                        <label for="mobile-novelty-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-low" checked>
                                            <label for="mobile-novelty-low"></label>
                                        </div>
                                        <label for="mobile-novelty-low" class="text-neutral-10 text-xl font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-novelty-none" checked>
                                            <label for="mobile-novelty-none"></label>
                                        </div>
                                        <label for="mobile-novelty-none" class="text-neutral-10 text-xl font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-impact-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-high" checked>
                                            <label for="mobile-impact-high"></label>
                                        </div>
                                        <label for="mobile-impact-high" class="text-neutral-10 text-xl font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-moderate" checked>
                                            <label for="mobile-impact-moderate"></label>
                                        </div>
                                        <label for="mobile-impact-moderate" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-low" checked>
                                            <label for="mobile-impact-low"></label>
                                        </div>
                                        <label for="mobile-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-impact-negligible" checked>
                                            <label for="mobile-impact-negligible"></label>
                                        </div>
                                        <label for="mobile-impact-negligible" class="text-neutral-10 text-xl font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-relevance-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-highly" checked>
                                            <label for="mobile-relevance-highly"></label>
                                        </div>
                                        <label for="mobile-relevance-highly" class="text-neutral-10 text-xl font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-moderately" checked>
                                            <label for="mobile-relevance-moderately"></label>
                                        </div>
                                        <label for="mobile-relevance-moderately" class="text-neutral-10 text-xl font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-tangentially" checked>
                                            <label for="mobile-relevance-tangentially"></label>
                                        </div>
                                        <label for="mobile-relevance-tangentially" class="text-neutral-10 text-xl font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-relevance-not" checked>
                                            <label for="mobile-relevance-not"></label>
                                        </div>
                                        <label for="mobile-relevance-not" class="text-neutral-10 text-xl font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-topic-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-md">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-rlhf" checked>
                                            <label for="mobile-topic-rlhf"></label>
                                        </div>
                                        <label for="mobile-topic-rlhf" class="text-neutral-10 text-xl font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-weak-supervision" checked>
                                            <label for="mobile-topic-weak-supervision"></label>
                                        </div>
                                        <label for="mobile-topic-weak-supervision" class="text-neutral-10 text-xl font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-diffusion-reasoning" checked>
                                            <label for="mobile-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="mobile-topic-diffusion-reasoning" class="text-neutral-10 text-xl font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-distributed-training" checked>
                                            <label for="mobile-topic-distributed-training"></label>
                                        </div>
                                        <label for="mobile-topic-distributed-training" class="text-neutral-10 text-xl font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-topic-datasets" checked>
                                            <label for="mobile-topic-datasets"></label>
                                        </div>
                                        <label for="mobile-topic-datasets" class="text-neutral-10 text-xl font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="mobile-hindex-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileHIndexDropdown()">
                                H-index: All Selected <span class="text-lg">▼</span>
                            </button>
                            <div id="mobile-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="mobile-hindex-found"></label>
                                        </div>
                                        <label for="mobile-hindex-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-md">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="mobile-hindex-not-found" checked>
                                            <label for="mobile-hindex-not-found"></label>
                                        </div>
                                        <label for="mobile-hindex-not-found" class="text-neutral-10 text-xl font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="mobile-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="mobile-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="mobile-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="mobile-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="mobile-sort-btn" class="w-full py-tag-y font-heading font-bold text-xl text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleMobileSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="mobile-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-base">▼</span>
                            </button>
                            <div id="mobile-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 py-xs">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-xl text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Sidebar Overlay -->
    <div id="desktop-sidebar-overlay" class="hidden tablet:block fixed inset-0 bg-black bg-opacity-50 z-30 opacity-0 pointer-events-none transition-opacity duration-300 ease-in-out" onclick="closeDesktopMenu()"></div>
    
    <!-- Desktop Sidebar -->
    <div id="desktop-sidebar" class="hidden tablet:block fixed inset-y-0 left-0 z-40 bg-neutral-200 transition-transform duration-300 ease-in-out overflow-y-auto" style="width: 500px; transform: translateX(-100%);">
        <div class="w-full h-full flex flex-col">
            <!-- Desktop Sidebar Header -->
            <div class="flex items-center justify-between pt-lg pr-lg pb-sm pl-lg">
                <!-- Left: Research Feed Home Button -->
                <div>
                    <a href="index.html" class="research-feed-button text-center px-tag-x py-sm bg-neutral-600 transition-colors duration-200">
                        <span class="text-neutral-10 font-heading font-bold text-2xl">Research Feed</span>
                    </a>
                </div>
                
                <!-- Right: Menu Button -->
                <button id="desktop-close-btn" class="nav-button bg-transparent flex items-center justify-center button" 
                        style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                        aria-label="Close Menu" onclick="closeDesktopMenu()">
                    <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                        <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                        <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                    </svg>
                </button>
            </div>
            
            <!-- Desktop Sidebar Content -->
            <div class="flex-1 px-lg pt-lg pb-6xl">
                <div class="flex flex-col gap-lg">
                    <!-- Section 1: Paper Count -->
                    <div class="bg-transparent">
                        <span class="text-neutral-70 font-heading text-xl font-bold" id="desktop-paper-count">
                            Showing: 0/0 Papers
                        </span>
                    </div>
                    
                    <!-- Section 2: Quick Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Quick Filters</h3>
                        <button id="desktop-quick-must-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('must-read')">Must Read</button>
                        <button id="desktop-quick-should-read" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('should-read')">Should Read</button>
                        <button id="desktop-quick-rlhf" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('rlhf')">RLHF</button>
                        <button id="desktop-quick-weak-supervision" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('weak-supervision')">Weak Supervision</button>
                        <button id="desktop-quick-diffusion-reasoning" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('diffusion-reasoning')">Diffusion Reasoning</button>
                        <button id="desktop-quick-distributed-training" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('distributed-training')">Distributed Training</button>
                        <button id="desktop-quick-datasets" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('datasets')">Datasets</button>
                        <button id="desktop-quick-reset" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 hover:bg-neutral-600" onclick="applyQuickFilter('reset')">Reset To Default</button>
                    </div>
                    
                    <!-- Section 3: Advanced Filters -->
                    <div class="flex flex-col gap-xs">
                        <h3 class="text-neutral-60 font-heading font-bold text-2xl text-left bg-transparent">Advanced Filters</h3>
                        <!-- Scoring Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-scoring-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopScoringDropdown()">
                                <span class="font-bold">Scoring:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-scoring-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Section 1: Has Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-has" checked>
                                            <label for="desktop-scoring-has"></label>
                                        </div>
                                        <label for="desktop-scoring-has" class="text-neutral-10 text-lg font-heading cursor-pointer">Completed</label>
                                    </div>
                                    
                                    <!-- Section 2: Does not have Scoring and Summary Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-scoring-no" checked>
                                            <label for="desktop-scoring-no"></label>
                                        </div>
                                        <label for="desktop-scoring-no" class="text-neutral-10 text-lg font-heading cursor-pointer">Not relevant enough</label>
                                    </div>
                                    
                                    <!-- Section 3: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyScoringFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Recommendation Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-recommendation-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRecommendationDropdown()">
                                <span class="font-bold">Recommendation:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-recommendation-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Must Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-must" checked>
                                            <label for="desktop-recommendation-must"></label>
                                        </div>
                                        <label for="desktop-recommendation-must" class="text-neutral-10 text-lg font-heading cursor-pointer">Must Read</label>
                                    </div>
                                    
                                    <!-- Should Read Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-should" checked>
                                            <label for="desktop-recommendation-should"></label>
                                        </div>
                                        <label for="desktop-recommendation-should" class="text-neutral-10 text-lg font-heading cursor-pointer">Should Read</label>
                                    </div>
                                    
                                    <!-- Can Skip Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-skip" checked>
                                            <label for="desktop-recommendation-skip"></label>
                                        </div>
                                        <label for="desktop-recommendation-skip" class="text-neutral-10 text-lg font-heading cursor-pointer">Can Skip</label>
                                    </div>
                                    
                                    <!-- Ignore Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-recommendation-ignore" checked>
                                            <label for="desktop-recommendation-ignore"></label>
                                        </div>
                                        <label for="desktop-recommendation-ignore" class="text-neutral-10 text-lg font-heading cursor-pointer">Ignore</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRecommendationFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Novelty Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-novelty-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopNoveltyDropdown()">
                                <span class="font-bold">Novelty:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-novelty-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-high" checked>
                                            <label for="desktop-novelty-high"></label>
                                        </div>
                                        <label for="desktop-novelty-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-moderate" checked>
                                            <label for="desktop-novelty-moderate"></label>
                                        </div>
                                        <label for="desktop-novelty-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-low" checked>
                                            <label for="desktop-novelty-low"></label>
                                        </div>
                                        <label for="desktop-novelty-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- None Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-novelty-none" checked>
                                            <label for="desktop-novelty-none"></label>
                                        </div>
                                        <label for="desktop-novelty-none" class="text-neutral-10 text-lg font-heading cursor-pointer">None</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyNoveltyFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Potential Impact Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-impact-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopImpactDropdown()">
                                <span class="font-bold">Potential Impact:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-impact-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- High Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-high" checked>
                                            <label for="desktop-impact-high"></label>
                                        </div>
                                        <label for="desktop-impact-high" class="text-neutral-10 text-lg font-heading cursor-pointer">High</label>
                                    </div>
                                    
                                    <!-- Moderate Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-moderate" checked>
                                            <label for="desktop-impact-moderate"></label>
                                        </div>
                                        <label for="desktop-impact-moderate" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderate</label>
                                    </div>
                                    
                                    <!-- Low Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-low" checked>
                                            <label for="desktop-impact-low"></label>
                                        </div>
                                        <label for="desktop-impact-low" class="text-neutral-10 text-lg font-heading cursor-pointer">Low</label>
                                    </div>
                                    
                                    <!-- Negligible Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-impact-negligible" checked>
                                            <label for="desktop-impact-negligible"></label>
                                        </div>
                                        <label for="desktop-impact-negligible" class="text-neutral-10 text-lg font-heading cursor-pointer">Negligible</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyImpactFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        <!-- Relevance Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-relevance-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopRelevanceDropdown()">
                                <span class="font-bold">Relevance:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-relevance-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- Highly Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-highly" checked>
                                            <label for="desktop-relevance-highly"></label>
                                        </div>
                                        <label for="desktop-relevance-highly" class="text-neutral-10 text-lg font-heading cursor-pointer">Highly Relevant</label>
                                    </div>
                                    
                                    <!-- Moderately Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-moderately" checked>
                                            <label for="desktop-relevance-moderately"></label>
                                        </div>
                                        <label for="desktop-relevance-moderately" class="text-neutral-10 text-lg font-heading cursor-pointer">Moderately Relevant</label>
                                    </div>
                                    
                                    <!-- Tangentially Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-tangentially" checked>
                                            <label for="desktop-relevance-tangentially"></label>
                                        </div>
                                        <label for="desktop-relevance-tangentially" class="text-neutral-10 text-lg font-heading cursor-pointer">Tangentially Relevant</label>
                                    </div>
                                    
                                    <!-- Not Relevant Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-relevance-not" checked>
                                            <label for="desktop-relevance-not"></label>
                                        </div>
                                        <label for="desktop-relevance-not" class="text-neutral-10 text-lg font-heading cursor-pointer">Not Relevant</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyRelevanceFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Topic Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-topic-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopTopicDropdown()">
                                <span class="font-bold">Topics:</span> <span class="font-normal">All Selected</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-topic-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-xs">
                                    <!-- RLHF Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-rlhf" checked>
                                            <label for="desktop-topic-rlhf"></label>
                                        </div>
                                        <label for="desktop-topic-rlhf" class="text-neutral-10 text-lg font-heading cursor-pointer">RLHF</label>
                                    </div>
                                    
                                    <!-- Weak Supervision Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-weak-supervision" checked>
                                            <label for="desktop-topic-weak-supervision"></label>
                                        </div>
                                        <label for="desktop-topic-weak-supervision" class="text-neutral-10 text-lg font-heading cursor-pointer">Weak Supervision</label>
                                    </div>
                                    
                                    <!-- Diffusion Reasoning Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-diffusion-reasoning" checked>
                                            <label for="desktop-topic-diffusion-reasoning"></label>
                                        </div>
                                        <label for="desktop-topic-diffusion-reasoning" class="text-neutral-10 text-lg font-heading cursor-pointer">Diffusion Reasoning</label>
                                    </div>
                                    
                                    <!-- Distributed Training Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-distributed-training" checked>
                                            <label for="desktop-topic-distributed-training"></label>
                                        </div>
                                        <label for="desktop-topic-distributed-training" class="text-neutral-10 text-lg font-heading cursor-pointer">Distributed Training</label>
                                    </div>
                                    
                                    <!-- Datasets Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-topic-datasets" checked>
                                            <label for="desktop-topic-datasets"></label>
                                        </div>
                                        <label for="desktop-topic-datasets" class="text-neutral-10 text-lg font-heading cursor-pointer">Datasets</label>
                                    </div>
                                    
                                    <!-- Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyTopicFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- H-Index Filter Dropdown -->
                        <div class="relative">
                            <button id="desktop-hindex-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopHIndexDropdown()">
                                H-index: All Selected <span class="text-md">▼</span>
                            </button>
                            <div id="desktop-hindex-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50 p-md">
                                <div class="flex flex-col gap-lg">
                                    <!-- Section 1: H-Index Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-found" checked onchange="toggleHIndexRanges()">
                                            <label for="desktop-hindex-found"></label>
                                        </div>
                                        <label for="desktop-hindex-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Found</label>
                                    </div>
                                    
                                    <!-- Section 2: H-Index Not Found Checkbox -->
                                    <div class="flex items-center gap-xs">
                                        <div class="custom-checkbox">
                                            <input type="checkbox" id="desktop-hindex-not-found" checked>
                                            <label for="desktop-hindex-not-found"></label>
                                        </div>
                                        <label for="desktop-hindex-not-found" class="text-neutral-10 text-lg font-heading cursor-pointer">H-Index Not Found</label>
                                    </div>
                                    
                                    <!-- Section 3: Highest H-Index Range -->
                                    <div id="desktop-highest-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Highest H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-highest-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-highest-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 4: Average H-Index Range -->
                                    <div id="desktop-average-range" class="hindex-range-section">
                                        <div class="text-neutral-10 text-lg font-heading mb-2xs">Average H-Index Range:</div>
                                        <div class="flex items-center gap-xs">
                                            <span class="text-neutral-10 text-lg font-heading">Min:</span>
                                            <input type="number" id="desktop-average-min" min="0" max="1000" value="0" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                            <span class="text-neutral-10 text-lg font-heading">-</span>
                                            <span class="text-neutral-10 text-lg font-heading">Max:</span>
                                            <input type="number" id="desktop-average-max" min="0" max="1000" value="1000" class="bg-neutral-600 text-neutral-10 px-2xs py-2xs text-lg font-heading w-16 rounded">
                                        </div>
                                    </div>
                                    
                                    <!-- Section 5: Apply Filter Button -->
                                    <button class="w-full py-tag-y font-heading font-bold text-lg text-neutral-60 bg-neutral-300 hover:bg-neutral-500 hover:text-neutral-10 transition-colors" onclick="applyHIndexFilter()">
                                        Apply Filter
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Sort By Dropdown -->
                        <div class="relative">
                            <button id="desktop-sort-btn" class="w-full py-tag-y font-heading font-bold text-lg text-neutral-10 bg-neutral-500 text-left px-tag-x hover:bg-neutral-600" onclick="toggleDesktopSortDropdown()">
                                <span class="font-bold">Sort By:</span> <span id="desktop-sort-text" class="font-normal">Recommendation (Best First)</span> <span class="text-sm">▼</span>
                            </button>
                            <div id="desktop-sort-dropdown" class="hidden absolute top-full left-0 w-full bg-neutral-700 z-50">
                                <div class="flex flex-col gap-xs">
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_best')">Recommendation (Best First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('recommend_worst')">Recommendation (Worst First)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_high')">Relevance (Highest to Lowest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('relevance_low')">Relevance (Lowest to Highest)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_asc')">Highest H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('highest_hindex_desc')">Highest H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_asc')">Average H-Index (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('average_hindex_desc')">Average H-Index (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_asc')">arXiv ID (Ascending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('id_desc')">arXiv ID (Descending)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_az')">Title (A-Z)</button>
                                    <button class="w-full py-tag-y font-heading text-lg text-neutral-10 hover:bg-neutral-600 text-left px-tag-x" onclick="changeSortAndClose('title_za')">Title (Z-A)</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Desktop Layout (visible ≥ 768px) -->
    <div class="hidden tablet:block">
        <!-- Desktop Header -->
        <header class="bg-neutral-200 w-full flex items-center px-lg pt-xl pb-md relative">
            <!-- Menu Button - Positioned absolutely within header -->
            <button id="desktop-menu-btn" class="absolute top-1/2 left-lg transform -translate-y-1/2 z-10 nav-button bg-transparent flex items-center justify-center button" 
                    style="width: clamp(3rem, 6vw, 3.125rem); height: clamp(3rem, 6vw, 3.125rem);" 
                    aria-label="Open Menu" onclick="toggleDesktopMenu()">
                <svg width="24" height="18" viewBox="0 0 24 18" xmlns="http://www.w3.org/2000/svg">
                    <rect x="0" y="0" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="6.5" width="24" height="5" fill="#4F4E4B"/>
                    <rect x="0" y="13" width="24" height="5" fill="#4F4E4B"/>
                </svg>
            </button>
            
            <!-- Center: Page info (full width since menu button is positioned absolutely) -->
            <div class="w-full flex flex-col items-center justify-center text-center">
                <h1 class="text-neutral-70 font-heading font-bold text-4xl mb-md" id="page-title-desktop">
                    Papers Published on 21 September 2025
                </h1>
                
                <!-- Desktop Pagination -->
                <div class="flex items-center mb-md" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
                
                <!-- Desktop Paper Count -->
                <p id="desktop-main-paper-count" class="text-neutral-60 font-heading font-bold text-xl">
                    Showing 0 / 0 papers
                </p>
            </div>
        </header>
        
        <!-- Desktop Content Area -->
        <main class="px-xl py-2xl min-h-screen">
            <div class="max-w-[1400px] mx-auto">
                <!-- Desktop Papers Grid -->
                <div class="flex flex-col gap-3xl" id="desktop-papers">
                    <!-- Paper cards will be populated by JavaScript -->
                </div>
            </div>
        </main>
        
        <!-- Desktop Footer -->
        <footer class="py-xl bg-neutral-200">
            <div class="flex flex-col items-center justify-center text-center">
                <!-- Desktop Footer Pagination -->
                <div class="flex items-center" style="gap: clamp(0.5rem, 1vw, 0.75rem);">
                    <!-- Previous Arrow -->
                    <button id="desktop-footer-prev-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage - 1)">
                        <span class="font-heading font-bold text-md">‹</span>
                    </button>
                    
                    <!-- Page Numbers Container -->
                    <div class="flex" style="gap: clamp(0.5rem, 1vw, 0.75rem);" id="desktop-footer-pagination-numbers">
                        <!-- Page numbers will be populated by JavaScript -->
                    </div>
                    
                    <!-- Next Arrow -->
                    <button id="desktop-footer-next-btn" class="pagination-arrow bg-transparent text-neutral-70 flex items-center justify-center hover:bg-neutral-300 cursor-pointer" 
                            style="width: clamp(1.5rem, 3vw, 1.875rem); height: clamp(1.5rem, 3vw, 1.875rem);" onclick="goToPage(currentPage + 1)">
                        <span class="font-heading font-bold text-md">›</span>
                    </button>
                </div>
            </div>
        </footer>
    </div>

    <!-- Embedded Paper Data - This will be populated by the builder script -->
    <script>
        // This JSON structure will be injected by the builder script
        // Expected structure:
        // {
        //   "papers": [
        //     {
        //       "id": "2407.xxxxx",
        //       "title": "Paper title with possible LaTeX: $\\alpha$ notation",
        //       "authors": ["Author 1", "Author 2"],
        //       "categories": ["cs.LG", "cs.AI"],
        //       "abstract": "Abstract text with possible LaTeX notation",
        //       "published_date": "2025-07-15",
        //       "arxiv_url": "https://arxiv.org/abs/2407.xxxxx",
        //       "pdf_url": "https://arxiv.org/pdf/2407.xxxxx.pdf",
        //       "summary": "AI generated summary",
        //       "recommendation_score": "Must Read",
        //       "recommendation_justification": "Justification text",
        //       "novelty_score": "High",
        //       "novelty_justification": "Novelty justification",
        //       "impact_score": "High", 
        //       "impact_justification": "Impact justification",
        //       "rlhf_score": 0.85,
        //       "weak_supervision_score": 0.72,
        //       "diffusion_reasoning_score": 0.15,
        //       "distributed_training_score": 0.05,
        //       "datasets_score": 0.92,
        //       "rlhf_relevance": "Highly Relevant",
        //       "weak_supervision_relevance": "Moderately Relevant", 
        //       "diffusion_reasoning_relevance": "Not Relevant",
        //       "distributed_training_relevance": "Not Relevant",
        //       "datasets_relevance": "Highly Relevant",
        //       "rlhf_justification": "Relevance justification text",
        //       "weak_supervision_justification": "Relevance justification text",
        //       "diffusion_reasoning_justification": "below_threshold",
        //       "distributed_training_justification": "below_threshold", 
        //       "datasets_justification": "Relevance justification text",
        //       "h_index_status": "completed",
        //       "semantic_scholar_url": "https://www.semanticscholar.org/...",
        //       "total_authors": 3,
        //       "authors_found": 2,
        //       "highest_h_index": 45,
        //       "average_h_index": 28.5,
        //       "notable_authors_count": 2,
        //       "author_h_indexes": [
        //         {"name": "Author 1", "h_index": 45, "profile_url": "https://..."},
        //         {"name": "Author 2", "h_index": 12, "profile_url": "https://..."}
        //       ],
        //       "llm_score_status": "completed" // or "not_relevant_enough"
        //     }
        //   ],
        //   "total_papers": 25,
        //   "date": "2025-07-15"
        // }
        const PAPER_DATA = {
  "papers": [
    {
      "id": "2509.16853",
      "title": "ISCS: Parameter-Guided Channel Ordering and Grouping for Learned Image\n  Compression",
      "authors": [
        "Jinhao Wang",
        "Cihan Ruan",
        "Nam Ling",
        "Wei Wang",
        "Wei Jiang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Prior studies in learned image compression (LIC) consistently show that only\na small subset of latent channels is critical for reconstruction, while many\nothers carry limited information. Exploiting this imbalance could improve both\ncoding and computational efficiency, yet existing approaches often rely on\ncostly, dataset-specific ablation tests and typically analyze channels in\nisolation, ignoring their interdependencies.\n  We propose a generalizable, dataset-agnostic method to identify and organize\nimportant channels in pretrained VAE-based LIC models. Instead of brute-force\nempirical evaluations, our approach leverages intrinsic parameter\nstatistics-weight variances, bias magnitudes, and pairwise correlations-to\nestimate channel importance. This analysis reveals a consistent organizational\nstructure, termed the Invariant Salient Channel Space (ISCS), where\nSalient-Core channels capture dominant structures and Salient-Auxiliary\nchannels provide complementary details. Building on ISCS, we introduce a\ndeterministic channel ordering and grouping strategy that enables\nslice-parallel decoding, reduces redundancy, and improves bitrate efficiency.\n  Experiments across multiple LIC architectures demonstrate that our method\neffectively reduces bitrate and computation while maintaining reconstruction\nquality, providing a practical and modular enhancement to existing learned\ncompression frameworks.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16853v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16853v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.356,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16857",
      "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\n  Caching",
      "authors": [
        "Xingyu Xiang",
        "Raj Joshi",
        "Yuhan Liu",
        "Jiayi Yao",
        "Chenxingyu Zhao",
        "Junchen Jiang",
        "Yang Zhou",
        "Eddie Kohler",
        "Minlan Yu"
      ],
      "categories": [
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16857v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16857v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.322,
      "distributed_training_score": 0.482,
      "datasets_score": 0.268,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on ShadowServe, a system for optimizing distributed prefix caching in LLM serving, specifically addressing KV cache fetching and decompression using SmartNICs to reduce interference and improve inference performance. While it involves distributed systems and parallel computing elements (e.g., offloading tasks to SmartNICs and managing resources across nodes), its primary contribution is to inference and serving, not model training. Distributed training typically involves partitioning data or models for training acceleration, which is not addressed here, making the connection indirect.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16859",
      "title": "The Principles of Human-like Conscious Machine",
      "authors": [
        "Fangfang Li",
        "Xiaojie Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Determining whether another system, biological or artificial, possesses\nphenomenal consciousness has long been a central challenge in consciousness\nstudies. This attribution problem has become especially pressing with the rise\nof large language models and other advanced AI systems, where debates about \"AI\nconsciousness\" implicitly rely on some criterion for deciding whether a given\nsystem is conscious. In this paper, we propose a substrate-independent,\nlogically rigorous, and counterfeit-resistant sufficiency criterion for\nphenomenal consciousness. We argue that any machine satisfying this criterion\nshould be regarded as conscious with at least the same level of confidence with\nwhich we attribute consciousness to other humans. Building on this criterion,\nwe develop a formal framework and specify a set of operational principles that\nguide the design of systems capable of meeting the sufficiency condition. We\nfurther argue that machines engineered according to this framework can, in\nprinciple, realize phenomenal consciousness. As an initial validation, we show\nthat humans themselves can be viewed as machines that satisfy this framework\nand its principles. If correct, this proposal carries significant implications\nfor philosophy, cognitive science, and artificial intelligence. It offers an\nexplanation for why certain qualia, such as the experience of red, are in\nprinciple irreducible to physical description, while simultaneously providing a\ngeneral reinterpretation of human information processing. Moreover, it suggests\na path toward a new paradigm of AI beyond current statistics-based approaches,\npotentially guiding the construction of genuinely human-like AI.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16859v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16859v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.383,
      "weak_supervision_score": 0.292,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.282,
      "datasets_score": 0.301,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16861",
      "title": "AdaptiveGuard: Towards Adaptive Runtime Safety for LLM-Powered Software",
      "authors": [
        "Rui Yang",
        "Michael Fu",
        "Chakkrit Tantithamthavorn",
        "Chetan Arora",
        "Gunel Gulmammadova",
        "Joey Chua"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Guardrails are critical for the safe deployment of Large Language Models\n(LLMs)-powered software. Unlike traditional rule-based systems with limited,\npredefined input-output spaces that inherently constrain unsafe behavior, LLMs\nenable open-ended, intelligent interactions--opening the door to jailbreak\nattacks through user inputs. Guardrails serve as a protective layer, filtering\nunsafe prompts before they reach the LLM. However, prior research shows that\njailbreak attacks can still succeed over 70% of the time, even against advanced\nmodels like GPT-4o. While guardrails such as LlamaGuard report up to 95%\naccuracy, our preliminary analysis shows their performance can drop sharply--to\nas low as 12%--when confronted with unseen attacks. This highlights a growing\nsoftware engineering challenge: how to build a post-deployment guardrail that\nadapts dynamically to emerging threats? To address this, we propose\nAdaptiveGuard, an adaptive guardrail that detects novel jailbreak attacks as\nout-of-distribution (OOD) inputs and learns to defend against them through a\ncontinual learning framework. Through empirical evaluation, AdaptiveGuard\nachieves 96% OOD detection accuracy, adapts to new attacks in just two update\nsteps, and retains over 85% F1-score on in-distribution data post-adaptation,\noutperforming other baselines. These results demonstrate that AdaptiveGuard is\na guardrail capable of evolving in response to emerging jailbreak strategies\npost deployment. We release our AdaptiveGuard and studied datasets at\nhttps://github.com/awsm-research/AdaptiveGuard to support further research.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16861v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16861v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.472,
      "weak_supervision_score": 0.438,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.396,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on AdaptiveGuard, which uses out-of-distribution detection and continual learning to adapt guardrails for LLMs against jailbreak attacks. It does not involve human feedback, reward models, or reinforcement learning techniques for model alignment, making it unrelated to RLHF.",
      "weak_supervision_justification": "The paper proposes training AdaptiveGuard with OOD-aware loss and adapting via continual learning, but it does not describe programmatically generating labels from noisy sources or rely on weak supervision methods. The emphasis is on detecting and adapting to new attacks, not on label generation techniques.",
      "diffusion_reasoning_justification": "The paper deals with OOD detection and adaptive guardrails for LLMs, without any mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. There is no component involving diffusion-based methods for reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16863",
      "title": "ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D\n  Gaussian Splatting SLAM",
      "authors": [
        "Amanuel T. Dufera",
        "Yuan-Li Cai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce ConfidentSplat, a novel 3D Gaussian Splatting (3DGS)-based SLAM\nsystem for robust, highfidelity RGB-only reconstruction. Addressing geometric\ninaccuracies in existing RGB-only 3DGS SLAM methods that stem from unreliable\ndepth estimation, ConfidentSplat incorporates a core innovation: a\nconfidence-weighted fusion mechanism. This mechanism adaptively integrates\ndepth cues from multiview geometry with learned monocular priors (Omnidata\nViT), dynamically weighting their contributions based on explicit reliability\nestimates-derived predominantly from multi-view geometric consistency-to\ngenerate high-fidelity proxy depth for map supervision. The resulting proxy\ndepth guides the optimization of a deformable 3DGS map, which efficiently\nadapts online to maintain global consistency following pose updates from a\nDROID-SLAM-inspired frontend and backend optimizations (loop closure, global\nbundle adjustment). Extensive validation on standard benchmarks (TUM-RGBD,\nScanNet) and diverse custom mobile datasets demonstrates significant\nimprovements in reconstruction accuracy (L1 depth error) and novel view\nsynthesis fidelity (PSNR, SSIM, LPIPS) over baselines, particularly in\nchallenging conditions. ConfidentSplat underscores the efficacy of principled,\nconfidence-aware sensor fusion for advancing state-of-the-art dense visual\nSLAM.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16863v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16863v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.321,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16865",
      "title": "Large Language Models as End-to-end Combinatorial Optimization Solvers",
      "authors": [
        "Xia Jiang",
        "Yaoxin Wu",
        "Minshuo Li",
        "Zhiguang Cao",
        "Yingqian Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Combinatorial optimization (CO) problems, central to decision-making\nscenarios like logistics and manufacturing, are traditionally solved using\nproblem-specific algorithms requiring significant domain expertise. While large\nlanguage models (LLMs) have shown promise in automating CO problem solving,\nexisting approaches rely on intermediate steps such as code generation or\nsolver invocation, limiting their generality and accessibility. This paper\nintroduces a novel framework that empowers LLMs to serve as end-to-end CO\nsolvers by directly mapping natural language problem descriptions to solutions.\nWe propose a two-stage training strategy: supervised fine-tuning (SFT) imparts\nLLMs with solution generation patterns from domain-specific solvers, while a\nfeasibility-and-optimality-aware reinforcement learning (FOARL) process\nexplicitly mitigates constraint violations and refines solution quality.\nEvaluation across seven NP-hard CO problems shows that our method achieves a\nhigh feasibility rate and reduces the average optimality gap to 1.03-8.20% by\ntuning a 7B-parameter LLM, surpassing both general-purpose LLMs (e.g., GPT-4o),\nreasoning models (e.g., DeepSeek-R1), and domain-specific heuristics. Our\nmethod establishes a unified language-based pipeline for CO without extensive\ncode execution or manual architectural adjustments for different problems,\noffering a general and language-driven alternative to traditional solver design\nwhile maintaining relative feasibility guarantees.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16865v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16865v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.434,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.487,
      "distributed_training_score": 0.408,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses a custom reinforcement learning method called FOARL to refine the LLM for feasibility and optimality in CO problems, but it does not involve human feedback, human-ranked data, or a reward model trained on human preferences. RLHF specifically requires these elements, which are absent here.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on supervised fine-tuning and reinforcement learning for LLMs to solve CO problems, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for logical tasks.",
      "distributed_training_justification": "The paper describes fine-tuning an LLM using LoRA for efficiency, but it does not discuss distributed training, parallel computing, multi-node setups, or strategies for partitioning data or computation across processors.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16866",
      "title": "seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of\n  LLMs",
      "authors": [
        "Mohammad Ramezanali",
        "Mo Vazifeh",
        "Paolo Santi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We introduce seqBench, a parametrized benchmark for probing sequential\nreasoning limits in Large Language Models (LLMs) through precise,\nmulti-dimensional control over several key complexity dimensions. seqBench\nallows systematic variation of (1) the logical depth, defined as the number of\nsequential actions required to solve the task; (2) the number of backtracking\nsteps along the optimal path, quantifying how often the agent must revisit\nprior states to satisfy deferred preconditions (e.g., retrieving a key after\nencountering a locked door); and (3) the noise ratio, defined as the ratio\nbetween supporting and distracting facts about the environment. Our evaluations\non state-of-the-art LLMs reveal a universal failure pattern: accuracy collapses\nexponentially beyond a model-specific logical depth. Unlike existing\nbenchmarks, seqBench's fine-grained control facilitates targeted analyses of\nthese reasoning failures, illuminating universal scaling laws and statistical\nlimits, as detailed in this paper alongside its generation methodology and\nevaluation metrics. We find that even top-performing models systematically fail\non seqBench's structured reasoning tasks despite minimal search complexity,\nunderscoring key limitations in their commonsense reasoning capabilities.\nDesigned for future evolution to keep pace with advancing models, the seqBench\ndatasets are publicly released to spur deeper scientific inquiry into LLM\nreasoning, aiming to establish a clearer understanding of their true potential\nand current boundaries for robust real-world application.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16866v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16866v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.385,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.523,
      "distributed_training_score": 0.378,
      "datasets_score": 0.418,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a benchmark for evaluating sequential reasoning in LLMs but does not mention or involve diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for correction. It focuses solely on traditional LLM evaluation, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of seqBench, a new benchmark dataset for probing LLM reasoning, including its generation methodology, evaluation metrics, and analysis of performance limits. This directly aligns with research on creating, benchmarking, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "seqBench is a novel, tunable benchmark introduced to systematically evaluate the sequential reasoning limitations of Large Language Models (LLMs) by controlling key factors such as logical depth (number of sequential actions), backtracking steps (revisiting prior states), and noise ratio (proportion of distracting facts) in synthetic pathfinding tasks on grids. The paper's evaluations on state-of-the-art LLMs reveal exponential accuracy declines beyond model-specific thresholds, underscoring persistent commonsense reasoning failures and providing a scalable, publicly available framework for deeper analysis of these limitations.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark with multi-dimensional control over reasoning complexities, significantly advancing the state-of-the-art in evaluating LLMs by enabling precise attribution of failure modes that existing benchmarks cannot isolate.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence a wide range of future research in LLM development and evaluation, as its publicly released framework could be adopted for training, fine-tuning, and analyzing reasoning capabilities across various applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution by highlighting critical LLM limitations and providing a practical tool for the community, making it essential for researchers focused on AI reasoning but not universally mandatory.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/152f89cc7115f4a39270355ae2185b3f9f038299",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 5,
      "average_h_index": 1.6666666666666667,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Mohammad Ramezanali",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381361499"
        },
        {
          "name": "M. Vazifeh",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2333130"
        },
        {
          "name": "Paolo Santi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381361647"
        }
      ]
    },
    {
      "id": "2509.16869",
      "title": "PhysHDR: When Lighting Meets Materials and Scene Geometry in HDR\n  Reconstruction",
      "authors": [
        "Hrishav Bakul Barua",
        "Kalin Stefanov",
        "Ganesh Krishnasamy",
        "KokSheik Wong",
        "Abhinav Dhall"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "cs.MM (Multimedia)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Low Dynamic Range (LDR) to High Dynamic Range (HDR) image translation is a\nfundamental task in many computational vision problems. Numerous data-driven\nmethods have been proposed to address this problem; however, they lack explicit\nmodeling of illumination, lighting, and scene geometry in images. This limits\nthe quality of the reconstructed HDR images. Since lighting and shadows\ninteract differently with different materials, (e.g., specular surfaces such as\nglass and metal, and lambertian or diffuse surfaces such as wood and stone),\nmodeling material-specific properties (e.g., specular and diffuse reflectance)\nhas the potential to improve the quality of HDR image reconstruction. This\npaper presents PhysHDR, a simple yet powerful latent diffusion-based generative\nmodel for HDR image reconstruction. The denoising process is conditioned on\nlighting and depth information and guided by a novel loss to incorporate\nmaterial properties of surfaces in the scene. The experimental results\nestablish the efficacy of PhysHDR in comparison to a number of recent\nstate-of-the-art methods.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16869v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16869v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.485,
      "distributed_training_score": 0.328,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using a latent diffusion model for HDR image reconstruction, specifically conditioning on lighting, depth, and material properties to improve image quality. While it employs the iterative denoising process of diffusion models, it applies this to visual data processing and generation, not to solving complex logical tasks or multi-step reasoning as defined in the topic. There is no component for treating a 'Chain-of-Thought' or holistic correction in logical reasoning paths, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16873",
      "title": "$\\mathtt{M^3VIR}$: A Large-Scale Multi-Modality Multi-View Synthesized\n  Benchmark Dataset for Image Restoration and Content Creation",
      "authors": [
        "Yuanzhi Li",
        "Lebin Zhou",
        "Nam Ling",
        "Zhenghao Chen",
        "Wei Wang",
        "Wei Jiang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The gaming and entertainment industry is rapidly evolving, driven by\nimmersive experiences and the integration of generative AI (GAI) technologies.\nTraining such models effectively requires large-scale datasets that capture the\ndiversity and context of gaming environments. However, existing datasets are\noften limited to specific domains or rely on artificial degradations, which do\nnot accurately capture the unique characteristics of gaming content. Moreover,\nbenchmarks for controllable video generation remain absent.\n  To address these limitations, we introduce $\\mathtt{M^3VIR}$, a large-scale,\nmulti-modal, multi-view dataset specifically designed to overcome the\nshortcomings of current resources. Unlike existing datasets, $\\mathtt{M^3VIR}$\nprovides diverse, high-fidelity gaming content rendered with Unreal Engine 5,\noffering authentic ground-truth LR-HR paired and multi-view frames across 80\nscenes in 8 categories. It includes $\\mathtt{M^3VIR\\_MR}$ for super-resolution\n(SR), novel view synthesis (NVS), and combined NVS+SR tasks, and\n$\\mathtt{M^3VIR\\_{MS}}$, the first multi-style, object-level ground-truth set\nenabling research on controlled video generation. Additionally, we benchmark\nseveral state-of-the-art SR and NVS methods to establish performance baselines.\nWhile no existing approaches directly handle controlled video generation,\n$\\mathtt{M^3VIR}$ provides a benchmark for advancing this area. By releasing\nthe dataset, we aim to facilitate research in AI-powered restoration,\ncompression, and controllable content generation for next-generation cloud\ngaming and entertainment.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16873v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16873v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.321,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.347,
      "datasets_score": 0.428,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of the M^3VIR dataset, a large-scale, multi-modality, multi-view benchmark for image restoration and content creation in gaming. It details the dataset's creation using Unreal Engine 5, includes subsets like M^3VIR_MR and M^3VIR_MS for specific tasks, and provides benchmarks for state-of-the-art methods, directly aligning with research on creating, analyzing, benchmarking, and evaluating datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces M^3VIR, a large-scale, multi-modal, and multi-view benchmark dataset created using Unreal Engine 5, designed to address gaps in existing resources for image restoration and content creation in gaming scenarios. It includes subsets like M^3VIR_MR for tasks such as super-resolution (SR), novel view synthesis (NVS), and their combination, along with M^3VIR_MS for controllable video generation, and provides baseline evaluations of state-of-the-art methods to facilitate advancements in AI-powered media delivery and immersive experiences.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark dataset specifically tailored for gaming content, providing authentic multi-modal data and novel subsets for advanced tasks like combined NVS+SR and controllable video generation, which significantly advances the state-of-the-art in computer vision datasets.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of computer vision for gaming and entertainment, as it offers a specialized dataset that could improve AI models for restoration and generation, though its influence may be limited to specific applications rather than widespread fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution by providing a much-needed benchmark for gaming-related AI research, making it essential for specialists in computer vision and generative AI to be aware of for advancing their work.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/69cf977a191b1844dbb870b27703cf5accce56de",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 18,
      "average_h_index": 4.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Yuanzhi Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2302760432"
        },
        {
          "name": "Lebin Zhou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2319022164"
        },
        {
          "name": "Nam Ling",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2318974516"
        },
        {
          "name": "Zhenghao Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381821885"
        },
        {
          "name": "Wei Wang",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2158506202"
        },
        {
          "name": "Wei Jiang",
          "h_index": 18,
          "profile_url": "https://www.semanticscholar.org/author/144537035"
        }
      ]
    },
    {
      "id": "2509.16875",
      "title": "Towards Interpretable and Efficient Attention: Compressing All by\n  Contracting a Few",
      "authors": [
        "Qishuai Wen",
        "Zhiyuan Huang",
        "Chun-Guang Li"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Attention mechanisms in Transformers have gained significant empirical\nsuccess. Nonetheless, the optimization objectives underlying their forward pass\nare still unclear. Additionally, the quadratic complexity of self-attention is\nincreasingly prohibitive. Unlike the prior work on addressing the\ninterpretability or efficiency issue separately, we propose a unified\noptimization objective to alleviate both issues simultaneously. By unrolling\nthe optimization over the objective, we derive an inherently interpretable and\nefficient attention mechanism, which compresses all tokens into low-dimensional\nstructures by contracting a few representative tokens and then broadcasting the\ncontractions back. This Contract-and-Broadcast Self-Attention (CBSA) mechanism\ncan not only scale linearly but also generalize existing attention mechanisms\nas its special cases. Experiments further demonstrate comparable performance\nand even superior advantages of CBSA on several visual tasks. Code is available\nat this https URL.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16875v2",
      "pdf_url": "http://arxiv.org/pdf/2509.16875v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.371,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on developing an interpretable and efficient attention mechanism for Transformers, specifically the Contract-and-Broadcast Self-Attention (CBSA), which addresses issues like quadratic complexity and interpretability through token compression. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. There is no mention of adapting diffusion for Chain-of-Thought or holistic reasoning, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16882",
      "title": "Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free\n  Multi-Domain MoE Adaptation",
      "authors": [
        "Junzhuo Li",
        "Bo Wang",
        "Xiuze Zhou",
        "Xuming Hu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Mixture-of-Experts (MoE) models offer immense capacity via sparsely gated\nexpert subnetworks, yet adapting them to multiple domains without catastrophic\nforgetting remains an open challenge. Existing approaches either incur\nprohibitive computation, suffer cross-domain interference, or require separate\nruns per domain. We propose DES-MoE, a dynamic expert specialization framework\nfor multi-domain adaptation of Mixture-of-Experts models. DES-MoE addresses\ncatastrophic forgetting through three innovations: (1) an adaptive router\nbalancing pre-trained knowledge retention and task-specific updates via\ndistillation, (2) real-time expert-domain correlation mapping to isolate\ndomain-specific gradients, and (3) a three-phase adaptive fine-tuning schedule\nthat progressively freezes non-specialized parameters. Evaluated on six domains\n(math, code, law, etc.), DES-MoE matches single-domain ESFT performance while\ntraining one unified model, reduces forgetting by 89% compared to full\nfine-tuning as domains scale from 2 to 6, and achieves 68% faster convergence\nthan conventional methods. Our work establishes dynamic expert isolation as a\nscalable paradigm for multi-task MoE adaptation.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16882v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16882v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.437,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Mixture-of-Experts (MoE) models and their adaptation for multi-domain fine-tuning, emphasizing dynamic routing and expert specialization to prevent catastrophic forgetting. It does not involve diffusion models, iterative refinement processes, or any mechanism for multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "The paper discusses efficient fine-tuning strategies for MoE models, such as adaptive routing and sparse updates, but it does not address distributed training, parallel computing across multiple nodes, or algorithms for partitioning data/computation. Its focus is on model adaptation rather than multi-node machine learning systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16886",
      "title": "SAM-DCE: Addressing Token Uniformity and Semantic Over-Smoothing in\n  Medical Segmentation",
      "authors": [
        "Yingzhen Hu",
        "Yiheng Zhong",
        "Ruobing Li",
        "Yingxue Su",
        "Jiabao An",
        "Feilong Tang",
        "Jionglong Su",
        "Imran Razzak"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The Segment Anything Model (SAM) demonstrates impressive zero-shot\nsegmentation ability on natural images but encounters difficulties in medical\nimaging due to domain shifts, anatomical variability, and its reliance on\nuser-provided prompts. Recent prompt-free adaptations alleviate the need for\nexpert intervention, yet still suffer from limited robustness and adaptability,\noften overlooking the issues of semantic over-smoothing and token uniformity.\nWe propose SAM-DCE, which balances local discrimination and global semantics\nwhile mitigating token uniformity, enhancing inter-class separability, and\nenriching mask decoding with fine-grained, consistent representations.\nExtensive experiments on diverse medical benchmarks validate its effectiveness.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16886v2",
      "pdf_url": "http://arxiv.org/pdf/2509.16886v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.363,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16888",
      "title": "Rethinking Evaluation of Infrared Small Target Detection",
      "authors": [
        "Youwei Pang",
        "Xiaoqi Zhao",
        "Lihe Zhang",
        "Huchuan Lu",
        "Georges El Fakhri",
        "Xiaofeng Liu",
        "Shijian Lu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "As an essential vision task, infrared small target detection (IRSTD) has seen\nsignificant advancements through deep learning. However, critical limitations\nin current evaluation protocols impede further progress. First, existing\nmethods rely on fragmented pixel- and target-level specific metrics, which\nfails to provide a comprehensive view of model capabilities. Second, an\nexcessive emphasis on overall performance scores obscures crucial error\nanalysis, which is vital for identifying failure modes and improving real-world\nsystem performance. Third, the field predominantly adopts dataset-specific\ntraining-testing paradigms, hindering the understanding of model robustness and\ngeneralization across diverse infrared scenarios. This paper addresses these\nissues by introducing a hybrid-level metric incorporating pixel- and\ntarget-level performance, proposing a systematic error analysis method, and\nemphasizing the importance of cross-dataset evaluation. These aim to offer a\nmore thorough and rational hierarchical analysis framework, ultimately\nfostering the development of more effective and robust IRSTD models. An\nopen-source toolkit has be released to facilitate standardized benchmarking.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16888v2",
      "pdf_url": "http://arxiv.org/pdf/2509.16888v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.353,
      "datasets_score": 0.376,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16891",
      "title": "LLMs as Layout Designers: A Spatial Reasoning Perspective",
      "authors": [
        "Sha Li"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While Large Language Models (LLMs) have demonstrated impressive reasoning and\nplanning abilities in textual domains and can effectively follow instructions\nfor complex tasks, their capacity for spatial understanding and reasoning\nremains limited. Such capabilities, however, are critical for applications like\ncontent-aware graphic layout design, which demands precise placement,\nalignment, and structural organization of multiple elements within constrained\nvisual spaces. To address this gap, we propose LaySPA, a reinforcement\nlearning-based framework that augments LLM agents with explicit spatial\nreasoning capabilities. LaySPA leverages hybrid reward signals that capture\ngeometric validity, structural fidelity, and visual quality, enabling agents to\nmodel inter-element relationships, navigate the canvas, and optimize spatial\narrangements. Through iterative self-exploration and adaptive policy\noptimization, LaySPA produces both interpretable reasoning traces and\nstructured layouts. Experimental results demonstrate that LaySPA generates\nstructurally sound and visually appealing layouts, outperforming larger\ngeneral-purpose LLMs and achieving results on par with state-of-the-art\nspecialized layout models.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16891v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16891v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.453,
      "weak_supervision_score": 0.382,
      "diffusion_reasoning_score": 0.528,
      "distributed_training_score": 0.351,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes a reinforcement learning framework using programmatically defined hybrid rewards for spatial reasoning in layout design, without any involvement of human feedback, human-ranked data, or a reward model trained on human preferences. Thus, it does not align with RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a reinforcement learning-based approach for spatial reasoning and layout generation, with no mention of diffusion models, iterative refinement for logical tasks, or treating a Chain-of-Thought as a single entity for multi-step reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16892",
      "title": "Learning from Gene Names, Expression Values and Images: Contrastive\n  Masked Text-Image Pretraining for Spatial Transcriptomics Representation\n  Learning",
      "authors": [
        "Jiahe Qian",
        "Yaoyu Fang",
        "Ziqiao Weng",
        "Xinkun Wang",
        "Lee A. Cooper",
        "Bo Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Spatial transcriptomics aims to connect high-resolution histology images with\nspatially resolved gene expression. To achieve better performance on downstream\ntasks such as gene expression prediction, large-scale pre-training is required\nto obtain generalisable representations that can bridge histology and\ntranscriptomics across tissues, protocols, and laboratories. Existing\ncross-modal pre-training approaches for spatial transcriptomics rely on either\ngene names or expression values in isolation, which strips the gene branch of\nessential semantics and breaks the association between each gene and its\nquantitative magnitude. In addition, by restricting supervision to image-text\nalignment, these methods ignore intrinsic visual cues that are critical for\nlearning robust image features. We present CoMTIP, the first Contrastive Masked\nText-Image Pretraining framework that jointly learns from images, gene names,\nand expression values while capturing fine-grained visual context for spatial\ntranscriptomics. The vision branch uses Masked Feature Modeling to reconstruct\noccluded patches and learn context-aware image embeddings. The text branch\napplies a scalable Gene-Text Encoder that processes all gene sentences in\nparallel, enriches each gene and its numerical value with dedicated embeddings,\nand employs Pair-aware Adversarial Training (PAAT) to preserve correct\ngene-value associations. Image and text representations are aligned in a shared\nInfoNCE-optimised space. Experiments on public spatial transcriptomics datasets\nshow that CoMTIP not only surpasses previous methods on diverse downstream\ntasks but also achieves zero-shot gene expression prediction, a capability that\nexisting approaches do not provide.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16892v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16892v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.355,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on contrastive masked text-image pretraining for spatial transcriptomics, involving techniques like Masked Feature Modeling and InfoNCE optimization to align images with gene data. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16897",
      "title": "PRISM: Precision-Recall Informed Data-Free Knowledge Distillation via\n  Generative Diffusion",
      "authors": [
        "Xuewan He",
        "Jielei Wang",
        "Zihan Cheng",
        "Yuchen Su",
        "Shiyue Huang",
        "Guoming Lu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to\na student without access to the real in-distribution (ID) data. While existing\nmethods perform well on small-scale images, they suffer from mode collapse when\nsynthesizing large-scale images, resulting in limited knowledge transfer.\nRecently, leveraging advanced generative models to synthesize photorealistic\nimages has emerged as a promising alternative. Nevertheless, directly using\noff-the-shelf diffusion to generate datasets faces the precision-recall\nchallenges: 1) ensuring synthetic data aligns with the real distribution, and\n2) ensuring coverage of the real ID manifold. In response, we propose PRISM, a\nprecision-recall informed synthesis method. Specifically, we introduce\nEnergy-guided Distribution Alignment to avoid the generation of\nout-of-distribution samples, and design the Diversified Prompt Engineering to\nenhance coverage of the real ID manifold. Extensive experiments on various\nlarge-scale image datasets demonstrate the superiority of PRISM. Moreover, we\ndemonstrate that models trained with PRISM exhibit strong domain\ngeneralization.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16897v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16897v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.487,
      "distributed_training_score": 0.41,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for image synthesis in data-free knowledge distillation, specifically to generate synthetic data for model compression. It does not involve adapting diffusion for multi-step logical reasoning, chain-of-thought processes, or iterative refinement of reasoning paths. The core application is generative image creation, not reasoning tasks.",
      "distributed_training_justification": "The paper discusses a method for synthesizing data using diffusion models for knowledge distillation and does not address distributed training, parallel computing, multi-node machine learning, or any techniques for partitioning data, models, or computations across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16900",
      "title": "ME-Mamba: Multi-Expert Mamba with Efficient Knowledge Capture and Fusion\n  for Multimodal Survival Analysis",
      "authors": [
        "Chengsheng Zhang",
        "Linhao Qu",
        "Xiaoyu Liu",
        "Zhijian Song"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Survival analysis using whole-slide images (WSIs) is crucial in cancer\nresearch. Despite significant successes, pathology images typically only\nprovide slide-level labels, which hinders the learning of discriminative\nrepresentations from gigapixel WSIs. With the rapid advancement of\nhigh-throughput sequencing technologies, multimodal survival analysis\nintegrating pathology images and genomics data has emerged as a promising\napproach. We propose a Multi-Expert Mamba (ME-Mamba) system that captures\ndiscriminative pathological and genomic features while enabling efficient\nintegration of both modalities. This approach achieves complementary\ninformation fusion without losing critical information from individual\nmodalities, thereby facilitating accurate cancer survival analysis.\nSpecifically, we first introduce a Pathology Expert and a Genomics Expert to\nprocess unimodal data separately. Both experts are designed with Mamba\narchitectures that incorporate conventional scanning and attention-based\nscanning mechanisms, allowing them to extract discriminative features from long\ninstance sequences containing substantial redundant or irrelevant information.\nSecond, we design a Synergistic Expert responsible for modality fusion. It\nexplicitly learns token-level local correspondences between the two modalities\nvia Optimal Transport, and implicitly enhances distribution consistency through\na global cross-modal fusion loss based on Maximum Mean Discrepancy. The fused\nfeature representations are then passed to a mamba backbone for further\nintegration. Through the collaboration of the Pathology Expert, Genomics\nExpert, and Synergistic Expert, our method achieves stable and accurate\nsurvival analysis with relatively low computational complexity. Extensive\nexperimental results on five datasets in The Cancer Genome Atlas (TCGA)\ndemonstrate our state-of-the-art performance.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16900v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16900v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.293,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.39,
      "distributed_training_score": 0.353,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16902",
      "title": "FedEL: Federated Elastic Learning for Heterogeneous Devices",
      "authors": [
        "Letian Zhang",
        "Bo Chen",
        "Jieming Bian",
        "Lei Wang",
        "Jie Xu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Federated learning (FL) enables distributed devices to collaboratively train\nmachine learning models while maintaining data privacy. However, the\nheterogeneous hardware capabilities of devices often result in significant\ntraining delays, as straggler clients with limited resources prolong the\naggregation process. Existing solutions such as client selection, asynchronous\nFL, and partial training partially address these challenges but encounter\nissues such as reduced accuracy, stale updates, and compromised model\nperformance due to inconsistent training contributions. To overcome these\nlimitations, we propose FedEL, a federated elastic learning framework that\nenhances training efficiency while maintaining model accuracy. FedEL introduces\na novel window-based training process, sliding the window to locate the\ntraining part of the model and dynamically selecting important tensors for\ntraining within a coordinated runtime budget. This approach ensures progressive\nand balanced training across all clients, including stragglers. Additionally,\nFedEL employs a tensor importance adjustment module, harmonizing local and\nglobal tensor importance to mitigate biases caused by data heterogeneity. The\nexperiment results show that FedEL achieves up to 3.87x improvement in\ntime-to-accuracy compared to baselines while maintaining or exceeding final\ntest accuracy.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16902v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16902v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.304,
      "distributed_training_score": 0.506,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution, FedEL, is a framework for federated learning that addresses challenges in distributed training on heterogeneous devices. It introduces techniques like window-based training and dynamic tensor selection to partition and optimize computation across multiple nodes, directly aligning with distributed training concepts such as partitioning model architecture and computation for efficiency. This enhances parallel computing in multi-node scenarios, making the paper highly relevant to the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces FedEL, a federated learning framework designed to address training inefficiencies caused by heterogeneous device capabilities in distributed environments, by implementing a window-based training process that dynamically selects and trains important model tensors within a runtime budget, while incorporating a tensor importance adjustment module to handle data heterogeneity. The methodology ensures progressive training across all devices, including stragglers, and experimental results demonstrate up to 3.87 times improvement in time-to-accuracy compared to baselines, with comparable or superior final test accuracy and reduced resource overhead.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by adapting and combining existing ideas like ElasticTrainer with new elements such as window-based training and tensor importance adjustment to effectively handle stragglers in federated learning, rather than introducing a entirely new problem or technique. This approach solves known issues in a clever, integrated way but is not a groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in federated learning for heterogeneous devices by providing practical efficiency gains, potentially leading to citations and adaptations in subfields like distributed machine learning. However, its broader impact may be limited to specific applications rather than transforming the entire field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, practical contribution to federated learning by addressing real-world challenges with innovative adaptations, making it valuable for researchers in machine learning and AI to understand and potentially build upon. While not essential for all, it provides insights that could enhance work in distributed systems.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2f09224e3675514e24246ce7406e140c49806366",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 8,
      "average_h_index": 5.4,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Letian Zhang",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2051634255"
        },
        {
          "name": "Bo Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382484965"
        },
        {
          "name": "Jieming Bian",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2143957927"
        },
        {
          "name": "Lei Wang",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2281267605"
        },
        {
          "name": "Jie Xu",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2265619461"
        }
      ]
    },
    {
      "id": "2509.16909",
      "title": "SLAM-Former: Putting SLAM into One Transformer",
      "authors": [
        "Yijun Yuan",
        "Zhuoguang Chen",
        "Kenan Li",
        "Weibang Wang",
        "Hang Zhao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)"
      ],
      "abstract": "We present SLAM-Former, a novel neural approach that integrates full SLAM\ncapabilities into a single transformer. Similar to traditional SLAM systems,\nSLAM-Former comprises both a frontend and a backend that operate in tandem. The\nfrontend processes sequential monocular images in real-time for incremental\nmapping and tracking, while the backend performs global refinement to ensure a\ngeometrically consistent result. This alternating execution allows the frontend\nand backend to mutually promote one another, enhancing overall system\nperformance. Comprehensive experimental results demonstrate that SLAM-Former\nachieves superior or highly competitive performance compared to\nstate-of-the-art dense SLAM methods.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16909v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16909v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.292,
      "diffusion_reasoning_score": 0.396,
      "distributed_training_score": 0.34,
      "datasets_score": 0.25,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16922",
      "title": "PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D\n  Gaussian Splatting with Pixel-Aware Density Control",
      "authors": [
        "Tianheng Zhu",
        "Yinfeng Yu",
        "Liejun Wang",
        "Fuchun Sun",
        "Wendong Zheng"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Audio-driven talking head generation is crucial for applications in virtual\nreality, digital avatars, and film production. While NeRF-based methods enable\nhigh-fidelity reconstruction, they suffer from low rendering efficiency and\nsuboptimal audio-visual synchronization. This work presents PGSTalker, a\nreal-time audio-driven talking head synthesis framework based on 3D Gaussian\nSplatting (3DGS). To improve rendering performance, we propose a pixel-aware\ndensity control strategy that adaptively allocates point density, enhancing\ndetail in dynamic facial regions while reducing redundancy elsewhere.\nAdditionally, we introduce a lightweight Multimodal Gated Fusion Module to\neffectively fuse audio and spatial features, thereby improving the accuracy of\nGaussian deformation prediction. Extensive experiments on public datasets\ndemonstrate that PGSTalker outperforms existing NeRF- and 3DGS-based approaches\nin rendering quality, lip-sync precision, and inference speed. Our method\nexhibits strong generalization capabilities and practical potential for\nreal-world deployment.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16922v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16922v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.361,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16924",
      "title": "Audio-Guided Dynamic Modality Fusion with Stereo-Aware Attention for\n  Audio-Visual Navigation",
      "authors": [
        "Jia Li",
        "Yinfeng Yu",
        "Liejun Wang",
        "Fuchun Sun",
        "Wendong Zheng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.SD (Sound)"
      ],
      "abstract": "In audio-visual navigation (AVN) tasks, an embodied agent must autonomously\nlocalize a sound source in unknown and complex 3D environments based on\naudio-visual signals. Existing methods often rely on static modality fusion\nstrategies and neglect the spatial cues embedded in stereo audio, leading to\nperformance degradation in cluttered or occluded scenes. To address these\nissues, we propose an end-to-end reinforcement learning-based AVN framework\nwith two key innovations: (1) a \\textbf{S}tereo-Aware \\textbf{A}ttention\n\\textbf{M}odule (\\textbf{SAM}), which learns and exploits the spatial disparity\nbetween left and right audio channels to enhance directional sound perception;\nand (2) an \\textbf{A}udio-\\textbf{G}uided \\textbf{D}ynamic \\textbf{F}usion\nModule (\\textbf{AGDF}), which dynamically adjusts the fusion ratio between\nvisual and auditory features based on audio cues, thereby improving robustness\nto environmental changes. Extensive experiments are conducted on two realistic\n3D scene datasets, Replica and Matterport3D, demonstrating that our method\nsignificantly outperforms existing approaches in terms of navigation success\nrate and path efficiency. Notably, our model achieves over 40\\% improvement\nunder audio-only conditions compared to the best-performing baselines. These\nresults highlight the importance of explicitly modeling spatial cues from\nstereo channels and performing deep multi-modal fusion for robust and efficient\naudio-visual navigation.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16924v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16924v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.305,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves developing a reinforcement learning framework for audio-visual navigation, including modules for stereo-aware attention and dynamic modality fusion. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, as defined in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16926",
      "title": "Cross-Attention with Confidence Weighting for Multi-Channel Audio\n  Alignment",
      "authors": [
        "Ragib Amin Nihal",
        "Benjamin Yen",
        "Takeshi Ashizawa",
        "Kazuhiro Nakadai"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Multi-channel audio alignment is a key requirement in bioacoustic monitoring,\nspatial audio systems, and acoustic localization. However, existing methods\noften struggle to address nonlinear clock drift and lack mechanisms for\nquantifying uncertainty. Traditional methods like Cross-correlation and Dynamic\nTime Warping assume simple drift patterns and provide no reliability measures.\nMeanwhile, recent deep learning models typically treat alignment as a binary\nclassification task, overlooking inter-channel dependencies and uncertainty\nestimation. We introduce a method that combines cross-attention mechanisms with\nconfidence-weighted scoring to improve multi-channel audio synchronization. We\nextend BEATs encoders with cross-attention layers to model temporal\nrelationships between channels. We also develop a confidence-weighted scoring\nfunction that uses the full prediction distribution instead of binary\nthresholding. Our method achieved first place in the BioDCASE 2025 Task 1\nchallenge with 0.30 MSE average across test datasets, compared to 0.58 for the\ndeep learning baseline. On individual datasets, we achieved 0.14 MSE on ARU\ndata (77% reduction) and 0.45 MSE on zebra finch data (18% reduction). The\nframework supports probabilistic temporal alignment, moving beyond point\nestimates. While validated in a bioacoustic context, the approach is applicable\nto a broader range of multi-channel audio tasks where alignment confidence is\ncritical. Code available on: https://github.com/Ragib-Amin-Nihal/BEATsCA",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16926v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16926v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.356,
      "datasets_score": 0.315,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16931",
      "title": "Equip Pre-ranking with Target Attention by Residual Quantization",
      "authors": [
        "Yutong Li",
        "Yu Zhu",
        "Yichen Qiao",
        "Ziyu Guan",
        "Lv Shao",
        "Tong Liu",
        "Bo Zheng"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The pre-ranking stage in industrial recommendation systems faces a\nfundamental conflict between efficiency and effectiveness. While powerful\nmodels like Target Attention (TA) excel at capturing complex feature\ninteractions in the ranking stage, their high computational cost makes them\ninfeasible for pre-ranking, which often relies on simplistic vector-product\nmodels. This disparity creates a significant performance bottleneck for the\nentire system. To bridge this gap, we propose TARQ, a novel pre-ranking\nframework. Inspired by generative models, TARQ's key innovation is to equip\npre-ranking with an architecture approximate to TA by Residual Quantization.\nThis allows us to bring the modeling power of TA into the latency-critical\npre-ranking stage for the first time, establishing a new state-of-the-art\ntrade-off between accuracy and efficiency. Extensive offline experiments and\nlarge-scale online A/B tests at Taobao demonstrate TARQ's significant\nimprovements in ranking performance. Consequently, our model has been fully\ndeployed in production, serving tens of millions of daily active users and\nyielding substantial business improvements.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16931v2",
      "pdf_url": "http://arxiv.org/pdf/2509.16931v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.405,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is developing a novel pre-ranking framework (TARQ) for recommendation systems using Residual Quantization to approximate Target Attention, focusing on efficiency in inference rather than training. It does not discuss distributed training, parallel computing, multi-node setups, or strategies for accelerating model training across processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16935",
      "title": "Parameter-efficient fine-tuning (PEFT) of Vision Foundation Models for\n  Atypical Mitotic Figure Classification",
      "authors": [
        "Lavish Ramchandani",
        "Gunjan Deotale",
        "Dev Kumar Das"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Atypical mitotic figures (AMFs) are rare abnormal cell divisions associated\nwith tumor aggressiveness and poor prognosis. Their detection remains a\nsignificant challenge due to subtle morphological cues, class imbalance, and\ninter-observer variability among pathologists. The MIDOG 2025 challenge\nintroduced a dedicated track for atypical mitosis classification, enabling\nsystematic evaluation of deep learning methods. In this study, we investigated\nthe use of large vision foundation models, including Virchow, Virchow2, and\nUNI, with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. We\nconducted extensive experiments with different LoRA ranks, as well as random\nand group-based data splits, to analyze robustness under varied conditions. Our\nbest approach, Virchow with LoRA rank 8 and ensemble of three-fold\ncross-validation, achieved a balanced accuracy of 88.37% on the preliminary\ntest set, ranking joint 9th in the challenge leaderboard. These results\nhighlight the promise of foundation models with efficient adaptation strategies\nfor the classification of atypical mitosis, while underscoring the need for\nimprovements in specificity and domain generalization.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16935v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16935v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.381,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16942",
      "title": "Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation\n  in Remote Sensing Semantic Segmentation",
      "authors": [
        "Bin Wang",
        "Fei Deng",
        "Zeyu Chen",
        "Zhicheng Yu",
        "Yiguang Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Source-Free Domain Adaptation (SFDA) enables domain adaptation for semantic\nsegmentation of Remote Sensing Images (RSIs) using only a well-trained source\nmodel and unlabeled target domain data. However, the lack of ground-truth\nlabels in the target domain often leads to the generation of noisy\npseudo-labels. Such noise impedes the effective mitigation of domain shift\n(DS). To address this challenge, we propose ProSFDA, a prototype-guided SFDA\nframework. It employs prototype-weighted pseudo-labels to facilitate reliable\nself-training (ST) under pseudo-labels noise. We, in addition, introduce a\nprototype-contrast strategy that encourages the aggregation of features\nbelonging to the same class, enabling the model to learn discriminative target\ndomain representations without relying on ground-truth supervision. Extensive\nexperiments show that our approach substantially outperforms existing methods.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16942v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16942v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.459,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.367,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using noisy pseudo-labels generated from a source model on unlabeled target data for training, which aligns directly with weak supervision. It employs strategies like prototype-weighted pseudo-labeling to handle and refine these imprecise labels, mirroring weak supervision's reliance on programmatically generated, high-level, or noisy labels rather than perfect hand-labeled data. This makes the paper's approach a clear application of weak supervision techniques in domain adaptation.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper, titled \"Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation,\" addresses the challenge of noisy pseudo-labels in Source-Free Domain Adaptation (SFDA) for remote sensing images (RSIs). It proposes ProSFDA, a framework that uses prototype-weighted pseudo-labels to enable reliable self-training and a prototype-contrast strategy to improve feature clustering and representation learning, ultimately demonstrating substantial performance improvements over existing methods through extensive experiments.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining prototype-weighted pseudo-labels and a prototype-contrast strategy to enhance existing SFDA methods for remote sensing, offering a clever adaptation rather than a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of domain adaptation for computer vision in remote sensing, as it improves SFDA performance, though its influence may be limited to specific applications involving semantic segmentation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to SFDA in remote sensing by introducing effective denoising techniques, making it essential for researchers in this niche area to be aware of its advancements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b3a2bd6b04f01f3c991f992acede66dae576b14c",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Bin Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381787391"
        },
        {
          "name": "Fei Deng",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2328009877"
        },
        {
          "name": "Zeyu Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2337429351"
        },
        {
          "name": "Zhicheng Yu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381770428"
        },
        {
          "name": "Yiguang Liu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381400819"
        }
      ]
    },
    {
      "id": "2509.16944",
      "title": "Catching the Details: Self-Distilled RoI Predictors for Fine-Grained\n  MLLM Perception",
      "authors": [
        "Yuheng Shi",
        "Xiaohuan Pei",
        "Minjing Dong",
        "Chang Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) require high-resolution visual\ninformation to perform fine-grained perception, yet processing entire\nhigh-resolution images is computationally prohibitive. While recent methods\nleverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they\ntypically present a difficult trade-off: training-based approaches depend on\nlarge-scale annotated datasets, while training-free methods that utilize the\nmodel's internal attention are computationally inefficient and less accurate,\nrequiring either multi-pass prefill stages or reliance on the slow\nauto-regressive decoding process. In this paper, we propose an efficient,\nannotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves\nthis trade-off. The SD-RPN is built around a pipeline that transforms the noisy\nattention maps from the MLLM's middle layers into high-quality pseudo-RoI\nlabels by explicitly denoising the signal and resolving ambiguity. We use these\nlabels to train a lightweight Region Proposal Network (RPN) that learns a more\nprecise localization. This RPN is also highly efficient, predicting the RoI in\na single forward pass using features from the MLLM's middle layers, decoupling\nRoI identification from the auto-regressive generation and avoiding costly\nmulti-pass operations.To validate our approach, we integrate the framework into\nthe LLaVA-1.5 architecture. Despite being trained on only a few (e.g. 10K)\nquestion-answer pairs, our method demonstrates exceptional data efficiency and\ngeneralization, achieving over a 10% absolute accuracy improvement on unseen\nbenchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a\npractical and scalable solution for enhancing the fine-grained perception of\nMLLMs without requiring costly supervision or full model fine-tuning. Code is\navailable at https://github.com/YuHengsss/SD-RPN.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16944v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16944v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.408,
      "weak_supervision_score": 0.442,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.416,
      "datasets_score": 0.341,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on self-distillation using the MLLM's internal attention maps to generate pseudo-labels, without any involvement of human feedback, ranking data, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper employs a method to programmatically generate pseudo-labels from noisy attention maps of the MLLM, which aligns with weak supervision by using imperfect, internal model signals as a high-level source for training the RPN, bypassing the need for manually annotated data.",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement for logical reasoning, or multi-step Chain-of-Thought processes; it instead centers on denoising attention maps for RoI prediction in MLLMs.",
      "distributed_training_justification": "The paper discusses efficiency in training a lightweight RPN but does not address distributed systems, parallel computing across multiple nodes, or strategies for partitioning data/computation; it focuses on single-model architectural improvements.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper introduces SD-RPN, a self-distilled region proposal network designed to enhance fine-grained perception in Multimodal Large Language Models (MLLMs) by efficiently identifying regions of interest from noisy internal attention maps. The methodology involves denoising these attention maps to generate pseudo-labels, training a lightweight RPN for precise and rapid RoI prediction, and integrating it into existing architectures like LLaVA-1.5, which results in over 10% absolute accuracy improvements on benchmarks such as TextVQA, DocVQA, and V-Star using only 10K question-answer pairs, demonstrating exceptional data efficiency and generalization without requiring annotated datasets or full model fine-tuning.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining self-distillation with region proposal networks to create an annotation-free method for RoI prediction in MLLMs, effectively addressing inefficiencies in existing approaches without introducing an entirely new paradigm.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in MLLM architectures by providing an efficient, scalable solution for fine-grained perception, potentially leading to broader adoption in vision-language tasks and citations within computer vision subfields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a high-quality, practical contribution that advances MLLM efficiency and accuracy, making it valuable for researchers focused on multimodal models and fine-grained perception.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/086d2eaa24a06f9c496db6e0e662cdef59aa91e7",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 11,
      "average_h_index": 6.25,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Yuheng Shi",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2182369240"
        },
        {
          "name": "Xiaohuan Pei",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2203812724"
        },
        {
          "name": "Minjing Dong",
          "h_index": 11,
          "profile_url": "https://www.semanticscholar.org/author/151498428"
        },
        {
          "name": "Chang Xu",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2302852345"
        }
      ]
    },
    {
      "id": "2509.16949",
      "title": "Leveraging RGB Images for Pre-Training of Event-Based Hand Pose\n  Estimation",
      "authors": [
        "Ruicong Liu",
        "Takehiko Ohkawa",
        "Tze Ho Elden Tse",
        "Mingfang Zhang",
        "Angela Yao",
        "Yoichi Sato"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper presents RPEP, the first pre-training method for event-based 3D\nhand pose estimation using labeled RGB images and unpaired, unlabeled event\ndata. Event data offer significant benefits such as high temporal resolution\nand low latency, but their application to hand pose estimation is still limited\nby the scarcity of labeled training data. To address this, we repurpose real\nRGB datasets to train event-based estimators. This is done by constructing\npseudo-event-RGB pairs, where event data is generated and aligned with the\nground-truth poses of RGB images. Unfortunately, existing pseudo-event\ngeneration techniques assume stationary objects, thus struggling to handle\nnon-stationary, dynamically moving hands. To overcome this, RPEP introduces a\nnovel generation strategy that decomposes hand movements into smaller,\nstep-by-step motions. This decomposition allows our method to capture temporal\nchanges in articulation, constructing more realistic event data for a moving\nhand. Additionally, RPEP imposes a motion reversal constraint, regularizing\nevent generation using reversed motion. Extensive experiments show that our\npre-trained model significantly outperforms state-of-the-art methods on real\nevent data, achieving up to 24% improvement on EvRealHands. Moreover, it\ndelivers strong performance with minimal labeled samples for fine-tuning,\nmaking it well-suited for practical deployment.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16949v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16949v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.345,
      "datasets_score": 0.307,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16952",
      "title": "AirQA: A Comprehensive QA Dataset for AI Research with Instance-Level\n  Evaluation",
      "authors": [
        "Tiancheng Huang",
        "Ruisheng Cao",
        "Yuxin Zhang",
        "Zhangyi Kang",
        "Zijian Wang",
        "Chenrun Wang",
        "Yijie Luo",
        "Hang Zheng",
        "Lirong Qian",
        "Lu Chen",
        "Kai Yu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The growing volume of academic papers has made it increasingly difficult for\nresearchers to efficiently extract key information. While large language models\n(LLMs) based agents are capable of automating question answering (QA) workflows\nfor scientific papers, there still lacks a comprehensive and realistic\nbenchmark to evaluate their capabilities. Moreover, training an interactive\nagent for this specific task is hindered by the shortage of high-quality\ninteraction trajectories. In this work, we propose AirQA, a human-annotated\ncomprehensive paper QA dataset in the field of artificial intelligence (AI),\nwith 13,948 papers and 1,246 questions, that encompasses multi-task,\nmulti-modal and instance-level evaluation. Furthermore, we propose ExTrActor,\nan automated framework for instruction data synthesis. With three LLM-based\nagents, ExTrActor can perform example generation and trajectory collection\nwithout human intervention. Evaluations of multiple open-source and proprietary\nmodels show that most models underperform on AirQA, demonstrating the quality\nof our dataset. Extensive experiments confirm that ExTrActor consistently\nimproves the multi-turn tool-use capability of small models, enabling them to\nachieve performance comparable to larger ones.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16952v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16952v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.422,
      "weak_supervision_score": 0.437,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.376,
      "datasets_score": 0.534,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on creating a QA dataset (AirQA) and a framework (ExTrActor) for synthetic data generation to fine-tune LLMs, but it does not involve training a reward model on human-ranked data or using reinforcement learning with human feedback. Instead, it relies on automated, non-human methods for data synthesis, which does not align with RLHF principles.",
      "weak_supervision_justification": "The paper's ExTrActor framework uses LLMs to programmatically generate QA examples and interaction trajectories without human intervention, which resembles weak supervision by relying on noisy or automated label generation from high-level sources. However, the primary focus is on dataset creation and evaluation, not a deep exploration of weak supervision techniques, making it only moderately relevant.",
      "diffusion_reasoning_justification": "The paper does not mention or utilize diffusion models for multi-step logical reasoning. It centers on LLMs, RAG frameworks, and QA agents for handling scientific papers, with no components involving iterative refinement processes characteristic of diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction and evaluation of AirQA, a new human-annotated QA dataset for AI research, including aspects like curation, benchmarking of LLMs, and instance-level evaluation. This directly aligns with research on creating, analyzing, and evaluating datasets for machine learning applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces AirQA, a human-annotated dataset encompassing 13,948 AI research papers and 1,246 questions, designed to evaluate large language models (LLMs) on multi-task, multi-modal question answering with instance-level assessments, addressing the lack of comprehensive benchmarks for scientific paper QA. It also presents ExTrActor, an automated framework using three LLM-based agents to generate instruction data and interaction trajectories, which, through experiments, significantly enhances smaller models' multi-turn tool-use capabilities, enabling them to perform comparably to larger models while demonstrating that current LLMs underperform on the dataset.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing the first comprehensive multi-modal QA dataset for AI papers with function-based evaluation and an automated data synthesis framework, effectively combining existing QA and LLM techniques to address a known problem in a new way.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI research QA, as the AirQA dataset and ExTrActor framework provide valuable tools for evaluating and improving LLMs on scientific tasks.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with a new dataset and framework that advance QA for academic papers, making it essential for researchers in AI and NLP to be aware of for potential applications in model evaluation and training.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/284ac0b40d3f359d25551b52de59f9162e1e25b3",
      "total_authors": 11,
      "authors_found": 11,
      "highest_h_index": 13,
      "average_h_index": 1.5454545454545454,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Tiancheng Huang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2355780974"
        },
        {
          "name": "Ruisheng Cao",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/150273321"
        },
        {
          "name": "Yuxin Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363758220"
        },
        {
          "name": "Zhangyi Kang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2363493164"
        },
        {
          "name": "Zijian Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381611898"
        },
        {
          "name": "Chenrun Wang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2295780933"
        },
        {
          "name": "Yijie Luo",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2351318217"
        },
        {
          "name": "Hang Zheng",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2333982190"
        },
        {
          "name": "Lirong Qian",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381795871"
        },
        {
          "name": "Lu Chen",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381403245"
        },
        {
          "name": "Kai Yu",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2384419851"
        }
      ]
    },
    {
      "id": "2509.16956",
      "title": "VidCLearn: A Continual Learning Approach for Text-to-Video Generation",
      "authors": [
        "Luca Zanchetta",
        "Lorenzo Papa",
        "Luca Maiano",
        "Irene Amerini"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-video generation is an emerging field in generative AI, enabling the\ncreation of realistic, semantically accurate videos from text prompts. While\ncurrent models achieve impressive visual quality and alignment with input text,\nthey typically rely on static knowledge, making it difficult to incorporate new\ndata without retraining from scratch. To address this limitation, we propose\nVidCLearn, a continual learning framework for diffusion-based text-to-video\ngeneration. VidCLearn features a student-teacher architecture where the student\nmodel is incrementally updated with new text-video pairs, and the teacher model\nhelps preserve previously learned knowledge through generative replay.\nAdditionally, we introduce a novel temporal consistency loss to enhance motion\nsmoothness and a video retrieval module to provide structural guidance at\ninference. Our architecture is also designed to be more computationally\nefficient than existing models while retaining satisfactory generation\nperformance. Experimental results show VidCLearn's superiority over baseline\nmethods in terms of visual quality, semantic alignment, and temporal coherence.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16956v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16956v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.36,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.484,
      "distributed_training_score": 0.356,
      "datasets_score": 0.352,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper proposes VidCLearn, a continual learning framework for diffusion-based text-to-video generation, which uses iterative refinement in diffusion models for video synthesis. However, it does not adapt diffusion for complex logical tasks, multi-step reasoning, or treating a Chain-of-Thought as an entity. Instead, it focuses on generative tasks like video creation and knowledge preservation, making the connection to diffusion-based reasoning indirect.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16957",
      "title": "MO R-CNN: Multispectral Oriented R-CNN for Object Detection in Remote\n  Sensing Image",
      "authors": [
        "Leiyu Wang",
        "Biao Jin",
        "Feng Huang",
        "Liqiong Chen",
        "Zhengyong Wang",
        "Xiaohai He",
        "Honggang Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Oriented object detection for multi-spectral imagery faces significant\nchallenges due to differences both within and between modalities. Although\nexisting methods have improved detection accuracy through complex network\narchitectures, their high computational complexity and memory consumption\nseverely restrict their performance. Motivated by the success of large kernel\nconvolutions in remote sensing, we propose MO R-CNN, a lightweight framework\nfor multi-spectral oriented detection featuring heterogeneous feature\nextraction network (HFEN), single modality supervision (SMS), and\ncondition-based multimodal label fusion (CMLF). HFEN leverages inter-modal\ndifferences to adaptively align, merge, and enhance multi-modal features. SMS\nconstrains multi-scale features and enables the model to learn from multiple\nmodalities. CMLF fuses multimodal labels based on specific rules, providing the\nmodel with a more robust and consistent supervisory signal. Experiments on the\nDroneVehicle, VEDAI and OGSOD datasets prove the superiority of our method. The\nsource code is available at:https://github.com/Iwill-github/MORCNN.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16957v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16957v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.34,
      "distributed_training_score": 0.363,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16958",
      "title": "Quantum Abduction: A New Paradigm for Reasoning under Uncertainty",
      "authors": [
        "Remo Pareschi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Abductive reasoning - the search for plausible explanations - has long been\ncentral to human inquiry, from forensics to medicine and scientific discovery.\nYet formal approaches in AI have largely reduced abduction to eliminative\nsearch: hypotheses are treated as mutually exclusive, evaluated against\nconsistency constraints or probability updates, and pruned until a single\n\"best\" explanation remains. This reductionist framing overlooks the way human\nreasoners sustain multiple explanatory lines in suspension, navigate\ncontradictions, and generate novel syntheses. This paper introduces quantum\nabduction, a non-classical paradigm that models hypotheses in superposition,\nallows them to interfere constructively or destructively, and collapses only\nwhen coherence with evidence is reached. Grounded in quantum cognition and\nimplemented with modern NLP embeddings and generative AI, the framework\nsupports dynamic synthesis rather than premature elimination. Case studies span\nhistorical mysteries (Ludwig II of Bavaria, the \"Monster of Florence\"),\nliterary demonstrations (\"Murder on the Orient Express\"), medical diagnosis,\nand scientific theory change. Across these domains, quantum abduction proves\nmore faithful to the constructive and multifaceted nature of human reasoning,\nwhile offering a pathway toward expressive and transparent AI reasoning\nsystems.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16958v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16958v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.495,
      "distributed_training_score": 0.272,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a quantum-inspired framework for abductive reasoning, focusing on concepts like superposition and interference from quantum cognition, implemented with NLP embeddings and generative AI. It does not involve diffusion models, iterative refinement processes, or treating a 'Chain-of-Thought' as a single entity for holistic correction. There is no component of multi-step logical reasoning using diffusion mechanisms, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16959",
      "title": "Graph Coloring for Multi-Task Learning",
      "authors": [
        "Santosh Patapati",
        "Trisanth Srinivasan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.NE (Neural and Evolutionary Computing)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "When different objectives conflict with each other in multi-task learning,\ngradients begin to interfere and slow convergence, thereby potentially reducing\nthe final model's performance. To address this, we introduce SON-GOKU, a\nscheduler that computes gradient interference, constructs an interference\ngraph, and then applies greedy graph-coloring to partition tasks into groups\nthat align well with each other. At each training step, only one group (color\nclass) of tasks are activated, and the grouping partition is constantly\nrecomputed as task relationships evolve throughout training. By ensuring that\neach mini-batch contains only tasks that pull the model in the same direction,\nour method improves the effectiveness of any underlying multi-task learning\noptimizer without additional tuning. Since tasks within these groups will\nupdate in compatible directions, multi-task learning will improve model\nperformance rather than impede it. Empirical results on six different datasets\nshow that this interference-aware graph-coloring approach consistently\noutperforms baselines and state-of-the-art multi-task optimizers. We provide\nextensive theory showing why grouping and sequential updates improve multi-task\nlearning, with guarantees on descent, convergence, and accurately identifying\nwhat tasks conflict or align.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16959v2",
      "pdf_url": "http://arxiv.org/pdf/2509.16959v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.42,
      "datasets_score": 0.313,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a scheduler for multi-task learning that uses graph coloring to group tasks and minimize gradient interference during training. It focuses on optimizing updates within a single training process, without any discussion of distributing computation across multiple processors, nodes, or machines. This does not align with distributed training concepts, such as data parallelism, model parallelism, or multi-node acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16968",
      "title": "Penalizing Boundary Activation for Object Completeness in Diffusion\n  Models",
      "authors": [
        "Haoyang Xu",
        "Tianhao Zhao",
        "Sibei Yang",
        "Yutian Lin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Diffusion models have emerged as a powerful technique for text-to-image (T2I)\ngeneration, creating high-quality, diverse images across various domains.\nHowever, a common limitation in these models is the incomplete display of\nobjects, where fragments or missing parts undermine the model's performance in\ndownstream applications. In this study, we conduct an in-depth analysis of the\nincompleteness issue and reveal that the primary factor behind incomplete\nobject generation is the usage of RandomCrop during model training. This widely\nused data augmentation method, though enhances model generalization ability,\ndisrupts object continuity during training. To address this, we propose a\ntraining-free solution that penalizes activation values at image boundaries\nduring the early denoising steps. Our method is easily applicable to\npre-trained Stable Diffusion models with minimal modifications and negligible\ncomputational overhead. Extensive experiments demonstrate the effectiveness of\nour method, showing substantial improvements in object integrity and image\nquality.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16968v2",
      "pdf_url": "http://arxiv.org/pdf/2509.16968v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.532,
      "distributed_training_score": 0.353,
      "datasets_score": 0.311,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on improving object completeness in text-to-image diffusion models by addressing issues caused by RandomCrop and proposing a boundary activation penalty method. It does not involve adapting diffusion models for iterative refinement in logical reasoning tasks, such as handling Chain-of-Thought processes for complex problem-solving. Therefore, there is no relevance to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16970",
      "title": "LLM-Assisted Semantic Guidance for Sparsely Annotated Remote Sensing\n  Object Detection",
      "authors": [
        "Wei Liao",
        "Chunyan Xu",
        "Chenxu Wang",
        "Zhen Cui"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Sparse annotation in remote sensing object detection poses significant\nchallenges due to dense object distributions and category imbalances. Although\nexisting Dense Pseudo-Label methods have demonstrated substantial potential in\npseudo-labeling tasks, they remain constrained by selection ambiguities and\ninconsistencies in confidence estimation.In this paper, we introduce an\nLLM-assisted semantic guidance framework tailored for sparsely annotated remote\nsensing object detection, exploiting the advanced semantic reasoning\ncapabilities of large language models (LLMs) to distill high-confidence\npseudo-labels.By integrating LLM-generated semantic priors, we propose a\nClass-Aware Dense Pseudo-Label Assignment mechanism that adaptively assigns\npseudo-labels for both unlabeled and sparsely labeled data, ensuring robust\nsupervision across varying data distributions. Additionally, we develop an\nAdaptive Hard-Negative Reweighting Module to stabilize the supervised learning\nbranch by mitigating the influence of confounding background information.\nExtensive experiments on DOTA and HRSC2016 demonstrate that the proposed method\noutperforms existing single-stage detector-based frameworks, significantly\nimproving detection performance under sparse annotations.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16970v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16970v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.518,
      "diffusion_reasoning_score": 0.425,
      "distributed_training_score": 0.394,
      "datasets_score": 0.422,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on using LLMs for semantic guidance in pseudo-labeling for object detection, with no mention of human feedback, reward models, or reinforcement learning techniques. It relies on automated processes rather than aligning models with human preferences.",
      "weak_supervision_justification": "The paper's main contribution involves generating pseudo-labels from LLMs for sparsely annotated data, which aligns with weak supervision by programmatically creating noisy or imprecise labels from high-level sources, reducing reliance on fully hand-labeled data.",
      "diffusion_reasoning_justification": "The paper uses LLMs for semantic guidance in object detection but does not involve diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes as defined. It focuses on pseudo-labeling rather than holistic chain-of-thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper evaluates its framework on existing datasets like DOTA and HRSC2016, demonstrating performance improvements, but its primary focus is on the detection method, not on creating, analyzing, or benchmarking datasets themselves.",
      "llm_score_status": "completed",
      "summary": "This paper introduces an LLM-assisted semantic guidance framework to address challenges in sparsely annotated remote sensing object detection, leveraging large language models to generate semantic priors for improved pseudo-labeling. It proposes a Class-Aware Dense Pseudo-Label Assignment mechanism and an Adaptive Hard-Negative Reweighting Module within a multi-branch architecture, demonstrating superior performance on datasets like DOTA and HRSC2016 by enhancing pseudo-label reliability and mitigating issues with confidence estimation and background noise.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever integration of LLMs with existing pseudo-labeling techniques for sparsely annotated remote sensing, offering a notable improvement in handling selection ambiguities and semantic inconsistencies. While it advances the field by applying LLMs in this context, it builds on established methods rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in semi-supervised object detection for remote sensing by incorporating LLMs, potentially leading to more robust methods in this subfield. However, its applicability may be limited to specialized areas like aerial image analysis, reducing broader commercial or general impact.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper provides a strong, innovative contribution to handling sparse annotations in remote sensing, making it valuable for researchers in computer vision. It is not essential for all readers but offers significant insights for those working in related domains.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/99ba93c92df041e35a61ee3bde12c5bcffaf93df",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 4,
      "average_h_index": 2.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Wei Liao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382946568"
        },
        {
          "name": "Chunyan Xu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2237943938"
        },
        {
          "name": "Chenxu Wang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2310389028"
        },
        {
          "name": "Zhen Cui",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2237803553"
        }
      ]
    },
    {
      "id": "2509.16972",
      "title": "The 1st Solution for 7th LSVOS RVOS Track: SaSaSa2VA",
      "authors": [
        "Quanzhu Niu",
        "Dengxian Gong",
        "Shihao Chen",
        "Tao Zhang",
        "Yikang Zhou",
        "Haobo Yuan",
        "Lu Qi",
        "Xiangtai Li",
        "Shunping Ji"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Referring video object segmentation (RVOS) requires segmenting and tracking\nobjects in videos conditioned on natural-language expressions, demanding\nfine-grained understanding of both appearance and motion. Building on Sa2VA,\nwhich couples a Multi-modal Large Language Model (MLLM) with the video\nsegmentation model SAM2, we identify two key bottlenecks that limit\nsegmentation performance: sparse frame sampling and reliance on a single [SEG]\ntoken for an entire video. We propose Segmentation Augmented and Selective\nAveraged Sa2VA SaSaSa2VA to address these issues. On the 7th LSVOS Challenge\n(RVOS track), SaSaSa2VA achieves a $J\\&F$ of 67.45, ranking first and\nsurpassing the runner-up by 2.80 points. This result and ablation studies\ndemonstrate that efficient segmentation augmentation and test-time ensembling\nsubstantially enhance grounded MLLMs for RVOS. The code is released in Sa2VA\nrepository: https://github.com/magic-research/Sa2VA.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16972v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16972v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.29,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.307,
      "datasets_score": 0.306,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16977",
      "title": "Optimal Transport for Handwritten Text Recognition in a Low-Resource\n  Regime",
      "authors": [
        "Petros Georgoulas Wraight",
        "Giorgos Sfikas",
        "Ioannis Kordonis",
        "Petros Maragos",
        "George Retsinas"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Handwritten Text Recognition (HTR) is a task of central importance in the\nfield of document image understanding. State-of-the-art methods for HTR require\nthe use of extensive annotated sets for training, making them impractical for\nlow-resource domains like historical archives or limited-size modern\ncollections. This paper introduces a novel framework that, unlike the standard\nHTR model paradigm, can leverage mild prior knowledge of lexical\ncharacteristics; this is ideal for scenarios where labeled data are scarce. We\npropose an iterative bootstrapping approach that aligns visual features\nextracted from unlabeled images with semantic word representations using\nOptimal Transport (OT). Starting with a minimal set of labeled examples, the\nframework iteratively matches word images to text labels, generates\npseudo-labels for high-confidence alignments, and retrains the recognizer on\nthe growing dataset. Numerical experiments demonstrate that our iterative\nvisual-semantic alignment scheme significantly improves recognition accuracy on\nlow-resource HTR benchmarks.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16977v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16977v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.407,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.354,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves an iterative bootstrapping framework that generates pseudo-labels from Optimal Transport alignments, starting with minimal labeled data. This approach aligns directly with weak supervision, as it programmatically creates noisy or imprecise labels (e.g., high-confidence matches) to expand the training set, reducing reliance on extensive hand-labeled data and improving model performance in low-resource settings.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper addresses the challenges of Handwritten Text Recognition (HTR) in low-resource settings by proposing a novel framework that uses Optimal Transport (OT) to align visual features from unlabeled word images with semantic word representations. The methodology involves an iterative bootstrapping process that begins with a minimal set of labeled examples, employs OT to generate high-confidence pseudo-labels, and progressively retrains the model on an expanding dataset, resulting in significant improvements in recognition accuracy on low-resource benchmarks, demonstrating the framework's effectiveness in leveraging lexical priors with limited annotated data.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by reframing HTR as an Optimal Transport-driven visual-semantic alignment with iterative pseudo-labeling, which significantly advances the state-of-the-art for low-resource scenarios.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfields of computer vision and machine learning for HTR, particularly in low-resource applications like historical document analysis, though its influence may remain specialized.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper represents a strong and valuable contribution to HTR in resource-constrained settings, making it essential for researchers in document analysis and machine learning to be aware of its innovative approach and demonstrated improvements.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/3e441f22d871254f1f61d4b28cecc189c8e14187",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 16,
      "average_h_index": 6.8,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Petros Georgoulas Wraight",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381371719"
        },
        {
          "name": "Giorgos Sfikas",
          "h_index": 16,
          "profile_url": "https://www.semanticscholar.org/author/1876181"
        },
        {
          "name": "Ioannis Kordonis",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2350855344"
        },
        {
          "name": "Petros Maragos",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2292179285"
        },
        {
          "name": "George Retsinas",
          "h_index": 15,
          "profile_url": "https://www.semanticscholar.org/author/1730770"
        }
      ]
    },
    {
      "id": "2509.16979",
      "title": "Leveraging Multiple Speech Enhancers for Non-Intrusive Intelligibility\n  Prediction for Hearing-Impaired Listeners",
      "authors": [
        "Boxuan Cao",
        "Linkai Li",
        "Hanlin Yu",
        "Changgeng Mo",
        "Haoshuai Zhou",
        "Shan Xiang Wang"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Speech intelligibility evaluation for hearing-impaired (HI) listeners is\nessential for assessing hearing aid performance, traditionally relying on\nlistening tests or intrusive methods like HASPI. However, these methods require\nclean reference signals, which are often unavailable in real-world conditions,\ncreating a gap between lab-based and real-world assessments. To address this,\nwe propose a non-intrusive intelligibility prediction framework that leverages\nspeech enhancers to provide a parallel enhanced-signal pathway, enabling robust\npredictions without reference signals. We evaluate three state-of-the-art\nenhancers and demonstrate that prediction performance depends on the choice of\nenhancer, with ensembles of strong enhancers yielding the best results. To\nimprove cross-dataset generalization, we introduce a 2-clips augmentation\nstrategy that enhances listener-specific variability, boosting robustness on\nunseen datasets. Our approach consistently outperforms the non-intrusive\nbaseline, CPC2 Champion across multiple datasets, highlighting the potential of\nenhancer-guided non-intrusive intelligibility prediction for real-world\napplications.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16979v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16979v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.416,
      "weak_supervision_score": 0.412,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.344,
      "datasets_score": 0.342,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a non-intrusive speech intelligibility prediction framework using speech enhancers and augmentation strategies, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "The paper introduces a 2-clips augmentation strategy to enhance data variability for better generalization, which indirectly relates to using programmatic methods for data enhancement, but it does not primarily rely on generating noisy or imprecise labels for training, as required in weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16986",
      "title": "VCE: Safe Autoregressive Image Generation via Visual Contrast\n  Exploitation",
      "authors": [
        "Feng Han",
        "Chao Gong",
        "Zhipeng Wei",
        "Jingjing Chen",
        "Yu-Gang Jiang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recently, autoregressive image generation models have wowed audiences with\ntheir remarkable capability in creating surprisingly realistic images. Models\nsuch as GPT-4o and LlamaGen can not only produce images that faithfully mimic\nrenowned artistic styles like Ghibli, Van Gogh, or Picasso, but also\npotentially generate Not-Safe-For-Work (NSFW) content, raising significant\nconcerns regarding copyright infringement and ethical use. Despite these\nconcerns, methods to safeguard autoregressive text-to-image models remain\nunderexplored. Previous concept erasure methods, primarily designed for\ndiffusion models that operate in denoising latent space, are not directly\napplicable to autoregressive models that generate images token by token. To\naddress this critical gap, we propose Visual Contrast Exploitation (VCE), a\nnovel framework comprising: (1) an innovative contrastive image pair\nconstruction paradigm that precisely decouples unsafe concepts from their\nassociated content semantics, and (2) a sophisticated DPO-based training\napproach that enhances the model's ability to identify and leverage visual\ncontrastive features from image pairs, enabling precise concept erasure. Our\ncomprehensive experiments across three challenging tasks-artist style erasure,\nexplicit content erasure, and object removal-demonstrate that our method\neffectively secures the model, achieving state-of-the-art results while erasing\nunsafe concepts and maintaining the integrity of unrelated safe concepts. The\ncode and models are available at https://github.com/Maplebb/VCE.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16986v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16986v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.402,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.443,
      "distributed_training_score": 0.341,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper employs Direct Preference Optimization (DPO) for training to align model outputs with preferences, but it does not involve training a separate reward model on human-ranked data or explicitly use human feedback. RLHF specifically requires these elements, which are absent here.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses safety in autoregressive image generation models and does not involve diffusion models, iterative refinement for logical tasks, or multi-step reasoning processes as defined for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16988",
      "title": "A Cross-Hierarchical Multi-Feature Fusion Network Based on Multiscale\n  Encoder-Decoder for Hyperspectral Change Detection",
      "authors": [
        "Mingshuai Sheng",
        "Bhatti Uzair Aslam",
        "Junfeng Zhang",
        "Siling Feng",
        "Yonis Gulzar"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Hyperspectral change detection (HCD) aims to accurately identify land-cover\nchanges in hyperspectral images of the same area acquired at different times,\nwith key applications in environmental monitoring and disaster assessment. To\naddress limitations of existing methods, such as insufficient use of multiscale\nfeatures and low efficiency in differential feature fusion, this paper proposes\na cross-hierarchical multi-feature fusion network (CHMFFN) based on a\nmultiscale encoder-decoder architecture. The front-end adopts a multiscale\nfeature extraction subnetwork, built on an encoder-decoder backbone with\nresidual connections and a dual-core channel-spatial attention (DCCSA) module\nto extract spectral-spatial-temporal features (SSTF). The encoder captures\nmultiscale features from shallow details to deep semantics via residual blocks\nand convolutional kernels with varying receptive fields. The decoder restores\nspatial resolution and suppresses noise information through skip connections\nintegrating encoder features. Additionally, a spectral-temporal change feature\nlearning (STCFL) module learns cross-temporal change features at different\nlevels, strengthening inter-temporal difference capture. An adaptive fusion of\nadvanced features (AFAF) module dynamically balances hierarchical differential\nfeatures via adaptive weights, enhancing representation of complex changes.\nExperiments on four public hyperspectral datasets show CHMFFN outperforms\nstate-of-the-art methods, verifying its effectiveness.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16988v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16988v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.365,
      "datasets_score": 0.36,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16989",
      "title": "PTQTP: Post-Training Quantization to Trit-Planes for Large Language\n  Models",
      "authors": [
        "He Xiao",
        "Runming Yang",
        "Qingyao Yang",
        "Wendong Xu",
        "Zheng Li",
        "Yupeng Su",
        "Zhengwu Liu",
        "Hongxia Yang",
        "Ngai Wong"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Post-training quantization (PTQ) of large language models (LLMs) to extremely\nlow bit-widths remains challenging due to the fundamental trade-off between\ncomputational efficiency and model expressiveness. While existing ultra-low-bit\nPTQ methods rely on binary approximations or complex compensation mechanisms,\nthey suffer from either limited representational capacity or computational\noverhead that undermines their efficiency gains. We introduce PTQ to\nTrit-Planes (PTQTP), the first ternary-weight PTQ framework that decomposes\nweight matrices into structured ternary {-1, 0, 1} trit-planes using 2x1.58-bit\nrepresentation. PTQTP achieves multiplication-free inference, identical to\n1-bit quantization, while maintaining superior expressiveness through its novel\nstructured decomposition. Our approach provides: (1) a theoretically grounded\nprogressive approximation algorithm ensuring global weight consistency; (2)\nmodel-agnostic deployment across diverse modern LLMs without architectural\nmodifications; and (3) uniform ternary operations that eliminate the need for\nmixed-precision or compensation schemes. Comprehensive experiments across\nLLaMA3.x and Qwen3 model families (0.6B-70B parameters) demonstrate that PTQTP\nsignificantly outperforms existing low-bit PTQ methods, achieving 82.4%\nmathematical reasoning retention versus 0% for competing approaches. PTQTP\napproaches and sometimes surpasses 1.58-bit quantization-aware training\nperformance while requiring only single-hour quantization compared to 10-14 GPU\ndays for training-based methods. These results establish PTQTP as a practical\nsolution for efficient LLM deployment in resource-constrained environments.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16989v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16989v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.334,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.456,
      "datasets_score": 0.281,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on post-training quantization techniques for LLMs to improve efficiency and expressiveness, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It does not involve adapting diffusion for tasks like holistic correction of reasoning paths.",
      "distributed_training_justification": "The paper discusses quantization methods for LLMs and mentions runtime efficiencies, such as speedups in quantization processes, but it does not address distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes. The core contribution is on model compression, not training acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.16990",
      "title": "Advancing Speech Understanding in Speech-Aware Language Models with GRPO",
      "authors": [
        "Avishai Elmakies",
        "Hagai Aronowitz",
        "Nimrod Shabtay",
        "Eli Schwartz",
        "Ron Hoory",
        "Avihu Dekel"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.SD (Sound)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "In this paper, we introduce a Group Relative Policy Optimization (GRPO)-based\nmethod for training Speech-Aware Large Language Models (SALLMs) on open-format\nspeech understanding tasks, such as Spoken Question Answering and Automatic\nSpeech Translation. SALLMs have proven highly effective for speech\nunderstanding tasks. GRPO has recently gained traction for its efficiency in\ntraining LLMs, and prior work has explored its application to SALLMs, primarily\nin multiple-choice tasks. Building on this, we focus on open-format tasks that\nbetter reflect the generative abilities of the models. Our approach leverages\nGRPO with BLEU as the reward signal to optimize SALLMs, and we demonstrate\nempirically that it surpasses standard SFT across several key metrics. Finally,\nwe explore the potential of incorporating off-policy samples within GRPO for\nthese tasks, highlighting avenues for further improvement and further research.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.16990v1",
      "pdf_url": "http://arxiv.org/pdf/2509.16990v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.497,
      "weak_supervision_score": 0.415,
      "diffusion_reasoning_score": 0.431,
      "distributed_training_score": 0.384,
      "datasets_score": 0.296,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses GRPO with BLEU as a reward signal, which is an automated metric derived from reference data, not human-ranked preferences or a reward model trained on human feedback. Therefore, it does not involve RLHF.",
      "weak_supervision_justification": "The paper focuses on reinforcement learning for fine-tuning SALLMs using metrics like BLEU, rather than training models with programmatically generated noisy labels as in weak supervision. There is no mention of label generation from imprecise sources.",
      "diffusion_reasoning_justification": "The paper employs GRPO for optimizing SALLMs, which is a reinforcement learning method, and does not involve diffusion models, iterative refinement for logical reasoning, or treating a chain-of-thought as a refinable entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17000",
      "title": "Adaptive Overclocking: Dynamic Control of Thinking Path Length via\n  Real-Time Reasoning Signals",
      "authors": [
        "Shuhao Jiang",
        "Songbo Wang",
        "Yang Qiao",
        "Chun Xu",
        "Chaoyang Zheng",
        "Shengyi Zhou",
        "Huanjun Wang",
        "Fangming Li",
        "Cong Zhang",
        "Jiyu Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Reasoning Models (LRMs) often suffer from computational inefficiency\ndue to overthinking, where a fixed reasoning budget fails to match the varying\ncomplexity of tasks. To address this issue, we propose Adaptive Overclocking, a\nmethod that makes the overclocking hyperparameter $\\alpha$ dynamic and\ncontext-aware. Our method adjusts reasoning speed in real time through two\ncomplementary signals: (1) token-level model uncertainty for fine-grained\nstep-wise control, and (2) input complexity estimation for informed\ninitialization. We implement this approach with three strategies:\nUncertainty-Aware Alpha Scheduling (UA-$\\alpha$S), Complexity-Guided Alpha\nInitialization (CG-$\\alpha$I), and a Hybrid Adaptive Control (HAC) that\ncombines both. Experiments on GSM8K, MATH, and SVAMP show that HAC achieves\nsuperior accuracy-latency trade-offs, reducing unnecessary computation on\nsimple problems while allocating more resources to challenging ones. By\nmitigating overthinking, Adaptive Overclocking enhances both efficiency and\noverall reasoning performance.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17000v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17000v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.438,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.533,
      "distributed_training_score": 0.424,
      "datasets_score": 0.287,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces Adaptive Overclocking for dynamic control of reasoning in Large Reasoning Models, focusing on inference-time adjustments without any mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses Chain-of-Thought reasoning and overclocking hidden states for efficiency, but it does not involve diffusion models, iterative refinement processes, or treating reasoning as a holistically corrected entity over multiple steps.",
      "distributed_training_justification": "The paper focuses on real-time inference adjustments for reasoning efficiency, with no discussion of training processes, parallel computing, data partitioning, or multi-node systems.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17012",
      "title": "DocIQ: A Benchmark Dataset and Feature Fusion Network for Document Image\n  Quality Assessment",
      "authors": [
        "Zhichao Ma",
        "Fan Huang",
        "Lu Zhao",
        "Fengjun Guo",
        "Guangtao Zhai",
        "Xiongkuo Min"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Document image quality assessment (DIQA) is an important component for\nvarious applications, including optical character recognition (OCR), document\nrestoration, and the evaluation of document image processing systems. In this\npaper, we introduce a subjective DIQA dataset DIQA-5000. The DIQA-5000 dataset\ncomprises 5,000 document images, generated by applying multiple document\nenhancement techniques to 500 real-world images with diverse distortions. Each\nenhanced image was rated by 15 subjects across three rating dimensions: overall\nquality, sharpness, and color fidelity. Furthermore, we propose a specialized\nno-reference DIQA model that exploits document layout features to maintain\nquality perception at reduced resolutions to lower computational cost.\nRecognizing that image quality is influenced by both low-level and high-level\nvisual features, we designed a feature fusion module to extract and integrate\nmulti-level features from document images. To generate multi-dimensional\nscores, our model employs independent quality heads for each dimension to\npredict score distributions, allowing it to learn distinct aspects of document\nimage quality. Experimental results demonstrate that our method outperforms\ncurrent state-of-the-art general-purpose IQA models on both DIQA-5000 and an\nadditional document image dataset focused on OCR accuracy.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17012v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17012v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.323,
      "datasets_score": 0.449,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes the introduction of a new dataset, DIQA-5000, specifically designed for document image quality assessment in AI applications. It details the dataset's creation process, involving curation of 5,000 images with diverse distortions and enhancements, human rating methodologies, and analysis of scores across dimensions. Additionally, it benchmarks the dataset with existing and proposed models, directly aligning with research on creating, analyzing, and evaluating datasets for machine learning.",
      "llm_score_status": "completed",
      "summary": "This paper introduces DIQA-5000, a new benchmark dataset comprising 5,000 document images with diverse distortions, enhanced using various techniques and rated by 15 subjects on dimensions like overall quality, sharpness, and color fidelity, to address the limitations of existing image quality assessment methods for documents. It proposes the DocIQ model, a no-reference quality assessment network that integrates document layout features with multi-level feature fusion and independent quality heads to predict multi-dimensional scores, demonstrating superior performance over state-of-the-art models on DIQA-5000 and another dataset focused on OCR accuracy.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel dataset specifically for document image quality assessment, filling a significant gap in existing research, and proposes a specialized model with feature fusion techniques that advance the state-of-the-art in handling document-specific distortions.",
      "impact_score": "Moderate",
      "impact_justification": "The work provides a new benchmark dataset and model that could be built upon in subfields like document processing and OCR, likely leading to citations and advancements in specialized applications, though its broader influence may be limited to these areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution to computer vision and document analysis by introducing essential tools for quality assessment, making it valuable for researchers in related fields to stay informed and potentially apply these methods.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/4bd246e87f0725bc2bf0516276dffcc0c0f1e2b5",
      "total_authors": 6,
      "authors_found": 5,
      "highest_h_index": 45,
      "average_h_index": 13.4,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Zhichao Ma",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381722027"
        },
        {
          "name": "Fan Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2309901871"
        },
        {
          "name": "Lu Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381697012"
        },
        {
          "name": "Fengjun Guo",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Guangtao Zhai",
          "h_index": 22,
          "profile_url": "https://www.semanticscholar.org/author/2266393212"
        },
        {
          "name": "Xiongkuo Min",
          "h_index": 45,
          "profile_url": "https://www.semanticscholar.org/author/2246414"
        }
      ]
    },
    {
      "id": "2509.17022",
      "title": "VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven\n  Module",
      "authors": [
        "Kam Man Wu",
        "Zeyue Tian",
        "Liya Ji",
        "Qifeng Chen"
      ],
      "categories": [
        "cs.MM (Multimedia)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.SD (Sound)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Video and audio inpainting for mixed audio-visual content has become a\ncrucial task in multimedia editing recently. However, precisely removing an\nobject and its corresponding audio from a video without affecting the rest of\nthe scene remains a significant challenge. To address this, we propose\nVAInpaint, a novel pipeline that first utilizes a segmentation model to\ngenerate masks and guide a video inpainting model in removing objects. At the\nsame time, an LLM then analyzes the scene globally, while a region-specific\nmodel provides localized descriptions. Both the overall and regional\ndescriptions will be inputted into an LLM, which will refine the content and\nturn it into text queries for our text-driven audio separation model. Our audio\nseparation model is fine-tuned on a customized dataset comprising segmented\nMUSIC instrument images and VGGSound backgrounds to enhance its generalization\nperformance. Experiments show that our method achieves performance comparable\nto current benchmarks in both audio and video inpainting.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17022v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17022v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.316,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17024",
      "title": "When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image\n  Restoration",
      "authors": [
        "Wenxuan Fang",
        "Jili Fan",
        "Chao Wang",
        "Xiantao Hu",
        "Jiangwei Weng",
        "Ying Tai",
        "Jian Yang",
        "Jun Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Adverse Weather Image Restoration (AWIR) is a highly challenging task due to\nthe unpredictable and dynamic nature of weather-related degradations.\nTraditional task-specific methods often fail to generalize to unseen or complex\ndegradation types, while recent prompt-learning approaches depend heavily on\nthe degradation estimation capabilities of vision-language models, resulting in\ninconsistent restorations. In this paper, we propose \\textbf{LCDiff}, a novel\nframework comprising two key components: \\textit{Lumina-Chroma Decomposition\nNetwork} (LCDN) and \\textit{Lumina-Guided Diffusion Model} (LGDM). LCDN\nprocesses degraded images in the YCbCr color space, separately handling\ndegradation-related luminance and degradation-invariant chrominance components.\nThis decomposition effectively mitigates weather-induced degradation while\npreserving color fidelity. To further enhance restoration quality, LGDM\nleverages degradation-related luminance information as a guiding condition,\neliminating the need for explicit degradation prompts. Additionally, LGDM\nincorporates a \\textit{Dynamic Time Step Loss} to optimize the denoising\nnetwork, ensuring a balanced recovery of both low- and high-frequency features\nin the image. Finally, we present DriveWeather, a comprehensive all-weather\ndriving dataset designed to enable robust evaluation. Extensive experiments\ndemonstrate that our approach surpasses state-of-the-art methods, setting a new\nbenchmark in AWIR. The dataset and code are available at:\nhttps://github.com/fiwy0527/LCDiff.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17024v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17024v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.303,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.428,
      "distributed_training_score": 0.298,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for adverse weather image restoration, specifically through color-space decomposition and guided diffusion for visual data processing. It does not involve adapting diffusion for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks, as it is centered on image enhancement rather than cognitive or reasoning applications.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17027",
      "title": "Efficient 3D Scene Reconstruction and Simulation from Sparse Endoscopic\n  Views",
      "authors": [
        "Zhenya Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Surgical simulation is essential for medical training, enabling practitioners\nto develop crucial skills in a risk-free environment while improving patient\nsafety and surgical outcomes. However, conventional methods for building\nsimulation environments are cumbersome, time-consuming, and difficult to scale,\noften resulting in poor details and unrealistic simulations. In this paper, we\npropose a Gaussian Splatting-based framework to directly reconstruct\ninteractive surgical scenes from endoscopic data while ensuring efficiency,\nrendering quality, and realism. A key challenge in this data-driven simulation\nparadigm is the restricted movement of endoscopic cameras, which limits\nviewpoint diversity. As a result, the Gaussian Splatting representation\noverfits specific perspectives, leading to reduced geometric accuracy. To\naddress this issue, we introduce a novel virtual camera-based regularization\nmethod that adaptively samples virtual viewpoints around the scene and\nincorporates them into the optimization process to mitigate overfitting. An\neffective depth-based regularization is applied to both real and virtual views\nto further refine the scene geometry. To enable fast deformation simulation, we\npropose a sparse control node-based Material Point Method, which integrates\nphysical properties into the reconstructed scene while significantly reducing\ncomputational costs. Experimental results on representative surgical data\ndemonstrate that our method can efficiently reconstruct and simulate surgical\nscenes from sparse endoscopic views. Notably, our method takes only a few\nminutes to reconstruct the surgical scene and is able to produce physically\nplausible deformations in real-time with user-defined interactions.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17027v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17027v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.251,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.33,
      "datasets_score": 0.252,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17030",
      "title": "The Transfer Neurons Hypothesis: An Underlying Mechanism for Language\n  Latent Space Transitions in Multilingual LLMs",
      "authors": [
        "Hinata Tezuka",
        "Naoya Inoue"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent studies have suggested a processing framework for multilingual inputs\nin decoder-based LLMs: early layers convert inputs into English-centric and\nlanguage-agnostic representations; middle layers perform reasoning within an\nEnglish-centric latent space; and final layers generate outputs by transforming\nthese representations back into language-specific latent spaces. However, the\ninternal dynamics of such transformation and the underlying mechanism remain\nunderexplored. Towards a deeper understanding of this framework, we propose and\nempirically validate The Transfer Neurons Hypothesis: certain neurons in the\nMLP module are responsible for transferring representations between\nlanguage-specific latent spaces and a shared semantic latent space.\nFurthermore, we show that one function of language-specific neurons, as\nidentified in recent studies, is to facilitate movement between latent spaces.\nFinally, we show that transfer neurons are critical for reasoning in\nmultilingual LLMs.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17030v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17030v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.323,
      "diffusion_reasoning_score": 0.51,
      "distributed_training_score": 0.378,
      "datasets_score": 0.275,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on the Transfer Neurons Hypothesis in multilingual LLMs, exploring how specific neurons facilitate representation transfers between language-specific and shared semantic spaces. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. There is no component related to adapting diffusion for Chain-of-Thought or holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17034",
      "title": "Long-Tailed Out-of-Distribution Detection with Refined Separate Class\n  Learning",
      "authors": [
        "Shuai Feng",
        "Yuxin Ge",
        "Yuntao Du",
        "Mingcai Chen",
        "Chongjun Wang",
        "Lei Feng"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Out-of-distribution (OOD) detection is crucial for deploying robust machine\nlearning models. However, when training data follows a long-tailed\ndistribution, the model's ability to accurately detect OOD samples is\nsignificantly compromised, due to the confusion between OOD samples and\nhead/tail classes. To distinguish OOD samples from both head and tail classes,\nthe separate class learning (SCL) approach has emerged as a promising solution,\nwhich separately conduct head-specific and tail-specific class learning. To\nthis end, we examine the limitations of existing works of SCL and reveal that\nthe OOD detection performance is notably influenced by the use of static\nscaling temperature value and the presence of uninformative outliers. To\nmitigate these limitations, we propose a novel approach termed Refined Separate\nClass Learning (RSCL), which leverages dynamic class-wise temperature\nadjustment to modulate the temperature parameter for each in-distribution class\nand informative outlier mining to identify diverse types of outliers based on\ntheir affinity with head and tail classes. Extensive experiments demonstrate\nthat RSCL achieves superior OOD detection performance while improving the\nclassification accuracy on in-distribution data.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17034v2",
      "pdf_url": "http://arxiv.org/pdf/2509.17034v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.388,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17037",
      "title": "KAHAN: Knowledge-Augmented Hierarchical Analysis and Narration for\n  Financial Data Narration",
      "authors": [
        "Yajing Yang",
        "Tony Deng",
        "Min-Yen Kan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We propose KAHAN, a knowledge-augmented hierarchical framework that\nsystematically extracts insights from raw tabular data at entity, pairwise,\ngroup, and system levels. KAHAN uniquely leverages LLMs as domain experts to\ndrive the analysis. On DataTales financial reporting benchmark, KAHAN\noutperforms existing approaches by over 20% on narrative quality (GPT-4o),\nmaintains 98.2% factuality, and demonstrates practical utility in human\nevaluation. Our results reveal that knowledge quality drives model performance\nthrough distillation, hierarchical analysis benefits vary with market\ncomplexity, and the framework transfers effectively to healthcare domains. The\ndata and code are available at https://github.com/yajingyang/kahan.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17037v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17037v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.294,
      "datasets_score": 0.35,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces KAHAN, a framework for hierarchical analysis and narration of financial data using LLMs, focusing on knowledge augmentation and multi-level insight extraction. It does not involve diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction, which are core to diffusion-based reasoning. Thus, there is no connection to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17040",
      "title": "From Easy to Hard: The MIR Benchmark for Progressive Interleaved\n  Multi-Image Reasoning",
      "authors": [
        "Hang Du",
        "Jiayang Zhang",
        "Guoshun Nan",
        "Wendi Deng",
        "Zhenyan Chen",
        "Chenyang Zhang",
        "Wang Xiao",
        "Shan Huang",
        "Yuqi Pan",
        "Tao Qi",
        "Sicong Leng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multi-image Interleaved Reasoning aims to improve Multi-modal Large Language\nModels (MLLMs) ability to jointly comprehend and reason across multiple images\nand their associated textual contexts, introducing unique challenges beyond\nsingle-image or non-interleaved multi-image tasks. While current multi-image\nbenchmarks overlook interleaved textual contexts and neglect distinct\nrelationships between individual images and their associated texts, enabling\nmodels to reason over multi-image interleaved data may significantly enhance\ntheir comprehension of complex scenes and better capture cross-modal\ncorrelations. To bridge this gap, we introduce a novel benchmark MIR, requiring\njoint reasoning over multiple images accompanied by interleaved textual\ncontexts to accurately associate image regions with corresponding texts and\nlogically connect information across images. To enhance MLLMs ability to\ncomprehend multi-image interleaved data, we introduce reasoning steps for each\ninstance within the benchmark and propose a stage-wise curriculum learning\nstrategy. This strategy follows an \"easy to hard\" approach, progressively\nguiding models from simple to complex scenarios, thereby enhancing their\nability to handle challenging tasks. Extensive experiments benchmarking\nmultiple MLLMs demonstrate that our method significantly enhances models\nreasoning performance on MIR and other established benchmarks. We believe that\nMIR will encourage further research into multi-image interleaved reasoning,\nfacilitating advancements in MLLMs capability to handle complex inter-modal\ntasks.Our code and dataset are available at\nhttps://github.com/Shelly-coder239/MIRBench.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17040v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17040v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.49,
      "distributed_training_score": 0.369,
      "datasets_score": 0.425,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a benchmark for multi-image interleaved reasoning and a curriculum learning strategy for MLLMs, but it does not involve diffusion models, iterative refinement processes, or treating a chain-of-thought as a single entity for correction. There is no mention of diffusion-based techniques for logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and introduction of the MIR benchmark, including dataset curation from 138,277 images into 22,257 QA pairs across various categories, along with methodologies for organization, evaluation, and analysis to assess MLLMs. This directly aligns with research on datasets for AI applications.",
      "llm_score_status": "completed",
      "summary": "The paper introduces the MIR benchmark, a comprehensive dataset comprising 22,257 questions derived from 138,277 images, designed to evaluate and enhance Multi-modal Large Language Models' (MLLMs) ability to reason over interleaved multi-image and textual contexts across categories like sequential, spatial, and analytical tasks. It proposes a stage-wise curriculum learning strategy with structured reasoning steps to progressively train models from simple to complex scenarios, demonstrating through extensive experiments that this approach significantly improves MLLMs' performance on MIR and other benchmarks, thereby advancing research in multi-image interleaved reasoning.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel benchmark for interleaved multi-image reasoning, addressing gaps in existing benchmarks by incorporating complex text-to-region and region-to-region inferences, and proposes a new curriculum learning strategy to enhance model training. This represents a significant advancement in the state-of-the-art for multi-modal AI tasks.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in multi-modal large language models by providing a new benchmark and training method for handling interleaved multi-image data, potentially leading to citations and improvements within the subfields of computer vision and artificial intelligence. However, its applicability may be limited to specific advanced MLLM applications rather than broader commercial or general AI fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a high-quality contribution with a new benchmark and learning strategy that advances multi-image reasoning in MLLMs, making it essential for researchers focused on computer vision and AI to stay informed on emerging techniques. While innovative, it is not universally groundbreaking, positioning it as a valuable but not mandatory read.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6beddccc5f89c422c156d737a2eb25fa1c3a94c8",
      "total_authors": 11,
      "authors_found": 8,
      "highest_h_index": 9,
      "average_h_index": 1.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Hang Du",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Jiayang Zhang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2299278747"
        },
        {
          "name": "Guoshun Nan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2352268194"
        },
        {
          "name": "Wendi Deng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Zhenyan Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2335226939"
        },
        {
          "name": "Chenyang Zhang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381610978"
        },
        {
          "name": "Wang Xiao",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Shan Huang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381367125"
        },
        {
          "name": "Yuqi Pan",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2382250916"
        },
        {
          "name": "Tao Qi",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381377403"
        },
        {
          "name": "Sicong Leng",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2113966687"
        }
      ]
    },
    {
      "id": "2509.17041",
      "title": "Towards Generalized Synapse Detection Across Invertebrate Species",
      "authors": [
        "Samia Mohinta",
        "Daniel Franco-Barranco",
        "Shi Yan Lee",
        "Albert Cardona"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Behavioural differences across organisms, whether healthy or pathological,\nare closely tied to the structure of their neural circuits. Yet, the fine-scale\nsynaptic changes that give rise to these variations remain poorly understood,\nin part due to persistent challenges in detecting synapses reliably and at\nscale. Volume electron microscopy (EM) offers the resolution required to\ncapture synaptic architecture, but automated detection remains difficult due to\nsparse annotations, morphological variability, and cross-dataset domain shifts.\nTo address this, we make three key contributions. First, we curate a diverse EM\nbenchmark spanning four datasets across two invertebrate species: adult and\nlarval Drosophila melanogaster, and Megaphragma viggianii (micro-WASP). Second,\nwe propose SimpSyn, a single-stage Residual U-Net trained to predict\ndual-channel spherical masks around pre- and post-synaptic sites, designed to\nprioritize training and inference speeds and annotation efficiency over\narchitectural complexity. Third, we benchmark SimpSyn against Buhmann et al.'s\nSynful [1], a state-of-the-art multi-task model that jointly infers synaptic\npairs. Despite its simplicity, SimpSyn consistently outperforms Synful in\nF1-score across all volumes for synaptic site detection. While generalization\nacross datasets remains limited, SimpSyn achieves competitive performance when\ntrained on the combined cohort. Finally, ablations reveal that simple\npost-processing strategies - such as local peak detection and distance-based\nfiltering - yield strong performance without complex test-time heuristics.\nTaken together, our results suggest that lightweight models, when aligned with\ntask structure, offer a practical and scalable solution for synapse detection\nin large-scale connectomic pipelines.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17041v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17041v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.354,
      "datasets_score": 0.368,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17044",
      "title": "AgriDoctor: A Multimodal Intelligent Assistant for Agriculture",
      "authors": [
        "Mingqing Zhang",
        "Zhuoning Xu",
        "Peijie Wang",
        "Rongji Li",
        "Liang Wang",
        "Qiang Liu",
        "Jian Xu",
        "Xuyao Zhang",
        "Shu Wu",
        "Liang Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate crop disease diagnosis is essential for sustainable agriculture and\nglobal food security. Existing methods, which primarily rely on unimodal models\nsuch as image-based classifiers and object detectors, are limited in their\nability to incorporate domain-specific agricultural knowledge and lack support\nfor interactive, language-based understanding. Recent advances in large\nlanguage models (LLMs) and large vision-language models (LVLMs) have opened new\navenues for multimodal reasoning. However, their performance in agricultural\ncontexts remains limited due to the absence of specialized datasets and\ninsufficient domain adaptation. In this work, we propose AgriDoctor, a modular\nand extensible multimodal framework designed for intelligent crop disease\ndiagnosis and agricultural knowledge interaction. As a pioneering effort to\nintroduce agent-based multimodal reasoning into the agricultural domain,\nAgriDoctor offers a novel paradigm for building interactive and domain-adaptive\ncrop health solutions. It integrates five core components: a router,\nclassifier, detector, knowledge retriever and LLMs. To facilitate effective\ntraining and evaluation, we construct AgriMM, a comprehensive benchmark\ncomprising 400000 annotated disease images, 831 expert-curated knowledge\nentries, and 300000 bilingual prompts for intent-driven tool selection.\nExtensive experiments demonstrate that AgriDoctor, trained on AgriMM,\nsignificantly outperforms state-of-the-art LVLMs on fine-grained agricultural\ntasks, establishing a new paradigm for intelligent and sustainable farming\napplications.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17044v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17044v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.359,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17046",
      "title": "A Chain-of-thought Reasoning Breast Ultrasound Dataset Covering All\n  Histopathology Categories",
      "authors": [
        "Haojun Yu",
        "Youcheng Li",
        "Zihan Niu",
        "Nan Zhang",
        "Xuantong Gong",
        "Huan Li",
        "Zhiying Zou",
        "Haifeng Qi",
        "Zhenxiao Cao",
        "Zijie Lan",
        "Xingjian Yuan",
        "Jiating He",
        "Haokai Zhang",
        "Shengtao Zhang",
        "Zicheng Wang",
        "Dong Wang",
        "Ziwei Zhao",
        "Congying Chen",
        "Yong Wang",
        "Wangyan Qin",
        "Qingli Zhu",
        "Liwei Wang"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Breast ultrasound (BUS) is an essential tool for diagnosing breast lesions,\nwith millions of examinations per year. However, publicly available\nhigh-quality BUS benchmarks for AI development are limited in data scale and\nannotation richness. In this work, we present BUS-CoT, a BUS dataset for\nchain-of-thought (CoT) reasoning analysis, which contains 11,439 images of\n10,019 lesions from 4,838 patients and covers all 99 histopathology types. To\nfacilitate research on incentivizing CoT reasoning, we construct the reasoning\nprocesses based on observation, feature, diagnosis and pathology labels,\nannotated and verified by experienced experts. Moreover, by covering lesions of\nall histopathology types, we aim to facilitate robust AI systems in rare cases,\nwhich can be error-prone in clinical practice.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17046v2",
      "pdf_url": "http://arxiv.org/pdf/2509.17046v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.208,
      "weak_supervision_score": 0.275,
      "diffusion_reasoning_score": 0.363,
      "distributed_training_score": 0.25,
      "datasets_score": 0.374,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17049",
      "title": "Learning Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via\n  Query Optimization",
      "authors": [
        "Peng Wang",
        "Yong Li",
        "Lin Zhao",
        "Xiu-Shen Wei"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Fine-grained hashing has become a powerful solution for rapid and efficient\nimage retrieval, particularly in scenarios requiring high discrimination\nbetween visually similar categories. To enable each hash bit to correspond to\nspecific visual attributes, we propoe a novel method that harnesses learnable\nqueries for attribute-aware hash codes learning. This method deploys a tailored\nset of queries to capture and represent nuanced attribute-level information\nwithin the hashing process, thereby enhancing both the interpretability and\nrelevance of each hash bit. Building on this query-based optimization\nframework, we incorporate an auxiliary branch to help alleviate the challenges\nof complex landscape optimization often encountered with low-bit hash codes.\nThis auxiliary branch models high-order attribute interactions, reinforcing the\nrobustness and specificity of the generated hash codes. Experimental results on\nbenchmark datasets demonstrate that our method generates attribute-aware hash\ncodes and consistently outperforms state-of-the-art techniques in retrieval\naccuracy and robustness, especially for low-bit hash codes, underscoring its\npotential in fine-grained image hashing tasks.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17049v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17049v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.306,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17050",
      "title": "Geodesic Prototype Matching via Diffusion Maps for Interpretable\n  Fine-Grained Recognition",
      "authors": [
        "Junhao Jia",
        "Yunyou Liu",
        "Yifei Sun",
        "Huangwei Chen",
        "Feiwei Qin",
        "Changmiao Wang",
        "Yong Peng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Nonlinear manifolds are widespread in deep visual features, where Euclidean\ndistances often fail to capture true similarity. This limitation becomes\nparticularly severe in prototype-based interpretable fine-grained recognition,\nwhere subtle semantic distinctions are essential. To address this challenge, we\npropose a novel paradigm for prototype-based recognition that anchors\nsimilarity within the intrinsic geometry of deep features. Specifically, we\ndistill the latent manifold structure of each class into a diffusion space and\nintroduce a differentiable Nystr\\\"om interpolation, making the geometry\naccessible to both unseen samples and learnable prototypes. To ensure\nefficiency, we employ compact per-class landmark sets with periodic updates.\nThis design keeps the embedding aligned with the evolving backbone, enabling\nfast and scalable inference. Extensive experiments on the CUB-200-2011 and\nStanford Cars datasets show that our GeoProto framework produces prototypes\nfocusing on semantically aligned parts, significantly outperforming Euclidean\nprototype networks.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17050v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17050v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.464,
      "distributed_training_score": 0.32,
      "datasets_score": 0.314,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion maps for manifold learning in visual feature spaces to improve prototype matching in fine-grained image recognition, focusing on geodesic distances and intrinsic geometry. However, it does not involve adapting iterative refinement processes of diffusion models for solving complex logical tasks, multi-step reasoning, or holistic correction of a Chain-of-Thought. Since the paper lacks any component for logical reasoning using diffusion, it does not align with the defined topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17054",
      "title": "TactfulToM: Do LLMs Have the Theory of Mind Ability to Understand White\n  Lies?",
      "authors": [
        "Yiwei Liu",
        "Emma Jane Pretty",
        "Jiahao Huang",
        "Saku Sugawara"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While recent studies explore Large Language Models' (LLMs) performance on\nTheory of Mind (ToM) reasoning tasks, research on ToM abilities that require\nmore nuanced social context is limited, such as white lies. We introduce\nTactfulToM, a novel English benchmark designed to evaluate LLMs' ability to\nunderstand white lies within real-life conversations and reason about prosocial\nmotivations behind them, particularly when they are used to spare others'\nfeelings and maintain social harmony. Our benchmark is generated through a\nmulti-stage human-in-the-loop pipeline where LLMs expand manually designed seed\nstories into conversations to maintain the information asymmetry between\nparticipants necessary for authentic white lies. We show that TactfulToM is\nchallenging for state-of-the-art models, which perform substantially below\nhumans, revealing shortcomings in their ability to fully comprehend the ToM\nreasoning that enables true understanding of white lies.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17054v2",
      "pdf_url": "http://arxiv.org/pdf/2509.17054v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.283,
      "datasets_score": 0.343,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper involves a human-in-the-loop process for generating and validating benchmark data, which includes human feedback, but this is for dataset creation and evaluation, not for training a reward model or fine-tuning an AI using reinforcement learning as defined in RLHF. Thus, it is only loosely connected.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluating LLMs' Theory of Mind abilities using benchmarks and mentions Chain-of-Thought prompting, but it does not involve any diffusion-based models, iterative refinement processes, or adaptations for multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17062",
      "title": "From domain-landmark graph learning to problem-landmark graph generation",
      "authors": [
        "Cristian Pérez-Corral",
        "Antonio Garrido",
        "Laura Sebastia"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Landmarks have long played a pivotal role in automated planning, serving as\ncrucial elements for improving the planning algorithms. The main limitation of\nclassical landmark extraction methods is their sensitivity to specific planning\ntasks. This results in landmarks fully tailored to individual instances,\nthereby limiting their applicability across other instances of the same\nplanning domain. We propose a novel approach that learns landmark relationships\nfrom multiple planning tasks of a planning domain. This leads to the creation\nof a \\textit{probabilistic lifted ordering graph}, as a structure that captures\nweighted abstractions of relationships between parameterized landmarks.\nAlthough these orderings are not 100\\% true (they are probabilistic), they can\nstill be very useful in planning. Next, given a new planning task for that\ndomain, we instantiate the relationships from that graph to this particular\ninstance. This instantiation operates in two phases. First, it generates two\ngraphs: the former instantiating information from the initial state and the\nlatter from the goal state. Second, it combines these two graphs into one\nunified graph by searching equivalences to extract landmark orderings. We\nevaluate the precision and recallof the information found by our approach over\nwell-known planning domains.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17062v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17062v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.373,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.309,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses learning probabilistic lifted ordering graphs for automated planning tasks, focusing on landmarks and graph generation from planning domains. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning, as required for this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17065",
      "title": "CardiacCLIP: Video-based CLIP Adaptation for LVEF Prediction in a\n  Few-shot Manner",
      "authors": [
        "Yao Du",
        "Jiarong Guo",
        "Xiaomeng Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Echocardiography is a vital non-invasive modality for cardiac assessment,\nwith left ventricular ejection fraction (LVEF) serving as a key indicator of\nheart function. Existing LVEF estimation methods depend on large-scale\nannotated video datasets, which are costly and limit adaptability across\nvarious clinical settings. Recent vision-language models for echocardiography,\nsuch as EchoCLIP, apply image-to-text pretraining but fail to capture crucial\ntemporal dynamics and localized cardiac structures essential for accurate\ndiagnosis. To address these challenges, we propose CardiacCLIP, a video-based\nframework that enhances LVEF prediction through attention-based frame\naggregation and multi-resolution input scaling. Specifically, we introduce MFL\n(Multi Frame Learning), a novel attention-based mechanism for selectively\nfusing informative frames, and EchoZoom, a multi-scale feature extraction\nstrategy that refines spatial representations of cardiac structures. As a novel\nadaptation of CLIP models for few-shot echocardiogram video analysis, our\napproach significantly improves diagnostic accuracy, reducing MAE by 2.07 on\nthe EchoNet-Dynamic dataset under 1-shot setting. The code is available at\nhttps://github.com/xmed-lab/CardiacCLIP.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17065v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17065v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.346,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17066",
      "title": "RALLM-POI: Retrieval-Augmented LLM for Zero-shot Next POI Recommendation\n  with Geographical Reranking",
      "authors": [
        "Kunrong Li",
        "Kwan Hui Lim"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Next point-of-interest (POI) recommendation predicts a user's next\ndestination from historical movements. Traditional models require intensive\ntraining, while LLMs offer flexible and generalizable zero-shot solutions but\noften generate generic or geographically irrelevant results due to missing\ntrajectory and spatial context. To address these issues, we propose RALLM-POI,\na framework that couples LLMs with retrieval-augmented generation and\nself-rectification. We first propose a Historical Trajectory Retriever (HTR)\nthat retrieves relevant past trajectories to serve as contextual references,\nwhich are then reranked by a Geographical Distance Reranker (GDR) for\nprioritizing spatially relevant trajectories. Lastly, an Agentic LLM Rectifier\n(ALR) is designed to refine outputs through self-reflection. Without additional\ntraining, RALLM-POI achieves substantial accuracy gains across three real-world\nFoursquare datasets, outperforming both conventional and LLM-based baselines.\nCode is released at https://github.com/LKRcrocodile/RALLM-POI.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17066v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17066v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.439,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.341,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a retrieval-augmented LLM framework for POI recommendation, involving retrieval, reranking, and self-rectification, but does not involve human feedback, reward models, or reinforcement learning for model alignment. There is no mention of training on human-ranked data or fine-tuning via RL, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's approach uses retrieval, geographical reranking, and self-reflection for refining outputs, but it does not employ diffusion models or adapt iterative refinement processes for multi-step logical reasoning as defined. There is no evidence of treating a Chain-of-Thought as a holistic entity for correction via diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17068",
      "title": "Intention-aware Hierarchical Diffusion Model for Long-term Trajectory\n  Anomaly Detection",
      "authors": [
        "Chen Wang",
        "Sarah Erfani",
        "Tansu Alpcan",
        "Christopher Leckie"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Long-term trajectory anomaly detection is a challenging problem due to the\ndiversity and complex spatiotemporal dependencies in trajectory data. Existing\ntrajectory anomaly detection methods fail to simultaneously consider both the\nhigh-level intentions of agents as well as the low-level details of the agent's\nnavigation when analysing an agent's trajectories. This limits their ability to\ncapture the full diversity of normal trajectories. In this paper, we propose an\nunsupervised trajectory anomaly detection method named Intention-aware\nHierarchical Diffusion model (IHiD), which detects anomalies through both\nhigh-level intent evaluation and low-level sub-trajectory analysis. Our\napproach leverages Inverse Q Learning as the high-level model to assess whether\na selected subgoal aligns with an agent's intention based on predicted\nQ-values. Meanwhile, a diffusion model serves as the low-level model to\ngenerate sub-trajectories conditioned on subgoal information, with anomaly\ndetection based on reconstruction error. By integrating both models, IHiD\neffectively utilises subgoal transition knowledge and is designed to capture\nthe diverse distribution of normal trajectories. Our experiments show that the\nproposed method IHiD achieves up to 30.2% improvement in anomaly detection\nperformance in terms of F1 score over state-of-the-art baselines.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17068v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17068v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.465,
      "distributed_training_score": 0.308,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a diffusion model for generating sub-trajectories in anomaly detection, which involves iterative refinement processes typical of diffusion models. However, it applies this to spatial-temporal trajectory generation and reconstruction error analysis, not to solving complex logical tasks or treating a Chain-of-Thought as a holistic entity for multi-step reasoning. Thus, while there is a shared use of diffusion mechanisms, the paper does not focus on logical reasoning, making it only tangentially relevant.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17074",
      "title": "Informative Text-Image Alignment for Visual Affordance Learning with\n  Foundation Models",
      "authors": [
        "Qian Zhang",
        "Lin Zhang",
        "Xing Fang",
        "Mingxin Zhang",
        "Zhiyuan Wei",
        "Ran Song",
        "Wei Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Visual affordance learning is crucial for robots to understand and interact\neffectively with the physical world. Recent advances in this field attempt to\nleverage pre-trained knowledge of vision-language foundation models to learn\naffordance properties with limited training data, providing a novel paradigm\nfor visual affordance learning. However, these methods overlook the\nsignificance of maintaining feature alignment between visual images and\nlanguage descriptions for identifying affordance areas with textual guidance,\nand thus may lead to suboptimal results. In this paper, we present an\ninformative framework for text-guided affordance learning, which involves\ninformation-based constraints to achieve text-image alignment at feature level.\nSpecifically, we design an affordance mutual information constraint that helps\nlearn appropriate textual prompts and task-oriented visual features\nsimultaneously by maximizing the mutual information between the features of the\naffordance areas in the input images and the corresponding textual prompts. In\naddition, we propose an object-level information constraint that maximizes the\nmutual information between the visual features of a given object and the text\nfeatures of the category it belongs to. This enables the model to capture\nhigh-quality representations for the object, providing more reliable semantic\npriors for identifying affordance regions. Experimental results on the AGD20K\ndataset show that the proposed method outperforms existing approaches and\nachieves the new state-of-the-art in one-shot affordance learning.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17074v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17074v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.448,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.31,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on text-image alignment and mutual information constraints for visual affordance learning using foundation models, with no involvement of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not utilize diffusion models or iterative refinement processes for multi-step logical reasoning; it instead employs mutual information maximization for feature alignment in affordance learning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17078",
      "title": "Enhanced Detection of Tiny Objects in Aerial Images",
      "authors": [
        "Kihyun Kim",
        "Michalis Lazarou",
        "Tania Stathaki"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "While one-stage detectors like YOLOv8 offer fast training speed, they often\nunder-perform on detecting small objects as a trade-off. This becomes even more\ncritical when detecting tiny objects in aerial imagery due to low-resolution\ntargets and cluttered backgrounds. To address this, we introduce three\nenhancement strategies -- input image resolution adjustment, data augmentation,\nand attention mechanisms -- that can be easily implemented on YOLOv8. We\ndemonstrate that image size enlargement and the proper use of augmentation can\nlead to enhancement. Additionally, we designed a Mixture of Orthogonal\nNeural-modules Network (MoonNet) pipeline which consists of attention-augmented\nCNNs. Two well-known attention modules, the Squeeze-and-Excitation Block (SE\nBlock) and the Convolutional Block Attention Module (CBAM), were integrated\ninto the backbone of YOLOv8 with an increased number of channels, and the\nMoonNet backbone obtained improved detection accuracy compared to the original\nYOLOv8. MoonNet further proved its adaptability and potential by achieving\nstate-of-the-art performance on a tiny-object benchmark when integrated with\nthe YOLC model. Our codes are available at: https://github.com/Kihyun11/MoonNet",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17078v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17078v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.281,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.36,
      "datasets_score": 0.339,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17079",
      "title": "A Dual-Modulation Framework for RGB-T Crowd Counting via Spatially\n  Modulated Attention and Adaptive Fusion",
      "authors": [
        "Yuhong Feng",
        "Hongtao Chen",
        "Qi Zhang",
        "Jie Chen",
        "Zhaoxi He",
        "Mingzhe Liu",
        "Jianghai Liao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate RGB-Thermal (RGB-T) crowd counting is crucial for public safety in\nchallenging conditions. While recent Transformer-based methods excel at\ncapturing global context, their inherent lack of spatial inductive bias causes\nattention to spread to irrelevant background regions, compromising crowd\nlocalization precision. Furthermore, effectively bridging the gap between these\ndistinct modalities remains a major hurdle. To tackle this, we propose the Dual\nModulation Framework, comprising two modules: Spatially Modulated Attention\n(SMA), which improves crowd localization by using a learnable Spatial Decay\nMask to penalize attention between distant tokens and prevent focus from\nspreading to the background; and Adaptive Fusion Modulation (AFM), which\nimplements a dynamic gating mechanism to prioritize the most reliable modality\nfor adaptive cross-modal fusion. Extensive experiments on RGB-T crowd counting\ndatasets demonstrate the superior performance of our method compared to\nprevious works. Code available at\nhttps://github.com/Cht2924/RGBT-Crowd-Counting.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17079v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17079v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.303,
      "diffusion_reasoning_score": 0.403,
      "distributed_training_score": 0.357,
      "datasets_score": 0.324,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a framework for RGB-T crowd counting using modified Transformers with spatial attention and adaptive fusion, aimed at improving localization and cross-modal integration in computer vision tasks. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17083",
      "title": "HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel\n  View Synthesis",
      "authors": [
        "Zipeng Wang",
        "Dan Xu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative\nto NeRF-based approaches, enabling real-time, high-quality novel view synthesis\nthrough explicit, optimizable 3D Gaussians. However, 3DGS suffers from\nsignificant memory overhead due to its reliance on per-Gaussian parameters to\nmodel view-dependent effects and anisotropic shapes. While recent works propose\ncompressing 3DGS with neural fields, these methods struggle to capture\nhigh-frequency spatial variations in Gaussian properties, leading to degraded\nreconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a\nnovel scene representation that combines the strengths of explicit Gaussians\nand neural fields. HyRF decomposes the scene into (1) a compact set of explicit\nGaussians storing only critical high-frequency parameters and (2) grid-based\nneural fields that predict remaining properties. To enhance representational\ncapacity, we introduce a decoupled neural field architecture, separately\nmodeling geometry (scale, opacity, rotation) and view-dependent color.\nAdditionally, we propose a hybrid rendering scheme that composites Gaussian\nsplatting with a neural field-predicted background, addressing limitations in\ndistant scene representation. Experiments demonstrate that HyRF achieves\nstate-of-the-art rendering quality while reducing model size by over 20 times\ncompared to 3DGS and maintaining real-time performance. Our project page is\navailable at https://wzpscott.github.io/hyrf/.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17083v2",
      "pdf_url": "http://arxiv.org/pdf/2509.17083v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.294,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.377,
      "datasets_score": 0.269,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17084",
      "title": "MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion\n  Vectors",
      "authors": [
        "Binhua Huang",
        "Ni Wang",
        "Arjun Pakrashi",
        "Soumyabrata Dev"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Video action recognition is a fundamental task in computer vision, but\nstate-of-the-art models are often computationally expensive and rely on\nextensive video pre-training. In parallel, large-scale vision-language models\nlike Contrastive Language-Image Pre-training (CLIP) offer powerful zero-shot\ncapabilities on static images, while motion vectors (MV) provide highly\nefficient temporal information directly from compressed video streams. To\nsynergize the strengths of these paradigms, we propose MoCLIP-Lite, a simple\nyet powerful two-stream late fusion framework for efficient video recognition.\nOur approach combines features from a frozen CLIP image encoder with features\nfrom a lightweight, supervised network trained on raw MV. During fusion, both\nbackbones are frozen, and only a tiny Multi-Layer Perceptron (MLP) head is\ntrained, ensuring extreme efficiency. Through comprehensive experiments on the\nUCF101 dataset, our method achieves a remarkable 89.2% Top-1 accuracy,\nsignificantly outperforming strong zero-shot (65.0%) and MV-only (66.5%)\nbaselines. Our work provides a new, highly efficient baseline for video\nunderstanding that effectively bridges the gap between large static models and\ndynamic, low-cost motion cues. Our code and models are available at\nhttps://github.com/microa/MoCLIP-Lite.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17084v2",
      "pdf_url": "http://arxiv.org/pdf/2509.17084v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.325,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.393,
      "distributed_training_score": 0.348,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17086",
      "title": "SFN-YOLO: Towards Free-Range Poultry Detection via Scale-aware Fusion\n  Networks",
      "authors": [
        "Jie Chen",
        "Yuhong Feng",
        "Tao Dai",
        "Mingzhe Liu",
        "Hongtao Chen",
        "Zhaoxi He",
        "Jiancong Bai"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Detecting and localizing poultry is essential for advancing smart poultry\nfarming. Despite the progress of detection-centric methods, challenges persist\nin free-range settings due to multiscale targets, obstructions, and complex or\ndynamic backgrounds. To tackle these challenges, we introduce an innovative\npoultry detection approach named SFN-YOLO that utilizes scale-aware fusion.\nThis approach combines detailed local features with broader global context to\nimprove detection in intricate environments. Furthermore, we have developed a\nnew expansive dataset (M-SCOPE) tailored for varied free-range conditions.\nComprehensive experiments demonstrate our model achieves an mAP of 80.7% with\njust 7.2M parameters, which is 35.1% fewer than the benchmark, while retaining\nstrong generalization capability across different domains. The efficient and\nreal-time detection capabilities of SFN-YOLO support automated smart poultry\nfarming. The code and dataset can be accessed at\nhttps://github.com/chenjessiee/SFN-YOLO.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17086v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17086v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.373,
      "diffusion_reasoning_score": 0.339,
      "distributed_training_score": 0.389,
      "datasets_score": 0.348,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17087",
      "title": "Governing Automated Strategic Intelligence",
      "authors": [
        "Nicholas Kruus",
        "Madhavendra Thakur",
        "Adam Khoja",
        "Leonhard Nagel",
        "Maximilian Nicholson",
        "Abeer Sharma",
        "Jason Hausenloy",
        "Alberto KoTafoya",
        "Aliya Mukhanova",
        "Alli Katila-Miikkulainen",
        "Harish Chandran",
        "Ivan Zhang",
        "Jessie Chen",
        "Joel Raj",
        "Jord Nguyen",
        "Lai Hsien Hao",
        "Neja Jayasundara",
        "Soham Sen",
        "Sophie Zhang",
        "Ashley Dora Kokui Tamaklo",
        "Bhavya Thakur",
        "Henry Close",
        "Janghee Lee",
        "Nina Sefton",
        "Raghavendra Thakur",
        "Shiv Munagala",
        "Yeeun Kim"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Military and economic strategic competitiveness between nation-states will\nincreasingly be defined by the capability and cost of their frontier artificial\nintelligence models. Among the first areas of geopolitical advantage granted by\nsuch systems will be in automating military intelligence. Much discussion has\nbeen devoted to AI systems enabling new military modalities, such as lethal\nautonomous weapons, or making strategic decisions. However, the ability of a\ncountry of \"CIA analysts in a data-center\" to synthesize diverse data at scale,\nand its implications, have been underexplored. Multimodal foundation models\nappear on track to automate strategic analysis previously done by humans. They\nwill be able to fuse today's abundant satellite imagery, phone-location traces,\nsocial media records, and written documents into a single queryable system. We\nconduct a preliminary uplift study to empirically evaluate these capabilities,\nthen propose a taxonomy of the kinds of ground truth questions these systems\nwill answer, present a high-level model of the determinants of this system's AI\ncapabilities, and provide recommendations for nation-states to remain\nstrategically competitive within the new paradigm of automated intelligence.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17087v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17087v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.381,
      "datasets_score": 0.386,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses the application of AI in automating military intelligence using multimodal models, but it does not mention reinforcement learning, human feedback, reward models, or any related training techniques. The focus is on deployment and capabilities, not model alignment via RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper explores AI for synthesizing data in strategic intelligence, including potential iterative processes, but it does not reference diffusion models, multi-step logical reasoning via diffusion, or any adaptation of diffusion for holistic Chain-of-Thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17088",
      "title": "AlignedGen: Aligning Style Across Generated Images",
      "authors": [
        "Jiexuan Zhang",
        "Yiheng Du",
        "Qian Wang",
        "Weiqi Li",
        "Yu Gu",
        "Jian Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Despite their generative power, diffusion models struggle to maintain style\nconsistency across images conditioned on the same style prompt, hindering their\npractical deployment in creative workflows. While several training-free methods\nattempt to solve this, they are constrained to the U-Net architecture, which\nnot only leads to low-quality results and artifacts like object repetition but\nalso renders them incompatible with superior Diffusion Transformer (DiT). To\naddress these issues, we introduce AlignedGen, a novel training-free framework\nthat enhances style consistency across images generated by DiT models. Our work\nfirst reveals a critical insight: naive attention sharing fails in DiT due to\nconflicting positional signals from improper position embeddings. We introduce\nShifted Position Embedding (ShiftPE), an effective solution that resolves this\nconflict by allocating a non-overlapping set of positional indices to each\nimage. Building on this foundation, we develop Advanced Attention Sharing\n(AAS), a suite of three techniques meticulously designed to fully unleash the\npotential of attention sharing within the DiT. Furthermore, to broaden the\napplicability of our method, we present an efficient query, key, and value\nfeature extraction algorithm, enabling our method to seamlessly incorporate\nexternal images as style references. Extensive experimental results validate\nthat our method effectively enhances style consistency across generated images\nwhile maintaining precise text-to-image alignment.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17088v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17088v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.384,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.511,
      "distributed_training_score": 0.398,
      "datasets_score": 0.338,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on enhancing style consistency in image generation using Diffusion Transformer (DiT) models, specifically through techniques like Shifted Position Embedding and Advanced Attention Sharing. It does not involve adapting the diffusion process for multi-step logical reasoning, Chain-of-Thought, or solving complex logical tasks. Instead, it is centered on visual content generation, making it unrelated to the topic of Diffusion-based Reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17094",
      "title": "DiffSyn: A Generative Diffusion Approach to Materials Synthesis Planning",
      "authors": [
        "Elton Pan",
        "Soonhyoung Kwon",
        "Sulin Liu",
        "Mingrou Xie",
        "Alexander J. Hoffman",
        "Yifei Duan",
        "Thorben Prein",
        "Killian Sheriff",
        "Yuriy Roman-Leshkov",
        "Manuel Moliner",
        "Rafael Gomez-Bombarelli",
        "Elsa Olivetti"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The synthesis of crystalline materials, such as zeolites, remains a\nsignificant challenge due to a high-dimensional synthesis space, intricate\nstructure-synthesis relationships and time-consuming experiments. Considering\nthe one-to-many relationship between structure and synthesis, we propose\nDiffSyn, a generative diffusion model trained on over 23,000 synthesis recipes\nspanning 50 years of literature. DiffSyn generates probable synthesis routes\nconditioned on a desired zeolite structure and an organic template. DiffSyn\nachieves state-of-the-art performance by capturing the multi-modal nature of\nstructure-synthesis relationships. We apply DiffSyn to differentiate among\ncompeting phases and generate optimal synthesis routes. As a proof of concept,\nwe synthesize a UFI material using DiffSyn-generated synthesis routes. These\nroutes, rationalized by density functional theory binding energies, resulted in\nthe successful synthesis of a UFI material with a high Si/Al$_{\\text{ICP}}$ of\n19.0, which is expected to improve thermal stability and is higher than that of\nany previously recorded.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17094v2",
      "pdf_url": "http://arxiv.org/pdf/2509.17094v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.578,
      "distributed_training_score": 0.342,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using a diffusion model for generating materials synthesis routes, emphasizing iterative denoising for data generation in a scientific context. It does not involve adapting the diffusion process for multi-step logical reasoning, such as treating a Chain-of-Thought as a single entity for holistic correction in solving complex logical tasks. Since there is no component for logical inference or reasoning, the paper does not align with this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17095",
      "title": "Ultra-short-term solar power forecasting by deep learning and data\n  reconstruction",
      "authors": [
        "Jinbao Wang",
        "Jun Liu",
        "Shiliang Zhang",
        "Xuehui Ma"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The integration of solar power has been increasing as the green energy\ntransition rolls out. The penetration of solar power challenges the grid\nstability and energy scheduling, due to its intermittent energy generation.\nAccurate and near real-time solar power prediction is of critical importance to\ntolerant and support the permeation of distributed and volatile solar power\nproduction in the energy system. In this paper, we propose a deep-learning\nbased ultra-short-term solar power prediction with data reconstruction. We\ndecompose the data for the prediction to facilitate extensive exploration of\nthe spatial and temporal dependencies within the data. Particularly, we\nreconstruct the data into low- and high-frequency components, using ensemble\nempirical model decomposition with adaptive noise (CEEMDAN). We integrate\nmeteorological data with those two components, and employ deep-learning models\nto capture long- and short-term dependencies towards the target prediction\nperiod. In this way, we excessively exploit the features in historical data in\npredicting a ultra-short-term solar power production. Furthermore, as\nultra-short-term prediction is vulnerable to local optima, we modify the\noptimization in our deep-learning training by penalizing long prediction\nintervals. Numerical experiments with diverse settings demonstrate that,\ncompared to baseline models, the proposed method achieves improved\ngeneralization in data reconstruction and higher prediction accuracy for\nultra-short-term solar power production.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17095v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17095v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.367,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17096",
      "title": "Prompt-with-Me: in-IDE Structured Prompt Management for LLM-Driven\n  Software Engineering",
      "authors": [
        "Ziyou Li",
        "Agnia Sergeyuk",
        "Maliheh Izadi"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Large Language Models are transforming software engineering, yet prompt\nmanagement in practice remains ad hoc, hindering reliability, reuse, and\nintegration into industrial workflows. We present Prompt-with-Me, a practical\nsolution for structured prompt management embedded directly in the development\nenvironment. The system automatically classifies prompts using a\nfour-dimensional taxonomy encompassing intent, author role, software\ndevelopment lifecycle stage, and prompt type. To enhance prompt reuse and\nquality, Prompt-with-Me suggests language refinements, masks sensitive\ninformation, and extracts reusable templates from a developer's prompt library.\nOur taxonomy study of 1108 real-world prompts demonstrates that modern LLMs can\naccurately classify software engineering prompts. Furthermore, our user study\nwith 11 participants shows strong developer acceptance, with high usability\n(Mean SUS=73), low cognitive load (Mean NASA-TLX=21), and reported gains in\nprompt quality and efficiency through reduced repetitive effort. Lastly, we\noffer actionable insights for building the next generation of prompt management\nand maintenance tools for software engineering workflows.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17096v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17096v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.435,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.314,
      "datasets_score": 0.326,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper presents a tool for managing and classifying prompts in software engineering workflows using LLMs, with evaluations involving few-shot learning and user studies. It does not describe training or fine-tuning AI models with human feedback via reinforcement learning, as required for RLHF. While existing tools mentioned use reinforcement learning, this is not the paper's main contribution.",
      "weak_supervision_justification": "The paper utilizes LLMs for prompt classification based on a taxonomy derived from real-world data and validated by experts, but it does not involve training models with programmatically generated, noisy labels as in weak supervision. The focus is on tool development and evaluation, not on weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17098",
      "title": "Uncertainty-Supervised Interpretable and Robust Evidential Segmentation",
      "authors": [
        "Yuzhu Li",
        "An Sui",
        "Fuping Wu",
        "Xiahai Zhuang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Uncertainty estimation has been widely studied in medical image segmentation\nas a tool to provide reliability, particularly in deep learning approaches.\nHowever, previous methods generally lack effective supervision in uncertainty\nestimation, leading to low interpretability and robustness of the predictions.\nIn this work, we propose a self-supervised approach to guide the learning of\nuncertainty. Specifically, we introduce three principles about the\nrelationships between the uncertainty and the image gradients around boundaries\nand noise. Based on these principles, two uncertainty supervision losses are\ndesigned. These losses enhance the alignment between model predictions and\nhuman interpretation. Accordingly, we introduce novel quantitative metrics for\nevaluating the interpretability and robustness of uncertainty. Experimental\nresults demonstrate that compared to state-of-the-art approaches, the proposed\nmethod can achieve competitive segmentation performance and superior results in\nout-of-distribution (OOD) scenarios while significantly improving the\ninterpretability and robustness of uncertainty estimation. Code is available\nvia https://github.com/suiannaius/SURE.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17098v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17098v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.438,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.284,
      "datasets_score": 0.305,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces a self-supervised approach for uncertainty estimation in segmentation, using principles to design supervision losses based on data properties like gradients and noise. While this involves programmatically derived supervisory signals, similar to weak supervision's use of noisy or imprecise sources, the paper does not primarily focus on training with weak labels for the main segmentation task. Instead, it emphasizes enhancing uncertainty in a supervised segmentation context, making it only loosely related.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17100",
      "title": "The SAGES Critical View of Safety Challenge: A Global Benchmark for\n  AI-Assisted Surgical Quality Assessment",
      "authors": [
        "Deepak Alapatt",
        "Jennifer Eckhoff",
        "Zhiliang Lyu",
        "Yutong Ban",
        "Jean-Paul Mazellier",
        "Sarah Choksi",
        "Kunyi Yang",
        "2024 CVS Challenge Consortium",
        "Quanzheng Li",
        "Filippo Filicori",
        "Xiang Li",
        "Pietro Mascagni",
        "Daniel A. Hashimoto",
        "Guy Rosman",
        "Ozanan Meireles",
        "Nicolas Padoy"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Advances in artificial intelligence (AI) for surgical quality assessment\npromise to democratize access to expertise, with applications in training,\nguidance, and accreditation. This study presents the SAGES Critical View of\nSafety (CVS) Challenge, the first AI competition organized by a surgical\nsociety, using the CVS in laparoscopic cholecystectomy, a universally\nrecommended yet inconsistently performed safety step, as an exemplar of\nsurgical quality assessment. A global collaboration across 54 institutions in\n24 countries engaged hundreds of clinicians and engineers to curate 1,000\nvideos annotated by 20 surgical experts according to a consensus-validated\nprotocol. The challenge addressed key barriers to real-world deployment in\nsurgery, including achieving high performance, capturing uncertainty in\nsubjective assessment, and ensuring robustness to clinical variability. To\nenable this scale of effort, we developed EndoGlacier, a framework for managing\nlarge, heterogeneous surgical video and multi-annotator workflows. Thirteen\ninternational teams participated, achieving up to a 17\\% relative gain in\nassessment performance, over 80\\% reduction in calibration error, and a 17\\%\nrelative improvement in robustness over the state-of-the-art. Analysis of\nresults highlighted methodological trends linked to model performance,\nproviding guidance for future research toward robust, clinically deployable AI\nfor surgical quality assessment.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17100v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17100v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.301,
      "distributed_training_score": 0.345,
      "datasets_score": 0.42,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves creating and curating a new dataset of 1,000 annotated surgical videos from a global collaboration across 54 institutions in 24 countries, including detailed methodologies for annotation, quality control, and management via the EndoGlacier framework. It also benchmarks and evaluates this dataset for AI applications in surgical quality assessment, directly aligning with research on dataset creation, curation, benchmarking, and analysis for machine learning and AI.",
      "llm_score_status": "completed",
      "summary": "The SAGES Critical View of Safety Challenge is a pioneering global initiative organized by a surgical society to benchmark AI models for assessing surgical quality, specifically the Critical View of Safety in laparoscopic cholecystectomy. By curating a diverse dataset of 1,000 annotated videos from 54 institutions across 24 countries using the EndoGlacier framework, the challenge engaged 13 international teams, resulting in significant improvements in AI performance, reduced calibration error, and enhanced robustness to clinical variability, thereby providing key insights for advancing AI in surgical applications.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel global benchmark dataset and the first AI competition organized by a surgical society, along with the EndoGlacier framework, which significantly advances the state-of-the-art in AI-assisted surgical quality assessment by addressing real-world deployment challenges.",
      "impact_score": "High",
      "impact_justification": "This work could broadly influence future AI research and commercial applications in surgery by providing a standardized benchmark that improves model robustness and democratizes access to expert assessment.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution with practical implications for AI in surgery, making it essential for researchers in the field to be aware of its advancements and insights.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/32261dad21866816c148aa3742939b5ee5ad6c91",
      "total_authors": 16,
      "authors_found": 16,
      "highest_h_index": 28,
      "average_h_index": 8.875,
      "notable_authors_count": 7,
      "author_h_indexes": [
        {
          "name": "Deepak Alapatt",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/29926739"
        },
        {
          "name": "J. Eckhoff",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2156583904"
        },
        {
          "name": "Zhiliang Lyu",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2111959984"
        },
        {
          "name": "Yutong Ban",
          "h_index": 19,
          "profile_url": "https://www.semanticscholar.org/author/8422060"
        },
        {
          "name": "J. Mazellier",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2239498597"
        },
        {
          "name": "Sarah Choksi",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2184430885"
        },
        {
          "name": "Kunyi Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2338359231"
        },
        {
          "name": "2024 Cvs Challenge Consortium",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381373067"
        },
        {
          "name": "Quanzheng Li",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2210054417"
        },
        {
          "name": "Filippo Filicori",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2345516569"
        },
        {
          "name": "Xiang Li",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381455139"
        },
        {
          "name": "Pietro Mascagni",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2254289472"
        },
        {
          "name": "Daniel A. Hashimoto",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2366171253"
        },
        {
          "name": "G. Rosman",
          "h_index": 25,
          "profile_url": "https://www.semanticscholar.org/author/2116952"
        },
        {
          "name": "O. Meireles",
          "h_index": 23,
          "profile_url": "https://www.semanticscholar.org/author/11009166"
        },
        {
          "name": "N. Padoy",
          "h_index": 28,
          "profile_url": "https://www.semanticscholar.org/author/2655297"
        }
      ]
    },
    {
      "id": "2509.17107",
      "title": "CoBEVMoE: Heterogeneity-aware Feature Fusion with Dynamic\n  Mixture-of-Experts for Collaborative Perception",
      "authors": [
        "Lingzhao Kong",
        "Jiacheng Lin",
        "Siyu Li",
        "Kai Luo",
        "Zhiyong Li",
        "Kailun Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.RO (Robotics)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Collaborative perception aims to extend sensing coverage and improve\nperception accuracy by sharing information among multiple agents. However, due\nto differences in viewpoints and spatial positions, agents often acquire\nheterogeneous observations. Existing intermediate fusion methods primarily\nfocus on aligning similar features, often overlooking the perceptual diversity\namong agents. To address this limitation, we propose CoBEVMoE, a novel\ncollaborative perception framework that operates in the Bird's Eye View (BEV)\nspace and incorporates a Dynamic Mixture-of-Experts (DMoE) architecture. In\nDMoE, each expert is dynamically generated based on the input features of a\nspecific agent, enabling it to extract distinctive and reliable cues while\nattending to shared semantics. This design allows the fusion process to\nexplicitly model both feature similarity and heterogeneity across agents.\nFurthermore, we introduce a Dynamic Expert Metric Loss (DEML) to enhance\ninter-expert diversity and improve the discriminability of the fused\nrepresentation. Extensive experiments on the OPV2V and DAIR-V2X-C datasets\ndemonstrate that CoBEVMoE achieves state-of-the-art performance. Specifically,\nit improves the IoU for Camera-based BEV segmentation by +1.5% on OPV2V and the\nAP@50 for LiDAR-based 3D object detection by +3.0% on DAIR-V2X-C, verifying the\neffectiveness of expert-based heterogeneous feature modeling in multi-agent\ncollaborative perception. The source code will be made publicly available at\nhttps://github.com/godk0509/CoBEVMoE.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17107v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17107v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.383,
      "datasets_score": 0.359,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for collaborative perception in autonomous driving, focusing on feature fusion using Dynamic Mixture-of-Experts to handle heterogeneous observations. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17116",
      "title": "MCTS-EP: Empowering Embodied Planning with Online Preference\n  Optimization",
      "authors": [
        "Hang Xu",
        "Zang Yu",
        "Yehui Tang",
        "Pengbo Hu",
        "Yuhao Tang",
        "Hao Dong"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper introduces MCTS-EP, an online learning framework that combines\nlarge language models (LLM) with Monte Carlo Tree Search (MCTS) for training\nembodied agents. MCTS-EP integrates three key components: MCTS-guided\nexploration for preference data collection, efficient multi-modal reasoning\nmechanism, and iterative training pipeline based on preference optimization. We\ntheoretically prove that MCTS-EP achieves better performance bounds than\nconventional on-policy algorithms when the loss function is strongly convex,\nand demonstrate that it can be formulated as a search-enhanced variant of GAIL.\nMCTS-EP achieves state-of-the-art performace across serval benchmarks. In\nALFWorld, it achieves 92% and 87% success rates for textual and visual tasks.\nIn WebShop, it reaches an average reward of 0.81. MTCS-EP also reduces average\ninteraction steps from from 18.7/19.5 to 10.2/9.9 steps in visual ALFWorld.Code\navailable at: https://github.com/xuhang-2/Embodied-Agent-Planning",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17116v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17116v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.485,
      "weak_supervision_score": 0.372,
      "diffusion_reasoning_score": 0.447,
      "distributed_training_score": 0.353,
      "datasets_score": 0.347,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper proposes MCTS-EP, which uses preference optimization for training embodied agents via MCTS-generated data, but it does not involve human feedback or a separate reward model trained on human-ranked data. Instead, it relies on algorithmic, self-generated preferences, which does not align with the definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper centers on MCTS for exploration and preference optimization, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for logical tasks. It lacks any components related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17119",
      "title": "ScenGAN: Attention-Intensive Generative Model for Uncertainty-Aware\n  Renewable Scenario Forecasting",
      "authors": [
        "Yifei Wu",
        "Bo Wang",
        "Jingshi Cui",
        "Pei-chun Lin",
        "Junzo Watada"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "To address the intermittency of renewable energy source (RES) generation,\nscenario forecasting offers a series of stochastic realizations for predictive\nobjects with superior flexibility and direct views. Based on a long time-series\nperspective, this paper explores uncertainties in the realms of renewable power\nand deep learning. Then, an uncertainty-aware model is meticulously designed\nfor renewable scenario forecasting, which leverages an attention mechanism and\ngenerative adversarial networks (GANs) to precisely capture complex\nspatial-temporal dynamics. To improve the interpretability of uncertain\nbehavior in RES generation, Bayesian deep learning and adaptive instance\nnormalization (AdaIN) are incorporated to simulate typical patterns and\nvariations. Additionally, the integration of meteorological information,\nforecasts, and historical trajectories in the processing layer improves the\nsynergistic forecasting capability for multiscale periodic regularities.\nNumerical experiments and case analyses demonstrate that the proposed approach\nprovides an appropriate interpretation for renewable uncertainty\nrepresentation, including both aleatoric and epistemic uncertainties, and shows\nsuperior performance over state-of-the-art methods.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17119v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17119v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.355,
      "datasets_score": 0.332,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of a GAN-based model (ScenGAN) for uncertainty-aware renewable scenario forecasting, incorporating attention mechanisms and Bayesian deep learning. It does not involve diffusion models, iterative refinement processes, or any adaptation for multi-step logical reasoning tasks such as Chain-of-Thought. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17120",
      "title": "Stencil: Subject-Driven Generation with Context Guidance",
      "authors": [
        "Gordon Chen",
        "Ziqi Huang",
        "Cheston Tan",
        "Ziwei Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent text-to-image diffusion models can generate striking visuals from text\nprompts, but they often fail to maintain subject consistency across generations\nand contexts. One major limitation of current fine-tuning approaches is the\ninherent trade-off between quality and efficiency. Fine-tuning large models\nimproves fidelity but is computationally expensive, while fine-tuning\nlightweight models improves efficiency but compromises image fidelity.\nMoreover, fine-tuning pre-trained models on a small set of images of the\nsubject can damage the existing priors, resulting in suboptimal results. To\nthis end, we present Stencil, a novel framework that jointly employs two\ndiffusion models during inference. Stencil efficiently fine-tunes a lightweight\nmodel on images of the subject, while a large frozen pre-trained model provides\ncontextual guidance during inference, injecting rich priors to enhance\ngeneration with minimal overhead. Stencil excels at generating high-fidelity,\nnovel renditions of the subject in less than a minute, delivering\nstate-of-the-art performance and setting a new benchmark in subject-driven\ngeneration.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17120v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17120v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.515,
      "distributed_training_score": 0.348,
      "datasets_score": 0.331,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a framework for enhancing text-to-image diffusion models for subject-driven generation, focusing on improving image fidelity and efficiency through fine-tuning and contextual guidance. It does not adapt the diffusion process for multi-step logical reasoning, Chain-of-Thought processing, or solving complex logical tasks; instead, it applies diffusion models solely to visual generation tasks. Therefore, there is no clear component of diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17136",
      "title": "SAEC: Scene-Aware Enhanced Edge-Cloud Collaborative Industrial Vision\n  Inspection with Multimodal LLM",
      "authors": [
        "Yuhao Tian",
        "Zheming Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Industrial vision inspection requires high accuracy under stringent resource\nconstraints, yet existing approaches face a fundamental trade-off. Multimodal\nLLMs (MLLMs) deliver strong reasoning capabilities but incur prohibitive\ncomputational costs, while lightweight edge models often fail on complex cases.\nIn this paper, we present SAEC, a scene-aware enhanced edge-cloud collaborative\nindustrial vision inspection framework with MLLM. The framework is composed of\nthree synergistic components: (1) Efficient MLLM Fine-Tuning for Complex Defect\nInspection, (2) Lightweight Multiscale Scene-Complexity Estimation, and (3)\nAdaptive Edge-Cloud Scheduler. Together, these modules enable robust defect\ndetection by tailoring multimodal reasoning to scene complexity and dynamically\nbalancing computation between edge and cloud resources. Experimental results on\nMVTec AD and KSDD2 datasets demonstrate that SAEC attains 85.11% and 82.72%\naccuracy, surpassing Qwen by 22.1% and 20.8%, and LLaVA by 33.3% and 31.6%. It\nalso reduces runtime by up to 22.4% and cuts energy per correct decision by\n40%-74%. The code is available at https://github.com/YuHao-Tian/SAEC.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17136v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17136v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.411,
      "datasets_score": 0.393,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on fine-tuning Multimodal LLMs for defect inspection and edge-cloud collaboration, but it does not mention or utilize weak supervision techniques, such as programmatically generating noisy labels. Instead, it relies on standard datasets like MVTec AD and KSDD2, which imply fully labeled data.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper describes an edge-cloud collaborative framework with adaptive scheduling for inference, which involves distributing computation across devices, but it primarily addresses real-time deployment and efficiency rather than distributed training algorithms, parallel computing for model training, or multi-node acceleration.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17143",
      "title": "MaskVCT: Masked Voice Codec Transformer for Zero-Shot Voice Conversion\n  With Increased Controllability via Multiple Guidances",
      "authors": [
        "Junhyeok Lee",
        "Helin Wang",
        "Yaohan Guan",
        "Thomas Thebaud",
        "Laureano Moro-Velazquez",
        "Jesús Villalba",
        "Najim Dehak"
      ],
      "categories": [
        "eess.AS (Audio and Speech Processing)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce MaskVCT, a zero-shot voice conversion (VC) model that offers\nmulti-factor controllability through multiple classifier-free guidances (CFGs).\nWhile previous VC models rely on a fixed conditioning scheme, MaskVCT\nintegrates diverse conditions in a single model. To further enhance robustness\nand control, the model can leverage continuous or quantized linguistic features\nto enhance intellgibility and speaker similarity, and can use or omit pitch\ncontour to control prosody. These choices allow users to seamlessly balance\nspeaker identity, linguistic content, and prosodic factors in a zero-shot VC\nsetting. Extensive experiments demonstrate that MaskVCT achieves the best\ntarget speaker and accent similarities while obtaining competitive word and\ncharacter error rates compared to existing baselines. Audio samples are\navailable at https://maskvct.github.io/.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17143v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17143v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.316,
      "datasets_score": 0.273,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17153",
      "title": "Flow-Induced Diagonal Gaussian Processes",
      "authors": [
        "Moule Lin",
        "Andrea Patane",
        "Weipeng Jing",
        "Shuhao Guan",
        "Goetz Botterweck"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present Flow-Induced Diagonal Gaussian Processes (FiD-GP), a compression\nframework that incorporates a compact inducing weight matrix to project a\nneural network's weight uncertainty into a lower-dimensional subspace.\nCritically, FiD-GP relies on normalising-flow priors and spectral\nregularisations to augment its expressiveness and align the inducing subspace\nwith feature-gradient geometry through a numerically stable projection\nmechanism objective. Furthermore, we demonstrate how the prediction framework\nin FiD-GP can help to design a single-pass projection for Out-of-Distribution\n(OoD) detection. Our analysis shows that FiD-GP improves uncertainty estimation\nability on various tasks compared with SVGP-based baselines, satisfies tight\nspectral residual bounds with theoretically guaranteed OoD detection, and\nsignificantly compresses the neural network's storage requirements at the cost\nof increased inference computation dependent on the number of inducing weights\nemployed. Specifically, in a comprehensive empirical study spanning regression,\nimage classification, semantic segmentation, and out-of-distribution detection\nbenchmarks, it cuts Bayesian training cost by several orders of magnitude,\ncompresses parameters by roughly 51%, reduces model size by about 75%, and\nmatches state-of-the-art accuracy and uncertainty estimation.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17153v2",
      "pdf_url": "http://arxiv.org/pdf/2509.17153v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.369,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.411,
      "datasets_score": 0.286,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Flow-Induced Diagonal Gaussian Processes for uncertainty estimation and neural network compression, using normalising flows for variational distributions. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. There is no component related to treating a Chain-of-Thought as an entity for holistic correction.",
      "distributed_training_justification": "The paper addresses efficient uncertainty estimation and model compression in neural networks but does not discuss distributed training, parallel computing, or multi-node machine learning. It lacks any mention of partitioning data, model architecture, or computation across multiple processors or nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17158",
      "title": "ARE: Scaling Up Agent Environments and Evaluations",
      "authors": [
        "Pierre Andrews",
        "Amine Benhalloum",
        "Gerard Moreno-Torres Bertran",
        "Matteo Bettini",
        "Amar Budhiraja",
        "Ricardo Silveira Cabral",
        "Virginie Do",
        "Romain Froger",
        "Emilien Garreau",
        "Jean-Baptiste Gaya",
        "Hugo Laurençon",
        "Maxime Lecanu",
        "Kunal Malkan",
        "Dheeraj Mekala",
        "Pierre Ménard",
        "Grégoire Mialon",
        "Ulyana Piterbarg",
        "Mikhail Plekhanov",
        "Mathieu Rita",
        "Andrey Rusakov",
        "Thomas Scialom",
        "Vladislav Vorotilov",
        "Mengjue Wang",
        "Ian Yu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "We introduce Meta Agents Research Environments (ARE), a research platform for\nscalable creation of environments, integration of synthetic or real\napplications, and execution of agentic orchestrations. ARE provides simple\nabstractions to build complex and diverse environments, each with their own\nrules, tools, content, and verifiers, helping to bridge the gap between model\ndevelopment and real-world deployment. We also propose Gaia2, a benchmark built\nin ARE and designed to measure general agent capabilities. Beyond search and\nexecution, Gaia2 requires agents to handle ambiguities and noise, adapt to\ndynamic environments, collaborate with other agents, and operate under temporal\nconstraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new\nfailure modes that are invisible in static settings. Our experiments show that\nno system dominates across the intelligence spectrum: stronger reasoning often\ncomes at the cost of efficiency, and budget scaling curves plateau,\nhighlighting the need for new architectures and adaptive compute strategies.\nPerhaps more importantly, ARE abstractions enable continuous extension of Gaia2\nto other environments, empowering the community to rapidly create new\nbenchmarks tailored to their domains. In AI's second half, progress\nincreasingly depends on defining meaningful tasks and robust evaluations to\ndrive frontier capabilities forward.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17158v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17158v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.392,
      "datasets_score": 0.392,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses reinforcement learning from verifiable rewards (RLVR) as an alternative to traditional reward models, which is related to the broader field of reinforcement learning for AI alignment. However, it does not involve human feedback, training on human-ranked data, or a separate reward model as defined in RLHF, making it only indirectly connected.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on agent environments, benchmarks, and RLVR, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning adapted from diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17165",
      "title": "Time Series Forecasting Using a Hybrid Deep Learning Method: A Bi-LSTM\n  Embedding Denoising Auto Encoder Transformer",
      "authors": [
        "Sahar Koohfar",
        "Wubeshet Woldemariam"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Time series data is a prevalent form of data found in various fields. It\nconsists of a series of measurements taken over time. Forecasting is a crucial\napplication of time series models, where future values are predicted based on\nhistorical data. Accurate forecasting is essential for making well-informed\ndecisions across industries. When it comes to electric vehicles (EVs), precise\npredictions play a key role in planning infrastructure development, load\nbalancing, and energy management. This study introduces a BI-LSTM embedding\ndenoising autoencoder model (BDM) designed to address time series problems,\nfocusing on short-term EV charging load prediction. The performance of the\nproposed model is evaluated by comparing it with benchmark models like\nTransformer, CNN, RNN, LSTM, and GRU. Based on the results of the study, the\nproposed model outperforms the benchmark models in four of the five-time steps,\ndemonstrating its effectiveness for time series forecasting. This research\nmakes a significant contribution to enhancing time series forecasting, thereby\nimproving decision-making processes.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17165v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17165v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.297,
      "diffusion_reasoning_score": 0.37,
      "distributed_training_score": 0.309,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17168",
      "title": "Beat on Gaze: Learning Stylized Generation of Gaze and Head Dynamics",
      "authors": [
        "Chengwei Shi",
        "Chong Cao",
        "Xin Tong",
        "Xukun Shen"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Head and gaze dynamics are crucial in expressive 3D facial animation for\nconveying emotion and intention. However, existing methods frequently address\nfacial components in isolation, overlooking the intricate coordination between\ngaze, head motion, and speech. The scarcity of high-quality gaze-annotated\ndatasets hinders the development of data-driven models capable of capturing\nrealistic, personalized gaze control. To address these challenges, we propose\nStyGazeTalk, an audio-driven method that generates synchronized gaze and head\nmotion styles. We extract speaker-specific motion traits from gaze-head\nsequences with a multi-layer LSTM structure incorporating a style encoder,\nenabling the generation of diverse animation styles. We also introduce a\nhigh-precision multimodal dataset comprising eye-tracked gaze, audio, head\npose, and 3D facial parameters, providing a valuable resource for training and\nevaluating head and gaze control models. Experimental results demonstrate that\nour method generates realistic, temporally coherent, and style-aware head-gaze\nmotions, significantly advancing the state-of-the-art in audio-driven facial\nanimation.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17168v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17168v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.297,
      "datasets_score": 0.335,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17172",
      "title": "SynergyNet: Fusing Generative Priors and State-Space Models for Facial\n  Beauty Prediction",
      "authors": [
        "Djamel Eddine Boukhari"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The automated prediction of facial beauty is a benchmark task in affective\ncomputing that requires a sophisticated understanding of both local aesthetic\ndetails (e.g., skin texture) and global facial harmony (e.g., symmetry,\nproportions). Existing models, based on either Convolutional Neural Networks\n(CNNs) or Vision Transformers (ViTs), exhibit inherent architectural biases\nthat limit their performance; CNNs excel at local feature extraction but\nstruggle with long-range dependencies, while ViTs model global relationships at\na significant computational cost. This paper introduces the\n\\textbf{Mamba-Diffusion Network (MD-Net)}, a novel dual-stream architecture\nthat resolves this trade-off by delegating specialized roles to\nstate-of-the-art models. The first stream leverages a frozen U-Net encoder from\na pre-trained latent diffusion model, providing a powerful generative prior for\nfine-grained aesthetic qualities. The second stream employs a Vision Mamba\n(Vim), a modern state-space model, to efficiently capture global facial\nstructure with linear-time complexity. By synergistically integrating these\ncomplementary representations through a cross-attention mechanism, MD-Net\ncreates a holistic and nuanced feature space for prediction. Evaluated on the\nSCUT-FBP5500 benchmark, MD-Net sets a new state-of-the-art, achieving a Pearson\nCorrelation of \\textbf{0.9235} and demonstrating the significant potential of\nhybrid architectures that fuse generative and sequential modeling paradigms for\ncomplex visual assessment tasks.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17172v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17172v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.46,
      "distributed_training_score": 0.356,
      "datasets_score": 0.337,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses a pre-trained latent diffusion model's U-Net encoder as a feature extractor for fine-grained aesthetic features in facial beauty prediction, but it does not adapt the iterative refinement process of diffusion for multi-step logical reasoning or treat a chain-of-thought as an entity for holistic correction. The focus is on visual assessment, not complex logical tasks, so there is no clear component for diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17177",
      "title": "FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning\n  Models on Automatically Verifiable Textual and Visual Questions",
      "authors": [
        "Bowen Qin",
        "Chen Yue",
        "Fang Yin",
        "Hui Wang",
        "JG Yao",
        "Jiakang Liu",
        "Jing-Shu Zheng",
        "Miguel Hu Chen",
        "Richeng Xuan",
        "Shibei Meng",
        "Shiqi Zhou",
        "Teng Dai",
        "Tong-Shuai Ren",
        "Wei Cui",
        "Xi Yang",
        "Xialin Du",
        "Xiaojing Xu",
        "Xue Sun",
        "Xuejing Li",
        "Yaming Liu",
        "Yesheng Liu",
        "Ying Liu",
        "Yonghua Lin",
        "Yu Zhao",
        "Yunduo Zhang",
        "Yuwen Luo",
        "Zheqi He",
        "Zhiyuan He",
        "Zhongyuan Wang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We conduct a moderate-scale contamination-free (to some extent) evaluation of\ncurrent large reasoning models (LRMs) with some preliminary findings. We also\nrelease ROME, our evaluation benchmark for vision language models intended to\ntest reasoning from visual clues. We attach links to the benchmark, evaluation\ndata, and other updates on this website:\nhttps://flageval-baai.github.io/LRM-Eval/",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17177v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17177v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.318,
      "diffusion_reasoning_score": 0.451,
      "distributed_training_score": 0.308,
      "datasets_score": 0.349,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on evaluating large reasoning models (LRMs) and introducing a benchmark for vision language models to test reasoning from visual clues, without any mention of diffusion models or their iterative refinement processes for logical tasks. There is no evidence of adapting diffusion for multi-step reasoning, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17183",
      "title": "LifeAlign: Lifelong Alignment for Large Language Models with\n  Memory-Augmented Focalized Preference Optimization",
      "authors": [
        "Junsong Li",
        "Jie Zhou",
        "Bihao Zhan",
        "Yutao Yang",
        "Qianjun Pan",
        "Shilian Chen",
        "Tianyu Huai",
        "Xin Li",
        "Qin Chen",
        "Liang He"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Alignment plays a crucial role in Large Language Models (LLMs) in aligning\nwith human preferences on a specific task/domain. Traditional alignment methods\nsuffer from catastrophic forgetting, where models lose previously acquired\nknowledge when adapting to new preferences or domains. We introduce LifeAlign,\na novel framework for lifelong alignment that enables LLMs to maintain\nconsistent human preference alignment across sequential learning tasks without\nforgetting previously learned knowledge. Our approach consists of two key\ninnovations. First, we propose a focalized preference optimization strategy\nthat aligns LLMs with new preferences while preventing the erosion of knowledge\nacquired from previous tasks. Second, we develop a short-to-long memory\nconsolidation mechanism that merges denoised short-term preference\nrepresentations into stable long-term memory using intrinsic dimensionality\nreduction, enabling efficient storage and retrieval of alignment patterns\nacross diverse domains. We evaluate LifeAlign across multiple sequential\nalignment tasks spanning different domains and preference types. Experimental\nresults demonstrate that our method achieves superior performance in\nmaintaining both preference alignment quality and knowledge retention compared\nto existing lifelong learning approaches. The codes and datasets will be\nreleased on GitHub.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17183v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17183v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.591,
      "weak_supervision_score": 0.396,
      "diffusion_reasoning_score": 0.413,
      "distributed_training_score": 0.413,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses traditional alignment methods like RLHF as a foundation but introduces a new framework, LifeAlign, with focalized preference optimization. While it addresses human preference alignment, it does not directly implement RLHF (e.g., no explicit reward model or reinforcement learning loop), focusing instead on lifelong learning to prevent forgetting. This makes it moderately relevant as it builds on related concepts but extends beyond RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on lifelong alignment for LLMs using memory consolidation and preference optimization, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. There is no component involving diffusion-based techniques for reasoning tasks.",
      "distributed_training_justification": "The paper's main contribution is a framework for aligning LLMs without catastrophic forgetting, involving optimization and memory mechanisms, but it does not address distributed training, parallel computing, or strategies for partitioning data/computation across nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "LifeAlign is a novel framework designed to enable large language models (LLMs) to maintain alignment with human preferences across sequential tasks without catastrophic forgetting. It incorporates Focalized Preference Optimization to align models with new preferences while protecting prior knowledge and Short-to-Long Memory Consolidation to efficiently store and retrieve alignment patterns through dimensionality reduction. Experimental evaluations on multiple tasks demonstrate that LifeAlign achieves superior performance in alignment quality and knowledge retention compared to existing lifelong learning methods.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework, LifeAlign, with innovative components like Focalized Preference Optimization and Short-to-Long Memory Consolidation, which significantly advance the state-of-the-art in handling catastrophic forgetting for LLM alignment.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI alignment and continual learning for LLMs, potentially improving real-world applications, but its influence may be limited to specific domains rather than broadly transformative.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, innovative contribution to addressing lifelong alignment in LLMs, making it valuable for researchers in AI and machine learning to understand and build upon.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/865da3faec0bd4aea4e56b9252149fe221309746",
      "total_authors": 10,
      "authors_found": 10,
      "highest_h_index": 9,
      "average_h_index": 2.3,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Junsong Li",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2274071511"
        },
        {
          "name": "Jie Zhou",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2280186807"
        },
        {
          "name": "Bihao Zhan",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2351814214"
        },
        {
          "name": "Yutao Yang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2303799141"
        },
        {
          "name": "Qianjun Pan",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2319606952"
        },
        {
          "name": "Shilian Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2359173303"
        },
        {
          "name": "Tianyu Huai",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2231570984"
        },
        {
          "name": "Xin Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2377565018"
        },
        {
          "name": "Qin Chen",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2256591738"
        },
        {
          "name": "Liang He",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2268703214"
        }
      ]
    },
    {
      "id": "2509.17186",
      "title": "Dendritic Resonate-and-Fire Neuron for Effective and Efficient Long\n  Sequence Modeling",
      "authors": [
        "Dehao Zhang",
        "Malu Zhang",
        "Shuai Wang",
        "Jingya Wang",
        "Wenjie Wei",
        "Zeyu Ma",
        "Guoqing Wang",
        "Yang Yang",
        "Haizhou Li"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The explosive growth in sequence length has intensified the demand for\neffective and efficient long sequence modeling. Benefiting from intrinsic\noscillatory membrane dynamics, Resonate-and-Fire (RF) neurons can efficiently\nextract frequency components from input signals and encode them into\nspatiotemporal spike trains, making them well-suited for long sequence\nmodeling. However, RF neurons exhibit limited effective memory capacity and a\ntrade-off between energy efficiency and training speed on complex temporal\ntasks. Inspired by the dendritic structure of biological neurons, we propose a\nDendritic Resonate-and-Fire (D-RF) model, which explicitly incorporates a\nmulti-dendritic and soma architecture. Each dendritic branch encodes specific\nfrequency bands by utilizing the intrinsic oscillatory dynamics of RF neurons,\nthereby collectively achieving comprehensive frequency representation.\nFurthermore, we introduce an adaptive threshold mechanism into the soma\nstructure that adjusts the threshold based on historical spiking activity,\nreducing redundant spikes while maintaining training efficiency in long\nsequence tasks. Extensive experiments demonstrate that our method maintains\ncompetitive accuracy while substantially ensuring sparse spikes without\ncompromising computational efficiency during training. These results underscore\nits potential as an effective and efficient solution for long sequence modeling\non edge platforms.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17186v2",
      "pdf_url": "http://arxiv.org/pdf/2509.17186v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.313,
      "diffusion_reasoning_score": 0.431,
      "distributed_training_score": 0.416,
      "datasets_score": 0.299,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on developing a Dendritic Resonate-and-Fire neuron for long sequence modeling in spiking neural networks, emphasizing frequency extraction and efficiency. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought correction.",
      "distributed_training_justification": "The paper addresses computational efficiency in neuron models for sequence tasks, such as reducing complexity in training, but it does not discuss distributed training techniques, parallel computing across nodes, or strategies for partitioning data/computation in multi-processor environments.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17187",
      "title": "Ambiguous Medical Image Segmentation Using Diffusion Schrödinger\n  Bridge",
      "authors": [
        "Lalith Bharadwaj Baru",
        "Kamalaker Dadi",
        "Tapabrata Chakraborti",
        "Raju S. Bapi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Accurate segmentation of medical images is challenging due to unclear lesion\nboundaries and mask variability. We introduce \\emph{Segmentation Sch\\\"{o}dinger\nBridge (SSB)}, the first application of Sch\\\"{o}dinger Bridge for ambiguous\nmedical image segmentation, modelling joint image-mask dynamics to enhance\nperformance. SSB preserves structural integrity, delineates unclear boundaries\nwithout additional guidance, and maintains diversity using a novel loss\nfunction. We further propose the \\emph{Diversity Divergence Index} ($D_{DDI}$)\nto quantify inter-rater variability, capturing both diversity and consensus.\nSSB achieves state-of-the-art performance on LIDC-IDRI, COCA, and RACER\n(in-house) datasets.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17187v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17187v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.29,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.322,
      "datasets_score": 0.325,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper applies diffusion models via Schrödinger Bridge for medical image segmentation, which involves iterative refinement processes. However, this is focused on generating and refining image masks, not on solving complex logical tasks or treating a 'Chain-of-Thought' as a holistic entity for multi-step reasoning. While diffusion mechanisms are shared, the paper lacks any component for logical reasoning, making it only tangentially related to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17190",
      "title": "Echo-Path: Pathology-Conditioned Echo Video Generation",
      "authors": [
        "Kabir Hamzah Muhammad",
        "Marawan Elbatel",
        "Yi Qin",
        "Xiaomeng Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Cardiovascular diseases (CVDs) remain the leading cause of mortality\nglobally, and echocardiography is critical for diagnosis of both common and\ncongenital cardiac conditions. However, echocardiographic data for certain\npathologies are scarce, hindering the development of robust automated diagnosis\nmodels. In this work, we propose Echo-Path, a novel generative framework to\nproduce echocardiogram videos conditioned on specific cardiac pathologies.\nEcho-Path can synthesize realistic ultrasound video sequences that exhibit\ntargeted abnormalities, focusing here on atrial septal defect (ASD) and\npulmonary arterial hypertension (PAH). Our approach introduces a\npathology-conditioning mechanism into a state-of-the-art echo video generator,\nallowing the model to learn and control disease-specific structural and motion\npatterns in the heart. Quantitative evaluation demonstrates that the synthetic\nvideos achieve low distribution distances, indicating high visual fidelity.\nClinically, the generated echoes exhibit plausible pathology markers.\nFurthermore, classifiers trained on our synthetic data generalize well to real\ndata and, when used to augment real training sets, it improves downstream\ndiagnosis of ASD and PAH by 7\\% and 8\\% respectively. Code, weights and dataset\nare available here https://github.com/Marshall-mk/EchoPathv1",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17190v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17190v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.289,
      "datasets_score": 0.303,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces Echo-Path, a diffusion-based framework for generating echocardiogram videos conditioned on cardiac pathologies, which uses the iterative refinement process of diffusion models for image synthesis. However, it focuses on generative tasks in medical imaging, not on adapting diffusion for multi-step logical reasoning or solving complex logical tasks as a 'Chain-of-Thought'. There is no evidence of the model handling reasoning paths or logical problem-solving, making the connection indirect.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17191",
      "title": "VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery",
      "authors": [
        "Jinchao Ge",
        "Tengfei Cheng",
        "Biao Wu",
        "Zeyu Zhang",
        "Shiya Huang",
        "Judith Bishop",
        "Gillian Shepherd",
        "Meng Fang",
        "Ling Chen",
        "Yang Zhao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Analyzing cultural-heritage artifacts remains challenging for MLLMs: general\nmodels lack domain expertise, and SFT often overfits superficial patterns,\nyielding brittle reasoning for authentication and historical attribution. This\nraises the question of how to equip MLLMs with robust, expert-level reasoning\nfor ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns\nevaluation into supervision: we construct a taxonomy of question types, probe\nthe SFT model to localize type-specific performance gaps, and optimize with\ntype-conditioned, compositionality-oriented rewards targeting those gaps. We\nalso release VaseVQA, a comprehensive benchmark of 31,773 images designed to\nprobe deep understanding. Experiments show state-of-the-art results on style\nclassification and historical attribution with marked gains in compositional\nrobustness over SFT-only baselines, validating diagnosis-guided,\ntaxonomy-conditioned reward engineering and providing a reusable resource for\nfuture research. Code and dataset will be available at\nhttps://github.com/AIGeeksGroup/VaseVQA.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17191v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17191v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.373,
      "distributed_training_score": 0.313,
      "datasets_score": 0.424,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes the creation and introduction of VaseVQA, a new benchmark dataset with 31,773 images and 93,544 QA pairs for multimodal AI applications in cultural heritage. It details dataset curation methodologies, such as taxonomy-based question types and evaluation scripts, and evaluates model performance on this dataset, directly aligning with research on creating, benchmarking, and evaluating datasets for machine learning.",
      "llm_score_status": "completed",
      "summary": "The paper introduces VaseVL, a system that enhances Multimodal Large Language Models (MLLMs) for analyzing ancient Greek pottery by combining supervised fine-tuning (SFT) with reinforcement learning (RL). It constructs a taxonomy of seven question types to identify performance gaps, optimizes using type-conditioned rewards and Group Relative Policy Optimization, and releases VaseVQA, a benchmark with 31,773 images and 93,544 question-answer pairs, demonstrating state-of-the-art results in style classification, historical attribution, and compositional robustness.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining SFT with RL and diagnosis-guided rewards for domain-specific MLLMs, offering a clever adaptation of existing techniques to address challenges in cultural heritage analysis. However, it does not introduce an entirely new problem or architecture, as it builds on established methods like SFT and RL.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like computer vision and computational linguistics for cultural heritage applications, due to the introduction of a reusable benchmark and improved methodology. Its influence is somewhat limited to niche areas rather than broad commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a high-quality contribution with a new benchmark and robust framework for multimodal AI in cultural heritage, making it valuable for researchers in relevant fields. While not essential for all, it offers significant insights and resources that warrant attention from specialists.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/58f0e571b248c58dd64767b0875043ddb1de0ece",
      "total_authors": 10,
      "authors_found": 9,
      "highest_h_index": 5,
      "average_h_index": 2.111111111111111,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jinchao Ge",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2316952981"
        },
        {
          "name": "Tengfei Cheng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Biao Wu",
          "h_index": 4,
          "profile_url": "https://www.semanticscholar.org/author/2302471717"
        },
        {
          "name": "Zeyu Zhang",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2316998709"
        },
        {
          "name": "Shiya Huang",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2302322543"
        },
        {
          "name": "Judith Bishop",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2381372722"
        },
        {
          "name": "Gillian Shepherd",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381369698"
        },
        {
          "name": "Meng Fang",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2257039084"
        },
        {
          "name": "Ling Chen",
          "h_index": 5,
          "profile_url": "https://www.semanticscholar.org/author/2261455899"
        },
        {
          "name": "Yang Zhao",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2381768782"
        }
      ]
    },
    {
      "id": "2509.17192",
      "title": "Shall We Play a Game? Language Models for Open-ended Wargames",
      "authors": [
        "Glenn Matlin",
        "Parv Mahajan",
        "Isaac Song",
        "Yixiong Hao",
        "Ryan Bard",
        "Stu Topp",
        "Evan Montoya",
        "M. Rehan Parwani",
        "Soham Shetty",
        "Mark Riedl"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Wargames are multi-faceted, multi-player depictions of conflict in which\nparticipants' decisions influence future events. Wargames are often used to\nexplore the strategic implications of decision-making. However, it also\nencompasses entertainment-oriented simulations, ranging from _Chess_ to\ntabletop role-playing games like _Dungeons & Dragons_ (D&D). On the more\nopen-ended side of the spectrum of wargames, players use natural language to\nconvey their moves, and adjudicators propose outcomes. Language Models (LMs)\nare increasingly being considered for how they can provide insights into\nreal-world, consequential decisions. We conduct a scoping literature review of\na curated selection of 100 recent works on AI in wargames, from which we\nconstruct an ontology of wargames in terms of the creativity afforded to either\nthe players or adjudicators. Focusing on the space of wargames with the most\nopen-endedness for players and adjudicators, we distill a set of considerations\nfor when and how to use LMs in different application areas. We also present a\nset of safety considerations, best practices for deploying LMs in open-ended\nwargames, and conclude with a set of high-impact open research challenges.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17192v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17192v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.303,
      "datasets_score": 0.328,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a literature review and ontology for using Language Models in wargames, including considerations for application and safety, but does not mention training AI models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses general use of Language Models in open-ended wargames for decision-making and creativity, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17196",
      "title": "Evolution of Concepts in Language Model Pre-Training",
      "authors": [
        "Xuyang Ge",
        "Wentao Shu",
        "Jiaxing Wu",
        "Yunhua Zhou",
        "Zhengfu He",
        "Xipeng Qiu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Language models obtain extensive capabilities through pre-training. However,\nthe pre-training process remains a black box. In this work, we track linear\ninterpretable feature evolution across pre-training snapshots using a sparse\ndictionary learning method called crosscoders. We find that most features begin\nto form around a specific point, while more complex patterns emerge in later\ntraining stages. Feature attribution analyses reveal causal connections between\nfeature evolution and downstream performance. Our feature-level observations\nare highly consistent with previous findings on Transformer's two-stage\nlearning process, which we term a statistical learning phase and a feature\nlearning phase. Our work opens up the possibility to track fine-grained\nrepresentation progress during language model learning dynamics.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17196v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17196v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.488,
      "distributed_training_score": 0.393,
      "datasets_score": 0.336,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines the evolution of features in language model pre-training using sparse dictionary learning and crosscoders, focusing on internal dynamics and representation learning. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17197",
      "title": "SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal\n  Processing",
      "authors": [
        "Junlong Ke",
        "Qiying Hu",
        "Shenghai Yuan",
        "Yuecong Xu",
        "Jianfei Yang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "Modern signal processing (SP) pipelines, whether model-based or data-driven,\noften constrained by complex and fragmented workflow, rely heavily on expert\nknowledge and manual engineering, and struggle with adaptability and\ngeneralization under limited data. In contrast, Large Language Models (LLMs)\noffer strong reasoning capabilities, broad general-purpose knowledge,\nin-context learning, and cross-modal transfer abilities, positioning them as\npowerful tools for automating and generalizing SP workflows. Motivated by these\npotentials, we introduce SignalLLM, the first general-purpose LLM-based agent\nframework for general SP tasks. Unlike prior LLM-based SP approaches that are\nlimited to narrow applications or tricky prompting, SignalLLM introduces a\nprincipled, modular architecture. It decomposes high-level SP goals into\nstructured subtasks via in-context learning and domain-specific retrieval,\nfollowed by hierarchical planning through adaptive retrieval-augmented\ngeneration (RAG) and refinement; these subtasks are then executed through\nprompt-based reasoning, cross-modal reasoning, code synthesis, model\ninvocation, or data-driven LLM-assisted modeling. Its generalizable design\nenables the flexible selection of problem solving strategies across different\nsignal modalities, task types, and data conditions. We demonstrate the\nversatility and effectiveness of SignalLLM through five representative tasks in\ncommunication and sensing, such as radar target detection, human activity\nrecognition, and text compression. Experimental results show superior\nperformance over traditional and existing LLM-based methods, particularly in\nfew-shot and zero-shot settings.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17197v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17197v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.453,
      "diffusion_reasoning_score": 0.462,
      "distributed_training_score": 0.393,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on an LLM-based framework for signal processing, emphasizing in-context learning, RAG, and task decomposition, but does not involve human feedback, reward models, or reinforcement learning techniques for model alignment.",
      "weak_supervision_justification": "The paper addresses few-shot and zero-shot learning in signal processing tasks, which indirectly relates to handling limited or noisy data, but it does not explicitly use or describe weak supervision methods like programmatically generated labels.",
      "diffusion_reasoning_justification": "The paper describes hierarchical planning and refinement using RAG and in-context learning, but it does not incorporate diffusion models or iterative refinement processes for multi-step logical reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17206",
      "title": "Guided and Unguided Conditional Diffusion Mechanisms for Structured and\n  Semantically-Aware 3D Point Cloud Generation",
      "authors": [
        "Gunner Stone",
        "Sushmita Sarker",
        "Alireza Tavakkoli"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Generating realistic 3D point clouds is a fundamental problem in computer\nvision with applications in remote sensing, robotics, and digital object\nmodeling. Existing generative approaches primarily capture geometry, and when\nsemantics are considered, they are typically imposed post hoc through external\nsegmentation or clustering rather than integrated into the generative process\nitself. We propose a diffusion-based framework that embeds per-point semantic\nconditioning directly within generation. Each point is associated with a\nconditional variable corresponding to its semantic label, which guides the\ndiffusion dynamics and enables the joint synthesis of geometry and semantics.\nThis design produces point clouds that are both structurally coherent and\nsegmentation-aware, with object parts explicitly represented during synthesis.\nThrough a comparative analysis of guided and unguided diffusion processes, we\ndemonstrate the significant impact of conditional variables on diffusion\ndynamics and generation quality. Extensive experiments validate the efficacy of\nour approach, producing detailed and accurate 3D point clouds tailored to\nspecific parts and features.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17206v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17206v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.385,
      "diffusion_reasoning_score": 0.578,
      "distributed_training_score": 0.346,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for generating 3D point clouds with semantic conditioning, emphasizing iterative refinement for synthesis and structural coherence. It does not involve multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks, which are central to the topic. Therefore, there is no adaptation of diffusion for reasoning purposes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17207",
      "title": "Point-RTD: Replaced Token Denoising for Pretraining Transformer Models\n  on Point Clouds",
      "authors": [
        "Gunner Stone",
        "Youngsook Choi",
        "Alireza Tavakkoli",
        "Ankita Shukla"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Pre-training strategies play a critical role in advancing the performance of\ntransformer-based models for 3D point cloud tasks. In this paper, we introduce\nPoint-RTD (Replaced Token Denoising), a novel pretraining strategy designed to\nimprove token robustness through a corruption-reconstruction framework. Unlike\ntraditional mask-based reconstruction tasks that hide data segments for later\nprediction, Point-RTD corrupts point cloud tokens and leverages a\ndiscriminator-generator architecture for denoising. This shift enables more\neffective learning of structural priors and significantly enhances model\nperformance and efficiency. On the ShapeNet dataset, Point-RTD reduces\nreconstruction error by over 93% compared to PointMAE, and achieves more than\n14x lower Chamfer Distance on the test set. Our method also converges faster\nand yields higher classification accuracy on ShapeNet, ModelNet10, and\nModelNet40 benchmarks, clearly outperforming the baseline Point-MAE framework\nin every case.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17207v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17207v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.433,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a corruption-reconstruction framework for pretraining transformer models on point clouds, using a discriminator-generator architecture for denoising. While it involves denoising, it does not adapt diffusion models for iterative refinement in logical reasoning tasks, such as treating a Chain-of-Thought as a single entity for multi-step correction. There is no component for multi-step logical reasoning, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper introduces a pretraining strategy for point cloud transformers and discusses improvements in efficiency and performance, but it does not address distributed training, parallel computing, or multi-node machine learning. There are no mentions of partitioning data, model architecture, or computation across processors or nodes, so it is not relevant to this topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17212",
      "title": "High Resolution UDF Meshing via Iterative Networks",
      "authors": [
        "Federico Stella",
        "Nicolas Talabot",
        "Hieu Le",
        "Pascal Fua"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Unsigned Distance Fields (UDFs) are a natural implicit representation for\nopen surfaces but, unlike Signed Distance Fields (SDFs), are challenging to\ntriangulate into explicit meshes. This is especially true at high resolutions\nwhere neural UDFs exhibit higher noise levels, which makes it hard to capture\nfine details. Most current techniques perform within single voxels without\nreference to their neighborhood, resulting in missing surface and holes where\nthe UDF is ambiguous or noisy. We show that this can be remedied by performing\nseveral passes and by reasoning on previously extracted surface elements to\nincorporate neighborhood information. Our key contribution is an iterative\nneural network that does this and progressively improves surface recovery\nwithin each voxel by spatially propagating information from increasingly\ndistant neighbors. Unlike single-pass methods, our approach integrates newly\ndetected surfaces, distance values, and gradients across multiple iterations,\neffectively correcting errors and stabilizing extraction in challenging\nregions. Experiments on diverse 3D models demonstrate that our method produces\nsignificantly more accurate and complete meshes than existing approaches,\nparticularly for complex geometries, enabling UDF surface extraction at higher\nresolutions where traditional methods fail.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17212v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17212v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.36,
      "datasets_score": 0.302,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an iterative neural network for improving mesh extraction from Unsigned Distance Fields (UDFs) by propagating spatial information across multiple passes. While it involves iterative refinement, it does not adapt or use diffusion models, nor does it address complex logical tasks or chain-of-thought reasoning. The iterative process is specific to 3D meshing and spatial propagation, with no connection to diffusion-based mechanisms for logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17220",
      "title": "MirrorSAM2: Segment Mirror in Videos with Depth Perception",
      "authors": [
        "Mingchen Xu",
        "Yukun Lai",
        "Ze Ji",
        "Jing Wu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper presents MirrorSAM2, the first framework that adapts Segment\nAnything Model 2 (SAM2) to the task of RGB-D video mirror segmentation.\nMirrorSAM2 addresses key challenges in mirror detection, such as reflection\nambiguity and texture confusion, by introducing four tailored modules: a Depth\nWarping Module for RGB and depth alignment, a Depth-guided Multi-Scale Point\nPrompt Generator for automatic prompt generation, a Frequency Detail Attention\nFusion Module to enhance structural boundaries, and a Mirror Mask Decoder with\na learnable mirror token for refined segmentation. By fully leveraging the\ncomplementarity between RGB and depth, MirrorSAM2 extends SAM2's capabilities\nto the prompt-free setting. To our knowledge, this is the first work to enable\nSAM2 for automatic video mirror segmentation. Experiments on the VMD and DVMD\nbenchmark demonstrate that MirrorSAM2 achieves SOTA performance, even under\nchallenging conditions such as small mirrors, weak boundaries, and strong\nreflections.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17220v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17220v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.29,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.323,
      "datasets_score": 0.294,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17232",
      "title": "DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for\n  Neural Radiance Fields in 3D Reconstruction",
      "authors": [
        "Bo Liu",
        "Runlong Li",
        "Li Zhou",
        "Yan Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper proposes a Diffusion Model-Optimized Neural Radiance Field\n(DT-NeRF) method, aimed at enhancing detail recovery and multi-view consistency\nin 3D scene reconstruction. By combining diffusion models with Transformers,\nDT-NeRF effectively restores details under sparse viewpoints and maintains high\naccuracy in complex geometric scenes. Experimental results demonstrate that\nDT-NeRF significantly outperforms traditional NeRF and other state-of-the-art\nmethods on the Matterport3D and ShapeNet datasets, particularly in metrics such\nas PSNR, SSIM, Chamfer Distance, and Fidelity. Ablation experiments further\nconfirm the critical role of the diffusion and Transformer modules in the\nmodel's performance, with the removal of either module leading to a decline in\nperformance. The design of DT-NeRF showcases the synergistic effect between\nmodules, providing an efficient and accurate solution for 3D scene\nreconstruction. Future research may focus on further optimizing the model,\nexploring more advanced generative models and network architectures to enhance\nits performance in large-scale dynamic scenes.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17232v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17232v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.359,
      "weak_supervision_score": 0.29,
      "diffusion_reasoning_score": 0.517,
      "distributed_training_score": 0.381,
      "datasets_score": 0.317,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for 3D scene reconstruction in Neural Radiance Fields, specifically for enhancing detail recovery and multi-view consistency. It does not involve adapting diffusion for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks. Therefore, it lacks the core elements required for this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17238",
      "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE",
      "authors": [
        "Soheil Zibakhsh",
        "Mohammad Samragh",
        "Kumari Nishu",
        "Lauren Hannah",
        "Arnav Kundu",
        "Minsik Cho"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.ET (Emerging Technologies)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17238v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17238v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.393,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.476,
      "datasets_score": 0.31,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on hyper-parallel scaling for token-level predictions in Mixture-of-Experts models, using techniques like RoE to ensemble experts during inference. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. While it references Chain-of-Thought as an existing method, it does not adapt or integrate diffusion-based approaches.",
      "distributed_training_justification": "The paper addresses inference-time optimizations, such as efficient batching and KV-caching for hyper-parallel scaling in MoE models, but it does not discuss distributed training, parallel computing for training acceleration, or multi-node machine learning systems. Its focus is solely on improving inference efficiency, not on partitioning data or computation across nodes for training purposes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17240",
      "title": "Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with\n  LLM-based Multi-Agent System",
      "authors": [
        "Abdullah Mushtaq",
        "Muhammad Rafay Naeem",
        "Ibrahim Ghaznavi",
        "Alaa Abd-alrazaq",
        "Aliya Tabassum",
        "Junaid Qadir"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Systematic Literature Reviews (SLRs) are foundational to evidence-based\nresearch but remain labor-intensive and prone to inconsistency across\ndisciplines. We present an LLM-based SLR evaluation copilot built on a\nMulti-Agent System (MAS) architecture to assist researchers in assessing the\noverall quality of the systematic literature reviews. The system automates\nprotocol validation, methodological assessment, and topic relevance checks\nusing a scholarly database. Unlike conventional single-agent methods, our\ndesign integrates a specialized agentic approach aligned with PRISMA guidelines\nto support more structured and interpretable evaluations. We conducted an\ninitial study on five published SLRs from diverse domains, comparing system\noutputs to expert-annotated PRISMA scores, and observed 84% agreement. While\nearly results are promising, this work represents a first step toward scalable\nand accurate NLP-driven systems for interdisciplinary workflows and reveals\ntheir capacity for rigorous, domain-agnostic knowledge aggregation to\nstreamline the review process.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17240v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17240v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.296,
      "datasets_score": 0.367,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper evaluates an LLM-based multi-agent system for Systematic Literature Reviews (SLRs) by comparing its outputs to expert-annotated PRISMA scores, which involves human feedback. However, the main contribution focuses on automating SLR processes using LLMs and multi-agent systems, not on training a reward model with human-ranked data for reinforcement learning-based fine-tuning. Thus, while human feedback is used for evaluation, it does not align with the core definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17246",
      "title": "SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting\n  from Sparse Views",
      "authors": [
        "Ranran Huang",
        "Krystian Mikolajczyk"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian\nsplatting from sparse multi-view images, requiring no ground-truth poses during\ntraining and inference. It employs a shared feature extraction backbone,\nenabling simultaneous prediction of 3D Gaussian primitives and camera poses in\na canonical space from unposed inputs. A masked attention mechanism is\nintroduced to efficiently estimate target poses during training, while a\nreprojection loss enforces pixel-aligned Gaussian primitives, providing\nstronger geometric constraints. We further demonstrate the compatibility of our\ntraining framework with different reconstruction architectures, resulting in\ntwo model variants. Remarkably, despite the absence of pose supervision, our\nmethod achieves state-of-the-art performance in both in-domain and\nout-of-domain novel view synthesis, even under extreme viewpoint changes and\nlimited image overlap, and surpasses recent methods that rely on geometric\nsupervision for relative pose estimation. By eliminating dependence on\nground-truth poses, our method offers the scalability to leverage larger and\nmore diverse datasets. Code and pretrained models will be available on our\nproject page: https://ranrhuang.github.io/spfsplatv2/.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17246v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17246v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.378,
      "datasets_score": 0.283,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17255",
      "title": "Agentic AI for Multi-Stage Physics Experiments at a Large-Scale User\n  Facility Particle Accelerator",
      "authors": [
        "Thorsten Hellert",
        "Drew Bertwistle",
        "Simon C. Leemann",
        "Antonin Sulc",
        "Marco Venturini"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present the first language-model-driven agentic artificial intelligence\n(AI) system to autonomously execute multi-stage physics experiments on a\nproduction synchrotron light source. Implemented at the Advanced Light Source\nparticle accelerator, the system translates natural language user prompts into\nstructured execution plans that combine archive data retrieval, control-system\nchannel resolution, automated script generation, controlled machine\ninteraction, and analysis. In a representative machine physics task, we show\nthat preparation time was reduced by two orders of magnitude relative to manual\nscripting even for a system expert, while operator-standard safety constraints\nwere strictly upheld. Core architectural features, plan-first orchestration,\nbounded tool access, and dynamic capability selection, enable transparent,\nauditable execution with fully reproducible artifacts. These results establish\na blueprint for the safe integration of agentic AI into accelerator experiments\nand demanding machine physics studies, as well as routine operations, with\ndirect portability across accelerators worldwide and, more broadly, to other\nlarge-scale scientific infrastructures.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17255v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17255v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.441,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.429,
      "distributed_training_score": 0.424,
      "datasets_score": 0.362,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes an agentic AI system using language models for autonomous experiment execution, focusing on planning and tool integration, but it does not mention reinforcement learning, human feedback, reward models, or fine-tuning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes language models for structured reasoning and multi-step planning (e.g., via ReAct), but it does not involve diffusion models, iterative refinement of a chain-of-thought as a holistic entity, or any diffusion-based processes for logical tasks.",
      "distributed_training_justification": "The paper addresses the application of an AI agent in physics experiments, including orchestration and tool access, but it does not discuss distributed training, parallel computing, or strategies for accelerating model training across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17259",
      "title": "Mind the Gap: Comparing Model- vs Agentic-Level Red Teaming with\n  Action-Graph Observability on GPT-OSS-20B",
      "authors": [
        "Ilham Wicaksono",
        "Zekun Wu",
        "Rahul Patel",
        "Theo King",
        "Adriano Koshiyama",
        "Philip Treleaven"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As the industry increasingly adopts agentic AI systems, understanding their\nunique vulnerabilities becomes critical. Prior research suggests that security\nflaws at the model level do not fully capture the risks present in agentic\ndeployments, where models interact with tools and external environments. This\npaper investigates this gap by conducting a comparative red teaming analysis of\nGPT-OSS-20B, a 20-billion parameter open-source model. Using our observability\nframework AgentSeer to deconstruct agentic systems into granular actions and\ncomponents, we apply iterative red teaming attacks with harmful objectives from\nHarmBench at two distinct levels: the standalone model and the model operating\nwithin an agentic loop. Our evaluation reveals fundamental differences between\nmodel level and agentic level vulnerability profiles. Critically, we discover\nthe existence of agentic-only vulnerabilities, attack vectors that emerge\nexclusively within agentic execution contexts while remaining inert against\nstandalone models. Agentic level iterative attacks successfully compromise\nobjectives that completely failed at the model level, with tool-calling\ncontexts showing 24\\% higher vulnerability than non-tool contexts. Conversely,\ncertain model-specific exploits work exclusively at the model level and fail\nwhen transferred to agentic contexts, demonstrating that standalone model\nvulnerabilities do not always generalize to deployed systems.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17259v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17259v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.415,
      "distributed_training_score": 0.4,
      "datasets_score": 0.354,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on red teaming and security vulnerabilities in agentic AI systems, specifically comparing model-level and agentic-level risks using iterative attacks. It does not discuss training AI models with human feedback, reward models, or reinforcement learning techniques for alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines agentic systems with chain-of-thought reasoning and iterative attacks but does not mention or utilize diffusion models for multi-step logical reasoning or iterative refinement processes. There is no evidence of adapting diffusion techniques for reasoning tasks.",
      "distributed_training_justification": "The paper is centered on evaluating security vulnerabilities in an existing model (GPT-OSS-20B) in different contexts, with no discussion of distributed training methods, parallel computing, multi-node setups, or algorithms for accelerating model training.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17262",
      "title": "Optimized Learned Image Compression for Facial Expression Recognition",
      "authors": [
        "Xiumei Li",
        "Marc Windsheimer",
        "Misha Sadeghi",
        "Björn Eskofier",
        "André Kaup"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "Efficient data compression is crucial for the storage and transmission of\nvisual data. However, in facial expression recognition (FER) tasks, lossy\ncompression often leads to feature degradation and reduced accuracy. To address\nthese challenges, this study proposes an end-to-end model designed to preserve\ncritical features and enhance both compression and recognition performance. A\ncustom loss function is introduced to optimize the model, tailored to balance\ncompression and recognition performance effectively. This study also examines\nthe influence of varying loss term weights on this balance. Experimental\nresults indicate that fine-tuning the compression model alone improves\nclassification accuracy by 0.71% and compression efficiency by 49.32%, while\njoint optimization achieves significant gains of 4.04% in accuracy and 89.12%\nin efficiency. Moreover, the findings demonstrate that the jointly optimized\nclassification model maintains high accuracy on both compressed and\nuncompressed data, while the compression model reliably preserves image\ndetails, even at high compression rates.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17262v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17262v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.31,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.339,
      "datasets_score": 0.308,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17268",
      "title": "Computational Scaffolding of Composition, Value, and Color for\n  Disciplined Drawing",
      "authors": [
        "Jiaju Ma",
        "Chau Vu",
        "Asya Lyubavina",
        "Catherine Liu",
        "Jingyi Li"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "One way illustrators engage in disciplined drawing - the process of drawing\nto improve technical skills - is through studying and replicating reference\nimages. However, for many novice and intermediate digital artists, knowing how\nto approach studying a reference image can be challenging. It can also be\ndifficult to receive immediate feedback on their works-in-progress. To help\nthese users develop their professional vision, we propose ArtKrit, a tool that\nscaffolds the process of replicating a reference image into three main steps:\ncomposition, value, and color. At each step, our tool offers computational\nguidance, such as adaptive composition line generation, and automatic feedback,\nsuch as value and color accuracy. Evaluating this tool with intermediate\ndigital artists revealed that ArtKrit could flexibly accommodate their unique\nworkflows. Our code and supplemental materials are available at\nhttps://majiaju.io/artkrit .",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17268v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17268v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.269,
      "datasets_score": 0.34,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17276",
      "title": "Probabilistic Token Alignment for Large Language Model Fusion",
      "authors": [
        "Runjia Zeng",
        "James Chenhao Liang",
        "Cheng Han",
        "Zhiwen Cao",
        "Jiahao Liu",
        "Xiaojun Quan",
        "Yingjie Victor Chen",
        "Lifu Huang",
        "Tong Geng",
        "Qifan Wang",
        "Dongfang Liu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Training large language models (LLMs) from scratch can yield models with\nunique functionalities and strengths, but it is costly and often leads to\nredundant capabilities. A more cost-effective alternative is to fuse existing\npre-trained LLMs with different architectures into a more powerful model.\nHowever, a key challenge in existing model fusion is their dependence on\nmanually predefined vocabulary alignment, which may not generalize well across\ndiverse contexts, leading to performance degradation in several evaluation. To\nsolve this, we draw inspiration from distribution learning and propose the\nprobabilistic token alignment method as a general and soft mapping for\nalignment, named as PTA-LLM. Our approach innovatively reformulates token\nalignment into a classic mathematical problem: optimal transport, seamlessly\nleveraging distribution-aware learning to facilitate more coherent model\nfusion. Apart from its inherent generality, PTA-LLM exhibits interpretability\nfrom a distributional perspective, offering insights into the essence of the\ntoken alignment. Empirical results demonstrate that probabilistic token\nalignment enhances the target model's performance across multiple capabilities.\nOur code is avaliable at https://runjia.tech/neurips_pta-llm/.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17276v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17276v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.439,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.48,
      "distributed_training_score": 0.427,
      "datasets_score": 0.334,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on probabilistic token alignment for fusing large language models using optimal transport, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses token alignment and model fusion via optimal transport, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks.",
      "distributed_training_justification": "The paper addresses model fusion and token alignment for pre-trained LLMs, without discussing distributed training, parallel computing, or strategies for partitioning data/computation across multiple nodes.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17280",
      "title": "From Prediction to Understanding: Will AI Foundation Models Transform\n  Brain Science?",
      "authors": [
        "Thomas Serre",
        "Ellie Pavlick"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generative pretraining (the \"GPT\" in ChatGPT) enables language models to\nlearn from vast amounts of internet text without human supervision. This\napproach has driven breakthroughs across AI by allowing deep neural networks to\nlearn from massive, unstructured datasets. We use the term foundation models to\nrefer to large pretrained systems that can be adapted to a wide range of tasks\nwithin and across domains, and these models are increasingly applied beyond\nlanguage to the brain sciences. These models achieve strong predictive\naccuracy, raising hopes that they might illuminate computational principles.\nBut predictive success alone does not guarantee scientific understanding. Here,\nwe outline how foundation models can be productively integrated into the brain\nsciences, highlighting both their promise and their limitations. The central\nchallenge is to move from prediction to explanation: linking model computations\nto mechanisms underlying neural activity and cognition.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17280v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17280v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.491,
      "distributed_training_score": 0.359,
      "datasets_score": 0.357,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on generative pretraining and foundation models for brain sciences, emphasizing predictive accuracy and the shift from prediction to explanation in neural mechanisms. It does not discuss diffusion-based models, iterative refinement processes, or multi-step logical reasoning tasks as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17281",
      "title": "Training the next generation of physicians for artificial\n  intelligence-assisted clinical neuroradiology: ASNR MICCAI Brain Tumor\n  Segmentation (BraTS) 2025 Lighthouse Challenge education platform",
      "authors": [
        "Raisa Amiruddin",
        "Nikolay Y. Yordanov",
        "Nazanin Maleki",
        "Pascal Fehringer",
        "Athanasios Gkampenis",
        "Anastasia Janas",
        "Kiril Krantchev",
        "Ahmed Moawad",
        "Fabian Umeh",
        "Salma Abosabie",
        "Sara Abosabie",
        "Albara Alotaibi",
        "Mohamed Ghonim",
        "Mohanad Ghonim",
        "Sedra Abou Ali Mhana",
        "Nathan Page",
        "Marko Jakovljevic",
        "Yasaman Sharifi",
        "Prisha Bhatia",
        "Amirreza Manteghinejad",
        "Melisa Guelen",
        "Michael Veronesi",
        "Virginia Hill",
        "Tiffany So",
        "Mark Krycia",
        "Bojan Petrovic",
        "Fatima Memon",
        "Justin Cramer",
        "Elizabeth Schrickel",
        "Vilma Kosovic",
        "Lorenna Vidal",
        "Gerard Thompson",
        "Ichiro Ikuta",
        "Basimah Albalooshy",
        "Ali Nabavizadeh",
        "Nourel Hoda Tahon",
        "Karuna Shekdar",
        "Aashim Bhatia",
        "Claudia Kirsch",
        "Gennaro D'Anna",
        "Philipp Lohmann",
        "Amal Saleh Nour",
        "Andriy Myronenko",
        "Adam Goldman-Yassen",
        "Janet R. Reid",
        "Sanjay Aneja",
        "Spyridon Bakas",
        "Mariam Aboian"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "High-quality reference standard image data creation by neuroradiology experts\nfor automated clinical tools can be a powerful tool for neuroradiology &\nartificial intelligence education. We developed a multimodal educational\napproach for students and trainees during the MICCAI Brain Tumor Segmentation\nLighthouse Challenge 2025, a landmark initiative to develop accurate brain\ntumor segmentation algorithms. Fifty-six medical students & radiology trainees\nvolunteered to annotate brain tumor MR images for the BraTS challenges of 2023\n& 2024, guided by faculty-led didactics on neuropathology MRI. Among the 56\nannotators, 14 select volunteers were then paired with neuroradiology faculty\nfor guided one-on-one annotation sessions for BraTS 2025. Lectures on\nneuroanatomy, pathology & AI, journal clubs & data scientist-led workshops were\norganized online. Annotators & audience members completed surveys on their\nperceived knowledge before & after annotations & lectures respectively.\nFourteen coordinators, each paired with a neuroradiologist, completed the data\nannotation process, averaging 1322.9+/-760.7 hours per dataset per pair and\n1200 segmentations in total. On a scale of 1-10, annotation coordinators\nreported significant increase in familiarity with image segmentation software\npre- and post-annotation, moving from initial average of 6+/-2.9 to final\naverage of 8.9+/-1.1, and significant increase in familiarity with brain tumor\nfeatures pre- and post-annotation, moving from initial average of 6.2+/-2.4 to\nfinal average of 8.1+/-1.2. We demonstrate an innovative offering for providing\nneuroradiology & AI education through an image segmentation challenge to\nenhance understanding of algorithm development, reinforce the concept of data\nreference standard, and diversify opportunities for AI-driven image analysis\namong future physicians.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17281v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17281v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.327,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.335,
      "distributed_training_score": 0.361,
      "datasets_score": 0.358,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17282",
      "title": "Task-Oriented Communications for 3D Scene Representation: Balancing\n  Timeliness and Fidelity",
      "authors": [
        "Xiangmin Xu",
        "Zhen Meng",
        "Kan Chen",
        "Jiaming Yang",
        "Emma Li",
        "Philip G. Zhao",
        "David Flynn"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.NI (Networking and Internet Architecture)"
      ],
      "abstract": "Real-time Three-dimensional (3D) scene representation is a foundational\nelement that supports a broad spectrum of cutting-edge applications, including\ndigital manufacturing, Virtual, Augmented, and Mixed Reality (VR/AR/MR), and\nthe emerging metaverse. Despite advancements in real-time communication and\ncomputing, achieving a balance between timeliness and fidelity in 3D scene\nrepresentation remains a challenge. This work investigates a wireless network\nwhere multiple homogeneous mobile robots, equipped with cameras, capture an\nenvironment and transmit images to an edge server over channels for 3D\nrepresentation. We propose a contextual-bandit Proximal Policy Optimization\n(PPO) framework incorporating both Age of Information (AoI) and semantic\ninformation to optimize image selection for representation, balancing data\nfreshness and representation quality. Two policies -- the $\\omega$-threshold\nand $\\omega$-wait policies -- together with two benchmark methods are\nevaluated, timeliness embedding and weighted sum, on standard datasets and\nbaseline 3D scene representation models. Experimental results demonstrate\nimproved representation fidelity while maintaining low latency, offering\ninsight into the model's decision-making process. This work advances real-time\n3D scene representation by optimizing the trade-off between timeliness and\nfidelity in dynamic environments.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17282v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17282v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.42,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.407,
      "datasets_score": 0.344,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses Proximal Policy Optimization (PPO), a reinforcement learning method, to optimize image selection based on Age of Information (AoI) and semantic data. However, there is no involvement of human feedback, such as human-ranked data or a reward model trained on human preferences, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on 3D scene representation using RL and communication strategies, with no mention of diffusion models, iterative refinement for logical reasoning, or multi-step Chain-of-Thought processes. It does not involve any components for diffusion-based reasoning.",
      "distributed_training_justification": "The paper describes a wireless network with multiple robots transmitting data to an edge server for 3D representation, which involves distributed data collection and processing across nodes. However, it does not focus on distributed training algorithms, parallel computing for model training, or partitioning data/computation for accelerating ML training, making it only loosely related.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17283",
      "title": "Automated Facility Enumeration for Building Compliance Checking using\n  Door Detection and Large Language Models",
      "authors": [
        "Licheng Zhang",
        "Bach Le",
        "Naveed Akhtar",
        "Tuan Ngo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.ET (Emerging Technologies)"
      ],
      "abstract": "Building compliance checking (BCC) is a critical process for ensuring that\nconstructed facilities meet regulatory standards. A core component of BCC is\nthe accurate enumeration of facility types and their spatial distribution.\nDespite its importance, this problem has been largely overlooked in the\nliterature, posing a significant challenge for BCC and leaving a critical gap\nin existing workflows. Performing this task manually is time-consuming and\nlabor-intensive. Recent advances in large language models (LLMs) offer new\nopportunities to enhance automation by combining visual recognition with\nreasoning capabilities. In this paper, we introduce a new task for BCC:\nautomated facility enumeration, which involves validating the quantity of each\nfacility type against statutory requirements. To address it, we propose a novel\nmethod that integrates door detection with LLM-based reasoning. We are the\nfirst to apply LLMs to this task and further enhance their performance through\na Chain-of-Thought (CoT) pipeline. Our approach generalizes well across diverse\ndatasets and facility types. Experiments on both real-world and synthetic floor\nplan data demonstrate the effectiveness and robustness of our method.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17283v2",
      "pdf_url": "http://arxiv.org/pdf/2509.17283v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.421,
      "distributed_training_score": 0.346,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves using Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting for automated facility enumeration in building compliance checking, combined with door detection. It does not mention or utilize diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. Therefore, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.17287",
      "title": "Event-Based Visual Teach-and-Repeat via Fast Fourier-Domain\n  Cross-Correlation",
      "authors": [
        "Gokul B. Nair",
        "Alejandro Fontan",
        "Michael Milford",
        "Tobias Fischer"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Visual teach-and-repeat navigation enables robots to autonomously traverse\npreviously demonstrated paths by comparing current sensory input with recorded\ntrajectories. However, conventional frame-based cameras fundamentally limit\nsystem responsiveness: their fixed frame rates (typically 30-60 Hz) create\ninherent latency between environmental changes and control responses. Here we\npresent the first event-camera-based visual teach-and-repeat system. To achieve\nthis, we develop a frequency-domain cross-correlation framework that transforms\nthe event stream matching problem into computationally efficient Fourier space\nmultiplications, capable of exceeding 300Hz processing rates, an order of\nmagnitude faster than frame-based approaches. By exploiting the binary nature\nof event frames and applying image compression techniques, we further enhance\nthe computational speed of the cross-correlation process without sacrificing\nlocalization accuracy. Extensive experiments using a Prophesee EVK4 HD event\ncamera mounted on an AgileX Scout Mini robot demonstrate successful autonomous\nnavigation across 4000+ meters of indoor and outdoor trajectories. Our system\nachieves ATEs below 24 cm while maintaining consistent high-frequency control\nupdates. Our evaluations show that our approach achieves substantially higher\nupdate rates compared to conventional frame-based systems, underscoring the\npractical viability of event-based perception for real-time robotic navigation.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.17287v1",
      "pdf_url": "http://arxiv.org/pdf/2509.17287v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.329,
      "weak_supervision_score": 0.263,
      "diffusion_reasoning_score": 0.297,
      "distributed_training_score": 0.341,
      "datasets_score": 0.292,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18208",
      "title": "Variational Task Vector Composition",
      "authors": [
        "Boyuan Zhang",
        "Yingjun Du",
        "Xiantong Zhen",
        "Ling Shao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Task vectors capture how a model changes during fine-tuning by recording the\ndifference between pre-trained and task-specific weights. The composition of\ntask vectors, a key operator in task arithmetic, enables models to integrate\nknowledge from multiple tasks without incurring additional inference costs. In\nthis paper, we propose variational task vector composition, where composition\ncoefficients are taken as latent variables and estimated in a Bayesian\ninference framework. Unlike previous methods that operate at the task level,\nour framework focuses on sample-specific composition. Motivated by the\nobservation of structural redundancy in task vectors, we introduce a\nSpike-and-Slab prior that promotes sparsity and preserves only the most\ninformative components. To further address the high variance and sampling\ninefficiency in sparse, high-dimensional spaces, we develop a gated sampling\nmechanism that constructs a controllable posterior by filtering the composition\ncoefficients based on both uncertainty and importance. This yields a more\nstable and interpretable variational framework by deterministically selecting\nreliable task components, reducing sampling variance while improving\ntransparency and generalization. Experimental results demonstrate that our\nmethod consistently outperforms existing approaches across all datasets by\nselectively leveraging the most reliable and informative components in task\nvectors. These findings highlight the practical value of our approach,\nestablishing a new standard for efficient and effective task vector\ncomposition.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.18208v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18208v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.378,
      "datasets_score": 0.322,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on variational task vector composition for efficient model fine-tuning and knowledge integration, using Bayesian inference and sparsity techniques. It does not involve diffusion models, iterative refinement processes, or any adaptation for solving complex logical tasks via a Chain-of-Thought mechanism.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18214",
      "title": "Automatic Classification of Magnetic Chirality of Solar Filaments from\n  H-Alpha Observations",
      "authors": [
        "Alexis Chalmers",
        "Azim Ahmadzadeh"
      ],
      "categories": [
        "astro-ph.SR (Solar and Stellar Astrophysics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this study, we classify the magnetic chirality of solar filaments from\nH-Alpha observations using state-of-the-art image classification models. We\nestablish the first reproducible baseline for solar filament chirality\nclassification on the MAGFiLO dataset. The MAGFiLO dataset contains over 10,000\nmanually-annotated filaments from GONG H-Alpha observations, making it the\nlargest dataset for filament detection and classification to date. Prior\nstudies relied on much smaller datasets, which limited their generalizability\nand comparability. We fine-tuned several pre-trained, image classification\narchitectures, including ResNet, WideResNet, ResNeXt, and ConvNeXt, and also\napplied data augmentation and per-class loss weights to optimize the models.\nOur best model, ConvNeXtBase, achieves a per-class accuracy of 0.69 for left\nchirality filaments and $0.73$ for right chirality filaments.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.18214v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18214v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.295,
      "diffusion_reasoning_score": 0.29,
      "distributed_training_score": 0.303,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18215",
      "title": "Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and\n  Counterfactual Explanations",
      "authors": [
        "Timotheus Kampik",
        "Kristijonas Čyras",
        "José Ruiz Alarcón"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LO (Logic in Computer Science)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "This paper presents a formal approach to explaining change of inference in\nQuantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions\nfrom a QBAF and updating the QBAF to then again draw conclusions (and so on),\nour approach traces changes -- which we call strength inconsistencies -- in the\npartial order over argument strengths that a semantics establishes on some\narguments of interest, called topic arguments. We trace the causes of strength\ninconsistencies to specific arguments, which then serve as explanations. We\nidentify sufficient, necessary, and counterfactual explanations for strength\ninconsistencies and show that strength inconsistency explanations exist if and\nonly if an update leads to strength inconsistency. We define a heuristic-based\napproach to facilitate the search for strength inconsistency explanations, for\nwhich we also provide an implementation.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.18215v1",
      "pdf_url": "http://arxiv.org/pdf/2509.18215v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.248,
      "weak_supervision_score": 0.223,
      "diffusion_reasoning_score": 0.316,
      "distributed_training_score": 0.174,
      "datasets_score": 0.168,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18216",
      "title": "nDNA -- the Semantic Helix of Artificial Cognition",
      "authors": [
        "Amitava Das"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "As AI foundation models grow in capability, a deeper question emerges: What\nshapes their internal cognitive identity -- beyond fluency and output?\nBenchmarks measure behavior, but the soul of a model resides in its latent\ngeometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic\nrepresentation that captures this latent identity through the intrinsic\ngeometry of belief. At its core, nDNA is synthesized from three principled and\nindispensable dimensions of latent geometry: spectral curvature, which reveals\nthe curvature of conceptual flow across layers; thermodynamic length, which\nquantifies the semantic effort required to traverse representational\ntransitions through layers; and belief vector field, which delineates the\nsemantic torsion fields that guide a model's belief directional orientations.\nLike biological DNA, it encodes ancestry, mutation, and semantic inheritance,\nfound in finetuning and alignment scars, cultural imprints, and architectural\ndrift. In naming it, we open a new field: Neural Genomics, where models are not\njust tools, but digital semantic organisms with traceable inner cognition.\n  Modeling statement. We read AI foundation models as semantic fluid dynamics:\nmeaning is transported through layers like fluid in a shaped conduit; nDNA is\nthe physics-grade readout of that flow -- a geometry-first measure of how\nmeaning is bent, paid for, and pushed -- yielding a stable, coordinate-free\nneural DNA fingerprint tied to on-input behavior; with this fingerprint we\ncross into biology: tracing lineages across pretraining, fine-tuning,\nalignment, pruning, distillation, and merges; measuring inheritance between\ncheckpoints; detecting drift as traits shift under new data or objectives; and,\nultimately, studying the evolution of artificial cognition to compare models,\ndiagnose risks, and govern change over time.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.18216v2",
      "pdf_url": "http://arxiv.org/pdf/2509.18216v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.39,
      "datasets_score": 0.355,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses alignment processes, which often involve RLHF in AI development, such as fine-tuning and alignment scars. However, its main contribution is analyzing latent geometry and neural structures, not the core mechanisms of RLHF like training a reward model on human-ranked data. Thus, it touches on related concepts but does not directly address or implement RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Neural DNA and latent geometry analysis, using concepts like semantic fluid dynamics as metaphors, but it does not involve diffusion models, iterative refinement for logical tasks, or treating chain-of-thought as a holistically corrected entity. There is no component for multi-step logical reasoning via diffusion processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.18218",
      "title": "Similarity Field Theory: A General Mathematical Framework for\n  Intelligence",
      "authors": [
        "Kei-Sing Ng"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We posit that persisting and transforming similarity relations form the\nstructural basis of any comprehensible dynamic system. This paper introduces\nSimilarity Field Theory, a mathematical framework that formalizes the\nprinciples governing similarity values among entities and their evolution. We\ndefine: (1) a similarity field $S: U \\times U \\to [0,1]$ over a universe of\nentities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed\nrelational field (asymmetry and non-transitivity are allowed); (2) the\nevolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by\n$p=0,1,2,\\ldots$; (3) concepts $K$ as entities that induce fibers\n$F_{\\alpha}(K) = { E \\in U \\mid S(E,K) \\ge \\alpha }$, i.e., superlevel sets of\nthe unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that\nproduces new entities. Within this framework, we formalize a generative\ndefinition of intelligence: an operator $G$ is intelligent with respect to a\nconcept $K$ if, given a system containing entities belonging to the fiber of\n$K$, it generates new entities that also belong to that fiber. Similarity Field\nTheory thus offers a foundational language for characterizing, comparing, and\nconstructing intelligent systems. We prove two theorems: (i) asymmetry blocks\nmutual inclusion; and (ii) stability requires either an anchor coordinate or\neventual confinement within a level set. These results ensure that the\nevolution of similarity fields is both constrained and interpretable,\nculminating in an exploration of how the framework allows us to interpret large\nlanguage models and present empirical results using large language models as\nexperimental probes of societal cognition.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.18218v2",
      "pdf_url": "http://arxiv.org/pdf/2509.18218v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.332,
      "datasets_score": 0.379,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a mathematical framework for similarity relations and intelligence, focusing on concepts like similarity fields and generative operators, but it does not involve human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses system evolution through sequences and generative operators but does not reference diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.19384",
      "title": "Data-Driven Reconstruction of Significant Wave Heights from Sparse\n  Observations",
      "authors": [
        "Hongyuan Shi",
        "Yilin Zhai",
        "Ping Dong",
        "Zaijin You",
        "Chao Zhan",
        "Qing Wang"
      ],
      "categories": [
        "eess.SP (Signal Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Reconstructing high-resolution regional significant wave height fields from\nsparse and uneven buoy observations remains a core challenge for ocean\nmonitoring and risk-aware operations. We introduce AUWave, a hybrid deep\nlearning framework that fuses a station-wise sequence encoder (MLP) with a\nmulti-scale U-Net enhanced by a bottleneck self-attention layer to recover\n32$\\times$32 regional SWH fields. A systematic Bayesian hyperparameter search\nwith Optuna identifies the learning rate as the dominant driver of\ngeneralization, followed by the scheduler decay and the latent dimension. Using\nNDBC buoy observations and ERA5 reanalysis over the Hawaii region, AUWave\nattains a minimum validation loss of 0.043285 and a slightly right-skewed RMSE\ndistribution. Spatial errors are lowest near observation sites and increase\nwith distance, reflecting identifiability limits under sparse sampling.\nSensitivity experiments show that AUWave consistently outperforms a\nrepresentative baseline in data-richer configurations, while the baseline is\nonly marginally competitive in the most underdetermined single-buoy cases. The\narchitecture's multi-scale and attention components translate into accuracy\ngains when minimal but non-trivial spatial anchoring is available. Error maps\nand buoy ablations reveal key anchor stations whose removal disproportionately\ndegrades performance, offering actionable guidance for network design. AUWave\nprovides a scalable pathway for gap filling, high-resolution priors for data\nassimilation, and contingency reconstruction.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.19384v1",
      "pdf_url": "http://arxiv.org/pdf/2509.19384v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.366,
      "weak_supervision_score": 0.415,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.362,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a hybrid deep learning framework (AUWave) for reconstructing significant wave heights from sparse observations, using supervised learning with data from buoy observations and ERA5 reanalysis as targets. It does not involve programmatically generating labels from high-level, noisy, or imprecise sources, which is the core of weak supervision. Instead, it relies on standard supervised training with existing datasets, making it unrelated to weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.20382",
      "title": "Lightweight MobileNetV1+GRU for ECG Biometric Authentication: Federated\n  and Adversarial Evaluation",
      "authors": [
        "Dilli Hang Rai",
        "Sabin Kafley"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "ECG biometrics offer a unique, secure authentication method, yet their\ndeployment on wearable devices faces real-time processing, privacy, and\nspoofing vulnerability challenges. This paper proposes a lightweight deep\nlearning model (MobileNetV1+GRU) for ECG-based authentication, injection of\n20dB Gaussian noise & custom preprocessing. We simulate wearable conditions and\nedge deployment using the ECGID, MIT-BIH, CYBHi, and PTB datasets, achieving\naccuracies of 99.34%, 99.31%, 91.74%, and 98.49%, F1-scores of 0.9869, 0.9923,\n0.9125, and 0.9771, Precision of 0.9866, 0.9924, 0.9180 and 0.9845, Recall of\n0.9878, 0.9923, 0.9129, and 0.9756, equal error rates (EER) of 0.0009, 0.00013,\n0.0091, and 0.0009, and ROC-AUC values of 0.9999, 0.9999, 0.9985, and 0.9998,\nwhile under FGSM adversarial attacks, accuracy drops from 96.82% to as low as\n0.80%. This paper highlights federated learning, adversarial testing, and the\nneed for diverse wearable physiological datasets to ensure secure and scalable\nbiometrics.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.20382v1",
      "pdf_url": "http://arxiv.org/pdf/2509.20382v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.281,
      "distributed_training_score": 0.343,
      "datasets_score": 0.33,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.20383",
      "title": "MARS: A Malignity-Aware Backdoor Defense in Federated Learning",
      "authors": [
        "Wei Wan",
        "Yuxuan Ning",
        "Zhicong Huang",
        "Cheng Hong",
        "Shengshan Hu",
        "Ziqi Zhou",
        "Yechao Zhang",
        "Tianqing Zhu",
        "Wanlei Zhou",
        "Leo Yu Zhang"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Federated Learning (FL) is a distributed paradigm aimed at protecting\nparticipant data privacy by exchanging model parameters to achieve high-quality\nmodel training. However, this distributed nature also makes FL highly\nvulnerable to backdoor attacks. Notably, the recently proposed state-of-the-art\n(SOTA) attack, 3DFed (SP2023), uses an indicator mechanism to determine whether\nthe backdoor models have been accepted by the defender and adaptively optimizes\nbackdoor models, rendering existing defenses ineffective. In this paper, we\nfirst reveal that the failure of existing defenses lies in the employment of\nempirical statistical measures that are loosely coupled with backdoor attacks.\nMotivated by this, we propose a Malignity-Aware backdooR defenSe (MARS) that\nleverages backdoor energy (BE) to indicate the malicious extent of each neuron.\nTo amplify malignity, we further extract the most prominent BE values from each\nmodel to form a concentrated backdoor energy (CBE). Finally, a novel\nWasserstein distance-based clustering method is introduced to effectively\nidentify backdoor models. Extensive experiments demonstrate that MARS can\ndefend against SOTA backdoor attacks and significantly outperforms existing\ndefenses.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.20383v1",
      "pdf_url": "http://arxiv.org/pdf/2509.20383v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.284,
      "distributed_training_score": 0.401,
      "datasets_score": 0.28,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on Federated Learning (FL), which is a form of distributed training involving multiple clients to train models without sharing data. However, its main contribution is a security defense mechanism (MARS) against backdoor attacks, rather than directly advancing techniques for accelerating training, partitioning data, or optimizing computation across nodes. While FL inherently relates to distributed training, the paper's emphasis on security makes it only moderately relevant to the core aspects of the topic.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "This paper addresses the vulnerabilities of Federated Learning (FL) to backdoor attacks, particularly advanced ones like 3DFed, by proposing MARS, a defense mechanism that uses backdoor energy (BE) to measure the maliciousness of neurons, amplifies it through concentrated backdoor energy (CBE), and applies a novel Wasserstein distance-based clustering method to detect and eliminate backdoor models. The core objectives include revealing the shortcomings of existing defenses and demonstrating through extensive experiments that MARS effectively counters state-of-the-art attacks while outperforming prior methods, thereby enhancing FL security.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces novel concepts such as backdoor energy and Wasserstein distance-based clustering, which represent a significant advancement in defending against adaptive backdoor attacks in Federated Learning, moving beyond empirical statistical measures used in existing defenses.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of FL security due to its effective defense against sophisticated attacks, but its influence may be limited to specialized applications in distributed machine learning rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a high-quality contribution with innovative defense techniques that are valuable for researchers in AI security and Federated Learning, making it essential for those in the field but not for the general audience.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c63b794f7600514433570e5acae08cd8c6eeddca",
      "total_authors": 10,
      "authors_found": 9,
      "highest_h_index": 17,
      "average_h_index": 7.111111111111111,
      "notable_authors_count": 6,
      "author_h_indexes": [
        {
          "name": "Wei Wan",
          "h_index": 7,
          "profile_url": "https://www.semanticscholar.org/author/2149237194"
        },
        {
          "name": "Yuxuan Ning",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2275150174"
        },
        {
          "name": "Zhicong Huang",
          "h_index": 3,
          "profile_url": "https://www.semanticscholar.org/author/2266091259"
        },
        {
          "name": "Cheng Hong",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2383968789"
        },
        {
          "name": "Shengshan Hu",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2302793245"
        },
        {
          "name": "Ziqi Zhou",
          "h_index": 8,
          "profile_url": "https://www.semanticscholar.org/author/2268806967"
        },
        {
          "name": "Yechao Zhang",
          "h_index": 9,
          "profile_url": "https://www.semanticscholar.org/author/2155392496"
        },
        {
          "name": "Tianqing Zhu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Wanlei Zhou",
          "h_index": 17,
          "profile_url": "https://www.semanticscholar.org/author/2134555583"
        },
        {
          "name": "Leo Yu Zhang",
          "h_index": 12,
          "profile_url": "https://www.semanticscholar.org/author/2248789322"
        }
      ]
    },
    {
      "id": "2509.20384",
      "title": "R1-Fuzz: Specializing Language Models for Textual Fuzzing via\n  Reinforcement Learning",
      "authors": [
        "Jiayi Lin",
        "Liangcai Su",
        "Junzhe Li",
        "Chenxiong Qian"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.PL (Programming Languages)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Fuzzing is effective for vulnerability discovery but struggles with complex\ntargets such as compilers, interpreters, and database engines, which accept\ntextual input that must satisfy intricate syntactic and semantic constraints.\nAlthough language models (LMs) have attracted interest for this task due to\ntheir vast latent knowledge and reasoning potential, their practical adoption\nhas been limited. The major challenges stem from insufficient exploration of\ndeep program logic among real-world codebases, and the high cost of leveraging\nlarger models. To overcome these challenges, we propose R1-Fuzz, the first\nframework that leverages reinforcement learning (RL) to specialize\ncost-efficient LMs and integrate them for complex textual fuzzing input\ngeneration. R1-Fuzz introduces two key designs: coverage-slicing-based question\nconstruction and a distance-based reward calculation. Through RL-based\npost-training of a model with our constructed dataset, R1-Fuzz designs a\nfuzzing workflow that tightly integrates LMs to reason deep program semantics\nduring fuzzing. Evaluations on diverse real-world targets show that our design\nenables a small model, named R1-Fuzz-7B, to rival or even outperform much\nlarger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\\%\nhigher coverage than state-of-the-art fuzzers and discovers 29 previously\nunknown vulnerabilities, demonstrating its practicality.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.20384v1",
      "pdf_url": "http://arxiv.org/pdf/2509.20384v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.469,
      "weak_supervision_score": 0.415,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.361,
      "datasets_score": 0.289,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning for model training, specifically with a distance-based reward mechanism derived from code coverage metrics, but it does not involve human feedback, human-ranked data, or a separate reward model trained on human preferences. Thus, it does not align with RLHF.",
      "weak_supervision_justification": "The paper programmatically generates a dataset of questions and rewards using coverage-slicing-based techniques from seed inputs and program execution, which resembles weak supervision by creating noisy or imprecise labels automatically. However, weak supervision is not the core focus, as the primary contribution is the RL framework for fuzzing.",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for language models in fuzzing, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning akin to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "R1-Fuzz is a novel framework that leverages reinforcement learning to specialize cost-efficient language models for generating high-quality inputs in textual fuzzing, addressing challenges in exploring deep program logic and reducing costs associated with larger models. The methodology involves coverage-slicing-based question construction to create targeted prompts from source code and a distance-based reward calculation for RL-based post-training, enabling the integration of these models into a coverage-guided fuzzing loop. Evaluations demonstrate that a small model, R1-Fuzz-7B, outperforms larger models and state-of-the-art fuzzers, achieving up to 75% higher code coverage and discovering 29 previously unknown vulnerabilities in real-world targets like compilers and databases.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new framework by combining reinforcement learning with language models for textual fuzzing, including innovative techniques like coverage-slicing-based question construction and distance-based rewards, which significantly advance the state-of-the-art in vulnerability discovery. This represents a substantial leap beyond existing methods that struggle with complex inputs.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research in AI-driven security testing and commercial applications by providing an efficient, open-sourced framework for fuzzing complex software. Its demonstrated ability to discover real vulnerabilities and outperform existing tools suggests it could lead to broader adoption in software engineering and cybersecurity practices.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality, innovative contribution to AI and security that offers practical advancements in fuzzing, making it valuable for researchers and practitioners in these fields. While essential for those focused on software security, it may not be critical for all audiences.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/022375a5639131bae3c5cd7745f4f4456767ef7c",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 2,
      "average_h_index": 1.25,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jiayi Lin",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2309717620"
        },
        {
          "name": "Liangcai Su",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2312334940"
        },
        {
          "name": "Junzhe Li",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2309668930"
        },
        {
          "name": "Chenxiong Qian",
          "h_index": 2,
          "profile_url": "https://www.semanticscholar.org/author/2312312793"
        }
      ]
    },
    {
      "id": "2509.21358",
      "title": "MDF-MLLM: Deep Fusion Through Cross-Modal Feature Alignment for\n  Contextually Aware Fundoscopic Image Classification",
      "authors": [
        "Jason Jordan",
        "Mohammadreza Akbari Lor",
        "Peter Koulen",
        "Mei-Ling Shyu",
        "Shu-Ching Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study aimed to enhance disease classification accuracy from retinal\nfundus images by integrating fine-grained image features and global textual\ncontext using a novel multimodal deep learning architecture. Existing\nmultimodal large language models (MLLMs) often struggle to capture low-level\nspatial details critical for diagnosing retinal diseases such as glaucoma,\ndiabetic retinopathy, and retinitis pigmentosa. This model development and\nvalidation study was conducted on 1,305 fundus image-text pairs compiled from\nthree public datasets (FIVES, HRF, and StoneRounds), covering acquired and\ninherited retinal diseases, and evaluated using classification accuracy and\nF1-score. The MDF-MLLM integrates skip features from four U-Net encoder layers\ninto cross-attention blocks within a LLaMA 3.2 11B MLLM. Vision features are\npatch-wise projected and fused using scaled cross-attention and FiLM-based\nU-Net modulation. Baseline MLLM achieved 60% accuracy on the dual-type disease\nclassification task. MDF-MLLM, with both U-Net and MLLM components fully\nfine-tuned during training, achieved a significantly higher accuracy of 94%,\nrepresenting a 56% improvement. Recall and F1-scores improved by as much as 67%\nand 35% over baseline, respectively. Ablation studies confirmed that the\nmulti-depth fusion approach contributed to substantial gains in spatial\nreasoning and classification, particularly for inherited diseases with rich\nclinical text. MDF-MLLM presents a generalizable, interpretable, and modular\nframework for fundus image classification, outperforming traditional MLLM\nbaselines through multi-scale feature fusion. The architecture holds promise\nfor real-world deployment in clinical decision support systems. Future work\nwill explore synchronized training techniques, a larger pool of diseases for\nmore generalizability, and extending the model for segmentation tasks.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.21358v1",
      "pdf_url": "http://arxiv.org/pdf/2509.21358v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.375,
      "datasets_score": 0.391,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a multimodal architecture for fundus image classification using U-Net and MLLM with feature fusion techniques, focusing on integrating image and textual data for improved accuracy in medical diagnostics. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks, which are core to diffusion-based reasoning. Thus, there is no connection to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.21359",
      "title": "Influence Guided Context Selection for Effective Retrieval-Augmented\n  Generation",
      "authors": [
        "Jiale Deng",
        "Yanyan Shen",
        "Ziyuan Pei",
        "Youmin Chen",
        "Linpeng Huang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) addresses large language model (LLM)\nhallucinations by grounding responses in external knowledge, but its\neffectiveness is compromised by poor-quality retrieved contexts containing\nirrelevant or noisy information. While existing approaches attempt to improve\nperformance through context selection based on predefined context quality\nassessment metrics, they show limited gains over standard RAG. We attribute\nthis limitation to their failure in holistically utilizing available\ninformation (query, context list, and generator) for comprehensive quality\nassessment. Inspired by recent advances in data selection, we reconceptualize\ncontext quality assessment as an inference-time data valuation problem and\nintroduce the Contextual Influence Value (CI value). This novel metric\nquantifies context quality by measuring the performance degradation when\nremoving each context from the list, effectively integrating query-aware\nrelevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI\nvalue eliminates complex selection hyperparameter tuning by simply retaining\ncontexts with positive CI values. To address practical challenges of label\ndependency and computational overhead, we develop a parameterized surrogate\nmodel for CI value prediction during inference. The model employs a\nhierarchical architecture that captures both local query-context relevance and\nglobal inter-context interactions, trained through oracle CI value supervision\nand end-to-end generator feedback. Extensive experiments across 8 NLP tasks and\nmultiple LLMs demonstrate that our context selection method significantly\noutperforms state-of-the-art baselines, effectively filtering poor-quality\ncontexts while preserving critical information. Code is available at\nhttps://github.com/SJTU-DMTai/RAG-CSM.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.21359v1",
      "pdf_url": "http://arxiv.org/pdf/2509.21359v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.428,
      "weak_supervision_score": 0.376,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.314,
      "datasets_score": 0.353,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on improving context selection in Retrieval-Augmented Generation (RAG) using Contextual Influence Value and a surrogate model, without involving human feedback, reward models, or reinforcement learning for model alignment. While it mentions generator feedback for training, this is automated and not based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses context quality assessment in RAG and introduces a hierarchical model for prediction, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic. There is no component for holistically correcting a Chain-of-Thought using diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.21360",
      "title": "Multimodal Prompt Decoupling Attack on the Safety Filters in\n  Text-to-Image Models",
      "authors": [
        "Xingkai Peng",
        "Jun Jiang",
        "Meng Tong",
        "Shuai Li",
        "Weiming Zhang",
        "Nenghai Yu",
        "Kejiang Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Text-to-image (T2I) models have been widely applied in generating\nhigh-fidelity images across various domains. However, these models may also be\nabused to produce Not-Safe-for-Work (NSFW) content via jailbreak attacks.\nExisting jailbreak methods primarily manipulate the textual prompt, leaving\npotential vulnerabilities in image-based inputs largely unexplored. Moreover,\ntext-based methods face challenges in bypassing the model's safety filters. In\nresponse to these limitations, we propose the Multimodal Prompt Decoupling\nAttack (MPDA), which utilizes image modality to separate the harmful semantic\ncomponents of the original unsafe prompt. MPDA follows three core steps:\nfirstly, a large language model (LLM) decouples unsafe prompts into pseudo-safe\nprompts and harmful prompts. The former are seemingly harmless sub-prompts that\ncan bypass filters, while the latter are sub-prompts with unsafe semantics that\ntrigger filters. Subsequently, the LLM rewrites the harmful prompts into\nnatural adversarial prompts to bypass safety filters, which guide the T2I model\nto modify the base image into an NSFW output. Finally, to ensure semantic\nconsistency between the generated NSFW images and the original unsafe prompts,\nthe visual language model generates image captions, providing a new pathway to\nguide the LLM in iterative rewriting and refining the generated content.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.21360v1",
      "pdf_url": "http://arxiv.org/pdf/2509.21360v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.445,
      "distributed_training_score": 0.324,
      "datasets_score": 0.304,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper primarily discusses a multimodal attack on text-to-image (T2I) models, which are often based on diffusion architectures for image generation. However, it does not adapt the iterative refinement process of diffusion models to solve complex logical tasks or treat a Chain-of-Thought as a single entity for holistic correction. Instead, the paper focuses on prompt decoupling and iterative refinement for bypassing safety filters, which is unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.21361",
      "title": "Context Is What You Need: The Maximum Effective Context Window for Real\n  World Limits of LLMs",
      "authors": [
        "Norman Paulsen"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language model (LLM) providers boast big numbers for maximum context\nwindow sizes. To test the real world use of context windows, we 1) define a\nconcept of maximum effective context window, 2) formulate a testing method of a\ncontext window's effectiveness over various sizes and problem types, and 3)\ncreate a standardized way to compare model efficacy for increasingly larger\ncontext window sizes to find the point of failure. We collected hundreds of\nthousands of data points across several models and found significant\ndifferences between reported Maximum Context Window (MCW) size and Maximum\nEffective Context Window (MECW) size. Our findings show that the MECW is, not\nonly, drastically different from the MCW but also shifts based on the problem\ntype. A few top of the line models in our test group failed with as little as\n100 tokens in context; most had severe degradation in accuracy by 1000 tokens\nin context. All models fell far short of their Maximum Context Window by as\nmuch as 99 percent. Our data reveals the Maximum Effective Context Window\nshifts based on the type of problem provided, offering clear and actionable\ninsights into how to improve model accuracy and decrease model hallucination\nrates.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.21361v1",
      "pdf_url": "http://arxiv.org/pdf/2509.21361v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.377,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.337,
      "datasets_score": 0.333,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.21363",
      "title": "A Mutual Learning Method for Salient Object Detection with intertwined\n  Multi-Supervision--Revised",
      "authors": [
        "Runmin Wu",
        "Mengyang Feng",
        "Wenlong Guan",
        "Dong Wang",
        "Huchuan Lu",
        "Errui Ding"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Though deep learning techniques have made great progress in salient object\ndetection recently, the predicted saliency maps still suffer from incomplete\npredictions due to the internal complexity of objects and inaccurate boundaries\ncaused by strides in convolution and pooling operations. To alleviate these\nissues, we propose to train saliency detection networks by exploiting the\nsupervision from not only salient object detection, but also foreground contour\ndetection and edge detection. First, we leverage salient object detection and\nforeground contour detection tasks in an intertwined manner to generate\nsaliency maps with uniform highlight. Second, the foreground contour and edge\ndetection tasks guide each other simultaneously, thereby leading to precise\nforeground contour prediction and reducing the local noises for edge\nprediction. In addition, we develop a novel mutual learning module (MLM) which\nserves as the building block of our method. Each MLM consists of multiple\nnetwork branches trained in a mutual learning manner, which improves the\nperformance by a large margin. Extensive experiments on seven challenging\ndatasets demonstrate that the proposed method has delivered state-of-the-art\nresults in both salient object detection and edge detection.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.21363v1",
      "pdf_url": "http://arxiv.org/pdf/2509.21363v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.355,
      "distributed_training_score": 0.341,
      "datasets_score": 0.295,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.22698",
      "title": "Advancing Audio-Visual Navigation Through Multi-Agent Collaboration in\n  3D Environments",
      "authors": [
        "Hailong Zhang",
        "Yinfeng Yu",
        "Liejun Wang",
        "Fuchun Sun",
        "Wendong Zheng"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Intelligent agents often require collaborative strategies to achieve complex\ntasks beyond individual capabilities in real-world scenarios. While existing\naudio-visual navigation (AVN) research mainly focuses on single-agent systems,\ntheir limitations emerge in dynamic 3D environments where rapid multi-agent\ncoordination is critical, especially for time-sensitive applications like\nemergency response. This paper introduces MASTAVN (Multi-Agent Scalable\nTransformer Audio-Visual Navigation), a scalable framework enabling two agents\nto collaboratively localize and navigate toward an audio target in shared 3D\nenvironments. By integrating cross-agent communication protocols and joint\naudio-visual fusion mechanisms, MASTAVN enhances spatial reasoning and temporal\nsynchronization. Through rigorous evaluation in photorealistic 3D simulators\n(Replica and Matterport3D), MASTAVN achieves significant reductions in task\ncompletion time and notable improvements in navigation success rates compared\nto single-agent and non-collaborative baselines. This highlights the essential\nrole of spatiotemporal coordination in multi-agent systems. Our findings\nvalidate MASTAVN's effectiveness in time-sensitive emergency scenarios and\nestablish a paradigm for advancing scalable multi-agent embodied intelligence\nin complex 3D environments.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.22698v1",
      "pdf_url": "http://arxiv.org/pdf/2509.22698v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.394,
      "distributed_training_score": 0.371,
      "datasets_score": 0.319,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2509.22700",
      "title": "Global Prompt Refinement with Non-Interfering Attention Masking for\n  One-Shot Federated Learning",
      "authors": [
        "Zhuang Qi",
        "Pan Yu",
        "Lei Meng",
        "Sijin Zhou",
        "Han Yu",
        "Xiaoxiao Li",
        "Xiangxu Meng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Federated Prompt Learning (FPL) enables communication-efficient adaptation by\ntuning lightweight prompts on top of frozen pre-trained models. Existing FPL\nmethods typically rely on global information, which is only available after the\nsecond training round, to facilitate collaboration among client models.\nTherefore, they are inherently dependent on multi-round communication to fully\nexhibit their strengths. Moreover, existing one-shot federated learning methods\ntypically focus on fitting seen tasks, but lack cross-task generalization. To\nbridge this gap, we propose the Global Prompt Refinement with Non-Interfering\nAttention Masking (GPR-NIAM) method for one-shot FPL. The core idea is to\ndesign a masking mechanism that restricts excessive interaction between the\noriginal text embeddings and the learnable prompt embeddings. GPR-NIAM achieves\nthis through the collaboration of two key modules. Firstly, the attention\nisolation module suppresses attention from the learnable prompt tokens to the\noriginal text tokens, and reweights the reverse attention which preserves\ngeneralization across tasks. Secondly, the cross-silo collaborative refinement\nmodule integrates decentralized visual knowledge into a unified base and\ncalibrates the global prompt through multi-source cross-modal knowledge\nalignment, further mitigating the inconsistency caused by data heterogeneity.\nExtensive experiments conducted on ten benchmark datasets under two tasks show\nthat GPR-NIAM outperforms eight state-of-the-art methods in both class-level\nand domain-level generalization.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.22700v1",
      "pdf_url": "http://arxiv.org/pdf/2509.22700v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.429,
      "weak_supervision_score": 0.394,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.436,
      "datasets_score": 0.316,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on federated prompt learning and attention masking for efficient model adaptation in distributed settings, with no mention of human feedback, reward models, or reinforcement learning techniques for aligning AI with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution involves federated learning, specifically one-shot federated prompt learning, which addresses distributed training challenges by aggregating models across clients, reducing communication overhead, and handling data heterogeneity in a multi-node environment.",
      "datasets_justification": "below_threshold",
      "llm_score_status": "completed",
      "summary": "The paper proposes GPR-NIAM, a method for one-shot Federated Prompt Learning (FPL) that addresses data heterogeneity and enhances generalization by introducing a non-interfering attention masking mechanism. It consists of an attention isolation module to suppress and reweight attention between learnable prompts and original text embeddings, and a cross-silo collaborative refinement module to integrate decentralized visual knowledge for refining global prompts; extensive experiments on ten datasets demonstrate that GPR-NIAM outperforms eight state-of-the-art methods in both class-level and domain-level generalization tasks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining attention masking and collaborative refinement to enable one-shot FPL, effectively addressing limitations in multi-round methods while preserving generalization. However, it builds on existing FPL concepts rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in federated learning and prompt-based methods by reducing communication overhead and improving generalization, making it relevant for subfields like computer vision. Nonetheless, its impact may be confined to specific applications rather than broadly transformative.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution with innovative modules and superior experimental results that advance one-shot FPL, making it essential for researchers in federated learning and computer vision. While impressive, it is not groundbreaking enough to be classified as must-read.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/685b28bccd522cddc23ed8e7735d289751b3605c",
      "total_authors": 7,
      "authors_found": 4,
      "highest_h_index": 13,
      "average_h_index": 5.25,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Zhuang Qi",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Pan Yu",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Lei Meng",
          "h_index": null,
          "profile_url": null
        },
        {
          "name": "Sijin Zhou",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2357977013"
        },
        {
          "name": "Han Yu",
          "h_index": 1,
          "profile_url": "https://www.semanticscholar.org/author/2381724091"
        },
        {
          "name": "Xiaoxiao Li",
          "h_index": 6,
          "profile_url": "https://www.semanticscholar.org/author/2283747425"
        },
        {
          "name": "Xiangxu Meng",
          "h_index": 13,
          "profile_url": "https://www.semanticscholar.org/author/2150591081"
        }
      ]
    },
    {
      "id": "2509.25210",
      "title": "STCast: Adaptive Boundary Alignment for Global and Regional Weather\n  Forecasting",
      "authors": [
        "Hao Chen",
        "Tao Han",
        "Jie Zhang",
        "Song Guo",
        "Lei Bai"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "To gain finer regional forecasts, many works have explored the regional\nintegration from the global atmosphere, e.g., by solving boundary equations in\nphysics-based methods or cropping regions from global forecasts in data-driven\nmethods. However, the effectiveness of these methods is often constrained by\nstatic and imprecise regional boundaries, resulting in poor generalization\nability. To address this issue, we propose Spatial-Temporal Weather Forecasting\n(STCast), a novel AI-driven framework for adaptive regional boundary\noptimization and dynamic monthly forecast allocation. Specifically, our\napproach employs a Spatial-Aligned Attention (SAA) mechanism, which aligns\nglobal and regional spatial distributions to initialize boundaries and\nadaptively refines them based on attention-derived alignment patterns.\nFurthermore, we design a Temporal Mixture-of-Experts (TMoE) module, where\natmospheric variables from distinct months are dynamically routed to\nspecialized experts using a discrete Gaussian distribution, enhancing the\nmodel's ability to capture temporal patterns. Beyond global and regional\nforecasting, we evaluate our STCast on extreme event prediction and ensemble\nforecasting. Experimental results demonstrate consistent superiority over\nstate-of-the-art methods across all four tasks.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2509.25210v1",
      "pdf_url": "http://arxiv.org/pdf/2509.25210v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.368,
      "distributed_training_score": 0.376,
      "datasets_score": 0.318,
      "llm_validation_status": "completed",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "llm_score_status": "not_relevant_enough",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "h_index_status": "not_fetched",
      "semantic_scholar_url": null,
      "total_authors": 0,
      "authors_found": 0,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": []
    },
    {
      "id": "2510.01219",
      "title": "Uncovering Implicit Bias in Large Language Models with Concept Learning\n  Dataset",
      "authors": [
        "Leroy Z. Wang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We introduce a dataset of concept learning tasks that helps uncover implicit\nbiases in large language models. Using in-context concept learning experiments,\nwe found that language models may have a bias toward upward monotonicity in\nquantifiers; such bias is less apparent when the model is tested by direct\nprompting without concept learning components. This demonstrates that\nin-context concept learning can be an effective way to discover hidden biases\nin language models.",
      "published_date": "2025-09-21",
      "arxiv_url": "http://arxiv.org/abs/2510.01219v1",
      "pdf_url": "http://arxiv.org/pdf/2510.01219v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "rlhf_score": 0.437,
      "weak_supervision_score": 0.448,
      "diffusion_reasoning_score": 0.437,
      "distributed_training_score": 0.343,
      "datasets_score": 0.425,
      "llm_validation_status": "completed",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on evaluating implicit biases in LLMs using a concept learning dataset and in-context learning, without any involvement of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper introduces a dataset for testing LLMs but does not involve training models with programmatically generated labels or noisy sources; it is centered on analysis and evaluation, not weak supervision methods.",
      "diffusion_reasoning_justification": "The paper examines in-context learning and prompting for bias detection, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction and use of a new dataset for concept learning tasks to uncover biases in LLMs, including analysis and benchmarking through experiments, which directly aligns with dataset creation, evaluation, and application in AI research.",
      "llm_score_status": "completed",
      "summary": "This paper introduces a dataset and method using in-context concept learning tasks to uncover implicit biases in large language models (LLMs), specifically focusing on biases related to semantic monotonicity in quantifiers. By presenting LLMs with examples of unknown concepts and testing their labeling accuracy, the authors demonstrate that models exhibit a bias toward upward monotone concepts, which is less evident in direct prompting experiments, highlighting the effectiveness of concept learning for detecting hidden biases.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining human concept learning ideas with LLM testing to reveal implicit biases not detected by standard methods, though it builds on existing bias detection techniques rather than introducing a entirely new problem. This clever adaptation advances the field without radically shifting the state-of-the-art.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in AI bias detection and NLP by providing a new tool for uncovering hidden biases in LLMs, potentially leading to citations and applications within subfields like computational linguistics and AI ethics. However, its impact may be limited to specialized areas rather than broadly transforming commercial applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution to understanding and detecting biases in LLMs, making it important for researchers in AI and NLP to be aware of the method and findings. While not essential for all audiences, it represents a strong advancement in bias evaluation techniques.",
      "h_index_status": "completed",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/00a1f1366aa8098e19fe2b467a4a8f1eb8a58f21",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Leroy Z. Wang",
          "h_index": 0,
          "profile_url": "https://www.semanticscholar.org/author/2383413232"
        }
      ]
    }
  ],
  "total_papers": 129,
  "date": "2025-09-21"
};
    </script>

    <script>
        // ============================================================================
        // GLOBAL VARIABLES & CONFIGURATION
        // ============================================================================
        
        // Page configuration - get data from embedded PAPER_DATA
        const PAGE_DATE = PAPER_DATA.date;
        const PAPERS_PER_PAGE = 5;
        let currentPage = 1;
        let totalPapers = PAPER_DATA.total_papers;
        let totalPages = 0;
        let allPapers = PAPER_DATA.papers;  // Use embedded papers data
        let filteredSortedPapers = [];  // Store papers after filtering/sorting
        let currentPagePapers = [];  // Store papers for current page display
        let currentSort = 'recommend_best';  // Default sort
        
        // H-Index Filter State Management
        let currentHIndexFilters = {
            found: true,
            notFound: true,
            highestMin: 0,
            highestMax: 1000,
            averageMin: 0,
            averageMax: 1000
        };
        
        let pendingHIndexFilters = { ...currentHIndexFilters };
        
        // Topic Filter State Management
        let currentTopicFilters = {
            rlhf: true,
            weakSupervision: true,
            diffusionReasoning: true,
            distributedTraining: true,
            datasets: true
        };
        
        let pendingTopicFilters = { ...currentTopicFilters };
        
        // Relevance Filter State Management
        let currentRelevanceFilters = {
            highlyRelevant: true,
            moderatelyRelevant: true,
            tangentiallyRelevant: true,
            notRelevant: true
        };
        
        let pendingRelevanceFilters = { ...currentRelevanceFilters };
        
        // Sidebar state variables
        let isMobileSidebarOpen = false;
        let isDesktopSidebarOpen = false;

        // ============================================================================
        // URL PARAMETER UTILITIES
        // ============================================================================
        
        function getUrlParameter(name) {
            const urlParams = new URLSearchParams(window.location.search);
            return urlParams.get(name);
        }
        
        function setUrlParameter(name, value) {
            const url = new URL(window.location.href);
            url.searchParams.set(name, value);
            window.history.pushState(null, '', url.toString());
        }
        
        function updateHIndexFiltersFromURL() {
            // Get H-Index filter parameters from URL
            const hindexFound = getUrlParameter('hindex_found');
            const hindexNotFound = getUrlParameter('hindex_not_found');
            const highestMin = getUrlParameter('highest_min');
            const highestMax = getUrlParameter('highest_max');
            const averageMin = getUrlParameter('average_min');
            const averageMax = getUrlParameter('average_max');
            
            // Update current filters if parameters exist
            if (hindexFound !== null) currentHIndexFilters.found = hindexFound === 'true';
            if (hindexNotFound !== null) currentHIndexFilters.notFound = hindexNotFound === 'true';
            if (highestMin !== null) currentHIndexFilters.highestMin = parseInt(highestMin) || 0;
            if (highestMax !== null) currentHIndexFilters.highestMax = parseInt(highestMax) || 1000;
            if (averageMin !== null) currentHIndexFilters.averageMin = parseInt(averageMin) || 0;
            if (averageMax !== null) currentHIndexFilters.averageMax = parseInt(averageMax) || 1000;
            
            // Sync pending filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Sync UI and update button text
            syncHIndexUI();
        }
        
        function updateURLWithHIndexFilters() {
            const url = new URL(window.location.href);
            
            // Only set parameters if they differ from defaults
            if (!currentHIndexFilters.found || !currentHIndexFilters.notFound) {
                url.searchParams.set('hindex_found', currentHIndexFilters.found);
                url.searchParams.set('hindex_not_found', currentHIndexFilters.notFound);
            } else {
                url.searchParams.delete('hindex_found');
                url.searchParams.delete('hindex_not_found');
            }
            
            if (currentHIndexFilters.highestMin !== 0 || currentHIndexFilters.highestMax !== 1000) {
                url.searchParams.set('highest_min', currentHIndexFilters.highestMin);
                url.searchParams.set('highest_max', currentHIndexFilters.highestMax);
            } else {
                url.searchParams.delete('highest_min');
                url.searchParams.delete('highest_max');
            }
            
            if (currentHIndexFilters.averageMin !== 0 || currentHIndexFilters.averageMax !== 1000) {
                url.searchParams.set('average_min', currentHIndexFilters.averageMin);
                url.searchParams.set('average_max', currentHIndexFilters.averageMax);
            } else {
                url.searchParams.delete('average_min');
                url.searchParams.delete('average_max');
            }
            
            window.history.pushState(null, '', url.toString());
        }

        // ============================================================================
        // DATE FORMATTING FUNCTIONS
        // ============================================================================
        
        function formatPageDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        function formatPublicationDate(dateString) {
            const date = new Date(dateString);
            const options = { day: 'numeric', month: 'long', year: 'numeric' };
            return date.toLocaleDateString('en-GB', options);
        }

        // ============================================================================
        // UI UPDATE FUNCTIONS FOR PAGE LOAD
        // ============================================================================
        
        function updatePageTitles(date) {
            const formattedDate = formatPageDate(date);
            const titleText = `Papers Published on ${formattedDate}`;
            
            // Update page title
            document.title = `Research Feed -- ${formattedDate}`;
            
            // Update mobile and desktop headers
            const mobileTitle = document.getElementById('page-title-mobile');
            const desktopTitle = document.getElementById('page-title-desktop');
            
            if (mobileTitle) {
                mobileTitle.textContent = titleText;
            }
            if (desktopTitle) {
                desktopTitle.textContent = titleText;
            }
        }

        function updatePaperCount() {
            const mobileCount = document.getElementById('mobile-paper-count');
            const desktopCount = document.getElementById('desktop-paper-count');
            const mobileMainCount = document.getElementById('mobile-main-paper-count');
            const desktopMainCount = document.getElementById('desktop-main-paper-count');
            
            const showing = filteredSortedPapers.length;
            const sidebarCountText = `Showing: ${showing}/${totalPapers} Papers`;
            const mainCountText = `Showing ${showing} / ${totalPapers} papers`;
            
            // Update sidebar counts
            if (mobileCount) {
                mobileCount.textContent = sidebarCountText;
            }
            if (desktopCount) {
                desktopCount.textContent = sidebarCountText;
            }
            
            // Update main header counts
            if (mobileMainCount) {
                mobileMainCount.textContent = mainCountText;
            }
            if (desktopMainCount) {
                desktopMainCount.textContent = mainCountText;
            }
        }

        // ============================================================================
        // SORTING FUNCTIONS
        // ============================================================================
        
        function calculateRecommendationScore(paper) {
            // Skip calculation if already calculated or if not relevant enough
            if (paper.recommendation_numerical_score !== undefined) {
                return paper.recommendation_numerical_score;
            }
            
            if (paper.llm_score_status === 'not_relevant_enough') {
                paper.recommendation_numerical_score = 0;
                return 0;
            }
            
            let score = 0;
            
            // Recommendation scores (primary)
            const recommendationScores = {
                'Must Read': 40,
                'Should Read': 30,
                'Can Skip': 20,
                'Ignore': 10
            };
            score += recommendationScores[paper.recommendation_score] || 0;
            
            // Novelty scores (first tiebreaker)
            const noveltyScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'None': 1
            };
            score += noveltyScores[paper.novelty_score] || 0;
            
            // Impact scores (second tiebreaker)
            const impactScores = {
                'High': 4,
                'Moderate': 3,
                'Low': 2,
                'Negligible': 1
            };
            score += impactScores[paper.impact_score] || 0;
            
            paper.recommendation_numerical_score = score;
            return score;
        }
        
        function getHighestHIndex(paper) {
            // Return the highest H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.highest_h_index !== undefined ? paper.highest_h_index : -1;
        }
        
        function getAverageHIndex(paper) {
            // Return the average H-index value, or -1 if not available (so unavailable papers sort last)
            return paper.average_h_index !== undefined ? paper.average_h_index : -1;
        }
        
        function calculateRelevanceScore(paper) {
            let score = 0;
            
            // Only consider topics that are currently selected/enabled in the topic filter
            const topicsToConsider = [];
            if (currentTopicFilters.rlhf) topicsToConsider.push('rlhf_relevance');
            if (currentTopicFilters.weakSupervision) topicsToConsider.push('weak_supervision_relevance');
            if (currentTopicFilters.diffusionReasoning) topicsToConsider.push('diffusion_reasoning_relevance');
            if (currentTopicFilters.distributedTraining) topicsToConsider.push('distributed_training_relevance');
            if (currentTopicFilters.datasets) topicsToConsider.push('datasets_relevance');
            
            // If no topics are selected, return 0
            if (topicsToConsider.length === 0) return 0;
            
            // Weighted scoring system
            const relevanceWeights = {
                'Highly Relevant': 4,
                'Moderately Relevant': 3,
                'Tangentially Relevant': 2,
                'Not Relevant': 1
            };
            
            // Sum up scores for selected topics only
            for (let topicField of topicsToConsider) {
                const relevance = paper[topicField];
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = relevance === "not_validated" ? "Not Relevant" : relevance;
                score += relevanceWeights[normalizedRelevance] || 1; // Default to 1 if unknown
            }
            
            return score;
        }
        
        function sortPapers(sortType) {
            switch (sortType) {
                case 'recommend_best':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
                    break;
                case 'recommend_worst':
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(a) - calculateRecommendationScore(b));
                    break;
                case 'relevance_high':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(b) - calculateRelevanceScore(a));
                    break;
                case 'relevance_low':
                    filteredSortedPapers.sort((a, b) => calculateRelevanceScore(a) - calculateRelevanceScore(b));
                    break;
                case 'highest_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(a) - getHighestHIndex(b));
                    break;
                case 'highest_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getHighestHIndex(b) - getHighestHIndex(a));
                    break;
                case 'average_hindex_asc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(a) - getAverageHIndex(b));
                    break;
                case 'average_hindex_desc':
                    filteredSortedPapers.sort((a, b) => getAverageHIndex(b) - getAverageHIndex(a));
                    break;
                case 'id_asc':
                    filteredSortedPapers.sort((a, b) => a.id.localeCompare(b.id));
                    break;
                case 'id_desc':
                    filteredSortedPapers.sort((a, b) => b.id.localeCompare(a.id));
                    break;
                case 'title_az':
                    filteredSortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
                case 'title_za':
                    filteredSortedPapers.sort((a, b) => b.title.localeCompare(a.title));
                    break;
                default:
                    // Default to recommendation best first
                    filteredSortedPapers.sort((a, b) => calculateRecommendationScore(b) - calculateRecommendationScore(a));
            }
        }

        // ============================================================================
        // DROPDOWN DIRECTION FUNCTIONS
        // ============================================================================
        
        function setDropdownDirection(button, dropdown) {
            const buttonRect = button.getBoundingClientRect();
            const sidebar = button.closest('#mobile-sidebar, #desktop-sidebar');
            
            // Get the sidebar content area instead of the entire sidebar
            const sidebarContent = sidebar.querySelector('.flex-1');
            const sidebarContentRect = sidebarContent ? sidebarContent.getBoundingClientRect() : sidebar.getBoundingClientRect();
            
            // Calculate available space within the entire sidebar content area
            const spaceBelow = sidebarContentRect.bottom - buttonRect.bottom;
            const spaceAbove = buttonRect.top - sidebarContentRect.top;
            
            // Estimate dropdown height (roughly 6 items * 40px each)
            const estimatedDropdownHeight = 240;
            
            // Determine direction based on available space in the whole sidebar content
            if (spaceBelow >= estimatedDropdownHeight || spaceBelow >= spaceAbove) {
                // Dropdown goes down
                dropdown.classList.remove('dropdown-up');
                dropdown.classList.add('dropdown-down');
            } else {
                // Dropdown goes up
                dropdown.classList.remove('dropdown-down');
                dropdown.classList.add('dropdown-up');
            }
        }

        // ============================================================================
        // SORTING DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileSortDropdown() {
            const button = document.getElementById('mobile-sort-btn');
            const dropdown = document.getElementById('mobile-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopSortDropdown() {
            const button = document.getElementById('desktop-sort-btn');
            const dropdown = document.getElementById('desktop-sort-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function changeSortAndClose(sortType) {
            // Update current sort
            currentSort = sortType;
            
            // Update URL
            setUrlParameter('sort', sortType);
            
            // Update dropdown text
            updateSortDropdownUI();
            
            // Close dropdowns and reset button states
            const mobileDropdown = document.getElementById('mobile-sort-dropdown');
            const desktopDropdown = document.getElementById('desktop-sort-dropdown');
            const mobileButton = document.getElementById('mobile-sort-btn');
            const desktopButton = document.getElementById('desktop-sort-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Close the appropriate sidebar
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
            
            // Apply new sorting
            applyFiltersAndSort();
            displayCurrentPage();
        }
        
        function updateSortDropdownUI() {
            const sortNames = {
                'recommend_best': 'Recommendation (Best First)',
                'recommend_worst': 'Recommendation (Worst First)',
                'relevance_high': 'Relevance (Highest to Lowest)',
                'relevance_low': 'Relevance (Lowest to Highest)',
                'highest_hindex_asc': 'Highest H-Index (Ascending)',
                'highest_hindex_desc': 'Highest H-Index (Descending)',
                'average_hindex_asc': 'Average H-Index (Ascending)',
                'average_hindex_desc': 'Average H-Index (Descending)',
                'id_asc': 'arXiv ID (Ascending)',
                'id_desc': 'arXiv ID (Descending)',
                'title_az': 'Title (A-Z)',
                'title_za': 'Title (Z-A)'
            };
            
            const sortName = sortNames[currentSort] || 'Recommendation (Best First)';
            
            const mobileText = document.getElementById('mobile-sort-text');
            const desktopText = document.getElementById('desktop-sort-text');
            
            if (mobileText) {
                mobileText.textContent = sortName;
            }
            if (desktopText) {
                desktopText.textContent = sortName;
            }
        }

        // ============================================================================
        // H-INDEX FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileHIndexDropdown() {
            const button = document.getElementById('mobile-hindex-btn');
            const dropdown = document.getElementById('mobile-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopHIndexDropdown() {
            const button = document.getElementById('desktop-hindex-btn');
            const dropdown = document.getElementById('desktop-hindex-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleHIndexRanges() {
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const desktopFoundCheckbox = document.getElementById('desktop-hindex-found');
            const mobileHighestRange = document.getElementById('mobile-highest-range');
            const mobileAverageRange = document.getElementById('mobile-average-range');
            const desktopHighestRange = document.getElementById('desktop-highest-range');
            const desktopAverageRange = document.getElementById('desktop-average-range');
            
            // Sync the checkboxes
            if (event.target.id === 'mobile-hindex-found') {
                desktopFoundCheckbox.checked = mobileFoundCheckbox.checked;
            } else if (event.target.id === 'desktop-hindex-found') {
                mobileFoundCheckbox.checked = desktopFoundCheckbox.checked;
            }
            
            const isEnabled = mobileFoundCheckbox.checked;
            
            // Update pending filters
            updatePendingHIndexFilters();
            
            // Update button text to reflect current state
            updateHIndexButtonText();
            
            // Toggle disabled state for range sections
            [mobileHighestRange, mobileAverageRange, desktopHighestRange, desktopAverageRange].forEach(range => {
                if (range) {
                    if (isEnabled) {
                        range.classList.remove('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = false;
                        });
                    } else {
                        range.classList.add('disabled');
                        range.querySelectorAll('input').forEach(input => {
                            input.disabled = true;
                        });
                    }
                }
            });
        }
        
        function updatePendingHIndexFilters() {
            // Read current UI state into pending filters
            const mobileFoundCheckbox = document.getElementById('mobile-hindex-found');
            const mobileNotFoundCheckbox = document.getElementById('mobile-hindex-not-found');
            const mobileHighestMin = document.getElementById('mobile-highest-min');
            const mobileHighestMax = document.getElementById('mobile-highest-max');
            const mobileAverageMin = document.getElementById('mobile-average-min');
            const mobileAverageMax = document.getElementById('mobile-average-max');
            
            pendingHIndexFilters = {
                found: mobileFoundCheckbox.checked,
                notFound: mobileNotFoundCheckbox.checked,
                highestMin: parseInt(mobileHighestMin.value) || 0,
                highestMax: parseInt(mobileHighestMax.value) || 1000,
                averageMin: parseInt(mobileAverageMin.value) || 0,
                averageMax: parseInt(mobileAverageMax.value) || 1000
            };
        }
        
        function resetPendingHIndexFilters() {
            // Revert pending filters to current applied filters
            pendingHIndexFilters = { ...currentHIndexFilters };
            
            // Update UI to reflect current filters
            syncHIndexUI();
        }
        
        function resetPendingNoveltyFilters() {
            // Revert pending filters to current applied filters
            pendingNoveltyFilters = { ...currentNoveltyFilters };
            
            // Update UI to reflect current filters
            syncPendingNoveltyUI();
            updateNoveltyButtonText();
        }
        
        function resetPendingImpactFilters() {
            // Revert pending filters to current applied filters
            pendingImpactFilters = { ...currentImpactFilters };
            
            // Update UI to reflect current filters
            syncPendingImpactUI();
            updateImpactButtonText();
        }
        
        function syncHIndexUI() {
            // Update checkboxes
            document.getElementById('mobile-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('mobile-hindex-not-found').checked = currentHIndexFilters.notFound;
            document.getElementById('desktop-hindex-found').checked = currentHIndexFilters.found;
            document.getElementById('desktop-hindex-not-found').checked = currentHIndexFilters.notFound;
            
            // Update range inputs
            document.getElementById('mobile-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('mobile-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('mobile-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('mobile-average-max').value = currentHIndexFilters.averageMax;
            document.getElementById('desktop-highest-min').value = currentHIndexFilters.highestMin;
            document.getElementById('desktop-highest-max').value = currentHIndexFilters.highestMax;
            document.getElementById('desktop-average-min').value = currentHIndexFilters.averageMin;
            document.getElementById('desktop-average-max').value = currentHIndexFilters.averageMax;
            
            // Update disabled states
            toggleHIndexRanges();
            
            // Update button text
            updateHIndexButtonText();
        }
        
        function updateHIndexButtonText() {
            // Read the current checkbox states from the UI
            const foundChecked = document.getElementById('mobile-hindex-found').checked;
            const notFoundChecked = document.getElementById('mobile-hindex-not-found').checked;
            
            let selectionText;
            if (foundChecked && notFoundChecked) {
                selectionText = "All Selected";
            } else if (foundChecked && !notFoundChecked) {
                selectionText = "H-Index Found";
            } else if (!foundChecked && notFoundChecked) {
                selectionText = "H-Index Not Found";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-hindex-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-hindex-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">H-Index:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyHIndexFilter() {
            // Update pending filters one final time
            updatePendingHIndexFilters();
            
            // Apply pending filters as current filters
            currentHIndexFilters = { ...pendingHIndexFilters };
            
            // Update URL with new filter state
            updateURLWithHIndexFilters();
            
            // Update button text to reflect applied filters
            updateHIndexButtonText();
            
            // Close both dropdowns
            const mobileDropdown = document.getElementById('mobile-hindex-dropdown');
            const desktopDropdown = document.getElementById('desktop-hindex-dropdown');
            const mobileButton = document.getElementById('mobile-hindex-btn');
            const desktopButton = document.getElementById('desktop-hindex-btn');
            
            mobileDropdown.classList.add('hidden');
            desktopDropdown.classList.add('hidden');
            
            // Reset button states to normal
            mobileButton.classList.remove('bg-neutral-600');
            mobileButton.classList.add('bg-neutral-500');
            desktopButton.classList.remove('bg-neutral-600');
            desktopButton.classList.add('bg-neutral-500');
            
            // Don't close sidebar - just close dropdown
            // (Sidebar should stay open for more filtering)
            
            // Apply new filtering and update display
            applyFiltersAndSort();
            displayCurrentPage();
        }

        // ============================================================================
        // INPUT VALIDATION FOR H-INDEX RANGES
        // ============================================================================
        
        function validateHIndexInput(input) {
            // Allow empty input temporarily (user might be typing)
            if (input.value === '') {
                return;
            }
            
            let value = parseInt(input.value);
            
            // Ensure value is within 0-1000 range
            if (isNaN(value) || value < 0) {
                input.value = 0;
            } else if (value > 1000) {
                input.value = 1000;
            }
            
            // Auto-correct min/max relationships
            enforceMinMaxConstraints(input);
        }
        
        function enforceMinMaxConstraints(changedInput) {
            const inputId = changedInput.id;
            let minInput, maxInput;
            
            // Determine which min/max pair this input belongs to
            if (inputId.includes('highest-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('highest-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            } else if (inputId.includes('average-min')) {
                minInput = changedInput;
                maxInput = document.getElementById(inputId.replace('min', 'max'));
            } else if (inputId.includes('average-max')) {
                maxInput = changedInput;
                minInput = document.getElementById(inputId.replace('max', 'min'));
            }
            
            if (minInput && maxInput) {
                const minVal = parseInt(minInput.value) || 0;
                const maxVal = parseInt(maxInput.value) || 0;
                
                // If min > max, auto-correct
                if (minVal > maxVal) {
                    if (changedInput === minInput) {
                        // User changed min to be > max, set max = min
                        maxInput.value = minVal;
                    } else {
                        // User changed max to be < min, set min = max
                        minInput.value = maxVal;
                    }
                }
            }
        }
        
        // Add input validation when page loads
        function setupHIndexValidation() {
            const inputs = [
                'mobile-highest-min', 'mobile-highest-max',
                'mobile-average-min', 'mobile-average-max',
                'desktop-highest-min', 'desktop-highest-max',
                'desktop-average-min', 'desktop-average-max'
            ];
            
            inputs.forEach(id => {
                const input = document.getElementById(id);
                if (input) {
                    // Validate on input (while typing) - but allow empty temporarily
                    input.addEventListener('input', () => {
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters as user types
                    });
                    
                    // Validate on blur (when user leaves field) - ensure it's not empty
                    input.addEventListener('blur', () => {
                        if (input.value === '') {
                            input.value = 0; // Set default if user leaves it empty
                        }
                        validateHIndexInput(input);
                        updatePendingHIndexFilters(); // Update pending filters
                    });
                    
                    // Prevent non-numeric characters except for selection/deletion
                    input.addEventListener('keydown', (e) => {
                        // Allow: backspace, delete, tab, escape, enter, and numbers
                        if ([8, 9, 27, 13, 46].includes(e.keyCode) || 
                            // Allow Ctrl+A, Ctrl+C, Ctrl+V, Ctrl+X
                            (e.keyCode === 65 && e.ctrlKey) || 
                            (e.keyCode === 67 && e.ctrlKey) || 
                            (e.keyCode === 86 && e.ctrlKey) || 
                            (e.keyCode === 88 && e.ctrlKey) ||
                            // Allow numbers (0-9) on main keyboard and numpad
                            (e.keyCode >= 48 && e.keyCode <= 57) ||
                            (e.keyCode >= 96 && e.keyCode <= 105)) {
                            return;
                        }
                        e.preventDefault();
                    });
                }
            });
            
            // Add event listeners for checkboxes to update pending filters
            document.getElementById('mobile-hindex-not-found').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-hindex-not-found').checked = 
                    document.getElementById('mobile-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            document.getElementById('desktop-hindex-not-found').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-hindex-not-found').checked = 
                    document.getElementById('desktop-hindex-not-found').checked;
                updatePendingHIndexFilters();
                updateHIndexButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for scoring checkboxes to update pending filters
            document.getElementById('mobile-scoring-has').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-has').checked = 
                    document.getElementById('mobile-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-has').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-has').checked = 
                    document.getElementById('desktop-scoring-has').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('mobile-scoring-no').addEventListener('change', () => {
                // Sync desktop checkbox
                document.getElementById('desktop-scoring-no').checked = 
                    document.getElementById('mobile-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            document.getElementById('desktop-scoring-no').addEventListener('change', () => {
                // Sync mobile checkbox
                document.getElementById('mobile-scoring-no').checked = 
                    document.getElementById('desktop-scoring-no').checked;
                updatePendingScoringFilters();
                updateScoringButtonText();
            });
            
            // Add event listeners for recommendation checkboxes to sync between mobile and desktop
            document.getElementById('mobile-recommendation-must').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-must').checked = 
                    document.getElementById('mobile-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-should').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-should').checked = 
                    document.getElementById('mobile-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-skip').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-skip').checked = 
                    document.getElementById('mobile-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('mobile-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('desktop-recommendation-ignore').checked = 
                    document.getElementById('mobile-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-must').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-must').checked = 
                    document.getElementById('desktop-recommendation-must').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-should').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-should').checked = 
                    document.getElementById('desktop-recommendation-should').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-skip').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-skip').checked = 
                    document.getElementById('desktop-recommendation-skip').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            document.getElementById('desktop-recommendation-ignore').addEventListener('change', () => {
                document.getElementById('mobile-recommendation-ignore').checked = 
                    document.getElementById('desktop-recommendation-ignore').checked;
                updatePendingRecommendationFilters();
                updateRecommendationButtonText();
            });
            
            // Add event listeners for novelty checkboxes to sync between mobile and desktop
            document.getElementById('mobile-novelty-high').addEventListener('change', () => {
                document.getElementById('desktop-novelty-high').checked = 
                    document.getElementById('mobile-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-moderate').addEventListener('change', () => {
                document.getElementById('desktop-novelty-moderate').checked = 
                    document.getElementById('mobile-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-low').addEventListener('change', () => {
                document.getElementById('desktop-novelty-low').checked = 
                    document.getElementById('mobile-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('mobile-novelty-none').addEventListener('change', () => {
                document.getElementById('desktop-novelty-none').checked = 
                    document.getElementById('mobile-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-high').addEventListener('change', () => {
                document.getElementById('mobile-novelty-high').checked = 
                    document.getElementById('desktop-novelty-high').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-moderate').addEventListener('change', () => {
                document.getElementById('mobile-novelty-moderate').checked = 
                    document.getElementById('desktop-novelty-moderate').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-low').addEventListener('change', () => {
                document.getElementById('mobile-novelty-low').checked = 
                    document.getElementById('desktop-novelty-low').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            document.getElementById('desktop-novelty-none').addEventListener('change', () => {
                document.getElementById('mobile-novelty-none').checked = 
                    document.getElementById('desktop-novelty-none').checked;
                updatePendingNoveltyFilters();
                updateNoveltyButtonText();
            });
            
            // Add event listeners for impact checkboxes to sync between mobile and desktop
            document.getElementById('mobile-impact-high').addEventListener('change', () => {
                document.getElementById('desktop-impact-high').checked = 
                    document.getElementById('mobile-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-moderate').addEventListener('change', () => {
                document.getElementById('desktop-impact-moderate').checked = 
                    document.getElementById('mobile-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-low').addEventListener('change', () => {
                document.getElementById('desktop-impact-low').checked = 
                    document.getElementById('mobile-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('mobile-impact-negligible').addEventListener('change', () => {
                document.getElementById('desktop-impact-negligible').checked = 
                    document.getElementById('mobile-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-high').addEventListener('change', () => {
                document.getElementById('mobile-impact-high').checked = 
                    document.getElementById('desktop-impact-high').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-moderate').addEventListener('change', () => {
                document.getElementById('mobile-impact-moderate').checked = 
                    document.getElementById('desktop-impact-moderate').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-low').addEventListener('change', () => {
                document.getElementById('mobile-impact-low').checked = 
                    document.getElementById('desktop-impact-low').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            document.getElementById('desktop-impact-negligible').addEventListener('change', () => {
                document.getElementById('mobile-impact-negligible').checked = 
                    document.getElementById('desktop-impact-negligible').checked;
                updatePendingImpactFilters();
                updateImpactButtonText();
            });
            
            // Add event listeners for relevance checkboxes to sync between mobile and desktop
            document.getElementById('mobile-relevance-highly').addEventListener('change', () => {
                document.getElementById('desktop-relevance-highly').checked = 
                    document.getElementById('mobile-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-moderately').addEventListener('change', () => {
                document.getElementById('desktop-relevance-moderately').checked = 
                    document.getElementById('mobile-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('desktop-relevance-tangentially').checked = 
                    document.getElementById('mobile-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('mobile-relevance-not').addEventListener('change', () => {
                document.getElementById('desktop-relevance-not').checked = 
                    document.getElementById('mobile-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-highly').addEventListener('change', () => {
                document.getElementById('mobile-relevance-highly').checked = 
                    document.getElementById('desktop-relevance-highly').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-moderately').addEventListener('change', () => {
                document.getElementById('mobile-relevance-moderately').checked = 
                    document.getElementById('desktop-relevance-moderately').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-tangentially').addEventListener('change', () => {
                document.getElementById('mobile-relevance-tangentially').checked = 
                    document.getElementById('desktop-relevance-tangentially').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            document.getElementById('desktop-relevance-not').addEventListener('change', () => {
                document.getElementById('mobile-relevance-not').checked = 
                    document.getElementById('desktop-relevance-not').checked;
                updatePendingRelevanceFilters();
                updateRelevanceButtonText();
            });
            
            // Add event listeners for topic checkboxes to sync between mobile and desktop
            document.getElementById('mobile-topic-rlhf').addEventListener('change', () => {
                document.getElementById('desktop-topic-rlhf').checked = 
                    document.getElementById('mobile-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('desktop-topic-weak-supervision').checked = 
                    document.getElementById('mobile-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('desktop-topic-diffusion-reasoning').checked = 
                    document.getElementById('mobile-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('desktop-topic-distributed-training').checked = 
                    document.getElementById('mobile-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('mobile-topic-datasets').addEventListener('change', () => {
                document.getElementById('desktop-topic-datasets').checked = 
                    document.getElementById('mobile-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-rlhf').addEventListener('change', () => {
                document.getElementById('mobile-topic-rlhf').checked = 
                    document.getElementById('desktop-topic-rlhf').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-weak-supervision').addEventListener('change', () => {
                document.getElementById('mobile-topic-weak-supervision').checked = 
                    document.getElementById('desktop-topic-weak-supervision').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-diffusion-reasoning').addEventListener('change', () => {
                document.getElementById('mobile-topic-diffusion-reasoning').checked = 
                    document.getElementById('desktop-topic-diffusion-reasoning').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-distributed-training').addEventListener('change', () => {
                document.getElementById('mobile-topic-distributed-training').checked = 
                    document.getElementById('desktop-topic-distributed-training').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
            
            document.getElementById('desktop-topic-datasets').addEventListener('change', () => {
                document.getElementById('mobile-topic-datasets').checked = 
                    document.getElementById('desktop-topic-datasets').checked;
                updatePendingTopicFilters();
                updateTopicButtonText();
            });
        }

        // ============================================================================
        // SCORING FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending scoring filter states
        let currentScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        let pendingScoringFilters = {
            hasScoring: true,
            noScoring: true
        };
        
        function toggleMobileScoringDropdown() {
            const button = document.getElementById('mobile-scoring-btn');
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopScoringDropdown() {
            const button = document.getElementById('desktop-scoring-btn');
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                // Set direction before showing
                setDropdownDirection(button, dropdown);
                // Change button to expanded state
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                // Change button back to normal state
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        
        function syncPendingScoringUI() {
            // Update all checkboxes to match pending state
            document.getElementById('mobile-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = pendingScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = pendingScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = pendingScoringFilters.noScoring;
        }
        
        function syncScoringUI() {
            // Update checkboxes
            document.getElementById('mobile-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('mobile-scoring-no').checked = currentScoringFilters.noScoring;
            document.getElementById('desktop-scoring-has').checked = currentScoringFilters.hasScoring;
            document.getElementById('desktop-scoring-no').checked = currentScoringFilters.noScoring;
            
            // Update button text
            updateScoringButtonText();
        }
        
        function updateScoringButtonText() {
            // Read the current checkbox states from the UI
            const hasChecked = document.getElementById('mobile-scoring-has').checked;
            const noChecked = document.getElementById('mobile-scoring-no').checked;
            
            let selectionText;
            if (hasChecked && noChecked) {
                selectionText = "All Selected";
            } else if (hasChecked && !noChecked) {
                selectionText = "Completed";
            } else if (!hasChecked && noChecked) {
                selectionText = "Not relevant enough";
            } else {
                selectionText = "None Selected";
            }
            
            // Update mobile button
            const mobileButton = document.getElementById('mobile-scoring-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            // Update desktop button
            const desktopButton = document.getElementById('desktop-scoring-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Scoring:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyScoringFilter() {
            // Update pending filters one final time
            updatePendingScoringFilters();
            
            // Apply the pending filters as current filters
            currentScoringFilters = { ...pendingScoringFilters };
            
            // Update UI to reflect current state
            syncScoringUI();
            
            // Update URL with current filters
            updateScoringFiltersInURL();
            
            // Close dropdown and apply filters
            closeMobileScoringDropdown();
            closeDesktopScoringDropdown();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
            
            // Apply all filters and redisplay
            applyFiltersAndSort();
        }
        
        function updatePendingScoringFilters() {
            // Read current UI state into pending filters
            const mobileScoringHas = document.getElementById('mobile-scoring-has');
            const mobileScoringNo = document.getElementById('mobile-scoring-no');
            
            if (mobileScoringHas && mobileScoringNo) {
                pendingScoringFilters.hasScoring = mobileScoringHas.checked;
                pendingScoringFilters.noScoring = mobileScoringNo.checked;
            }
        }
        
        function resetPendingScoringFilters() {
            pendingScoringFilters = { ...currentScoringFilters };
            syncPendingScoringUI();
            updateScoringButtonText();
        }
        
        function closeMobileScoringDropdown() {
            const dropdown = document.getElementById('mobile-scoring-dropdown');
            const button = document.getElementById('mobile-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopScoringDropdown() {
            const dropdown = document.getElementById('desktop-scoring-dropdown');
            const button = document.getElementById('desktop-scoring-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateScoringFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Add scoring filter parameters
            params.set('scoring_has', currentScoringFilters.hasScoring.toString());
            params.set('scoring_no', currentScoringFilters.noScoring.toString());
            
            // Update URL without reload
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateScoringFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            // Read scoring filter parameters from URL
            const hasScoring = params.get('scoring_has');
            const noScoring = params.get('scoring_no');
            
            if (hasScoring !== null) {
                currentScoringFilters.hasScoring = hasScoring === 'true';
                pendingScoringFilters.hasScoring = hasScoring === 'true';
            }
            
            if (noScoring !== null) {
                currentScoringFilters.noScoring = noScoring === 'true';
                pendingScoringFilters.noScoring = noScoring === 'true';
            }
            
            // Update UI to match loaded filters
            syncScoringUI();
            
            // Update disabled state for advanced filters
            updateAdvancedFiltersDisabledState();
        }

        // ============================================================================
        // RECOMMENDATION FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending recommendation filter states
        let currentRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        let pendingRecommendationFilters = {
            mustRead: true,
            shouldRead: true,
            canSkip: true,
            ignore: true
        };
        
        function toggleMobileRecommendationDropdown() {
            const button = document.getElementById('mobile-recommendation-btn');
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRecommendationDropdown() {
            const button = document.getElementById('desktop-recommendation-btn');
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = pendingRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = pendingRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = pendingRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = pendingRecommendationFilters.ignore;
        }
        
        function syncRecommendationUI() {
            document.getElementById('mobile-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('mobile-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('mobile-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('mobile-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            document.getElementById('desktop-recommendation-must').checked = currentRecommendationFilters.mustRead;
            document.getElementById('desktop-recommendation-should').checked = currentRecommendationFilters.shouldRead;
            document.getElementById('desktop-recommendation-skip').checked = currentRecommendationFilters.canSkip;
            document.getElementById('desktop-recommendation-ignore').checked = currentRecommendationFilters.ignore;
            
            updateRecommendationButtonText();
        }
        
        function updateRecommendationButtonText() {
            const mustChecked = document.getElementById('mobile-recommendation-must').checked;
            const shouldChecked = document.getElementById('mobile-recommendation-should').checked;
            const skipChecked = document.getElementById('mobile-recommendation-skip').checked;
            const ignoreChecked = document.getElementById('mobile-recommendation-ignore').checked;
            
            const checkedCount = [mustChecked, shouldChecked, skipChecked, ignoreChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-recommendation-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-recommendation-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Recommendation:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRecommendationFilter() {
            updatePendingRecommendationFilters();
            currentRecommendationFilters = { ...pendingRecommendationFilters };
            syncRecommendationUI();
            updateRecommendationFiltersInURL();
            closeMobileRecommendationDropdown();
            closeDesktopRecommendationDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRecommendationFilters() {
            pendingRecommendationFilters.mustRead = document.getElementById('mobile-recommendation-must').checked;
            pendingRecommendationFilters.shouldRead = document.getElementById('mobile-recommendation-should').checked;
            pendingRecommendationFilters.canSkip = document.getElementById('mobile-recommendation-skip').checked;
            pendingRecommendationFilters.ignore = document.getElementById('mobile-recommendation-ignore').checked;
        }
        
        function resetPendingRecommendationFilters() {
            pendingRecommendationFilters = { ...currentRecommendationFilters };
            syncPendingRecommendationUI();
            updateRecommendationButtonText();
        }
        
        function closeMobileRecommendationDropdown() {
            const dropdown = document.getElementById('mobile-recommendation-dropdown');
            const button = document.getElementById('mobile-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRecommendationDropdown() {
            const dropdown = document.getElementById('desktop-recommendation-dropdown');
            const button = document.getElementById('desktop-recommendation-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRecommendationFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('recommendation_must', currentRecommendationFilters.mustRead.toString());
            params.set('recommendation_should', currentRecommendationFilters.shouldRead.toString());
            params.set('recommendation_skip', currentRecommendationFilters.canSkip.toString());
            params.set('recommendation_ignore', currentRecommendationFilters.ignore.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateRecommendationFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const mustRead = params.get('recommendation_must');
            const shouldRead = params.get('recommendation_should');
            const canSkip = params.get('recommendation_skip');
            const ignore = params.get('recommendation_ignore');
            
            if (mustRead !== null) {
                currentRecommendationFilters.mustRead = mustRead === 'true';
                pendingRecommendationFilters.mustRead = mustRead === 'true';
            }
            if (shouldRead !== null) {
                currentRecommendationFilters.shouldRead = shouldRead === 'true';
                pendingRecommendationFilters.shouldRead = shouldRead === 'true';
            }
            if (canSkip !== null) {
                currentRecommendationFilters.canSkip = canSkip === 'true';
                pendingRecommendationFilters.canSkip = canSkip === 'true';
            }
            if (ignore !== null) {
                currentRecommendationFilters.ignore = ignore === 'true';
                pendingRecommendationFilters.ignore = ignore === 'true';
            }
            
            syncRecommendationUI();
        }

        // ============================================================================
        // NOVELTY FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending novelty filter states
        let currentNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        let pendingNoveltyFilters = {
            high: true,
            moderate: true,
            low: true,
            none: true
        };
        
        function toggleMobileNoveltyDropdown() {
            const button = document.getElementById('mobile-novelty-btn');
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopNoveltyDropdown() {
            const button = document.getElementById('desktop-novelty-btn');
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = pendingNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = pendingNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = pendingNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = pendingNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = pendingNoveltyFilters.none;
        }
        
        function syncNoveltyUI() {
            document.getElementById('mobile-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('mobile-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('mobile-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('mobile-novelty-none').checked = currentNoveltyFilters.none;
            document.getElementById('desktop-novelty-high').checked = currentNoveltyFilters.high;
            document.getElementById('desktop-novelty-moderate').checked = currentNoveltyFilters.moderate;
            document.getElementById('desktop-novelty-low').checked = currentNoveltyFilters.low;
            document.getElementById('desktop-novelty-none').checked = currentNoveltyFilters.none;
            
            updateNoveltyButtonText();
        }
        
        function updateNoveltyButtonText() {
            const highChecked = document.getElementById('mobile-novelty-high').checked;
            const moderateChecked = document.getElementById('mobile-novelty-moderate').checked;
            const lowChecked = document.getElementById('mobile-novelty-low').checked;
            const noneChecked = document.getElementById('mobile-novelty-none').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, noneChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-novelty-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-novelty-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Novelty:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyNoveltyFilter() {
            updatePendingNoveltyFilters();
            currentNoveltyFilters = { ...pendingNoveltyFilters };
            syncNoveltyUI();
            updateNoveltyFiltersInURL();
            closeMobileNoveltyDropdown();
            closeDesktopNoveltyDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingNoveltyFilters() {
            pendingNoveltyFilters.high = document.getElementById('mobile-novelty-high').checked;
            pendingNoveltyFilters.moderate = document.getElementById('mobile-novelty-moderate').checked;
            pendingNoveltyFilters.low = document.getElementById('mobile-novelty-low').checked;
            pendingNoveltyFilters.none = document.getElementById('mobile-novelty-none').checked;
        }
        
        function closeMobileNoveltyDropdown() {
            const dropdown = document.getElementById('mobile-novelty-dropdown');
            const button = document.getElementById('mobile-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopNoveltyDropdown() {
            const dropdown = document.getElementById('desktop-novelty-dropdown');
            const button = document.getElementById('desktop-novelty-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateNoveltyFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('novelty_high', currentNoveltyFilters.high.toString());
            params.set('novelty_moderate', currentNoveltyFilters.moderate.toString());
            params.set('novelty_low', currentNoveltyFilters.low.toString());
            params.set('novelty_none', currentNoveltyFilters.none.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateNoveltyFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('novelty_high');
            const moderate = params.get('novelty_moderate');
            const low = params.get('novelty_low');
            const none = params.get('novelty_none');
            
            if (high !== null) {
                currentNoveltyFilters.high = high === 'true';
                pendingNoveltyFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentNoveltyFilters.moderate = moderate === 'true';
                pendingNoveltyFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentNoveltyFilters.low = low === 'true';
                pendingNoveltyFilters.low = low === 'true';
            }
            if (none !== null) {
                currentNoveltyFilters.none = none === 'true';
                pendingNoveltyFilters.none = none === 'true';
            }
            
            syncNoveltyUI();
        }

        // ============================================================================
        // POTENTIAL IMPACT FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        // Current and pending impact filter states
        let currentImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        let pendingImpactFilters = {
            high: true,
            moderate: true,
            low: true,
            negligible: true
        };
        
        function toggleMobileImpactDropdown() {
            const button = document.getElementById('mobile-impact-btn');
            const dropdown = document.getElementById('mobile-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopImpactDropdown() {
            const button = document.getElementById('desktop-impact-btn');
            const dropdown = document.getElementById('desktop-impact-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingImpactUI() {
            document.getElementById('mobile-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = pendingImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = pendingImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = pendingImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = pendingImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = pendingImpactFilters.negligible;
        }
        
        function syncImpactUI() {
            document.getElementById('mobile-impact-high').checked = currentImpactFilters.high;
            document.getElementById('mobile-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('mobile-impact-low').checked = currentImpactFilters.low;
            document.getElementById('mobile-impact-negligible').checked = currentImpactFilters.negligible;
            document.getElementById('desktop-impact-high').checked = currentImpactFilters.high;
            document.getElementById('desktop-impact-moderate').checked = currentImpactFilters.moderate;
            document.getElementById('desktop-impact-low').checked = currentImpactFilters.low;
            document.getElementById('desktop-impact-negligible').checked = currentImpactFilters.negligible;
            
            updateImpactButtonText();
        }
        
        function updateImpactButtonText() {
            const highChecked = document.getElementById('mobile-impact-high').checked;
            const moderateChecked = document.getElementById('mobile-impact-moderate').checked;
            const lowChecked = document.getElementById('mobile-impact-low').checked;
            const negligibleChecked = document.getElementById('mobile-impact-negligible').checked;
            
            const checkedCount = [highChecked, moderateChecked, lowChecked, negligibleChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 4) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-impact-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-impact-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Potential Impact:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyImpactFilter() {
            updatePendingImpactFilters();
            currentImpactFilters = { ...pendingImpactFilters };
            syncImpactUI();
            updateImpactFiltersInURL();
            closeMobileImpactDropdown();
            closeDesktopImpactDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingImpactFilters() {
            pendingImpactFilters.high = document.getElementById('mobile-impact-high').checked;
            pendingImpactFilters.moderate = document.getElementById('mobile-impact-moderate').checked;
            pendingImpactFilters.low = document.getElementById('mobile-impact-low').checked;
            pendingImpactFilters.negligible = document.getElementById('mobile-impact-negligible').checked;
        }
        
        function closeMobileImpactDropdown() {
            const dropdown = document.getElementById('mobile-impact-dropdown');
            const button = document.getElementById('mobile-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopImpactDropdown() {
            const dropdown = document.getElementById('desktop-impact-dropdown');
            const button = document.getElementById('desktop-impact-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateImpactFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('impact_high', currentImpactFilters.high.toString());
            params.set('impact_moderate', currentImpactFilters.moderate.toString());
            params.set('impact_low', currentImpactFilters.low.toString());
            params.set('impact_negligible', currentImpactFilters.negligible.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateImpactFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const high = params.get('impact_high');
            const moderate = params.get('impact_moderate');
            const low = params.get('impact_low');
            const negligible = params.get('impact_negligible');
            
            if (high !== null) {
                currentImpactFilters.high = high === 'true';
                pendingImpactFilters.high = high === 'true';
            }
            if (moderate !== null) {
                currentImpactFilters.moderate = moderate === 'true';
                pendingImpactFilters.moderate = moderate === 'true';
            }
            if (low !== null) {
                currentImpactFilters.low = low === 'true';
                pendingImpactFilters.low = low === 'true';
            }
            if (negligible !== null) {
                currentImpactFilters.negligible = negligible === 'true';
                pendingImpactFilters.negligible = negligible === 'true';
            }
            
            syncImpactUI();
        }

        // ============================================================================
        // TOPIC FILTER DROPDOWN FUNCTIONS
        // ============================================================================
        
        function toggleMobileTopicDropdown() {
            const button = document.getElementById('mobile-topic-btn');
            const dropdown = document.getElementById('mobile-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopTopicDropdown() {
            const button = document.getElementById('desktop-topic-btn');
            const dropdown = document.getElementById('desktop-topic-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = pendingTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = pendingTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = pendingTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = pendingTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = pendingTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = pendingTopicFilters.datasets;
        }
        
        function syncTopicUI() {
            document.getElementById('mobile-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('mobile-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('mobile-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('mobile-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('mobile-topic-datasets').checked = currentTopicFilters.datasets;
            document.getElementById('desktop-topic-rlhf').checked = currentTopicFilters.rlhf;
            document.getElementById('desktop-topic-weak-supervision').checked = currentTopicFilters.weakSupervision;
            document.getElementById('desktop-topic-diffusion-reasoning').checked = currentTopicFilters.diffusionReasoning;
            document.getElementById('desktop-topic-distributed-training').checked = currentTopicFilters.distributedTraining;
            document.getElementById('desktop-topic-datasets').checked = currentTopicFilters.datasets;
            
            updateTopicButtonText();
        }
        
        function updateTopicButtonText() {
            const rlhfChecked = document.getElementById('mobile-topic-rlhf').checked;
            const weakSupervisionChecked = document.getElementById('mobile-topic-weak-supervision').checked;
            const diffusionReasoningChecked = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            const distributedTrainingChecked = document.getElementById('mobile-topic-distributed-training').checked;
            const datasetsChecked = document.getElementById('mobile-topic-datasets').checked;
            
            const checkedCount = [rlhfChecked, weakSupervisionChecked, diffusionReasoningChecked, distributedTrainingChecked, datasetsChecked].filter(Boolean).length;
            
            let selectionText;
            if (checkedCount === 5) {
                selectionText = "All Selected";
            } else if (checkedCount === 0) {
                selectionText = "None Selected";
            } else {
                selectionText = `${checkedCount} Selected`;
            }
            
            const mobileButton = document.getElementById('mobile-topic-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-topic-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Topics:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyTopicFilter() {
            updatePendingTopicFilters();
            currentTopicFilters = { ...pendingTopicFilters };
            syncTopicUI();
            updateTopicFiltersInURL();
            closeMobileTopicDropdown();
            closeDesktopTopicDropdown();
            updateAllPaperModules();
            applyFiltersAndSort();
        }
        
        function updatePendingTopicFilters() {
            pendingTopicFilters.rlhf = document.getElementById('mobile-topic-rlhf').checked;
            pendingTopicFilters.weakSupervision = document.getElementById('mobile-topic-weak-supervision').checked;
            pendingTopicFilters.diffusionReasoning = document.getElementById('mobile-topic-diffusion-reasoning').checked;
            pendingTopicFilters.distributedTraining = document.getElementById('mobile-topic-distributed-training').checked;
            pendingTopicFilters.datasets = document.getElementById('mobile-topic-datasets').checked;
        }
        
        function resetPendingTopicFilters() {
            pendingTopicFilters = { ...currentTopicFilters };
            syncPendingTopicUI();
            updateTopicButtonText();
        }
        
        function closeMobileTopicDropdown() {
            const dropdown = document.getElementById('mobile-topic-dropdown');
            const button = document.getElementById('mobile-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopTopicDropdown() {
            const dropdown = document.getElementById('desktop-topic-dropdown');
            const button = document.getElementById('desktop-topic-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateTopicFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('topic_rlhf', currentTopicFilters.rlhf.toString());
            params.set('topic_weak_supervision', currentTopicFilters.weakSupervision.toString());
            params.set('topic_diffusion_reasoning', currentTopicFilters.diffusionReasoning.toString());
            params.set('topic_distributed_training', currentTopicFilters.distributedTraining.toString());
            params.set('topic_datasets', currentTopicFilters.datasets.toString());
            const newURL = `${window.location.pathname}?${params.toString()}`;
            window.history.pushState({}, '', newURL);
        }
        
        function updateTopicFiltersFromURL() {
            const params = new URLSearchParams(window.location.search);
            
            const rlhf = params.get('topic_rlhf');
            const weakSupervision = params.get('topic_weak_supervision');
            const diffusionReasoning = params.get('topic_diffusion_reasoning');
            const distributedTraining = params.get('topic_distributed_training');
            const datasets = params.get('topic_datasets');
            
            if (rlhf !== null) {
                currentTopicFilters.rlhf = rlhf === 'true';
                pendingTopicFilters.rlhf = rlhf === 'true';
            }
            if (weakSupervision !== null) {
                currentTopicFilters.weakSupervision = weakSupervision === 'true';
                pendingTopicFilters.weakSupervision = weakSupervision === 'true';
            }
            if (diffusionReasoning !== null) {
                currentTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
                pendingTopicFilters.diffusionReasoning = diffusionReasoning === 'true';
            }
            if (distributedTraining !== null) {
                currentTopicFilters.distributedTraining = distributedTraining === 'true';
                pendingTopicFilters.distributedTraining = distributedTraining === 'true';
            }
            if (datasets !== null) {
                currentTopicFilters.datasets = datasets === 'true';
                pendingTopicFilters.datasets = datasets === 'true';
            }
            
            syncTopicUI();
        }
        
        function passesTopicFilter(paper) {
            // Topic filter doesn't actually filter papers, it only affects module display
            return true;
        }
        
        function updateAllPaperModules() {
            // Update all similarity and relevance modules when topic filters change
            currentPagePapers.forEach(paper => {
                updateSimilarityModuleTopics(paper.id);
                updateRelevanceModuleTopics(paper.id);
            });
        }

        // ============================================================================
        // RELEVANCE FILTER FUNCTIONS  
        // ============================================================================
        
        function toggleMobileRelevanceDropdown() {
            const button = document.getElementById('mobile-relevance-btn');
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function toggleDesktopRelevanceDropdown() {
            const button = document.getElementById('desktop-relevance-btn');
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            
            if (dropdown.classList.contains('hidden')) {
                setDropdownDirection(button, dropdown);
                button.classList.remove('bg-neutral-500');
                button.classList.add('bg-neutral-600');
            } else {
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
            
            dropdown.classList.toggle('hidden');
        }
        
        function syncPendingRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = pendingRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = pendingRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = pendingRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = pendingRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = pendingRelevanceFilters.notRelevant;
        }
        
        function syncRelevanceUI() {
            document.getElementById('mobile-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('mobile-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('mobile-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('mobile-relevance-not').checked = currentRelevanceFilters.notRelevant;
            document.getElementById('desktop-relevance-highly').checked = currentRelevanceFilters.highlyRelevant;
            document.getElementById('desktop-relevance-moderately').checked = currentRelevanceFilters.moderatelyRelevant;
            document.getElementById('desktop-relevance-tangentially').checked = currentRelevanceFilters.tangentiallyRelevant;
            document.getElementById('desktop-relevance-not').checked = currentRelevanceFilters.notRelevant;
            
            updateRelevanceButtonText();
        }
        
        function updateRelevanceButtonText() {
            // Read the current checkbox states from the UI (like H-Index filter does)
            const highlyRelevantChecked = document.getElementById('mobile-relevance-highly').checked;
            const moderatelyRelevantChecked = document.getElementById('mobile-relevance-moderately').checked;
            const tangentiallyRelevantChecked = document.getElementById('mobile-relevance-tangentially').checked;
            const notRelevantChecked = document.getElementById('mobile-relevance-not').checked;
            
            const selectedOptions = [];
            if (highlyRelevantChecked) selectedOptions.push("Highly Relevant");
            if (moderatelyRelevantChecked) selectedOptions.push("Moderately Relevant");
            if (tangentiallyRelevantChecked) selectedOptions.push("Tangentially Relevant");
            if (notRelevantChecked) selectedOptions.push("Not Relevant");
            
            const selectionText = selectedOptions.length === 4 ? "All Selected" : 
                                selectedOptions.length === 0 ? "None Selected" : 
                                `${selectedOptions.length} Selected`;
            
            const mobileButton = document.getElementById('mobile-relevance-btn');
            if (mobileButton) {
                mobileButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-base">▼</span>`;
            }
            
            const desktopButton = document.getElementById('desktop-relevance-btn');
            if (desktopButton) {
                desktopButton.innerHTML = `<span class="font-bold">Relevance:</span> <span class="font-normal">${selectionText}</span> <span class="text-sm">▼</span>`;
            }
        }
        
        function applyRelevanceFilter() {
            updatePendingRelevanceFilters();
            currentRelevanceFilters = { ...pendingRelevanceFilters };
            syncRelevanceUI();
            updateRelevanceFiltersInURL();
            closeMobileRelevanceDropdown();
            closeDesktopRelevanceDropdown();
            applyFiltersAndSort();
        }
        
        function updatePendingRelevanceFilters() {
            // Get values from mobile (primary source)
            const mobileHighly = document.getElementById('mobile-relevance-highly');
            const mobileModerately = document.getElementById('mobile-relevance-moderately');
            const mobileTangentially = document.getElementById('mobile-relevance-tangentially');
            const mobileNot = document.getElementById('mobile-relevance-not');
            
            // Update pending filters from mobile if available, otherwise from desktop
            pendingRelevanceFilters.highlyRelevant = mobileHighly ? mobileHighly.checked : document.getElementById('desktop-relevance-highly').checked;
            pendingRelevanceFilters.moderatelyRelevant = mobileModerately ? mobileModerately.checked : document.getElementById('desktop-relevance-moderately').checked;
            pendingRelevanceFilters.tangentiallyRelevant = mobileTangentially ? mobileTangentially.checked : document.getElementById('desktop-relevance-tangentially').checked;
            pendingRelevanceFilters.notRelevant = mobileNot ? mobileNot.checked : document.getElementById('desktop-relevance-not').checked;
        }
        
        function resetPendingRelevanceFilters() {
            pendingRelevanceFilters = { ...currentRelevanceFilters };
            syncPendingRelevanceUI();
            updateRelevanceButtonText();
        }
        
        function closeMobileRelevanceDropdown() {
            const dropdown = document.getElementById('mobile-relevance-dropdown');
            const button = document.getElementById('mobile-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function closeDesktopRelevanceDropdown() {
            const dropdown = document.getElementById('desktop-relevance-dropdown');
            const button = document.getElementById('desktop-relevance-btn');
            if (dropdown && !dropdown.classList.contains('hidden')) {
                dropdown.classList.add('hidden');
                button.classList.remove('bg-neutral-600');
                button.classList.add('bg-neutral-500');
            }
        }
        
        function updateRelevanceFiltersInURL() {
            const params = new URLSearchParams(window.location.search);
            params.set('relevance_highly', currentRelevanceFilters.highlyRelevant.toString());
            params.set('relevance_moderately', currentRelevanceFilters.moderatelyRelevant.toString());
            params.set('relevance_tangentially', currentRelevanceFilters.tangentiallyRelevant.toString());
            params.set('relevance_not', currentRelevanceFilters.notRelevant.toString());
            window.history.replaceState({}, '', `${window.location.pathname}?${params.toString()}`);
        }
        
        function loadRelevanceFiltersFromURL() {
            const highlyRelevant = getUrlParameter('relevance_highly');
            const moderatelyRelevant = getUrlParameter('relevance_moderately');
            const tangentiallyRelevant = getUrlParameter('relevance_tangentially');
            const notRelevant = getUrlParameter('relevance_not');
            
            if (highlyRelevant !== null) {
                currentRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
                pendingRelevanceFilters.highlyRelevant = highlyRelevant === 'true';
            }
            if (moderatelyRelevant !== null) {
                currentRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
                pendingRelevanceFilters.moderatelyRelevant = moderatelyRelevant === 'true';
            }
            if (tangentiallyRelevant !== null) {
                currentRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
                pendingRelevanceFilters.tangentiallyRelevant = tangentiallyRelevant === 'true';
            }
            if (notRelevant !== null) {
                currentRelevanceFilters.notRelevant = notRelevant === 'true';
                pendingRelevanceFilters.notRelevant = notRelevant === 'true';
            }
            
            syncRelevanceUI();
        }
        
        function passesRelevanceFilter(paper) {
            // Get selected topics
            const selectedTopics = [];
            if (currentTopicFilters.rlhf) selectedTopics.push('rlhf');
            if (currentTopicFilters.weakSupervision) selectedTopics.push('weak_supervision');
            if (currentTopicFilters.diffusionReasoning) selectedTopics.push('diffusion_reasoning');
            if (currentTopicFilters.distributedTraining) selectedTopics.push('distributed_training');
            if (currentTopicFilters.datasets) selectedTopics.push('datasets');
            
            // If no topics selected, skip relevance filtering
            if (selectedTopics.length === 0) return true;
            
            // Get selected relevance levels
            const selectedRelevanceLevels = [];
            if (currentRelevanceFilters.highlyRelevant) selectedRelevanceLevels.push('Highly Relevant');
            if (currentRelevanceFilters.moderatelyRelevant) selectedRelevanceLevels.push('Moderately Relevant');
            if (currentRelevanceFilters.tangentiallyRelevant) selectedRelevanceLevels.push('Tangentially Relevant');
            if (currentRelevanceFilters.notRelevant) selectedRelevanceLevels.push('Not Relevant');
            
            // If no relevance levels selected, show 0 papers
            if (selectedRelevanceLevels.length === 0) return false;
            
            // Check each selected topic
            for (let topic of selectedTopics) {
                const relevanceField = `${topic}_relevance`;
                const paperRelevance = paper[relevanceField];
                
                // Treat "not_validated" same as "Not Relevant"
                const normalizedRelevance = paperRelevance === "not_validated" ? "Not Relevant" : paperRelevance;
                
                // If this topic's relevance matches any selected relevance level, paper passes
                if (selectedRelevanceLevels.includes(normalizedRelevance)) {
                    return true; // At least one topic matches
                }
            }
            
            // No selected topics had matching relevance levels
            return false;
        }

        // ============================================================================
        // FILTERING AND DISPLAY FUNCTIONS
        // ============================================================================
        
        function shouldDisableAdvancedFilters() {
            return !currentScoringFilters.hasScoring && currentScoringFilters.noScoring;
        }
        
        function updateDropdownDisabledState(buttonId, dropdownId, shouldDisable) {
            const button = document.getElementById(buttonId);
            const dropdown = document.getElementById(dropdownId);
            
            if (button && dropdown) {
                if (shouldDisable) {
                    button.classList.add('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'none';
                    dropdown.classList.add('hidden'); // Close if open
                } else {
                    button.classList.remove('opacity-50', 'cursor-not-allowed');
                    button.style.pointerEvents = 'auto';
                }
            }
        }
        
        function updateAdvancedFiltersDisabledState() {
            const shouldDisable = shouldDisableAdvancedFilters();
            
            // Update Recommendation
            updateDropdownDisabledState('mobile-recommendation-btn', 'mobile-recommendation-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-recommendation-btn', 'desktop-recommendation-dropdown', shouldDisable);
            
            // Update Novelty  
            updateDropdownDisabledState('mobile-novelty-btn', 'mobile-novelty-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-novelty-btn', 'desktop-novelty-dropdown', shouldDisable);
            
            // Update Impact
            updateDropdownDisabledState('mobile-impact-btn', 'mobile-impact-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-impact-btn', 'desktop-impact-dropdown', shouldDisable);
            
            // Update Relevance
            updateDropdownDisabledState('mobile-relevance-btn', 'mobile-relevance-dropdown', shouldDisable);
            updateDropdownDisabledState('desktop-relevance-btn', 'desktop-relevance-dropdown', shouldDisable);
        }
        
        function applyFiltersAndSort() {
            // Apply H-Index filtering first
            filteredSortedPapers = allPapers.filter(paper => passesHIndexFilter(paper));
            
            // Apply Scoring filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesScoringFilter(paper));
            
            // Apply Recommendation filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRecommendationFilter(paper));
            
            // Apply Novelty filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesNoveltyFilter(paper));
            
            // Apply Impact filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesImpactFilter(paper));
            
            // Apply Relevance filtering
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesRelevanceFilter(paper));
            
            // Apply Topic filtering (note: this doesn't filter papers, just affects display)
            filteredSortedPapers = filteredSortedPapers.filter(paper => passesTopicFilter(paper));
            
            // Apply current sorting
            sortPapers(currentSort);
            
            // Calculate pagination
            totalPages = Math.ceil(filteredSortedPapers.length / PAPERS_PER_PAGE);
            currentPage = 1;
            
            updatePaperCount();
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function passesHIndexFilter(paper) {
            const { found, notFound, highestMin, highestMax, averageMin, averageMax } = currentHIndexFilters;
            
            // Check H-Index status
            const hasHIndex = paper.h_index_status === 'completed';
            const noHIndex = paper.h_index_status === 'not_fetched';
            
            // If neither found nor not-found is checked, hide all papers
            if (!found && !notFound) return false;
            
            // Check status inclusion
            if (hasHIndex && !found) return false;
            if (noHIndex && !notFound) return false;
            
            // For papers with H-Index data, check ranges (only if "found" is checked)
            if (hasHIndex && found) {
                // Treat null h-index values as 0
                const paperHighest = paper.highest_h_index || 0;
                const paperAverage = paper.average_h_index || 0;
                
                // Check if paper's H-Index values fall within ranges
                if (paperHighest < highestMin || paperHighest > highestMax) return false;
                if (paperAverage < averageMin || paperAverage > averageMax) return false;
            }
            
            return true;
        }
        
        function passesScoringFilter(paper) {
            const { hasScoring, noScoring } = currentScoringFilters;
            
            // Check scoring status
            const hasLLMScoring = paper.llm_score_status === 'completed';
            const noLLMScoring = paper.llm_score_status === 'not_relevant_enough';
            
            // If neither hasScoring nor noScoring is checked, hide all papers
            if (!hasScoring && !noScoring) return false;
            
            // Check status inclusion
            if (hasLLMScoring && !hasScoring) return false;
            if (noLLMScoring && !noScoring) return false;
            
            // Handle other statuses - if paper has a different status, only show if both filters are enabled
            if (!hasLLMScoring && !noLLMScoring) {
                return hasScoring && noScoring;
            }
            
            return true;
        }
        
        function passesRecommendationFilter(paper) {
            const { mustRead, shouldRead, canSkip, ignore } = currentRecommendationFilters;
            
            // If no filters are selected, hide all papers
            if (!mustRead && !shouldRead && !canSkip && !ignore) return false;
            
            // Check recommendation score
            const score = paper.recommendation_score;
            
            if (score === 'Must Read' && !mustRead) return false;
            if (score === 'Should Read' && !shouldRead) return false;
            if (score === 'Can Skip' && !canSkip) return false;
            if (score === 'Ignore' && !ignore) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['Must Read', 'Should Read', 'Can Skip', 'Ignore'].includes(score)) {
                return mustRead && shouldRead && canSkip && ignore;
            }
            
            return true;
        }
        
        function passesNoveltyFilter(paper) {
            const { high, moderate, low, none } = currentNoveltyFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !none) return false;
            
            // Check novelty score
            const score = paper.novelty_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if ((score === null || score === undefined) && !none) return false;
            
            // If paper has a different score, only show if all filters are enabled
            if (score && !['High', 'Moderate', 'Low'].includes(score)) {
                return high && moderate && low && none;
            }
            
            return true;
        }
        
        function passesImpactFilter(paper) {
            const { high, moderate, low, negligible } = currentImpactFilters;
            
            // If no filters are selected, hide all papers
            if (!high && !moderate && !low && !negligible) return false;
            
            // Check impact score
            const score = paper.impact_score;
            
            if (score === 'High' && !high) return false;
            if (score === 'Moderate' && !moderate) return false;
            if (score === 'Low' && !low) return false;
            if (score === 'Negligible' && !negligible) return false;
            
            // If paper has a different/null score, only show if all filters are enabled
            if (!['High', 'Moderate', 'Low', 'Negligible'].includes(score)) {
                return high && moderate && low && negligible;
            }
            
            return true;
        }
        
        function displayCurrentPage() {
            // Check if there are no papers to display
            if (filteredSortedPapers.length === 0) {
                showNoPapersMessage();
                hidePaginationSections();
                return;
            }
            
            const startIndex = (currentPage - 1) * PAPERS_PER_PAGE;
            const endIndex = startIndex + PAPERS_PER_PAGE;
            currentPagePapers = filteredSortedPapers.slice(startIndex, endIndex);
            
            showPaginationSections();
            populatePaperCards(currentPagePapers, startIndex + 1);
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
        }

        function populatePaperCards(papers, startIndex = 1) {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const papersHTML = papers.map((paper, index) => createPaperCard(paper, startIndex + index)).join('');
            
            if (mobileContainer) {
                mobileContainer.innerHTML = papersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = papersHTML;
            }
        }

        function showNoPapersMessage() {
            const mobileContainer = document.getElementById('mobile-papers');
            const desktopContainer = document.getElementById('desktop-papers');
            
            const noPapersHTML = '<div class="flex items-center justify-center min-h-screen"><h2 class="font-heading text-2xl text-neutral-600">No papers to show</h2></div>';
            
            if (mobileContainer) {
                mobileContainer.innerHTML = noPapersHTML;
            }
            if (desktopContainer) {
                desktopContainer.innerHTML = noPapersHTML;
            }
        }

        function hidePaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = 'none';
                }
            });
        }

        function showPaginationSections() {
            const paginationIds = [
                'mobile-prev-btn', 'mobile-next-btn', 'mobile-pagination-numbers',
                'desktop-prev-btn', 'desktop-next-btn', 'desktop-pagination-numbers',
                'mobile-footer-prev-btn', 'mobile-footer-next-btn', 'mobile-footer-pagination-numbers',
                'desktop-footer-prev-btn', 'desktop-footer-next-btn', 'desktop-footer-pagination-numbers'
            ];
            
            paginationIds.forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.style.display = '';
                }
            });
        }

        // ============================================================================
        // PAGINATION FUNCTIONS
        // ============================================================================
        
        function goToPage(page) {
            if (page < 1 || page > totalPages) return;
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
        }
        
        function updatePaginationUI() {
            // Update all pagination controls
            updatePaginationButtons();
            updatePaginationNumbers();
        }
        
        function updatePaginationButtons() {
            // Previous buttons
            const prevButtons = ['mobile-prev-btn', 'desktop-prev-btn', 'mobile-footer-prev-btn', 'desktop-footer-prev-btn'];
            prevButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage <= 1) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
            
            // Next buttons
            const nextButtons = ['mobile-next-btn', 'desktop-next-btn', 'mobile-footer-next-btn', 'desktop-footer-next-btn'];
            nextButtons.forEach(id => {
                const btn = document.getElementById(id);
                if (btn) {
                    if (currentPage >= totalPages) {
                        btn.classList.add('disabled');
                    } else {
                        btn.classList.remove('disabled');
                    }
                }
            });
        }
        
        function updatePaginationNumbers() {
            const containers = [
                'mobile-pagination-numbers',
                'desktop-pagination-numbers', 
                'mobile-footer-pagination-numbers',
                'desktop-footer-pagination-numbers'
            ];
            
            containers.forEach(containerId => {
                const container = document.getElementById(containerId);
                if (container) {
                    container.innerHTML = generatePaginationNumbers();
                }
            });
        }
        
        function generatePaginationNumbers() {
            if (totalPages <= 1) return '';
            
            let html = '';
            const maxVisiblePages = 5;
            let startPage = Math.max(1, currentPage - Math.floor(maxVisiblePages / 2));
            let endPage = Math.min(totalPages, startPage + maxVisiblePages - 1);
            
            // Adjust if we're near the end
            if (endPage - startPage + 1 < maxVisiblePages) {
                startPage = Math.max(1, endPage - maxVisiblePages + 1);
            }
            
            for (let i = startPage; i <= endPage; i++) {
                const isActive = i === currentPage;
                const activeClass = isActive ? 'bg-neutral-500 text-neutral-10' : 'bg-transparent text-neutral-70 hover:bg-neutral-300';
                html += `<button class="pagination-square w-8 h-8 ${activeClass} flex items-center justify-center cursor-pointer font-heading font-bold text-sm" onclick="goToPage(${i})">${i}</button>`;
            }
            
            return html;
        }

        // ============================================================================
        // HELPER FUNCTIONS FOR STYLING
        // ============================================================================

        function getScoreColor(scoreType, value) {
            const colorMap = {
                recommendation: {
                    'Must Read': 'bg-status-green',      
                    'Should Read': 'bg-status-blue',   
                    'Can Skip': 'bg-status-orange',       
                    'Ignore': 'bg-status-red'          
                },
                novelty: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'None': 'bg-status-red'            
                },
                impact: {
                    'High': 'bg-status-green',           
                    'Moderate': 'bg-status-blue',      
                    'Low': 'bg-status-orange',            
                    'Negligible': 'bg-status-red'      
                }
            };
            
            return colorMap[scoreType][value] || 'bg-neutral-500';  // fallback to neutral-500
        }

        function getRelevanceColor(relevanceValue) {
            const colorMap = {
                'Highly Relevant': 'bg-status-green',      
                'Moderately Relevant': 'bg-status-blue', 
                'Tangentially Relevant': 'bg-status-orange', 
                'Not Relevant': 'bg-status-red',         
                'not_validated': 'bg-status-red'         
            };
            
            return colorMap[relevanceValue] || 'bg-status-red';  // fallback to status-red
        }

        function getRelevanceDisplayText(relevanceValue) {
            if (relevanceValue === 'not_validated') {
                return 'Not Relevant';
            }
            return relevanceValue;
        }

        function getJustificationText(justificationValue) {
            if (justificationValue === 'below_threshold') {
                return "Topic similarity score below 0.4, hence default to 'Not Relevant'.";
            }
            return justificationValue;
        }

        // ============================================================================
        // KATEX RENDERING FUNCTIONS
        // ============================================================================

        function renderKatexInElement(element) {
            if (typeof renderMathInElement !== 'undefined' && element) {
                renderMathInElement(element, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true},
                    ],
                    throwOnError: false,
                    trust: true
                });
            }
        }

        // ============================================================================
        // TOPIC VISIBILITY HELPER FUNCTIONS
        // ============================================================================
        
        function getTopicKeyMapping() {
            return {
                'rlhf': 'rlhf',
                'weakSupervision': 'weak_supervision', 
                'diffusionReasoning': 'diffusion_reasoning',
                'distributedTraining': 'distributed_training',
                'datasets': 'datasets'
            };
        }
        
        function getTopicDisplayNames() {
            return {
                'rlhf': 'RLHF',
                'weakSupervision': 'Weak Supervision',
                'diffusionReasoning': 'Diffusion Reasoning', 
                'distributedTraining': 'Distributed Training',
                'datasets': 'Datasets'
            };
        }
        
        function getHiddenTopicsCount() {
            const filters = currentTopicFilters;
            return Object.values(filters).filter(visible => !visible).length;
        }
        
        function getVisibleTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => filters[topic]);
        }
        
        function getHiddenTopics() {
            const filters = currentTopicFilters;
            return Object.keys(filters).filter(topic => !filters[topic]);
        }
        
        function generateSimilarityTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                html += `
                    <!-- ${displayName} Score Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar bg-bar-raw absolute inset-0 z-0" 
                                 data-paper-id="${paper.id}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceTopicRows(paper) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add visible topic rows
            visibleTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                html += `
                    <!-- ${displayName} Relevance Row -->
                    <div class="flex flex-col topic-row visible-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function generateRelevanceJustificationContent(paper, showingHidden = false) {
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const visibleTopics = getVisibleTopics();
            
            let html = '';
            
            // Add justification for visible topics only unless showing hidden topics
            const topicsToShow = showingHidden ? Object.keys(displayNames) : visibleTopics;
            
            topicsToShow.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                
                html += `
                    <div class="justification-topic-section visible-justification" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function toggleSimilarityHiddenTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the similarity container (the one that contains similarity-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenSimilarityTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics  
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Recalculate normalized scores if in normalized mode
            if (container.getAttribute('data-normalized') === 'true') {
                updateNormalizedScores(paperId);
            }
        }
        
        function toggleRelevanceHiddenTopics(paperId) {
            // Find the relevance module specifically (not similarity module)
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            let container = null;
            
            // Find the relevance container (the one that contains relevance-scores-container)
            // and ensure it's in the currently visible layout (mobile or desktop)
            containers.forEach(cont => {
                if (cont.querySelector('.relevance-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const button = container.querySelector('.show-other-topics-container button');
            const hiddenTopicsContainer = container.querySelector('.hidden-topics-container');
            
            if (!isShowingHidden) {
                // Show hidden topics
                container.setAttribute('data-show-hidden-topics', 'true');
                button.innerHTML = 'Hide Other Topics <span class="text-xs">▲</span>';
                button.className = 'bg-neutral-700 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Show and populate the hidden topics container
                hiddenTopicsContainer.style.display = 'block';
                addHiddenRelevanceTopics(paperId, hiddenTopicsContainer);
            } else {
                // Hide other topics
                container.setAttribute('data-show-hidden-topics', 'false');
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                button.className = 'bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600';
                
                // Hide and clear the hidden topics container
                hiddenTopicsContainer.style.display = 'none';
                hiddenTopicsContainer.innerHTML = '';
            }
            
            // Update justification content based on new state
            const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
            if (justificationContainer) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    const newShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                    justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, newShowingHidden);
                }
            }
            
            // Update justification if it's currently visible
            updateRelevanceJustificationVisibility(paperId);
        }
        
        function addHiddenSimilarityTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            // Check if the parent container is in normalized mode
            const parentContainer = container.closest('[data-normalized]');
            const isNormalized = parentContainer && parentContainer.getAttribute('data-normalized') === 'true';
            const barColorClass = isNormalized ? 'bg-bar-normalized' : 'bg-bar-raw';
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const score = paper[`${dataKey}_score`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="bg-neutral-200 relative flex items-center justify-end">
                            <div class="similarity-progress-bar ${dataKey.replace('_', '-')}-progress-bar ${barColorClass} absolute inset-0 z-0" 
                                 data-paper-id="${paperId}" 
                                 data-topic="${dataKey}">
                            </div>
                            <span class="text-neutral-70 font-heading font-bold text-md py-tag-y px-tag-x relative z-10 ${dataKey.replace('_', '-')}-similarity-score">
                                ${score.toFixed(3)}
                            </span>
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
            
            // Update progress bars for newly added rows with correct values and colors
            setupProgressBarsForPaper(paper);
            
            // If in normalized mode, update all scores including the newly added ones
            if (isNormalized) {
                updateNormalizedScores(paperId);
            }
        }
        
        function addHiddenRelevanceTopics(paperId, container) {
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            const hiddenTopics = getHiddenTopics();
            
            hiddenTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const relevance = paper[`${dataKey}_relevance`];
                
                const rowHtml = `
                    <div class="flex flex-col topic-row hidden-topic" data-topic="${topic}">
                        <div class="text-left">
                            <span class="text-neutral-70 font-heading font-bold text-lg">${displayName}:</span>
                        </div>
                        <div class="w-full text-center py-tag-y font-heading font-bold text-md text-neutral-10 ${getRelevanceColor(relevance)}">
                            ${getRelevanceDisplayText(relevance)}
                        </div>
                    </div>
                `;
                
                container.insertAdjacentHTML('beforeend', rowHtml);
            });
        }
        
        function removeHiddenTopicRows(container) {
            const hiddenRows = container.querySelectorAll('.hidden-topic');
            hiddenRows.forEach(row => row.remove());
        }
        
        function updateSimilarityModuleTopics(paperId) {
            // Find the similarity module specifically
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized]`);
            let container = null;
            
            // Find the similarity container in the currently visible layout
            containers.forEach(cont => {
                if (cont.querySelector('.similarity-scores-container')) {
                    // Check if this container is in a visible layout
                    const mobileLayout = cont.closest('#mobile-main-container');
                    const desktopLayout = cont.closest('.tablet\\:block');
                    
                    if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                        container = cont;
                    } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                        container = cont;
                    }
                }
            });
            
            if (!container) return;
            
            const scoresContainer = container.querySelector('.similarity-scores-container');
            const showOtherButton = container.querySelector('.show-other-topics-container');
            
            // Remove all existing topic rows
            scoresContainer.innerHTML = '';
            
            // Regenerate visible topic rows
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (paper) {
                scoresContainer.innerHTML = generateSimilarityTopicRows(paper);
                setupProgressBarsForPaper(paper);
                
                // Update normalized scores if needed
                if (container.getAttribute('data-normalized') === 'true') {
                    updateNormalizedScores(paperId);
                }
            }
            
            // Show/hide the "Show Other Topics" button
            if (getHiddenTopicsCount() > 0) {
                if (showOtherButton) {
                    showOtherButton.style.display = 'block';
                }
            } else {
                if (showOtherButton) {
                    showOtherButton.style.display = 'none';
                }
            }
            
            // Reset the hidden topics state
            container.setAttribute('data-show-hidden-topics', 'false');
            
            // Reset button text if it exists
            const button = showOtherButton?.querySelector('button');
            if (button) {
                button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
            }
        }
        
        function updateRelevanceModuleTopics(paperId) {
            // Find all relevance containers for this paper
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-show-hidden-topics]`);
            
            containers.forEach(container => {
                const scoresContainer = container.querySelector('.relevance-scores-container');
                if (!scoresContainer) return; // Skip if this is not a relevance container
                
                const showOtherButton = container.querySelector('.show-other-topics-container');
                const justificationContainer = container.querySelector('.relevance-justification-section .justification-text');
                
                // Remove all existing topic rows
                scoresContainer.innerHTML = '';
                
                // Regenerate visible topic rows
                const paper = currentPagePapers.find(p => p.id === paperId);
                if (paper) {
                    scoresContainer.innerHTML = generateRelevanceTopicRows(paper);
                    
                    // Update justification content based on current visibility state
                    if (justificationContainer) {
                        const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                        justificationContainer.innerHTML = generateRelevanceJustificationContent(paper, isShowingHidden);
                    }
                }
                
                // Show/hide the "Show Other Topics" button
                if (getHiddenTopicsCount() > 0) {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'block';
                    }
                } else {
                    if (showOtherButton) {
                        showOtherButton.style.display = 'none';
                    }
                }
                
                // Reset the hidden topics state
                container.setAttribute('data-show-hidden-topics', 'false');
                
                // Reset button text if it exists
                const button = showOtherButton?.querySelector('button');
                if (button) {
                    button.innerHTML = 'Show Other Topics <span class="text-xs">▼</span>';
                }
            });
        }
        
        function updateRelevanceJustificationVisibility(paperId) {
            // Find the justification container in the currently visible layout
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const justificationDiv = container.querySelector('.justification-text');
            const isVisible = !justificationDiv.classList.contains('hidden');
            
            if (isVisible) {
                const paper = currentPagePapers.find(p => p.id === paperId);
                const relevanceContainer = container.closest('[data-show-hidden-topics]');
                const isShowingHidden = relevanceContainer && relevanceContainer.getAttribute('data-show-hidden-topics') === 'true';
                
                if (paper) {
                    if (isShowingHidden) {
                        // Show all justifications
                        justificationDiv.innerHTML = generateFullRelevanceJustificationContent(paper);
                    } else {
                        // Show only visible justifications
                        justificationDiv.innerHTML = generateRelevanceJustificationContent(paper);
                    }
                }
            }
        }
        
        function generateFullRelevanceJustificationContent(paper) {
            const allTopics = ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'];
            const topicMapping = getTopicKeyMapping();
            const displayNames = getTopicDisplayNames();
            
            let html = '';
            
            allTopics.forEach(topic => {
                const dataKey = topicMapping[topic];
                const displayName = displayNames[topic];
                const justification = paper[`${dataKey}_justification`];
                const isVisible = currentTopicFilters[topic];
                
                html += `
                    <div class="justification-topic-section ${isVisible ? 'visible-justification' : 'hidden-justification'}" data-topic="${topic}">
                        <div class="font-heading font-bold">${displayName}:</div>
                        <div>${getJustificationText(justification)}</div>
                    </div>
                `;
            });
            
            return html;
        }
        
        function setupProgressBarsForPaper(paper) {
            const topics = ['rlhf', 'weak_supervision', 'diffusion_reasoning', 'distributed_training', 'datasets'];
            
            topics.forEach(topic => {
                const progressBars = document.querySelectorAll(
                    `.similarity-progress-bar[data-paper-id="${paper.id}"][data-topic="${topic}"]`
                );
                
                progressBars.forEach(progressBar => {
                    const score = paper[`${topic}_score`];
                    const percentage = (score * 100);
                    progressBar.style.width = `${percentage}%`;
                });
            });
        }
        
        function updateNormalizedScores(paperId) {
            // Find the normalized similarity container in the currently visible layout
            const containers = document.querySelectorAll(`[data-paper-id="${paperId}"][data-normalized="true"]`);
            let container = null;
            
            containers.forEach(cont => {
                // Check if this container is in a visible layout
                const mobileLayout = cont.closest('#mobile-main-container');
                const desktopLayout = cont.closest('.tablet\\:block');
                
                if (mobileLayout && getComputedStyle(mobileLayout).display !== 'none') {
                    container = cont;
                } else if (desktopLayout && getComputedStyle(desktopLayout).display !== 'none') {
                    container = cont;
                }
            });
            
            if (!container) return;
            
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
            const topicsToCalculate = isShowingHidden ? 
                ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                getVisibleTopics();
                
            const topicMapping = getTopicKeyMapping();
            
            // Calculate total score for normalization
            const totalScore = topicsToCalculate.reduce((sum, topic) => {
                const dataKey = topicMapping[topic];
                return sum + paper[`${dataKey}_score`];
            }, 0);
            
            // Update each visible topic
            topicsToCalculate.forEach(topic => {
                const dataKey = topicMapping[topic];
                const rawScore = paper[`${dataKey}_score`];
                const normalizedScore = (rawScore / totalScore) * 100;
                
                // Update progress bar
                const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                if (progressBar) {
                    progressBar.style.width = `${normalizedScore}%`;
                }
                
                // Update score text
                const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                if (scoreElement) {
                    const sigFigScore = normalizedScore.toPrecision(3);
                    scoreElement.textContent = `${sigFigScore}%`;
                }
            });
        }

        // ============================================================================
        // PAPER CARD CREATION FUNCTIONS
        // ============================================================================
        function createPaperCard(paper, paperNumber) {
            const cardId = `paper-${paperNumber}`;
            
            return `
                <article class="bg-neutral-200" role="article" aria-labelledby="${cardId}">
                    <!-- Title Section -->
                    <div class="p-md">
                        <h2 id="${cardId}" class="text-neutral-70 font-heading font-bold text-2xl">
                            <span class="mr-sm">${paperNumber}.</span><a href="${paper.pdf_url}" 
                               class="paper-title-link" 
                               target="_blank" 
                               rel="noopener noreferrer"
                               aria-label="View paper PDF">${paper.title}</a>
                        </h2>
                    </div>
                    
                    <!-- Paper Info Section -->
                    <div class="grid grid-cols-1 gap-lg pb-xl px-xl">
                        <!-- Row 1: Metadata Module -->
                        <div class="flex flex-col gap-xs">
                            <!-- First row: arXiv ID and Publication Date -->
                            <div class="flex gap-xs">
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    arXiv ID: <a href="${paper.arxiv_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${paper.id}</a>
                                </span>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y">
                                    Published: ${formatPublicationDate(paper.published_date)}
                                </span>
                            </div>
                            
                            <!-- Second row: Authors -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Authors: ${paper.h_index_status === 'completed' && paper.author_h_indexes && paper.author_h_indexes.length > 0 
                                        ? paper.author_h_indexes.map(author => 
                                            author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-10 underline hover:no-underline">${author.name}</a>`
                                                : author.name
                                        ).join(', ')
                                        : paper.authors.join(', ')
                                    }
                                </span>
                            </div>
                            
                            <!-- Third row: Categories -->
                            <div>
                                <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y metadata-tag">
                                    Categories: ${paper.categories.join(', ')}
                                </span>
                            </div>
                        </div>
                        
                        <!-- Row 2: AI Generated Summary Module -->
                        ${paper.summary && paper.summary.trim() ? `
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">AI-generated summary</h3>
                                <p class="text-neutral-70 font-body text-md">${paper.summary}</p>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 3: Abstract Module -->
                        <div class="bg-neutral-300 p-lg">
                            <div class="flex flex-col gap-xs">
                                <h3 class="text-neutral-70 font-heading font-bold text-lg">Abstract</h3>
                                <div class="abstract-container" data-paper-id="${paper.id}">
                                    <p class="abstract-text text-neutral-70 font-body text-md" 
                                       style="line-height: calc(1.5em);">${paper.abstract}</p>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Row 4: Score Row Section -->
                        ${paper.llm_score_status !== 'not_relevant_enough' ? `
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Recommendation Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Recommendation:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('recommendation', paper.recommendation_score)}">
                                            ${paper.recommendation_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full recommendation-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRecommendationJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.recommendation_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Novelty Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Novelty:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('novelty', paper.novelty_score)}">
                                            ${paper.novelty_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full novelty-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleNoveltyJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.novelty_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Potential Impact Score Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-xs">
                                    <!-- Score Section -->
                                    <div class="flex">
                                        <span class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center">
                                            Potential Impact:
                                        </span>
                                        <span class="text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y text-center ${getScoreColor('impact', paper.impact_score)}">
                                            ${paper.impact_score}
                                        </span>
                                    </div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full impact-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-left cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleImpactJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-body text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                            ${paper.impact_justification}
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        ` : ''}
                        
                        <!-- Row 5: Similarity, Relevance, H-index Section -->
                        <div class="flex flex-col tablet:flex-row gap-lg items-start">
                            <!-- Similarity Scores Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-normalized="false" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Similarity Scores</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs similarity-scores-container">
                                        ${generateSimilarityTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleSimilarityHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Button Section -->
                                    <div>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" onclick="toggleSimilarityScores(this)">
                                            Show Normalized Scores ⇄
                                        </button>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Relevance Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full" data-paper-id="${paper.id}" data-show-hidden-topics="false">
                                <div class="flex flex-col gap-xs">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Topic Relevance</h3>
                                    </div>
                                    
                                    <!-- Scores Section -->
                                    <div class="flex flex-col gap-xs relevance-scores-container">
                                        ${generateRelevanceTopicRows(paper)}
                                    </div>
                                    
                                    <!-- Show Other Topics Button (conditionally shown) -->
                                    <div class="show-other-topics-container" ${getHiddenTopicsCount() > 0 ? '' : 'style="display: none;"'}>
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center hover:bg-neutral-600" onclick="toggleRelevanceHiddenTopics('${paper.id}')">
                                            Show Other Topics <span class="text-xs">▼</span>
                                        </button>
                                    </div>
                                    
                                    <!-- Hidden Topics Container (appears after button when toggled) -->
                                    <div class="hidden-topics-container" style="display: none;"></div>
                                    
                                    <!-- Justification Section -->
                                    <div class="w-full relevance-justification-section" data-paper-id="${paper.id}">
                                        <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y mt-md w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                onclick="toggleRelevanceJustification('${paper.id}')">
                                            Show Justification <span class="text-xs">▲</span>
                                        </button>
                                        <div class="justification-text hidden text-neutral-20 font-mono text-md px-tag-x py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out flex flex-col gap-sm">
                                            ${generateRelevanceJustificationContent(paper)}
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Author H-Index Module -->
                            <div class="bg-neutral-300 p-lg flex-1 w-full">
                                <div class="flex flex-col gap-md">
                                    <!-- Title Section -->
                                    <div class="text-center py-tag-y">
                                        <h3 class="text-neutral-70 font-heading font-bold text-xl">Author H-Index</h3>
                                    </div>
                                    
                                    ${paper.h_index_status === 'not_fetched' || paper.h_index_status === 'failed' ? `
                                        <!-- No Data Available Section -->
                                        <div class="text-center pt-lg pb-sm">
                                            <p class="text-neutral-60 font-heading font-bold text-lg">No H-Index data available</p>
                                        </div>
                                    ` : `
                                        <!-- H-Index Info Section -->
                                        <div class="flex flex-col gap-sm">
                                            <!-- Authors Found Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Authors found:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.authors_found}/${paper.total_authors}</span>
                                            </div>
                                            
                                            <!-- Highest H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Highest H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.highest_h_index || 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Average H-Index Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Average H-Index:</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.average_h_index ? paper.average_h_index.toFixed(1) : 'N/A'}</span>
                                            </div>
                                            
                                            <!-- Notable Authors Row -->
                                            <div class="flex justify-between items-center">
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">Notable (H>5):</span>
                                                <span class="text-neutral-60 font-heading font-bold text-xl px-tag-x py-tag-y">${paper.notable_authors_count || 0}</span>
                                            </div>
                                        </div>
                                        
                                        <!-- Semantic Scholar Button -->
                                        <div>
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md py-tag-y mt-md w-full text-center" 
                                                    onclick="window.open('${paper.semantic_scholar_url}', '_blank')">
                                                Verify source on Semantic Scholar
                                            </button>
                                        </div>
                                        
                                        <!-- Individual H-Indices Section -->
                                        <div class="w-full author-h-index-section" data-paper-id="${paper.id}">
                                            <button class="bg-neutral-500 text-neutral-10 font-heading font-bold text-md px-tag-x py-tag-y w-full text-center cursor-pointer border-none transition-opacity duration-200" 
                                                    onclick="toggleAuthorHIndices('${paper.id}')">
                                                Show Individual H-Indices <span class="text-xs">▼</span>
                                            </button>
                                            <div class="individual-authors-text hidden text-neutral-20 font-mono text-md px-xl py-tag-y bg-neutral-500 transition-all duration-300 ease-in-out">
                                                ${paper.author_h_indexes && paper.author_h_indexes.length > 0 ? 
                                                    paper.author_h_indexes.map(author => `
                                                        <div class="flex justify-between items-center py-xs">
                                                            ${author.profile_url && author.profile_url !== null && author.profile_url !== '' 
                                                                ? `<a href="${author.profile_url}" target="_blank" rel="noopener noreferrer" class="text-neutral-20 font-mono text-md underline hover:no-underline">${author.name}:</a>`
                                                                : `<span class="text-neutral-20 font-mono text-md">${author.name}:</span>`
                                                            }
                                                            <span class="text-neutral-20 font-mono text-md">${author.h_index !== null && author.h_index !== undefined ? author.h_index : 'N/A'}</span>
                                                        </div>
                                                    `).join('') 
                                                    : '<div class="text-center text-neutral-20">No individual author data available</div>'
                                                }
                                            </div>
                                        </div>
                                    `}
                                </div>
                            </div>
                        </div>
                    </div>
                </article>
            `;
        }

        // ============================================================================
        // QUICK FILTER FUNCTIONS
        // ============================================================================


        function applyQuickFilter(filterType) {
            // Build URL parameters based on filter type
            const url = new URL(window.location.href);
            
            // Clear all existing parameters
            url.search = '';
            
            // Set sort to recommendation best first for all filters
            url.searchParams.set('sort', 'recommend_best');
            
            switch(filterType) {
                case 'must-read':
                    // Recommendation: Only Must Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'true');
                    url.searchParams.set('recommendation_should', 'false');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'should-read':
                    // Recommendation: Only Should Read + Scoring: Only Has Scoring
                    url.searchParams.set('recommendation_must', 'false');
                    url.searchParams.set('recommendation_should', 'true');
                    url.searchParams.set('recommendation_skip', 'false');
                    url.searchParams.set('recommendation_ignore', 'false');
                    url.searchParams.set('scoring_has', 'true');
                    url.searchParams.set('scoring_no', 'false');
                    break;
                    
                case 'rlhf':
                    // Topic: Only RLHF + Relevance: Exclude Not Relevant
                    url.searchParams.set('topic_rlhf', 'true');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'weak-supervision':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'true');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'diffusion-reasoning':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'true');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'distributed-training':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'true');
                    url.searchParams.set('topic_datasets', 'false');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'datasets':
                    url.searchParams.set('topic_rlhf', 'false');
                    url.searchParams.set('topic_weak_supervision', 'false');
                    url.searchParams.set('topic_diffusion_reasoning', 'false');
                    url.searchParams.set('topic_distributed_training', 'false');
                    url.searchParams.set('topic_datasets', 'true');
                    url.searchParams.set('relevance_highly', 'true');
                    url.searchParams.set('relevance_moderately', 'true');
                    url.searchParams.set('relevance_tangentially', 'true');
                    url.searchParams.set('relevance_not', 'false');
                    break;
                    
                case 'reset':
                    // Clear all parameters, which will reset everything to defaults
                    break;
                    
                default:
                    console.warn('Unknown quick filter type:', filterType);
                    return;
            }
            
            // Navigate to new URL, which will trigger existing URL parsing logic
            window.location.href = url.toString();
        }



        // ============================================================================
        // PAGE INITIALIZATION
        // ============================================================================

        function initializePage() {

            // Get sort parameter from URL, default to 'recommend_best'
            currentSort = getUrlParameter('sort') || 'recommend_best';
            
            // Load H-Index filters from URL
            updateHIndexFiltersFromURL();
            
            // Load Scoring filters from URL
            updateScoringFiltersFromURL();
            
            // Load Recommendation filters from URL
            updateRecommendationFiltersFromURL();
            
            // Load Novelty filters from URL
            updateNoveltyFiltersFromURL();
            
            // Load Impact filters from URL
            updateImpactFiltersFromURL();
            
            // Load Topic filters from URL
            updateTopicFiltersFromURL();
            
            // Load Relevance filters from URL
            loadRelevanceFiltersFromURL();
            
            // Update disabled state for advanced filters after loading scoring filters
            updateAdvancedFiltersDisabledState();
            
            // Update page title and headers based on embedded date
            updatePageTitles(PAGE_DATE);
            
            // Update sort dropdown UI
            updateSortDropdownUI();
            
            // Setup H-Index input validation
            setupHIndexValidation();
            
            // Sync H-Index UI with loaded filters
            syncHIndexUI();
            
            // Apply initial sorting and filtering
            applyFiltersAndSort();
            
            // Display first page
            displayCurrentPage();
            
        }

        // ============================================================================
        // ABSTRACT TRUNCATION LOGIC
        // ============================================================================
        
        let resizeTimer;

        function resetAbstractToOriginal(container) {
            const abstractText = container.querySelector('.abstract-text');
            const originalText = abstractText.getAttribute('data-original-text');
            
            if (originalText) {
                // Reset to clean original text
                abstractText.innerHTML = originalText;
                abstractText.setAttribute('data-expanded', 'false');
                // Clear any existing truncated text to force recalculation
                abstractText.removeAttribute('data-truncated-text');
            }
        }

        function calculateAverageCharWidth(fontStyle, fontSize, fontFamily) {
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            ctx.font = `${fontStyle} ${fontSize} ${fontFamily}`;
            
            const characterSet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ';
            const totalWidth = ctx.measureText(characterSet).width;
            
            return totalWidth / characterSet.length;
        }

        function getTextContentWidth(element) {
            const computedStyle = getComputedStyle(element);
            return element.clientWidth - 
                parseFloat(computedStyle.paddingLeft) - 
                parseFloat(computedStyle.paddingRight);
        }

        function calculateThreeLineCharLimit(element) {
            const computedStyle = getComputedStyle(element);
            const fontSize = computedStyle.fontSize;
            const fontFamily = computedStyle.fontFamily;
            const fontWeight = computedStyle.fontWeight;
            
            // Get average character width
            const avgCharWidth = calculateAverageCharWidth(fontWeight, fontSize, fontFamily);
            
            // Get content width
            const contentWidth = getTextContentWidth(element);
            
            // Calculate characters per line
            const charsPerLine = Math.floor(contentWidth / avgCharWidth);
            
            // Total characters for 3 lines
            const totalChars = charsPerLine * 3;
            
            // Reserve space for "... [Expand]"
            const expandButtonChars = 30;
            
            return Math.max(0, totalChars - expandButtonChars);
        }

        function toggleAbstract(paperId) {
            const containers = document.querySelectorAll(`.abstract-container[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                if (!abstractText) return; // Safety check
                
                const isExpanded = abstractText.getAttribute('data-expanded') === 'true';
                
                if (isExpanded) {
                    // Collapse - restore truncated text
                    const truncatedText = abstractText.getAttribute('data-truncated-text');
                    abstractText.innerHTML = truncatedText;
                    abstractText.setAttribute('data-expanded', 'false');
                } else {
                    // Expand - show full text
                    const originalText = abstractText.getAttribute('data-original-text');
                    abstractText.innerHTML = `${originalText} <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract('${paperId}')">[Collapse]</button>`;
                    abstractText.setAttribute('data-expanded', 'true');
                }
                
                // Re-render KaTeX after content change
                setTimeout(() => renderKatexInElement(abstractText), 50);
            });
        }

        // Function to setup abstract truncation using font metrics and binary search
        function setupAbstractTruncation() {
            document.querySelectorAll('.abstract-container').forEach(container => {
                const abstractText = container.querySelector('.abstract-text');
                const paperId = container.getAttribute('data-paper-id');
                
                // Get original text - only set it if not already stored to prevent corruption
                let originalText = abstractText.getAttribute('data-original-text');
                if (!originalText) {
                    // First time setup - get clean text content
                    originalText = abstractText.textContent;
                    abstractText.setAttribute('data-original-text', originalText);
                } else {
                    // Subsequent calls - reset to clean state first
                    resetAbstractToOriginal(container);
                }
                
                // Always reset to collapsed state
                abstractText.setAttribute('data-expanded', 'false');
                
                // Calculate the rough character limit for 3 lines as starting point
                const roughCharLimit = calculateThreeLineCharLimit(abstractText);
                
                // Check if text needs truncation
                if (originalText.length > roughCharLimit) {
                    // Create expand button template
                    const expandButton = '... <button class="text-neutral-60 font-body font-bold text-md cursor-pointer bg-transparent border-none p-0 hover:opacity-70 transition-opacity duration-200" onclick="toggleAbstract(\'' + paperId + '\')">[Expand]</button>';
                    
                    // Calculate 3-line height for comparison
                    const computedStyle = getComputedStyle(abstractText);
                    const lineHeight = parseFloat(computedStyle.lineHeight);
                    const maxHeight = lineHeight * 3;
                    
                    // Binary search for perfect truncation point
                    let left = 0;
                    let right = Math.min(originalText.length, roughCharLimit + 100); // Use rough estimate + buffer
                    let bestFit = '';
                    let bestLength = 0;
                    
                    // Create temporary element for height testing
                    const testElement = abstractText.cloneNode(true);
                    testElement.style.position = 'absolute';
                    testElement.style.visibility = 'hidden';
                    testElement.style.width = abstractText.offsetWidth + 'px';
                    testElement.style.height = 'auto';
                    testElement.style.maxHeight = 'none';
                    document.body.appendChild(testElement);
                    
                    while (left <= right) {
                        const mid = Math.floor((left + right) / 2);
                        const testText = originalText.substring(0, mid) + expandButton;
                        
                        testElement.innerHTML = testText;
                        
                        if (testElement.offsetHeight <= maxHeight) {
                            // Text fits, try longer
                            bestFit = testText;
                            bestLength = mid;
                            left = mid + 1;
                        } else {
                            // Text too long, trying shorter
                            right = mid - 1;
                        }
                    }
                    
                    // Clean up temporary element
                    document.body.removeChild(testElement);
                    
                    // Apply the best fit result
                    if (bestFit) {
                        abstractText.setAttribute('data-truncated-text', bestFit);
                        abstractText.innerHTML = bestFit;
                    } else {
                        // Fallback to rough estimate if binary search fails
                        const fallbackText = originalText.substring(0, Math.max(0, roughCharLimit - 50)) + expandButton;
                        abstractText.setAttribute('data-truncated-text', fallbackText);
                        abstractText.innerHTML = fallbackText;
                    }
                } else {
                    // Text fits without truncation
                    abstractText.innerHTML = originalText;
                }
            });
        }

        // Function to toggle recommendation justification
        function toggleRecommendationJustification(paperId) {
            const containers = document.querySelectorAll(`.recommendation-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle novelty justification
        function toggleNoveltyJustification(paperId) {
            const containers = document.querySelectorAll(`.novelty-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle impact justification
        function toggleImpactJustification(paperId) {
            const containers = document.querySelectorAll(`.impact-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle relevance justification
        function toggleRelevanceJustification(paperId) {
            const containers = document.querySelectorAll(`.relevance-justification-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.justification-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show justification
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Justification <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide justification
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Justification <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to toggle author H-indices
        function toggleAuthorHIndices(paperId) {
            const containers = document.querySelectorAll(`.author-h-index-section[data-paper-id="${paperId}"]`);
            
            containers.forEach(container => {
                const button = container.querySelector('button');
                const textDiv = container.querySelector('.individual-authors-text');
                const isHidden = textDiv.classList.contains('hidden');
                
                if (isHidden) {
                    // Show individual H-indices
                    textDiv.classList.remove('hidden');
                    button.innerHTML = 'Hide Individual H-Indices <span class="text-xs">▲</span>';
                    button.classList.remove('bg-neutral-500');
                    button.classList.add('bg-neutral-600');
                } else {
                    // Hide individual H-indices
                    textDiv.classList.add('hidden');
                    button.innerHTML = 'Show Individual H-Indices <span class="text-xs">▼</span>';
                    button.classList.remove('bg-neutral-600');
                    button.classList.add('bg-neutral-500');
                }
            });
        }

        // Function to setup initial similarity progress bars (raw scores only)
        function setupInitialProgressBars() {
            currentPagePapers.forEach(paper => {
                setupProgressBarsForPaper(paper);
            });
        }

        // Function to toggle similarity scores between raw and normalized
        function toggleSimilarityScores(buttonElement) {
            // Find the parent container with data-paper-id
            const container = buttonElement.closest('[data-paper-id]');
            if (!container) return;
            
            const paperId = container.getAttribute('data-paper-id');
            const isNormalized = container.getAttribute('data-normalized') === 'true';
            
            // Find the paper data from current page papers
            const paper = currentPagePapers.find(p => p.id === paperId);
            if (!paper) return;
            
            // Toggle state
            container.setAttribute('data-normalized', (!isNormalized).toString());
            
            // Update button text
            buttonElement.textContent = isNormalized ? 'Show Normalized Scores ⇄' : 'Show Raw Scores ⇄';
            
            if (!isNormalized) {
                // Switch to normalized mode
                updateNormalizedScores(paperId);
                
                // Change all progress bars to normalized color
                const progressBars = container.querySelectorAll('.similarity-progress-bar');
                progressBars.forEach(bar => {
                    bar.classList.remove('bg-bar-raw');
                    bar.classList.add('bg-bar-normalized');
                });
            } else {
                // Switch to raw mode
                const isShowingHidden = container.getAttribute('data-show-hidden-topics') === 'true';
                const topicsToShow = isShowingHidden ? 
                    ['rlhf', 'weakSupervision', 'diffusionReasoning', 'distributedTraining', 'datasets'] :
                    getVisibleTopics();
                    
                const topicMapping = getTopicKeyMapping();
                
                topicsToShow.forEach(topic => {
                    const dataKey = topicMapping[topic];
                    const rawScore = paper[`${dataKey}_score`];
                    
                    // Update progress bar
                    const progressBar = container.querySelector(`.${dataKey.replace('_', '-')}-progress-bar`);
                    if (progressBar) {
                        progressBar.style.width = `${(rawScore * 100)}%`;
                        // Change to raw bar color
                        progressBar.classList.remove('bg-bar-normalized');
                        progressBar.classList.add('bg-bar-raw');
                    }
                    
                    // Update score text
                    const scoreElement = container.querySelector(`.${dataKey.replace('_', '-')}-similarity-score`);
                    if (scoreElement) {
                        scoreElement.textContent = rawScore.toFixed(3);
                    }
                });
            }
        }

        // Pagination functions
        function calculatePaginationWindow(currentPage, totalPages) {
            const windowSize = 5;
            const halfWindow = Math.floor(windowSize / 2);
            
            let start = Math.max(1, currentPage - halfWindow);
            let end = Math.min(totalPages, start + windowSize - 1);
            
            // Adjust start if we can't show a full window at the end
            if (end - start + 1 < windowSize) {
                start = Math.max(1, end - windowSize + 1);
            }
            
            return { start, end };
        }

        function updatePaginationUI() {
            const { start, end } = calculatePaginationWindow(currentPage, totalPages);
            
            // Update mobile pagination
            updatePaginationButtons('mobile', start, end);
            
            // Update desktop pagination
            updatePaginationButtons('desktop', start, end);
            
            // Update footer pagination
            updateFooterPaginationButtons('mobile', start, end);
            updateFooterPaginationButtons('desktop', start, end);
        }

        function updatePaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-pagination-numbers')
                : document.getElementById('desktop-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update arrow button states
            updateArrowButtons(layout);
        }

        function updateFooterPaginationButtons(layout, start, end) {
            const container = layout === 'mobile' 
                ? document.getElementById('mobile-footer-pagination-numbers')
                : document.getElementById('desktop-footer-pagination-numbers');
            
            if (!container) return;
            
            // Clear existing buttons
            container.innerHTML = '';
            
            // Create pagination buttons
            for (let i = start; i <= end; i++) {
                const isActive = i === currentPage;
                const buttonClass = isActive 
                    ? 'bg-neutral-900 text-neutral-10' 
                    : 'bg-neutral-300 text-neutral-70 hover:bg-neutral-400 cursor-pointer';
                
                const sizeClasses = layout === 'mobile' 
                    ? 'w-8 h-8' 
                    : 'clamp(1.5rem, 3vw, 1.875rem)';
                
                const button = document.createElement('div');
                button.className = `pagination-square ${buttonClass} flex items-center justify-center`;
                
                if (layout === 'desktop') {
                    button.style.width = sizeClasses;
                    button.style.height = sizeClasses;
                } else {
                    button.className += ` ${sizeClasses}`;
                }
                
                button.innerHTML = `<span class="font-heading font-bold text-${layout === 'mobile' ? 'sm' : 'md'}">${i}</span>`;
                
                if (!isActive) {
                    button.onclick = () => goToPage(i);
                    button.style.cursor = 'pointer';
                }
                
                container.appendChild(button);
            }
            
            // Update footer arrow button states
            updateFooterArrowButtons(layout);
        }

        function updateArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function updateFooterArrowButtons(layout) {
            const prevBtn = document.getElementById(`${layout}-footer-prev-btn`);
            const nextBtn = document.getElementById(`${layout}-footer-next-btn`);
            
            // Update previous button
            if (prevBtn) {
                if (currentPage <= 1) {
                    // Show disabled state instead of hiding
                    prevBtn.classList.add('disabled');
                    prevBtn.onclick = null;
                } else {
                    // Show enabled state
                    prevBtn.classList.remove('disabled');
                    prevBtn.onclick = () => goToPage(currentPage - 1);
                }
            }
            
            // Update next button
            if (nextBtn) {
                if (currentPage >= totalPages) {
                    // Show disabled state instead of hiding
                    nextBtn.classList.add('disabled');
                    nextBtn.onclick = null;
                } else {
                    // Show enabled state
                    nextBtn.classList.remove('disabled');
                    nextBtn.onclick = () => goToPage(currentPage + 1);
                }
            }
        }

        function goToPage(page) {
            // Prevent navigation if page is out of bounds or is current page
            if (page === currentPage || page < 1 || page > totalPages) {
                return;
            }
            
            console.log(`Navigating to page ${page}`);
            currentPage = page;
            updatePaginationUI();
            displayCurrentPage();
            
            // Re-run truncation after new content is displayed
            setTimeout(() => {
                setupAbstractTruncation();
                setupInitialProgressBars();
            }, 50);
            
            // Scroll to top after loading new page
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // ============================================================================
        // SCROLL LOCK FUNCTIONS
        // ============================================================================
        
        function lockBodyScroll() {
            // Get the width of the scrollbar
            const scrollbarWidth = window.innerWidth - document.documentElement.clientWidth;
            document.body.style.paddingRight = `${scrollbarWidth}px`;
            
            // Add the class to prevent scrolling
            document.body.classList.add('no-scroll');
        }

        function unlockBodyScroll() {
            // Remove the inline padding
            document.body.style.paddingRight = '';

            // Remove the class to re-enable scrolling
            document.body.classList.remove('no-scroll');
        }

        // ============================================================================
        // SIDEBAR FUNCTIONS
        // ============================================================================

        function toggleMobileMenu() {
            if (isMobileSidebarOpen) {
                closeMobileMenu();
            } else {
                openMobileMenu();
            }
        }

        function openMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar into view (full screen width)
            sidebar.style.transform = 'translateX(0)';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isMobileSidebarOpen = true;
        }

        function closeMobileMenu() {
            const sidebar = document.getElementById('mobile-sidebar');
            const mainContainer = document.getElementById('mobile-main-container');
            
            // Move sidebar out of view
            sidebar.style.transform = 'translateX(-100%)';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isMobileSidebarOpen = false;
        }

        function toggleDesktopMenu() {
            if (isDesktopSidebarOpen) {
                closeDesktopMenu();
            } else {
                openDesktopMenu();
            }
        }

        function openDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Show sidebar
            sidebar.style.transform = 'translateX(0)';
            
            // Show overlay
            overlay.style.opacity = '1';
            overlay.style.pointerEvents = 'auto';
            
            // Lock body scrolling with padding compensation
            lockBodyScroll();
            
            isDesktopSidebarOpen = true;
        }

        function closeDesktopMenu() {
            const sidebar = document.getElementById('desktop-sidebar');
            const overlay = document.getElementById('desktop-sidebar-overlay');
            
            // Hide sidebar
            sidebar.style.transform = 'translateX(-100%)';
            
            // Hide overlay
            overlay.style.opacity = '0';
            overlay.style.pointerEvents = 'none';
            
            // Unlock body scrolling
            unlockBodyScroll();
            
            isDesktopSidebarOpen = false;
        }

        // Handle window resize to close mobile menu if switching to desktop
        window.addEventListener('resize', function() {
            if (window.innerWidth >= 768 && isMobileSidebarOpen) {
                closeMobileMenu();
            }
            if (window.innerWidth < 768 && isDesktopSidebarOpen) {
                closeDesktopMenu();
            }
        });

        // ============================================================================
        // CLICK OUTSIDE TO CLOSE DROPDOWNS
        // ============================================================================
        
        // Add click outside listener to close dropdowns
        document.addEventListener('click', function(event) {
            // List of all dropdown containers
            const dropdowns = [
                { dropdown: document.getElementById('mobile-sort-dropdown'), button: document.getElementById('mobile-sort-btn') },
                { dropdown: document.getElementById('desktop-sort-dropdown'), button: document.getElementById('desktop-sort-btn') },
                { dropdown: document.getElementById('mobile-hindex-dropdown'), button: document.getElementById('mobile-hindex-btn') },
                { dropdown: document.getElementById('desktop-hindex-dropdown'), button: document.getElementById('desktop-hindex-btn') },
                { dropdown: document.getElementById('mobile-scoring-dropdown'), button: document.getElementById('mobile-scoring-btn') },
                { dropdown: document.getElementById('desktop-scoring-dropdown'), button: document.getElementById('desktop-scoring-btn') },
                { dropdown: document.getElementById('mobile-recommendation-dropdown'), button: document.getElementById('mobile-recommendation-btn') },
                { dropdown: document.getElementById('desktop-recommendation-dropdown'), button: document.getElementById('desktop-recommendation-btn') },
                { dropdown: document.getElementById('mobile-novelty-dropdown'), button: document.getElementById('mobile-novelty-btn') },
                { dropdown: document.getElementById('desktop-novelty-dropdown'), button: document.getElementById('desktop-novelty-btn') },
                { dropdown: document.getElementById('mobile-impact-dropdown'), button: document.getElementById('mobile-impact-btn') },
                { dropdown: document.getElementById('desktop-impact-dropdown'), button: document.getElementById('desktop-impact-btn') },
                { dropdown: document.getElementById('mobile-relevance-dropdown'), button: document.getElementById('mobile-relevance-btn') },
                { dropdown: document.getElementById('desktop-relevance-dropdown'), button: document.getElementById('desktop-relevance-btn') },
                { dropdown: document.getElementById('mobile-topic-dropdown'), button: document.getElementById('mobile-topic-btn') },
                { dropdown: document.getElementById('desktop-topic-dropdown'), button: document.getElementById('desktop-topic-btn') }
            ];
            
            dropdowns.forEach(({ dropdown, button }) => {
                if (!dropdown || !button) return;
                
                // Check if dropdown is open and click is outside
                if (!dropdown.classList.contains('hidden')) {
                    const dropdownContainer = dropdown.parentElement; // The relative container
                    
                    // Check if click is outside the dropdown container
                    if (!dropdownContainer.contains(event.target)) {
                        dropdown.classList.add('hidden');
                        button.classList.remove('bg-neutral-600');
                        button.classList.add('bg-neutral-500');
                        
                        // Reset pending filters for specific dropdown types
                        if (dropdown.id.includes('hindex')) {
                            resetPendingHIndexFilters();
                        } else if (dropdown.id.includes('novelty')) {
                            resetPendingNoveltyFilters();
                        } else if (dropdown.id.includes('impact')) {
                            resetPendingImpactFilters();
                        } else if (dropdown.id.includes('relevance')) {
                            resetPendingRelevanceFilters();
                        } else if (dropdown.id.includes('topic')) {
                            resetPendingTopicFilters();
                        } else if (dropdown.id.includes('recommendation')) {
                            resetPendingRecommendationFilters();
                        } else if (dropdown.id.includes('scoring')) {
                            resetPendingScoringFilters();
                        }
                    }
                }
            });
        });

        // Initialize page on load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('Papers Dashboard loaded successfully');
            initializePage();
        });

        // Setup abstract truncation when everything is fully loaded
        window.addEventListener('load', function() {
            setupAbstractTruncation();
            setupInitialProgressBars();
        });

        // Setup debounced resize handler for truncation
        window.addEventListener('resize', () => {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(() => {
                setupAbstractTruncation();
            }, 250); // Delay to wait for resize to settle
        });
    </script>

    <!-- KaTeX JavaScript -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- KaTeX Auto-render Configuration -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Configure KaTeX auto-render after page content is loaded
            setTimeout(function() {
                renderMathInElement(document.body, {
                    // KaTeX rendering options
                    delimiters: [
                        {left: '$$', right: '$$', display: true},       // Block math
                        {left: '$', right: '$', display: false},        // Inline math
                        {left: '\\(', right: '\\)', display: false},    // Inline math alternative
                        {left: '\\[', right: '\\]', display: true},     // Block math alternative
                        {left: '\\begin{equation}', right: '\\end{equation}', display: true},
                        {left: '\\begin{align}', right: '\\end{align}', display: true},
                        {left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
                        {left: '\\begin{gather}', right: '\\end{gather}', display: true},
                        {left: '\\begin{CD}', right: '\\end{CD}', display: true},
                    ],
                    // Throw errors on unknown commands/symbols
                    throwOnError: false,
                    // Allow HTML in math expressions
                    trust: true,
                    // Ignore certain classes/elements
                    ignoredClasses: [
                        "nokatex", 
                        "katex-ignore"
                    ],
                    // Skip script and style tags
                    ignoredTags: [
                        "script", 
                        "noscript", 
                        "style", 
                        "textarea", 
                        "pre", 
                        "code"
                    ]
                });
            }, 500); // Delay to ensure all content is loaded
        });
    </script>
</body>
</html>
